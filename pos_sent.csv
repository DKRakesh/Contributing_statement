idx,text,main_heading,heading,topic,paper_idx,BIO,BIO_1,BIO_2,offset1,pro1,offset2,pro2,offset3,pro3,mask,bi_labels,labels
2,Learning Phrase Representations using RNN Encoder - Decoder for Statistical Machine Translation,title,title,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob']",1,0.0,1,0.0045662100456621,1,0.0,1,1,research-problem
15,"Along this line of research on using neural networks for SMT , this paper focuses on a novel neural network architecture that can be used as apart of the conventional phrase - based SMT system .",Introduction,Introduction,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O']",6,0.05555555555555555,14,0.0639269406392694,6,0.15789473684210525,1,1,research-problem
16,"The proposed neural network architecture , which we will refer to as an RNN Encoder - Decoder , consists of two recurrent neural networks ( RNN ) that act as an encoder and a decoder pair .",Introduction,Introduction,machine-translation,0,"['O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'O', 'B', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'B', 'I', 'O', 'B', 'O', 'O', 'B', 'O', 'O']",,,7,0.06481481481481481,15,0.0684931506849315,7,0.18421052631578946,1,1,model
17,"The encoder maps a variable - length source sequence to a fixed - length vector , and the decoder maps the vector representation back to a variable - length target sequence .",Introduction,Introduction,machine-translation,0,"['O', 'O', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",8,0.07407407407407407,16,0.0730593607305936,8,0.21052631578947367,1,1,model
18,The two networks are trained jointly to maximize the conditional probability of the target sequence given a source sequence .,Introduction,Introduction,machine-translation,0,"['O', 'O', 'O', 'O', 'B', 'B', 'B', 'I', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'B-p', 'B-b', 'B-p', 'I-p', 'O', 'B-b', 'I-b', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",9,0.08333333333333333,17,0.0776255707762557,9,0.23684210526315788,1,1,model
19,"Additionally , we propose to use a rather sophisticated hidden unit in order to improve both the memory capacity and the ease of training .",Introduction,Introduction,machine-translation,0,"['O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'B', 'I', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'B-b', 'I-b', 'I-b', 'O', 'O', 'B-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",10,0.09259259259259259,18,0.0821917808219178,10,0.2631578947368421,1,1,model
135,The baseline phrase - based SMT system was built using Moses with default settings .,Data and Baseline System,Data and Baseline System,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'B', 'B', 'B', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-b', 'B-p', 'B-ob', 'I-ob', 'O']",15,0.9375,134,0.6118721461187214,15,0.9375,1,1,baselines
159,"As expected , adding features computed by neural networks consistently improves the performance over the baseline performance .",Neural Language Model,Quantitative Analysis,machine-translation,0,"['O', 'O', 'O', 'B', 'I', 'B', 'I', 'B', 'I', 'O', 'B', 'I', 'I', 'B', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'B-b', 'I-b', 'B-p', 'I-p', 'B-ob', 'I-ob', 'O', 'B-p', 'I-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",22,0.3188405797101449,158,0.7214611872146118,8,0.4,1,1,results
160,The best performance was achieved when we used both CSLM and the phrase scores from the RNN Encoder - Decoder .,Neural Language Model,Quantitative Analysis,machine-translation,0,"['O', 'B', 'I', 'I', 'I', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'O']","['O', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",23,0.3333333333333333,159,0.726027397260274,9,0.45,1,1,results
2,Neural Machine Translation in Linear Time,title,,machine-translation,1,"['B', 'I', 'I', 'I', 'I', 'I']",,,1,0.0,1,0.004975124378109453,1,0.0,1,1,research-problem
10,The ByteNet decoder attains state - of - the - art performance on character - level language modelling and outperforms the previous best results obtained with recurrent networks .,abstract,abstract,machine-translation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.7777777777777778,9,0.04477611940298507,7,0.7777777777777778,1,1,research-problem
11,"The ByteNet also achieves state - of - the - art performance on character - to - character machine translation on the English - to - German WMT translation task , surpassing comparable neural translation models that are based on recurrent networks with attentional pooling and run in quadratic time .",abstract,abstract,machine-translation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.8888888888888888,10,0.04975124378109453,8,0.8888888888888888,1,1,research-problem
14,"In neural language modelling , a neural network estimates a distribution over sequences of words or characters that belong to a given language .",Introduction,Introduction,machine-translation,1,"['O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.02702702702702703,13,0.06467661691542288,1,0.02702702702702703,1,1,research-problem
15,"In neural machine translation , the network estimates a distribution over sequences in the target language conditioned on a given sequence in the source language .",Introduction,Introduction,machine-translation,1,"['O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.05405405405405406,14,0.06965174129353234,2,0.05405405405405406,1,1,research-problem
31,The ByteNet is the instance within this family of models that uses one - dimensional convolutional neural networks ( CNN ) of fixed depth for both the encoder and the decoder ) .,Introduction,Introduction,machine-translation,1,"['O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'B', 'I', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'O', 'O']",,,18,0.4864864864864865,30,0.14925373134328357,18,0.4864864864864865,1,1,
32,The two CNNs use increasing factors of dilation to rapidly grow the receptive fields ; a similar technique is also used in .,Introduction,Introduction,machine-translation,1,"['O', 'B', 'I', 'B', 'B', 'I', 'I', 'I', 'O', 'B', 'I', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",,,19,0.5135135135135135,31,0.15422885572139303,19,0.5135135135135135,1,1,
33,The convolutions in the decoder CNN are masked to prevent the network from seeing future tokens in the target sequence .,Introduction,Introduction,machine-translation,1,"['O', 'B', 'B', 'O', 'B', 'I', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'B', 'I', 'O']",,,20,0.5405405405405406,32,0.15920398009950248,20,0.5405405405405406,1,1,
34,The network has beneficial computational and learning properties .,Introduction,Introduction,machine-translation,1,"['O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",21,0.5675675675675675,33,0.16417910447761194,21,0.5675675675675675,1,1,model
35,"From a computational perspective , the network has a running time that is linear in the length of the source and target sequences ( up to a constant c ?",Introduction,Introduction,machine-translation,1,"['B', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-b', 'I-b', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.5945945945945946,34,0.1691542288557214,22,0.5945945945945946,1,1,model
38,"From a learning perspective , the representation of the source sequence in the ByteNet is resolution preserving ; the representation sidesteps the need for memorization and allows for maximal bandwidth between encoder and decoder .",Introduction,Introduction,machine-translation,1,"['O', 'O', 'B', 'I', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-b', 'I-b', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.6756756756756757,37,0.18407960199004975,25,0.6756756756756757,1,1,model
156,"The ByteNet Decoder that we use for the result has 30 residual blocks split into six sets of five blocks each ; for the five blocks in each set the dilation rates are , respectively , 1 , 2 , 4 , 8 and 16 .",Model Test,Character Prediction,machine-translation,1,"['O', 'B', 'I', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'B', 'I', 'I', 'B', 'I', 'B', 'I', 'I', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']",,,11,0.22916666666666666,155,0.7711442786069652,4,0.0975609756097561,1,1,experiments
157,The masked kernel has size 3 .,Model Test,Character Prediction,machine-translation,1,"['O', 'B', 'I', 'O', 'B', 'I', 'O']","['O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['O', 'B-p', 'I-p', 'O', 'B-ob', 'I-ob', 'O']",12,0.25,156,0.7761194029850746,5,0.12195121951219512,1,1,experiments
159,The number of hidden units dis 512 .,Model Test,Character Prediction,machine-translation,1,"['O', 'B', 'I', 'I', 'I', 'B', 'I', 'O']","['O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O']","['O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-ob', 'I-ob', 'O']",14,0.2916666666666667,158,0.7860696517412935,7,0.17073170731707318,1,1,experiments
161,For the optimization we use Adam with a learning rate of 0.0003 and a weight decay term of 0.0001 .,Model Test,Character Prediction,machine-translation,1,"['O', 'O', 'B', 'O', 'O', 'B', 'O', 'O', 'B', 'I', 'O', 'B', 'O', 'O', 'B', 'I', 'O', 'O', 'B', 'O']","['O', 'O', 'B-p', 'O', 'O', 'B-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O', 'O', 'B-p', 'I-p', 'O', 'O', 'B-n', 'O']","['O', 'O', 'B-p', 'O', 'O', 'B-b', 'O', 'O', 'B-p', 'I-p', 'O', 'B-ob', 'O', 'O', 'B-p', 'I-p', 'O', 'O', 'B-ob', 'O']",16,0.3333333333333333,160,0.7960199004975125,9,0.21951219512195122,1,1,experiments
162,We apply dropout to the last ReLU layer before the softmax dropping units with a probability of 0.1 .,Model Test,Character Prediction,machine-translation,1,"['O', 'B', 'B', 'B', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'B', 'O', 'B', 'I', 'I', 'O']","['O', 'B-p', 'B-n', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['O', 'B-p', 'B-b', 'B-p', 'O', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'O']",17,0.3541666666666667,161,0.8009950248756219,10,0.24390243902439024,1,1,experiments
164,"At each step we sample a batch of sequences of 500 characters each , use the first 100 characters as the minimum context and predict the latter 400 characters .",Model Test,Character Prediction,machine-translation,1,"['O', 'O', 'O', 'O', 'B', 'O', 'B', 'B', 'I', 'O', 'B', 'I', 'I', 'O', 'B', 'O', 'B', 'I', 'I', 'B', 'B', 'I', 'I', 'O', 'B', 'O', 'B', 'I', 'I', 'O']",,,19,0.3958333333333333,163,0.8109452736318408,12,0.2926829268292683,1,1,experiments
167,The ByteNet decoder achieves 1.31 bits / character on the test set .,Model Test,Character Prediction,machine-translation,1,"['O', 'O', 'O', 'B', 'B', 'I', 'I', 'I', 'B', 'O', 'B', 'I', 'O']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['O', 'O', 'O', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'B-p', 'O', 'B-b', 'I-b', 'O']",22,0.4583333333333333,166,0.8258706467661692,15,0.36585365853658536,1,1,experiments
174,The ByteNet used in the experiments has 30 residual blocks in the encoder and 30 residual blocks in the decoder .,Model Test,Character - Level Machine Translation,machine-translation,1,"['O', 'B', 'O', 'B', 'O', 'O', 'O', 'B', 'I', 'I', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O']","['O', 'B-n', 'O', 'B-p', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']","['O', 'B-b', 'O', 'B-p', 'O', 'O', 'O', 'B-b', 'I-b', 'I-b', 'B-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'O']",29,0.6041666666666666,173,0.8606965174129353,22,0.5365853658536586,1,1,experiments
175,"As in the ByteNet Decoder , the residual blocks are arranged in sets of five with corresponding dilation rates of 1 , 2 , 4 , 8 and 16 .",Model Test,Character - Level Machine Translation,machine-translation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'B', 'I', 'B', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']",,,30,0.625,174,0.8656716417910447,23,0.5609756097560976,1,1,experiments
176,For this task we use the residual blocks with ReLUs ( .,Model Test,Character - Level Machine Translation,machine-translation,1,"['O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'B', 'B', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'B-p', 'B-ob', 'O', 'O']",31,0.6458333333333334,175,0.8706467661691543,24,0.5853658536585366,1,1,experiments
177,The number of hidden units dis 800 .,Model Test,Character - Level Machine Translation,machine-translation,1,"['O', 'O', 'O', 'B', 'I', 'B', 'I', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'B-ob', 'I-ob', 'O']",32,0.6666666666666666,176,0.8756218905472637,25,0.6097560975609756,1,1,experiments
178,"The size of the kernel in the source network is 3 , whereas the size of the masked kernel in the target network is 3 .",Model Test,Character - Level Machine Translation,machine-translation,1,"['O', 'B', 'I', 'I', 'I', 'B', 'O', 'B', 'I', 'O', 'B', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'O', 'O', 'B', 'I', 'O', 'O', 'O']","['O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O']","['O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-p', 'O', 'B-b', 'I-b', 'O', 'B-ob', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'O', 'O', 'B-b', 'I-b', 'O', 'O', 'O']",33,0.6875,177,0.8805970149253731,26,0.6341463414634146,1,1,experiments
179,For the optimization we use Adam with a learning rate of 0.0003 .,Model Test,Character - Level Machine Translation,machine-translation,1,"['O', 'O', 'B', 'O', 'B', 'B', 'B', 'O', 'B', 'I', 'I', 'I', 'O']","['O', 'O', 'B-n', 'O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'B-b', 'O', 'B-p', 'B-b', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",34,0.7083333333333334,178,0.8855721393034826,27,0.6585365853658537,1,1,experiments
180,Each sentence is padded with special characters to the nearest greater multiple of 50 ; 20 % of further padding is ap - plied to each source sentence as apart of dynamic unfolding ( eq. 2 ) .,Model Test,Character - Level Machine Translation,machine-translation,1,"['O', 'B', 'O', 'B', 'I', 'B', 'I', 'O', 'O', 'B', 'I', 'I', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-b', 'O', 'B-p', 'I-p', 'B-b', 'I-b', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",35,0.7291666666666666,179,0.8905472636815921,28,0.6829268292682927,1,1,experiments
181,Each pair of sentences is mapped to a bucket based on the pair of padded lengths for efficient batching during training .,Model Test,Character - Level Machine Translation,machine-translation,1,"['O', 'B', 'I', 'I', 'O', 'B', 'B', 'O', 'B', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'B', 'B', 'I', 'I', 'I', 'O']","['O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'B-b', 'I-b', 'I-b', 'O', 'B-p', 'B-p', 'O', 'B-b', 'B-p', 'I-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",36,0.75,180,0.8955223880597015,29,0.7073170731707317,1,1,experiments
182,We use vanilla beam search according to the total likelihood of the generated candidate and accept only candidates which end in a end -of - sentence token .,Model Test,Character - Level Machine Translation,machine-translation,1,"['O', 'O', 'B', 'I', 'I', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'I-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",37,0.7708333333333334,181,0.900497512437811,30,0.7317073170731707,1,1,experiments
183,We use abeam of size 12 .,Model Test,Character - Level Machine Translation,machine-translation,1,"['O', 'O', 'O', 'B', 'B', 'I', 'O']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O']","['O', 'O', 'O', 'B-p', 'B-ob', 'I-ob', 'O']",38,0.7916666666666666,182,0.9054726368159204,31,0.7560975609756098,1,1,experiments
186,"On NewsTest 2014 the ByteNet achieves the highest performance in character - level and subword - level neural machine translation , and compared to the word - level systems it is second only to the version of GNMT that uses word - pieces .",Model Test,Character - Level Machine Translation,machine-translation,1,"['O', 'B', 'I', 'O', 'O', 'B', 'O', 'B', 'I', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'B', 'B', 'O', 'B', 'I', 'I', 'I', 'O', 'B', 'B', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'B', 'B', 'I', 'I', 'O']",,,41,0.8541666666666666,185,0.9203980099502488,34,0.8292682926829268,1,1,experiments
187,"On NewsTest 2015 , to our knowledge , ByteNet achieves the best published results to date .",Model Test,Character - Level Machine Translation,machine-translation,1,"['O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'O']","['O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'B-b', 'I-b', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",42,0.875,186,0.9253731343283582,35,0.8536585365853658,1,1,experiments
6,"We propose anew simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .",abstract,abstract,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.1875,5,0.022222222222222223,3,0.1875,1,1,research-problem
22,Numerous efforts have since continued to push the boundaries of recurrent language models and encoder - decoder architectures .,Introduction,Introduction,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",2,0.18181818181818182,21,0.09333333333333334,2,0.18181818181818182,1,1,research-problem
30,"In this work we propose the Transformer , a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output .",Introduction,Introduction,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",10,0.9090909090909091,29,0.1288888888888889,10,0.9090909090909091,1,1,research-problem
50,The encoder is composed of a stack of N = 6 identical layers .,Model Architecture,Encoder and Decoder Stacks,machine-translation,2,"['O', 'O', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O']",,,8,0.07339449541284404,49,0.21777777777777776,2,0.15384615384615385,1,1,
51,Each layer has two sub-layers .,Model Architecture,Encoder and Decoder Stacks,machine-translation,2,"['O', 'O', 'O', 'B', 'I', 'O']",,,9,0.08256880733944955,50,0.2222222222222222,3,0.23076923076923078,1,1,
52,"The first is a multi-head self - attention mechanism , and the second is a simple , positionwise fully connected feed - forward network .",Model Architecture,Encoder and Decoder Stacks,machine-translation,2,"['O', 'B', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'B', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']",,,10,0.09174311926605505,51,0.22666666666666666,4,0.3076923076923077,1,1,
53,"We employ a residual connection around each of the two sub-layers , followed by layer normalization .",Model Architecture,Encoder and Decoder Stacks,machine-translation,2,"['O', 'B', 'O', 'B', 'I', 'B', 'O', 'O', 'O', 'B', 'I', 'O', 'B', 'I', 'B', 'I', 'O']",,,11,0.10091743119266056,52,0.2311111111111111,5,0.38461538461538464,1,1,
57,The decoder is also composed of a stack of N = 6 identical layers .,Model Architecture,Encoder and Decoder Stacks,machine-translation,2,"['O', 'O', 'O', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O']",,,15,0.13761467889908258,56,0.24888888888888888,9,0.6923076923076923,1,1,
58,"In addition to the two sub-layers in each encoder layer , the decoder inserts a third sub - layer , which performs multi-head attention over the output of the encoder stack .",Model Architecture,Encoder and Decoder Stacks,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'B', 'B', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'O']",,,16,0.14678899082568808,57,0.25333333333333335,10,0.7692307692307693,1,1,
59,"Similar to the encoder , we employ residual connections around each of the sub-layers , followed by layer normalization .",Model Architecture,Encoder and Decoder Stacks,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'I', 'B', 'B', 'I', 'I', 'I', 'O', 'B', 'I', 'B', 'I', 'O']",,,17,0.1559633027522936,58,0.2577777777777778,11,0.8461538461538461,1,1,
60,We also modify the self - attention sub - layer in the decoder stack to prevent positions from attending to subsequent positions .,Model Architecture,Encoder and Decoder Stacks,machine-translation,2,"['O', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'B', 'O', 'B', 'I', 'O', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'O']",,,18,0.1651376146788991,59,0.26222222222222225,12,0.9230769230769231,1,1,
63,"An attention function can be described as mapping a query and a set of key - value pairs to an output , where the query , keys , values , and output are all vectors .",Model Architecture,Attention,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'B', 'I', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'B', 'O', 'B', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",,,21,0.1926605504587156,62,0.27555555555555555,1,0.3333333333333333,1,1,
64,"The output is computed as a weighted sum of the values , where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key .",Model Architecture,Attention,machine-translation,2,"['O', 'O', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'B', 'O', 'B', 'I', 'O']",,,22,0.2018348623853211,63,0.28,2,0.6666666666666666,1,1,
103,"In addition to attention sub - layers , each of the layers in our encoder and decoder contains a fully connected feed - forward network , which is applied to each position separately and identically .",Model Architecture,See.,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'B', 'I', 'B', 'I', 'I', 'I', 'I', 'O']",,,61,0.5596330275229358,102,0.4533333333333333,12,0.7058823529411765,1,1,
104,This consists of two linear transformations with a ReLU activation in between .,Model Architecture,See.,machine-translation,2,"['O', 'B', 'O', 'B', 'I', 'I', 'B', 'O', 'B', 'I', 'O', 'O', 'O']",,,62,0.5688073394495413,103,0.4577777777777778,13,0.7647058823529411,1,1,
110,"Similarly to other sequence transduction models , we use learned embeddings to convert the input tokens and output tokens to vectors of dimension d model .",Model Architecture,Embeddings and Softmax,machine-translation,2,"['O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'I', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",,,68,0.6238532110091743,109,0.48444444444444446,1,0.25,1,1,
111,We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next - token probabilities .,Model Architecture,Embeddings and Softmax,machine-translation,2,"['O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'B', 'I', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'O']",,,69,0.6330275229357798,110,0.4888888888888889,2,0.5,1,1,
115,"Since our model contains no recurrence and no convolution , in order for the model to make use of the order of the sequence , we must inject some information about the relative or absolute position of the : Maximum path lengths , per-layer complexity and minimum number of sequential operations for different layer types .",Model Architecture,Positional Encoding,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",,,73,0.6697247706422018,114,0.5066666666666667,1,0.5,1,1,
116,"n is the sequence length , dis the representation dimension , k is the kernel size of convolutions and r the size of the neighborhood in restricted self - attention .",Model Architecture,Positional Encoding,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",74,0.6788990825688074,115,0.5111111111111111,2,1.0,1,1,
117,Layer Type Complexity per Layer Sequential Maximum Path Length Operations,Model Architecture,Positional Encoding,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",75,0.6880733944954128,116,0.5155555555555555,0,0.0,1,1,
118,tokens in the sequence .,Model Architecture,Positional Encoding,machine-translation,2,"['B', 'I', 'I', 'I', 'O']",,,76,0.6972477064220184,117,0.52,1,0.029411764705882353,1,1,
119,"To this end , we add "" positional encodings "" to the input embeddings at the bottoms of the encoder and decoder stacks .",Model Architecture,Positional Encoding,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'O', 'B', 'O', 'B', 'I', 'B', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'O']",,,77,0.7064220183486238,118,0.5244444444444445,2,0.058823529411764705,1,1,
161,We trained our models on one machine with 8 NVIDIA P100 GPUs .,Training Data and Batching,Hardware and Schedule,machine-translation,2,"['O', 'B', 'O', 'B', 'B', 'B', 'I', 'B', 'B', 'I', 'I', 'I', 'O']","['O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'B-p', 'O', 'B-b', 'B-p', 'B-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",7,0.28,160,0.7111111111111111,1,0.2,1,1,experimental-setup
163,"We trained the base models fora total of 100,000 steps or 12 hours .",Training Data and Batching,Hardware and Schedule,machine-translation,2,"['O', 'O', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'B-b', 'I-b', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",9,0.36,162,0.72,3,0.6,1,1,experimental-setup
165,"The big models were trained for 300,000 steps ( 3.5 days ) .",Training Data and Batching,Hardware and Schedule,machine-translation,2,"['O', 'B', 'I', 'O', 'O', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'B-b', 'I-b', 'O', 'O', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",11,0.44,164,0.7288888888888889,5,1.0,1,1,experimental-setup
167,"We used the Adam optimizer with ? 1 = 0.9 , ? 2 = 0.98 and = 10 ?9 .",Training Data and Batching,Optimizer,machine-translation,2,"['O', 'B', 'O', 'B', 'I', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'B-p', 'O', 'B-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",13,0.52,166,0.7377777777777778,1,0.25,1,1,experimental-setup
170,We used warmup_steps = 4000 .,Training Data and Batching,Optimizer,machine-translation,2,"['O', 'O', 'B', 'B', 'B', 'O']","['O', 'O', 'B-n', 'B-p', 'B-n', 'O']","['O', 'O', 'B-b', 'B-p', 'B-ob', 'O']",16,0.64,169,0.7511111111111111,4,1.0,1,1,experimental-setup
173,Residual Dropout,Training Data and Batching,,machine-translation,2,"['B', 'I']","['B-n', 'I-n']","['B-b', 'I-b']",19,0.76,172,0.7644444444444445,2,0.25,1,1,experimental-setup
174,"We apply dropout to the output of each sub - layer , before it is added to the sub - layer input and normalized .",Training Data and Batching,Residual Dropout,machine-translation,2,"['O', 'B', 'B', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-b', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.8,173,0.7688888888888888,3,0.375,1,1,experimental-setup
175,"In addition , we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks .",Training Data and Batching,Residual Dropout,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",21,0.84,174,0.7733333333333333,4,0.5,1,1,experimental-setup
177,Label Smoothing,Training Data and Batching,,machine-translation,2,"['B', 'I']","['B-n', 'I-n']","['B-b', 'I-b']",23,0.92,176,0.7822222222222223,6,0.75,1,1,experimental-setup
178,"During training , we employed label smoothing of value ls = 0.1 .",Training Data and Batching,Label Smoothing,machine-translation,2,"['O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",24,0.96,177,0.7866666666666666,7,0.875,1,1,experimental-setup
182,"On the WMT 2014 English - to - German translation task , the big transformer model ( Transformer ( big ) in ) outperforms the best previously reported models ( including ensembles ) by more than 2.0 BLEU , establishing anew state - of - the - art BLEU score of 28.4 .",Results,Machine Translation,machine-translation,2,"['O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'B', 'B', 'I', 'I', 'I', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'B', 'I', 'O', 'B', 'O']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'O']","['O', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'O', 'B-ob', 'O']",2,0.14285714285714285,181,0.8044444444444444,1,0.07692307692307693,1,1,results
186,"On the WMT 2014 English - to - French translation task , our big model achieves a BLEU score of 41.0 , outperforming all of the previously published single models , at less than 1 / 4 the training cost of the previous state - of - the - art model .",Results,Machine Translation,machine-translation,2,"['O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'O', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'O', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.42857142857142855,185,0.8222222222222222,5,0.38461538461538464,1,1,results
215,"Our results in show that despite the lack of task - specific tuning our model performs surprisingly well , yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar .",Model Variations,English Constituency Parsing,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'I', 'O', 'B', 'B', 'I', 'B', 'B', 'I', 'I', 'I', 'B', 'I', 'I', 'O', 'O', 'B', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-b', 'I-b', 'O', 'B-p', 'B-b', 'I-b', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'I-p', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",20,0.9523809523809523,214,0.9511111111111111,10,0.9090909090909091,1,1,results
2,Deep Recurrent Models with Fast - Forward Connections for Neural Machine Translation,title,title,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I']",,,1,0.0,1,0.003194888178913738,1,0.0,1,1,research-problem
4,Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years .,abstract,abstract,machine-translation,3,"['O', 'B', 'I', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-ob', 'I-ob', 'O', 'B-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.1111111111111111,3,0.009584664536741214,1,0.1111111111111111,1,1,research-problem
31,"In this work , we introduce anew type of linear connections for multi - layer recurrent networks .",Introduction,Introduction,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",18,0.12080536912751678,30,0.09584664536741214,18,0.6206896551724138,1,1,model
32,"These connections , which are called fast - forward connections , play an essential role in building a deep topology with depth of 16 .",Introduction,Introduction,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'B', 'B', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'B', 'B', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'O']",19,0.12751677852348994,31,0.09904153354632587,19,0.6551724137931034,1,1,model
33,"In addition , we introduce an interleaved bi-directional architecture to stack LSTM layers in the encoder .",Introduction,Introduction,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'B', 'I', 'B', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",20,0.1342281879194631,32,0.10223642172523961,20,0.6896551724137931,1,1,model
204,We use 256 dimensional word embeddings for both the source and target languages .,Model settings,Model settings,machine-translation,3,"['O', 'B', 'B', 'I', 'B', 'I', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'B-p', 'B-n', 'I-n', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'B-p', 'B-ob', 'I-ob', 'B-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",3,0.0967741935483871,203,0.6485623003194888,3,0.3,1,1,experimental-setup
205,"All LSTM layers , including the 2n e layers in the encoder and then d layers in the decoder , have 512 memory cells .",Model settings,Model settings,machine-translation,3,"['O', 'B', 'I', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'B', 'B', 'I', 'I', 'O']","['O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['O', 'B-b', 'I-b', 'O', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'B-p', 'B-b', 'I-b', 'I-b', 'O']",4,0.12903225806451613,204,0.6517571884984026,4,0.4,1,1,experimental-setup
208,"For each LSTM layer , the activation functions for gates , inputs and outputs are sigmoid , tanh , and tanh respectively .",Model settings,Model settings,machine-translation,3,"['O', 'B', 'I', 'I', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O']","['O', 'B-b', 'I-b', 'I-b', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O']",7,0.22580645161290322,207,0.6613418530351438,7,0.7,1,1,experimental-setup
214,"Since there are non-linear activations in the recurrent computation , a larger learning rate l r = 5 10 ? 4 is used , while for the feed - forward computation a smaller learning rate l f = 4 10 ? 5 is used .",Model settings,Optimization,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'O', 'O', 'B', 'I', 'I', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'B', 'B', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'O']",,,13,0.41935483870967744,213,0.6805111821086262,2,0.1,1,1,experimental-setup
224,The dropout ratio pd is 0.1 .,Model settings,Optimization,machine-translation,3,"['O', 'B', 'I', 'O', 'O', 'B', 'O']","['O', 'B-p', 'I-p', 'O', 'O', 'B-n', 'O']","['O', 'B-p', 'I-p', 'O', 'O', 'B-ob', 'O']",23,0.7419354838709677,223,0.7124600638977636,12,0.6,1,1,experimental-setup
229,We use 4 ? 8 GPU machines ( each has 4 K40 GPU cards ) running for 10 days to train the full model with parallelization at the data batch level .,Model settings,Optimization,machine-translation,3,"['O', 'O', 'B', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'B', 'I', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-ob', 'I-ob', 'B-p', 'I-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",28,0.9032258064516129,228,0.7284345047923323,17,0.85,1,1,experimental-setup
241,"From Deep - ED , we obtain the BLEU score of 36.3 , which outperforms Enc - Dec model by 4.8 BLEU points .",Single models,Single models,machine-translation,3,"['B', 'B', 'I', 'I', 'O', 'O', 'B', 'O', 'B', 'I', 'B', 'B', 'O', 'O', 'B', 'B', 'I', 'I', 'I', 'B', 'B', 'I', 'I', 'O']","['B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['B-p', 'B-b', 'I-b', 'I-b', 'O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'B-p', 'B-b', 'O', 'O', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'O']",4,0.2857142857142857,240,0.7667731629392971,4,0.2857142857142857,1,1,results
244,"For Deep - Att , the performance is further improved to 37.7 .",Single models,Single models,machine-translation,3,"['O', 'B', 'I', 'I', 'O', 'O', 'B', 'I', 'I', 'I', 'O', 'B', 'O']","['O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'O']","['O', 'B-b', 'I-b', 'I-b', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-ob', 'O']",7,0.5,243,0.7763578274760383,7,0.5,1,1,results
2,Unsupervised Neural Machine Translation with Weight Sharing,title,,machine-translation,4,"['B', 'I', 'I', 'I', 'O', 'O', 'O']",,,1,0.0,1,0.0041841004184100415,1,0.0,1,1,research-problem
4,Unsupervised neural machine translation ( NMT ) is a recently proposed approach for machine translation which aims to train the model without using any labeled data .,abstract,abstract,machine-translation,4,"['O', 'O', 'B', 'I', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-ob', 'I-ob', 'O', 'B-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.2,3,0.012552301255230125,1,0.2,1,1,research-problem
5,"The models proposed for unsupervised NMT often use only one shared encoder to map the pairs of sentences from different languages to a shared - latent space , which is weak in keeping the unique and internal characteristics of each language , such as the style , terminology , and sentence structure .",abstract,abstract,machine-translation,4,"['O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.4,4,0.016736401673640166,2,0.4,1,1,research-problem
26,"In order to address this issue , we extend the encoder - shared model , i.e. , the model with one shared encoder , by leveraging two independent encoders with each for one language .",Introduction,Introduction,machine-translation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",17,0.4857142857142857,25,0.10460251046025104,17,0.4857142857142857,1,1,model
27,"Similarly , two independent decoders are utilized .",Introduction,Introduction,machine-translation,4,"['O', 'O', 'B', 'I', 'I', 'O', 'B', 'O']","['O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O']","['O', 'O', 'B-ob', 'I-ob', 'I-ob', 'O', 'B-p', 'O']",18,0.5142857142857142,26,0.1087866108786611,18,0.5142857142857142,1,1,model
28,"For each language , the encoder and its corresponding decoder perform an AE , where the encoder generates the latent representations from the perturbed input sentences and the decoder reconstructs the sentences from the latent representations .",Introduction,Introduction,machine-translation,4,"['O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'O', 'O', 'O', 'B', 'I', 'O', 'B', 'I', 'B', 'O', 'B', 'I', 'B', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",,,19,0.5428571428571428,27,0.11297071129707113,19,0.5428571428571428,1,1,model
29,"To map the latent representations from different languages to a shared - latent space , we propose the weightsharing constraint to the two AEs .",Introduction,Introduction,machine-translation,4,"['B', 'I', 'O', 'B', 'I', 'B', 'B', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'B', 'O', 'B', 'I', 'O', 'O', 'B', 'I', 'O']","['B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O']","['B-p', 'I-p', 'O', 'B-b', 'I-b', 'B-p', 'B-b', 'I-b', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'O', 'O', 'B-ob', 'I-ob', 'O']",20,0.5714285714285714,28,0.11715481171548117,20,0.5714285714285714,1,1,model
33,"For cross - language translation , we utilize the backtranslation following .",Introduction,Introduction,machine-translation,4,"['O', 'B', 'I', 'I', 'I', 'O', 'O', 'B', 'O', 'B', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'O', 'O']","['O', 'B-b', 'I-b', 'I-b', 'I-b', 'O', 'O', 'B-p', 'O', 'B-ob', 'O', 'O']",24,0.6857142857142857,32,0.13389121338912133,24,0.6857142857142857,1,1,model
34,"Additionally , two different generative adversarial networks ( GAN ) , namely the local and global GAN , are proposed to further improve the cross - language translation .",Introduction,Introduction,machine-translation,4,"['O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'O', 'B', 'I', 'B', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'O']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'B-p', 'I-p', 'B-p', 'I-p', 'I-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",25,0.7142857142857143,33,0.13807531380753138,25,0.7142857142857143,1,1,model
35,"We utilize the local GAN to constrain the source and target latent representations to have the same distribution , whereby the encoder tries to fool a local discriminator which is simultaneously trained to distinguish the language of a given latent representation .",Introduction,Introduction,machine-translation,4,"['O', 'B', 'O', 'B', 'I', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'B', 'I', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-b', 'I-b', 'B-p', 'I-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'O', 'B-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.7428571428571429,34,0.14225941422594143,26,0.7428571428571429,1,1,model
36,"We apply the global GAN to finetune the corresponding generator , i.e. , the composition of the encoder and decoder of the other language , where a global discriminator is leveraged to guide the training of the generator by assessing how far the generated sentence is from the true data distribution 1 .",Introduction,Introduction,machine-translation,4,"['O', 'B', 'O', 'B', 'I', 'B', 'I', 'O', 'B', 'I', 'O', 'B', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-b', 'I-b', 'B-p', 'I-p', 'O', 'B-b', 'I-b', 'O', 'B-p', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.7714285714285715,35,0.14644351464435146,27,0.7714285714285715,1,1,model
168,"Following the base model in , we set the dimension of word embedding as 512 , dropout rate as 0.1 and the head number as 8 .",Experiments and Results,Model Hyper - parameters and Evaluation,machine-translation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'B', 'B', 'O', 'B', 'I', 'O', 'B', 'O', 'O', 'B', 'I', 'O', 'B', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'O', 'B-b', 'I-b', 'O', 'B-ob', 'O', 'O', 'B-b', 'I-b', 'O', 'B-ob', 'O']",26,0.7428571428571429,167,0.698744769874477,22,0.7096774193548387,1,1,hyperparameters
169,We use beam search with abeam size of 4 and length penalty ? = 0.6 .,Experiments and Results,Model Hyper - parameters and Evaluation,machine-translation,4,"['O', 'B', 'B', 'I', 'B', 'O', 'O', 'B', 'B', 'O', 'B', 'I', 'B', 'I', 'B', 'O']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'O', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O']","['O', 'B-p', 'B-b', 'I-b', 'B-p', 'O', 'O', 'B-p', 'B-ob', 'O', 'B-b', 'I-b', 'B-p', 'I-p', 'B-ob', 'O']",27,0.7714285714285715,168,0.702928870292887,23,0.7419354838709677,1,1,hyperparameters
170,The model is implemented in TensorFlow and trained on up to four K80 GPUs synchronously in a multi - GPU setup on a single machine .,Experiments and Results,Model Hyper - parameters and Evaluation,machine-translation,4,"['O', 'O', 'O', 'B', 'B', 'B', 'O', 'B', 'B', 'B', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'B', 'I', 'O']",,,28,0.8,169,0.7071129707112971,24,0.7741935483870968,1,1,hyperparameters
179,Word - by - word translation ( WBW ) The first baseline we consider is a system that performs word - by - word translations using the inferred bilingual dictionary .,Baseline Systems,Baseline Systems,machine-translation,4,"['B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.1111111111111111,178,0.7447698744769874,1,0.08333333333333333,1,1,baselines
181,Lample et al .,Baseline Systems,Baseline Systems,machine-translation,4,"['B', 'I', 'I', 'I']","['B-n', 'I-n', 'I-n', 'I-n']","['B-ob', 'I-ob', 'I-ob', 'I-ob']",3,0.3333333333333333,180,0.7531380753138075,3,0.25,1,1,baselines
188,Supervised training,,,machine-translation,4,"['B', 'I']","['B-n', 'I-n']","['B-ob', 'I-ob']",0,0.0,187,0.7824267782426778,10,0.8333333333333334,1,1,baselines
199,And the best translation performance is achieved when only one layer is shared in our system .,Results and Analysis,Results and Analysis,machine-translation,4,"['O', 'O', 'B', 'I', 'I', 'O', 'B', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'B-b', 'I-b', 'I-b', 'O', 'B-p', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",8,0.36363636363636365,198,0.8284518828451883,8,0.36363636363636365,1,1,results
200,"When all of the four layers are shared , i.e. , only one shared encoder is utilized , we get poor translation performance in all of the three translation tasks .",Results and Analysis,Results and Analysis,machine-translation,4,"['O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'I', 'I', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",9,0.4090909090909091,199,0.8326359832635983,9,0.4090909090909091,1,1,results
220,"The most critical component is the weight - sharing constraint , which is vital to map sentences of different languages to the shared - latent space .",Ablation study,Ablation study,machine-translation,4,"['O', 'B', 'I', 'I', 'O', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'B', 'I', 'I', 'I', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'B-p', 'I-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'B-p', 'I-p', 'I-p', 'O', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",5,0.45454545454545453,219,0.9163179916317992,5,0.45454545454545453,1,1,ablation-analysis
221,The embedding - reinforced encoder also brings some improvement on all of the translation tasks .,Ablation study,Ablation study,machine-translation,4,"['O', 'B', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'B', 'B', 'I', 'I', 'I', 'I', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'B-b', 'I-b', 'I-b', 'I-b', 'O', 'B-p', 'I-p', 'I-p', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",6,0.5454545454545454,220,0.9205020920502092,6,0.5454545454545454,1,1,ablation-analysis
222,"When we remove the directional self - attention , we getup to - 0.3 BLEU points decline .",Ablation study,Ablation study,machine-translation,4,"['O', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",7,0.6363636363636364,221,0.9246861924686193,7,0.6363636363636364,1,1,ablation-analysis
224,The GANs also significantly improve the translation performance of our system .,Ablation study,Ablation study,machine-translation,4,"['B', 'I', 'O', 'B', 'I', 'O', 'B', 'I', 'O', 'B', 'I', 'O']","['B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['B-b', 'I-b', 'O', 'B-p', 'I-p', 'O', 'B-p', 'I-p', 'O', 'B-ob', 'I-ob', 'O']",9,0.8181818181818182,223,0.9330543933054394,9,0.8181818181818182,1,1,ablation-analysis
2,Tilde 's Machine Translation Systems for WMT 2018,title,title,machine-translation,5,"['O', 'O', 'B', 'I', 'O', 'O', 'O', 'O']",,,1,0.0,1,0.006993006993006993,1,0.0,1,1,research-problem
4,The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation .,abstract,abstract,machine-translation,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'O']",1,0.25,3,0.02097902097902098,1,0.25,1,1,research-problem
9,Neural machine translation ( NMT ) is a rapidly changing research area .,Introduction,Introduction,machine-translation,5,"['B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.06666666666666667,8,0.055944055944055944,1,0.06666666666666667,1,1,research-problem
25,"For the WMT 2018 shared task on news translation , Tilde submitted both constrained and unconstrained NMT systems ( 7 in total ) .",System Overview,System Overview,machine-translation,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0136986301369863,24,0.16783216783216784,1,0.1111111111111111,1,1,model
26,The following is a list of the five MT systems submitted :,System Overview,System Overview,machine-translation,5,"['O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O']",,,2,0.0273972602739726,25,0.17482517482517482,2,0.2222222222222222,1,1,
27,Constrained English - Estonian and Estonian - English NMT systems ( tilde - c-nmt ) that were deployed as ensembles of averaged factored data ( see Section 3 ) Transformer models .,System Overview,System Overview,machine-translation,5,"['B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",,,3,0.0410958904109589,26,0.18181818181818182,3,0.3333333333333333,1,1,
28,The models were trained using parallel data and back - translated data in a 1 - to - 1 proportion .,System Overview,System Overview,machine-translation,5,"['O', 'O', 'O', 'B', 'I', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']",,,4,0.0547945205479452,27,0.1888111888111888,4,0.4444444444444444,1,1,
29,Unconstrained English - Estonian and Estonian - English NMT systems ( tilde - nc - nmt ) that were deployed as averaged Transformer models .,System Overview,System Overview,machine-translation,5,"['B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.0684931506849315,28,0.1958041958041958,5,0.5555555555555556,1,1,model
30,"These models were also trained using back - translated data similarly to the constrained systems , however , the data , taking into account their relatively large size , were not factored .",System Overview,System Overview,machine-translation,5,"['O', 'O', 'O', 'O', 'B', 'I', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'B', 'O']","['O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O']","['O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-b', 'I-b', 'I-b', 'I-b', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-ob', 'O']",6,0.0821917808219178,29,0.20279720279720279,6,0.6666666666666666,1,1,model
31,A constrained Estonian - English NMT system ( tilde - c - nmt - comb ) that is a system combination of six factored data NMT systems .,System Overview,System Overview,machine-translation,5,"['O', 'B', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",7,0.0958904109589041,30,0.2097902097902098,7,0.7777777777777778,1,1,model
32,Constrained English - Estonian and Estonian - English NMT systems ( tilde - c - nmt - 2 bt ) averaged from multiple best NMT models .,System Overview,System Overview,machine-translation,5,"['B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.1095890410958904,31,0.21678321678321677,8,0.8888888888888888,1,1,model
33,The models were trained using two sets of back - translated data in a 1 - to - 1 proportion to the clean parallel data - one set was backtranslated using a system trained on parallelonly data and the other set -using an NMT system trained on parallel data and the first set of back - translated data .,System Overview,System Overview,machine-translation,5,"['O', 'O', 'O', 'B', 'I', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.1232876712328767,32,0.22377622377622378,9,1.0,1,1,model
129,The results ( see ) show that the Transformer models achieved better results than the MLSTM - based models .,Results,Results,machine-translation,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",2,0.25,128,0.8951048951048951,2,0.25,1,1,results
130,"For the constrained scenarios , both ensembles of averaged models achieved higher scores than each individual averaged model .",Results,Results,machine-translation,5,"['B', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'B', 'B', 'I', 'B', 'B', 'I', 'I', 'I', 'O']","['B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['B-p', 'O', 'B-b', 'I-b', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'B-p', 'B-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",3,0.375,129,0.9020979020979021,3,0.375,1,1,results
131,It is also evident that the unconstrained models ( tilde - nc - nmt ) achieved the best results .,Results,Results,machine-translation,5,"['O', 'O', 'O', 'B', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O']","['O', 'O', 'O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O']","['O', 'O', 'O', 'B-p', 'O', 'O', 'B-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-b', 'I-b', 'O']",4,0.5,130,0.9090909090909091,4,0.5,1,1,results
2,FRAGE : Frequency - Agnostic Word Representation,title,title,machine-translation,6,"['O', 'O', 'B', 'I', 'I', 'I', 'I']",,,1,0.0,1,0.003436426116838488,1,0.0,1,1,research-problem
5,"Although it is widely accepted that words with similar semantics should be close to each other in the embedding space , we find that word embeddings learned in several tasks are biased towards word frequency : the embeddings of highfrequency and low - frequency words lie in different subregions of the embedding space , and the embedding of a rare word and a popular word can be far from each other even if they are semantically similar .",abstract,abstract,machine-translation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.3333333333333333,4,0.013745704467353952,2,0.3333333333333333,1,1,research-problem
22,"Interestingly , the learned embeddings of rare words and popular words behave differently .",Introduction,Introduction,machine-translation,6,"['O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",12,0.35294117647058826,21,0.07216494845360824,12,0.35294117647058826,1,1,research-problem
26,"As shown in ( a ) and 1 ( b ) , the embeddings of rare words and popular words actually lie in different subregions of the space .",Introduction,Introduction,machine-translation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",16,0.47058823529411764,25,0.0859106529209622,16,0.47058823529411764,1,1,research-problem
28,We argue that the different behaviors of the embeddings of popular words and rare words are problematic .,Introduction,Introduction,machine-translation,6,"['O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",18,0.5294117647058824,27,0.09278350515463918,18,0.5294117647058824,1,1,research-problem
36,"To address this problem , in this paper , we propose an adversarial training method to learn FRequency - AGnostic word Embedding ( FRAGE ) .",Introduction,Introduction,machine-translation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",26,0.7647058823529411,35,0.12027491408934708,26,0.7647058823529411,1,1,approach
37,"For a given NLP task , in addition to minimize the task - specific loss by optimizing the task - specific parameters together with word embeddings , we introduce another discriminator , which takes a word embedding as input and classifies whether it is a popular / rare word .",Introduction,Introduction,machine-translation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'B', 'I', 'B', 'I', 'O', 'O', 'B', 'B', 'I', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'B', 'O', 'B', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-p', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'B-ob', 'I-ob', 'O', 'O', 'B-p', 'B-b', 'I-b', 'O', 'O', 'O', 'B-b', 'I-b', 'I-b', 'O', 'B-p', 'O', 'B-p', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",27,0.7941176470588235,36,0.12371134020618557,27,0.7941176470588235,1,1,approach
38,"The discriminator optimizes its parameters to maximize its classification accuracy , while word embeddings are optimized towards a low task - dependent loss as well as fooling the discriminator to mis-classify the popular and rare words .",Introduction,Introduction,machine-translation,6,"['O', 'B', 'B', 'O', 'O', 'B', 'I', 'O', 'B', 'I', 'O', 'O', 'B', 'I', 'O', 'B', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'B', 'O', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'O']","['O', 'B-n', 'B-p', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'B-b', 'B-p', 'O', 'O', 'B-p', 'I-p', 'O', 'B-ob', 'I-ob', 'O', 'O', 'B-b', 'I-b', 'O', 'B-p', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'B-p', 'O', 'O', 'B-p', 'I-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",28,0.8235294117647058,37,0.12714776632302405,28,0.8235294117647058,1,1,approach
39,"When the whole training process converges and the system achieves an equilibrium , the discriminator can not well differentiate popular words from rare words .",Introduction,Introduction,machine-translation,6,"['B', 'B', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'B', 'O', 'B', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'B', 'I', 'B', 'B', 'I', 'O']","['B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'B-p', 'O', 'B-b', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'O']",29,0.8529411764705882,38,0.13058419243986255,29,0.8529411764705882,1,1,approach
178,"We use the skip - gram model as our baseline model , and train the embeddings using Enwik9 6 .",Experiment,Settings,machine-translation,6,"['O', 'B', 'O', 'B', 'I', 'I', 'I', 'B', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.4117647058823529,177,0.6082474226804123,3,0.09090909090909091,1,1,experiments
179,"We test the baseline and our method on three datasets : RG65 , WS and RW .",Experiment,Settings,machine-translation,6,"['O', 'B', 'O', 'B', 'I', 'I', 'I', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",22,0.43137254901960786,178,0.6116838487972509,4,0.12121212121212122,1,1,experiments
183,The goal is to predict the next word conditioned on previous words and the task is evaluated by perplexity .,Experiment,Settings,machine-translation,6,"['O', 'B', 'O', 'O', 'B', 'I', 'I', 'I', 'B', 'I', 'B', 'I', 'O', 'O', 'O', 'O', 'B', 'I', 'B', 'O']","['O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O']","['O', 'B-p', 'O', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'B-ob', 'I-ob', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-ob', 'O']",26,0.5098039215686274,182,0.6254295532646048,8,0.24242424242424243,1,1,experiments
184,"We do experiments on two widely used datasets , Penn Treebank ( PTB ) and WikiText - 2 ( WT2 ) .",Experiment,Settings,machine-translation,6,"['O', 'O', 'B', 'I', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'B-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",27,0.5294117647058824,183,0.6288659793814433,9,0.2727272727272727,1,1,experiments
185,"We choose two recent works as our baselines : the AWD - LSTM model and the AWD - LSTM - MoS model , which achieves state - of - the - art performance .",Experiment,Settings,machine-translation,6,"['O', 'B', 'B', 'I', 'I', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",28,0.5490196078431373,184,0.6323024054982818,10,0.30303030303030304,1,1,experiments
187,"We choose two datasets : WMT14 English - German and IWSLT14 German - English datasets , which are evaluated in terms of BLEU score .",Experiment,Settings,machine-translation,6,"['O', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'O', 'B', 'O']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'O']","['O', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-ob', 'O']",30,0.5882352941176471,186,0.6391752577319587,12,0.36363636363636365,1,1,experiments
188,"We use Transformer as the baseline model , which achieves state - of - the - art accuracy on multiple translation datasets .",Experiment,Settings,machine-translation,6,"['O', 'B', 'B', 'B', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-b', 'B-p', 'O', 'B-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",31,0.6078431372549019,187,0.6426116838487973,13,0.3939393939393939,1,1,experiments
191,"Following the setting in , we implement a Recurrent CNN - based model and test it on AG 's news corpus ( AGs ) , IMDB movie review dataset ( IMDB ) and 20 Newsgroups ( 20 NG ) .",Experiment,Settings,machine-translation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'B-p', 'I-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",34,0.6666666666666666,190,0.6529209621993127,16,0.48484848484848486,1,1,experiments
217,"In all these settings , our method outperforms the two baselines .",Language Modeling,Language Modeling,machine-translation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-ob', 'I-ob', 'O']",3,0.17647058823529413,216,0.7422680412371134,3,0.5,1,1,results
218,"On PTB dataset , our method improves the AWD - LSTM and AWD - LSTM - MoS baseline by 0.8/1.2/1.0 and 0.76/1.13/1.15 points in test set at different checkpoints .",Language Modeling,Language Modeling,machine-translation,6,"['O', 'B', 'I', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'B', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-b', 'I-b', 'O', 'O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.23529411764705882,217,0.7457044673539519,4,0.6666666666666666,1,1,results
219,"On WT2 dataset , which contains more rare words , our method achieves larger improvements .",Language Modeling,Language Modeling,machine-translation,6,"['O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'I', 'O']","['O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O']","['O', 'B-b', 'I-b', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-ob', 'I-ob', 'O']",5,0.29411764705882354,218,0.7491408934707904,5,0.8333333333333334,1,1,results
220,"We improve the results of AWD - LSTM and AWD - LSTM - MoS by 2.3/2.4/2.7 and 1.15/1.72/1.54 in terms of test perplexity , respectively .",Language Modeling,Language Modeling,machine-translation,6,"['O', 'B', 'I', 'I', 'I', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'B', 'B', 'I', 'I', 'B', 'I', 'I', 'B', 'I', 'O', 'O', 'O']","['O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O']","['O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'B-b', 'I-b', 'I-b', 'B-p', 'I-p', 'I-p', 'B-ob', 'I-ob', 'O', 'O', 'O']",6,0.35294117647058826,219,0.7525773195876289,6,1.0,1,1,results
223,We outperform the baselines for 1.06/0.71 in the term of BLEU in transformer_base and transformer_big settings in WMT14 English - German : BLEU scores on test set on WMT2014 English - German and IWSLT German - English tasks .,Language Modeling,Machine Translation,machine-translation,6,"['O', 'B', 'O', 'B', 'B', 'B', 'B', 'I', 'I', 'I', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-b', 'B-p', 'B-ob', 'B-p', 'I-p', 'I-p', 'I-p', 'B-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.5294117647058824,222,0.7628865979381443,2,0.4,1,1,results
225,The model learned from adversarial training also outperforms original one in IWSLT14 German - English task by 0.85 .,Language Modeling,Machine Translation,machine-translation,6,"['O', 'B', 'B', 'I', 'B', 'I', 'O', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'B', 'B', 'O']","['O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['O', 'B-b', 'B-p', 'I-p', 'B-ob', 'I-ob', 'O', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'O']",11,0.6470588235294118,224,0.7697594501718213,4,0.8,1,1,results
229,Our method outperforms the baseline method for 1.26%/0.66%/0.44 % on three different datasets .,Language Modeling,Text Classification,machine-translation,6,"['O', 'O', 'B', 'O', 'B', 'O', 'B', 'B', 'I', 'B', 'B', 'I', 'I', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['O', 'O', 'B-p', 'O', 'B-b', 'O', 'B-p', 'B-ob', 'I-ob', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'O']",15,0.8823529411764706,228,0.7835051546391752,2,0.5,1,1,results
4,The capacity of a neural network to absorb information is limited by its number of parameters .,abstract,abstract,machine-translation,7,"['O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",1,0.010869565217391304,3,0.00804289544235925,1,0.02040816326530612,1,1,research-problem
5,"Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation .",abstract,abstract,machine-translation,7,"['B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",2,0.021739130434782608,4,0.010723860589812333,2,0.04081632653061224,1,1,research-problem
16,Exploiting scale in both training data and model size has been central to the success of deep learning .,abstract,abstract,machine-translation,7,"['B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.14130434782608695,15,0.040214477211796246,13,0.2653061224489796,1,1,research-problem
45,Our approach to conditional computation is to introduce anew type of general purpose neural network component : a Sparsely - Gated Mixture - of - Experts Layer ( MoE ) .,abstract,abstract,machine-translation,7,"['O', 'O', 'B', 'B', 'I', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'B-p', 'B-b', 'I-b', 'O', 'B-p', 'I-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",42,0.45652173913043476,44,0.11796246648793565,42,0.8571428571428571,1,1,approach
46,"The MoE consists of a number of experts , each a simple feed - forward neural network , and a trainable gating network which selects a sparse combination of the experts to process each input ( see ) .",abstract,abstract,machine-translation,7,"['O', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'I-p', 'O', 'B-b', 'I-b', 'I-b', 'O', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'B-b', 'I-b', 'I-b', 'O', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O']",43,0.4673913043478261,45,0.12064343163538874,43,0.8775510204081632,1,1,approach
47,All parts of the network are trained jointly by back - propagation .,abstract,abstract,machine-translation,7,"['B', 'I', 'I', 'O', 'B', 'O', 'B', 'B', 'B', 'B', 'I', 'I', 'O']","['B-p', 'I-p', 'I-p', 'O', 'B-n', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['B-p', 'I-p', 'I-p', 'O', 'B-ob', 'O', 'B-p', 'B-b', 'B-p', 'B-b', 'I-b', 'I-b', 'O']",44,0.4782608695652174,46,0.12332439678284182,44,0.8979591836734694,1,1,approach
186,"In addition to a baseline LSTM model , we trained models augmented with MoE layers containing 32 , experts .",Training the Gating Network,Computational,machine-translation,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'O']",90,0.743801652892562,185,0.4959785522788204,3,0.1111111111111111,1,1,tasks
190,"When training over the full 100 billion words , test perplexity improves significantly up to 65536 experts ( 68 billion parameters ) , dropping 39 % lower than the computationally matched baseline , but degrades at 131072 experts , possibly a result of too much sparsity .",Training the Gating Network,Computational,machine-translation,7,"['B', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'B', 'I', 'B', 'I', 'B', 'I', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O', 'B-p', 'I-p', 'B-b', 'I-b', 'B-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",94,0.7768595041322314,189,0.5067024128686327,7,0.25925925925925924,1,1,tasks
195,Our model was a modified version of the GNMT model described in .,Training the Gating Network,Computational,machine-translation,7,"['O', 'B', 'B', 'I', 'B', 'I', 'O', 'O', 'B', 'I', 'O', 'O', 'O']","['O', 'B-p', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O']","['O', 'B-p', 'B-p', 'I-p', 'B-ob', 'I-ob', 'O', 'O', 'B-b', 'I-b', 'O', 'O', 'O']",99,0.8181818181818182,194,0.5201072386058981,12,0.4444444444444444,1,1,tasks
196,"To reduce computation , we decreased the number of LSTM layers in the encoder and decoder from 9 and 8 to 3 and 2 respectively .",Training the Gating Network,Computational,machine-translation,7,"['B', 'I', 'B', 'O', 'O', 'B', 'O', 'O', 'O', 'B', 'I', 'B', 'O', 'B', 'I', 'I', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['B-p', 'I-p', 'B-n', 'O', 'O', 'B-p', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['B-p', 'I-p', 'B-ob', 'O', 'O', 'B-p', 'O', 'O', 'O', 'B-b', 'I-b', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",100,0.8264462809917356,195,0.5227882037533512,13,0.48148148148148145,1,1,tasks
197,We inserted MoE layers in both the encoder ( between layers 2 and 3 ) and the decoder ( between layers 1 and 2 ) .,Training the Gating Network,Computational,machine-translation,7,"['O', 'B', 'B', 'I', 'B', 'I', 'O', 'B', 'O', 'B', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'B', 'O', 'O', 'B', 'I', 'I', 'I', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O']","['O', 'B-p', 'B-b', 'I-b', 'B-p', 'I-p', 'O', 'B-b', 'O', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'B-b', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O']",101,0.8347107438016529,196,0.5254691689008043,14,0.5185185185185185,1,1,tasks
198,"Each MoE layer contained up to 2048 experts each with about two million parameters , adding a total of about 8 billion parameters to the models .",Training the Gating Network,Computational,machine-translation,7,"['O', 'B', 'I', 'B', 'I', 'I', 'B', 'I', 'I', 'B', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-b', 'I-b', 'B-p', 'I-p', 'I-p', 'B-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",102,0.8429752066115702,197,0.5281501340482574,15,0.5555555555555556,1,1,tasks
206,Our approach achieved BLEU scores of 40.56 and 26.03 on the WMT ' 14 En?Fr and En ? De benchmarks .,Training the Gating Network,Computational,machine-translation,7,"['O', 'O', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",110,0.9090909090909091,205,0.5495978552278821,23,0.8518518518518519,1,1,tasks
210,"On the Google Production dataset , our model achieved 1.01 higher test BLEU score even after training for only one sixth of the time .",Training the Gating Network,Computational,machine-translation,7,"['B', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'B', 'I', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'B-b', 'I-b', 'B-p', 'I-p', 'I-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",114,0.9421487603305785,209,0.5603217158176944,27,1.0,1,1,tasks
214,The MoE model achieves 19 % lower perplexity on the dev set than the multilingual GNMT model .,Training the Gating Network,Computational,machine-translation,7,"['O', 'O', 'O', 'B', 'B', 'I', 'I', 'I', 'B', 'O', 'B', 'I', 'B', 'O', 'B', 'I', 'I', 'O']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'O', 'B-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'O']",118,0.9752066115702479,213,0.5710455764075067,3,0.5,1,1,tasks
215,"On BLEU score , the MoE model significantly beats the multilingual GNMT model on 11 of the 12 language pairs ( by as much as 5.84 points ) , and even beats the monolingual GNMT models on 8 of 12 language pairs .",Training the Gating Network,Computational,machine-translation,7,"['B', 'B', 'I', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'B', 'I', 'O', 'O', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'O']","['B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['B-p', 'B-b', 'I-b', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-ob', 'I-ob', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-b', 'I-b', 'I-b', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",119,0.9834710743801653,214,0.5737265415549598,4,0.6666666666666666,1,1,tasks
2,Published as a conference paper at ICLR 2015 NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE,,,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",,,2,0.2222222222222222,1,0.0030211480362537764,1,0.0,1,1,research-problem
7,"In this paper , we conjecture that the use of a fixed - length vector is a bottleneck in improving the performance of this basic encoder - decoder architecture , and propose to extend this by allowing a model to automatically ( soft - ) search for parts of a source sentence that are relevant to predicting a target word , without having to form these parts as a hard segment explicitly .",,,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.7777777777777778,6,0.01812688821752266,4,0.6666666666666666,1,1,research-problem
11,"Neural machine translation is a newly emerging approach to machine translation , recently proposed by , and .",INTRODUCTION,INTRODUCTION,machine-translation,8,"['O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.010869565217391304,10,0.030211480362537766,1,0.023255813953488372,1,1,research-problem
20,"In order to address this issue , we introduce an extension to the encoder - decoder model which learns to align and translate jointly .",INTRODUCTION,INTRODUCTION,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'B', 'I', 'B', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O', 'B-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",10,0.10869565217391304,19,0.05740181268882175,10,0.23255813953488372,1,1,model
21,"Each time the proposed model generates a word in a translation , it ( soft - ) searches fora set of positions in a source sentence where the most relevant information is concentrated .",INTRODUCTION,INTRODUCTION,machine-translation,8,"['B', 'I', 'O', 'O', 'O', 'B', 'O', 'B', 'B', 'O', 'B', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'O', 'O', 'B', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'O']","['B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['B-b', 'I-b', 'O', 'O', 'O', 'B-p', 'O', 'B-b', 'B-p', 'O', 'B-ob', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'O', 'B-b', 'I-b', 'I-b', 'O', 'O', 'B-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",11,0.11956521739130435,20,0.06042296072507553,11,0.2558139534883721,1,1,model
22,The model then predicts a target word based on the context vectors associated with these source positions and all the previous generated target words .,INTRODUCTION,INTRODUCTION,machine-translation,8,"['O', 'O', 'O', 'B', 'O', 'B', 'I', 'B', 'I', 'O', 'B', 'I', 'B', 'I', 'O', 'B', 'I', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'B-p', 'I-p', 'O', 'B-b', 'I-b', 'B-p', 'I-p', 'O', 'B-ob', 'I-ob', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",12,0.13043478260869565,21,0.0634441087613293,12,0.27906976744186046,1,1,model
118,We train two types of models .,MODELS,MODELS,machine-translation,8,"['O', 'B', 'B', 'I', 'I', 'I', 'O']",,,1,0.07692307692307693,117,0.35347432024169184,1,0.07692307692307693,1,1,
119,"The first one is an RNN Encoder - Decoder ( RNNencdec , , and the other is the proposed model , to which we refer as RNNsearch .",MODELS,MODELS,machine-translation,8,"['O', 'B', 'I', 'O', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O']",,,2,0.15384615384615385,118,0.3564954682779456,2,0.15384615384615385,1,1,
120,"We train each model twice : first with the sentences of length up to 30 words ( RNNencdec - 30 , RNNsearch - 30 ) and then with the sentences of length up to 50 word ( RNNencdec - 50 , RNNsearch - 50 ) .",MODELS,MODELS,machine-translation,8,"['O', 'O', 'B', 'I', 'I', 'O', 'O', 'B', 'O', 'B', 'B', 'I', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",,,3,0.23076923076923078,119,0.3595166163141994,3,0.23076923076923078,1,1,
121,The encoder and decoder of the RNNencdec have 1000 hidden units each .,MODELS,MODELS,machine-translation,8,"['O', 'B', 'I', 'I', 'B', 'O', 'B', 'B', 'B', 'B', 'I', 'I', 'O']",,,4,0.3076923076923077,120,0.36253776435045315,4,0.3076923076923077,1,1,
122,The encoder of the RNNsearch consists of forward and backward recurrent neural networks ( RNN ) each having 1000 hidden units .,MODELS,MODELS,machine-translation,8,"['O', 'B', 'I', 'O', 'B', 'B', 'I', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'B', 'I', 'B', 'I', 'I', 'O']",,,5,0.38461538461538464,121,0.36555891238670696,5,0.38461538461538464,1,1,
123,It s decoder has 1000 hidden units .,MODELS,MODELS,machine-translation,8,"['O', 'O', 'B', 'O', 'B', 'B', 'I', 'O']",,,6,0.46153846153846156,122,0.3685800604229607,6,0.46153846153846156,1,1,
124,"In both cases , we use a multilayer network with a single maxout hidden layer to compute the conditional probability of each target word .",MODELS,MODELS,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'B', 'I', 'O', 'B', 'I', 'B', 'B', 'I', 'I', 'O']",,,7,0.5384615384615384,123,0.3716012084592145,7,0.5384615384615384,1,1,
141,"In , we see that the performance of RNNencdec dramatically drops as the length of the sentences increases .",QUANTITATIVE RESULTS,QUANTITATIVE RESULTS,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'B', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-b', 'B-p', 'I-p', 'I-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",9,0.13846153846153847,140,0.4229607250755287,9,0.6923076923076923,1,1,results
142,"On the other hand , both RNNsearch - 30 and RNNsearch - 50 are more robust to the length of the sentences .",QUANTITATIVE RESULTS,QUANTITATIVE RESULTS,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'B', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O', 'B-p', 'I-p', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",10,0.15384615384615385,141,0.4259818731117825,10,0.7692307692307693,1,1,results
143,"RNNsearch - 50 , especially , shows no performance deterioration even with sentences of length 50 or more .",QUANTITATIVE RESULTS,QUANTITATIVE RESULTS,machine-translation,8,"['B', 'I', 'I', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'B', 'B', 'B', 'I', 'B', 'I', 'I', 'O']","['B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-p', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['B-b', 'I-b', 'I-b', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-p', 'B-b', 'B-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'O']",11,0.16923076923076924,142,0.42900302114803623,11,0.8461538461538461,1,1,results
144,This superiority of the proposed model over the basic encoder - decoder is further confirmed by the fact that the RNNsearch - 30 even outperforms RNNencdec - 50 ( see ) .,QUANTITATIVE RESULTS,QUANTITATIVE RESULTS,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'B', 'B', 'B', 'I', 'I', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O']",12,0.18461538461538463,143,0.43202416918429004,12,0.9230769230769231,1,1,results
2,Under review as a conference paper at ICLR 2018 COMPRESSING WORD EMBEDDINGS VIA DEEP COMPOSITIONAL CODE LEARNING,title,title,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O']",,,1,0.0,1,0.003484320557491289,1,0.0,1,1,research-problem
5,Deploying neural NLP models to mobile devices requires compressing the word embeddings without any significant sacrifices in performance .,abstract,abstract,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",2,0.2,4,0.013937282229965157,2,0.2,1,1,research-problem
24,"Thus , it is becoming more important to compress the size of NLP models for deployment to devices with limited memory or storage capacity .",INTRODUCTION,INTRODUCTION,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.07042253521126761,23,0.08013937282229965,10,0.19230769230769232,1,1,research-problem
32,"Following the intuition of creating partially shared embeddings , instead of assigning each word a unique ID , we represent each word w with a code C w = ( C 1 w , C 2 w , ... , C M w ) .",INTRODUCTION,INTRODUCTION,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'I', 'I', 'B', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-b', 'I-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.1267605633802817,31,0.10801393728222997,18,0.34615384615384615,1,1,approach
37,"Once we have obtained such compact codes for all words in the vocabulary , we use embedding vectors to represent the codes rather than the unique words .",INTRODUCTION,Each component,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'I', 'B', 'I', 'O', 'O', 'B', 'I', 'O', 'B', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-b', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-b', 'I-b', 'B-p', 'I-p', 'O', 'O', 'B-p', 'I-p', 'O', 'B-ob', 'I-ob', 'O']",23,0.1619718309859155,36,0.1254355400696864,23,0.4423076923076923,1,1,approach
38,"More specifically , we create M codebooks E 1 , E 2 , ... , EM , each containing K codeword vectors .",INTRODUCTION,Each component,machine-translation,9,"['O', 'O', 'O', 'O', 'B', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'B', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'B-p', 'B-b', 'I-b', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'O']",24,0.16901408450704225,37,0.1289198606271777,24,0.46153846153846156,1,1,approach
39,The embedding of a word is computed by summing up the codewords corresponding to all the components in the code as,INTRODUCTION,Each component,machine-translation,9,"['O', 'B', 'I', 'I', 'I', 'O', 'B', 'B', 'I', 'I', 'O', 'B', 'B', 'I', 'B', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'B-b', 'I-b', 'I-b', 'I-b', 'O', 'B-p', 'B-p', 'I-p', 'I-p', 'O', 'B-b', 'B-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",25,0.176056338028169,38,0.13240418118466898,25,0.4807692307692308,1,1,approach
55,"In this work , we utilize such codes fora different purpose , that is , constructing word embeddings with drastically fewer parameters .",INTRODUCTION,Each component,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",41,0.2887323943661972,54,0.18815331010452963,41,0.7884615384615384,1,1,research-problem
59,We utilize the Gumbel - softmax trick to find the best discrete codes that minimize the loss .,INTRODUCTION,Each component,machine-translation,9,"['O', 'B', 'O', 'B', 'I', 'I', 'I', 'B', 'I', 'O', 'B', 'I', 'I', 'O', 'B', 'O', 'B', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'O']","['O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'O', 'B-b', 'I-b', 'I-b', 'O', 'B-p', 'O', 'B-ob', 'O']",45,0.31690140845070425,58,0.20209059233449478,45,0.8653846153846154,1,1,approach
171,"In each iteration , a small batch of the embeddings is sampled uniformly from the baseline embedding matrix .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'O', 'B', 'I', 'B', 'O', 'B', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'O', 'B-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'O']",14,0.12962962962962962,170,0.5923344947735192,6,0.5,1,1,experiments
173,"In our experiments , the batch size is set to 128 .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'B', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'O', 'O', 'B-n', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'O', 'O', 'B-ob', 'O']",16,0.14814814814814814,172,0.5993031358885017,8,0.6666666666666666,1,1,experiments
174,We use Adam optimizer with a fixed learning rate of 0.0001 .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,"['O', 'B', 'B', 'I', 'B', 'O', 'B', 'I', 'I', 'B', 'B', 'O']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['O', 'B-p', 'B-b', 'I-b', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'O']",17,0.1574074074074074,173,0.6027874564459931,9,0.75,1,1,experiments
175,The training is run for 200K iterations .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,"['O', 'B', 'O', 'O', 'O', 'B', 'I', 'O']","['O', 'B-p', 'O', 'O', 'O', 'B-n', 'I-n', 'O']","['O', 'B-p', 'O', 'O', 'O', 'B-ob', 'I-ob', 'O']",18,0.16666666666666666,174,0.6062717770034843,10,0.8333333333333334,1,1,experiments
177,"We evenly distribute the model training to 4 GPUs using the nccl package , so that one round of code learning takes around 15 minutes to complete .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,"['O', 'O', 'B', 'O', 'B', 'I', 'O', 'B', 'B', 'B', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'B-p', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'O', 'B-ob', 'B-p', 'B-p', 'O', 'B-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.18518518518518517,176,0.6132404181184669,12,1.0,1,1,experiments
186,The model is composed of a single LSTM layer with 150 hidden units and a softmax layer for predicting the binary label .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,"['O', 'O', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'B', 'B', 'I', 'I', 'O', 'O', 'B', 'I', 'B', 'I', 'O', 'B', 'I', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'O', 'O', 'B-b', 'I-b', 'B-p', 'I-p', 'O', 'B-ob', 'I-ob', 'O']",29,0.26851851851851855,185,0.6445993031358885,8,0.25806451612903225,1,1,experiments
187,"For the baseline model , the embedding layer contains a large 75 K 300 embedding matrix initialized by GloVe embeddings .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,"['O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'B', 'I', 'B', 'I', 'O']","['O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['O', 'O', 'B-b', 'I-b', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'B-ob', 'I-ob', 'O']",30,0.2777777777777778,186,0.6480836236933798,9,0.2903225806451613,1,1,experiments
188,"For the compressed models based on the compositional coding , the embedding layer maintains a matrix of basis vectors .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,"['O', 'O', 'B', 'I', 'B', 'I', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'B', 'B', 'I', 'O']","['O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['O', 'O', 'B-b', 'I-b', 'B-p', 'I-p', 'O', 'B-b', 'I-b', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-b', 'B-p', 'B-ob', 'I-ob', 'O']",31,0.28703703703703703,187,0.6515679442508711,10,0.3225806451612903,1,1,experiments
190,The embedding parameters for both models remain fixed during the training .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,"['O', 'B', 'I', 'B', 'O', 'O', 'B', 'I', 'B', 'O', 'B', 'O']",,,33,0.3055555555555556,189,0.6585365853658537,12,0.3870967741935484,1,1,experiments
191,"For the models with network pruning , the sparse embedding matrix is finetuned during training .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,"['O', 'O', 'B', 'B', 'B', 'I', 'O', 'O', 'B', 'I', 'I', 'O', 'B', 'B', 'B', 'O']","['O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-p', 'B-n', 'O']","['O', 'O', 'B-b', 'B-p', 'B-ob', 'I-ob', 'O', 'O', 'B-b', 'I-b', 'I-b', 'O', 'B-p', 'B-p', 'B-ob', 'O']",34,0.3148148148148148,190,0.662020905923345,13,0.41935483870967744,1,1,experiments
193,The models are trained with Adam optimizer for 15 epochs with a fixed learning rate of 0.0001 .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,"['O', 'O', 'O', 'B', 'B', 'B', 'I', 'B', 'B', 'I', 'O', 'O', 'B', 'I', 'I', 'B', 'B', 'O']",,,36,0.3333333333333333,192,0.6689895470383276,15,0.4838709677419355,1,1,experiments
206,"For our proposed methods , the maximum loss - free compression rate is achieved by a 16 32 coding scheme .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'B', 'I', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-ob', 'I-ob', 'O', 'B-p', 'I-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'O']",49,0.4537037037037037,205,0.7142857142857143,28,0.9032258064516129,1,1,experiments
208,We also found the classification accuracy can be substantially improved with a slightly lower compression rate .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,"['O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'B', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'B-b', 'I-b', 'O', 'O', 'B-p', 'I-p', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",51,0.4722222222222222,207,0.7212543554006968,30,0.967741935483871,1,1,experiments
225,The model has a standard bi-directional encoder composed of two LSTM layers similar to .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,"['O', 'O', 'O', 'O', 'B', 'I', 'I', 'B', 'I', 'B', 'I', 'I', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'O', 'O', 'O']",68,0.6296296296296297,224,0.7804878048780488,15,0.35714285714285715,1,1,experiments
226,The decoder contains two LSTM layers .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,"['O', 'B', 'B', 'B', 'B', 'I', 'O']","['O', 'B-n', 'B-p', 'B-n', 'B-p', 'I-p', 'O']","['O', 'B-ob', 'B-p', 'B-b', 'B-p', 'I-p', 'O']",69,0.6388888888888888,225,0.7839721254355401,16,0.38095238095238093,1,1,experiments
227,Residual connection with a scaling factor of 1 / 2 is applied to the two decoder states to compute the outputs .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,"['B', 'I', 'B', 'O', 'B', 'I', 'B', 'B', 'I', 'I', 'O', 'B', 'B', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['B-b', 'I-b', 'B-p', 'O', 'B-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'O', 'B-p', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O']",70,0.6481481481481481,226,0.7874564459930313,17,0.40476190476190477,1,1,experiments
228,All LSTMs and embeddings have 256 hidden units in the IWSLT14 task and 1000 hidden units in ASPEC task .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,"['B', 'I', 'I', 'I', 'O', 'B', 'B', 'I', 'O', 'O', 'B', 'I', 'O', 'B', 'O', 'O', 'O', 'B', 'I', 'O']","['B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'O', 'O', 'B-n', 'I-n', 'O']","['B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-p', 'B-b', 'I-b', 'O', 'O', 'B-ob', 'I-ob', 'O', 'B-p', 'O', 'O', 'O', 'B-ob', 'I-ob', 'O']",71,0.6574074074074074,227,0.7909407665505227,18,0.42857142857142855,1,1,experiments
229,The decoder states are firstly linearly transformed to 600 - dimensional vectors before computing the final softmax .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'B', 'I', 'B', 'I', 'I', 'I', 'I', 'B', 'I', 'O', 'B', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'O', 'B-ob', 'I-ob', 'O']",72,0.6666666666666666,228,0.794425087108014,19,0.4523809523809524,1,1,experiments
230,Dropout with a rate of 0.2 is applied everywhere except the recurrent computation .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,"['B', 'O', 'O', 'B', 'I', 'I', 'O', 'B', 'B', 'B', 'O', 'B', 'I', 'O']","['B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['B-p', 'O', 'O', 'B-b', 'I-b', 'I-b', 'O', 'B-p', 'B-b', 'B-p', 'O', 'B-ob', 'I-ob', 'O']",73,0.6759259259259259,229,0.7979094076655052,20,0.47619047619047616,1,1,experiments
231,"We apply Key - Value Attention to the first decoder , where the query is the sum of the feedback embedding and the previous decoder state and the keys are computed by linear transformation of encoder states .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,"['O', 'B', 'B', 'I', 'I', 'I', 'B', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",74,0.6851851851851852,230,0.8013937282229965,21,0.5,1,1,experiments
233,All models are trained by Nesterov 's accelerated gradient with an initial learning rate of 0.25 .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,"['O', 'O', 'O', 'B', 'I', 'B', 'I', 'I', 'I', 'B', 'O', 'B', 'I', 'I', 'B', 'B', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'O']",76,0.7037037037037037,232,0.8083623693379791,23,0.5476190476190477,1,1,experiments
237,"Similar to the code learning , the training is distributed to 4 GPUs , each GPU computes a mini-batch of 16 samples .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'O', 'B', 'B', 'O', 'O', 'O', 'B', 'O', 'B', 'O', 'B', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'O', 'O', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-b', 'O', 'B-p', 'O', 'B-ob', 'B-p', 'O', 'O', 'O', 'B-b', 'O', 'B-p', 'O', 'B-ob', 'I-ob', 'O']",80,0.7407407407407407,236,0.8222996515679443,27,0.6428571428571429,1,1,experiments
246,The loss - free compression rate reaches 92 % on ASPEC dataset by pruning 90 % of the connections .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,"['O', 'B', 'I', 'I', 'I', 'I', 'B', 'I', 'I', 'B', 'B', 'I', 'B', 'I', 'B', 'I', 'I', 'I', 'I', 'O']","['O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'B-p', 'B-b', 'I-b', 'B-p', 'I-p', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O']",89,0.8240740740740741,245,0.8536585365853658,36,0.8571428571428571,1,1,experiments
247,"However , with the same pruning ratio , a modest performance loss is observed in IWSLT14 dataset .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'B', 'B', 'B', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-p', 'B-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'O', 'B-p', 'B-p', 'B-b', 'I-b', 'O']",90,0.8333333333333334,246,0.8571428571428571,37,0.8809523809523809,1,1,experiments
248,"For the models using compositional coding , the loss - free compression rate is 94 % for the IWSLT14 dataset and 99 % for the ASPEC dataset .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,"['O', 'O', 'O', 'B', 'B', 'I', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'O', 'B', 'I', 'B', 'O', 'B', 'I', 'O', 'B', 'I', 'O', 'O', 'B', 'I', 'O']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O']","['O', 'O', 'O', 'B-p', 'B-b', 'I-b', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'O', 'B-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'O', 'B-b', 'I-b', 'O', 'O', 'B-ob', 'I-ob', 'O']",91,0.8425925925925926,247,0.8606271777003485,38,0.9047619047619048,1,1,experiments
2,"0,000 + Times Accelerated Robust Subset Selection ( ARSS )",title,title,named-entity-recognition,0,"['O', 'O', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'O']",,,1,0.0,1,0.0036900369003690036,1,0.0,1,1,research-problem
4,Subset selection from massive data with noised information is increasingly popular for various applications .,abstract,abstract,named-entity-recognition,0,"['B', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.14285714285714285,3,0.01107011070110701,1,0.14285714285714285,1,1,research-problem
7,"Specifically in the subset selection area , this is the first attempt to employ the p ( 0 < p ? 1 ) - norm based measure for the representation loss , preventing large errors from dominating our objective .",abstract,abstract,named-entity-recognition,0,"['O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.5714285714285714,6,0.02214022140221402,4,0.5714285714285714,1,1,research-problem
48,"In this paper , we propose an accelerated robust subset selection method to highly raise the speed on the one hand , and to boost the robustness on the other .",Introduction,Contributions .,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",37,0.3978494623655914,47,0.17343173431734318,25,0.14124293785310735,1,1,approach
49,"To this end , we use the p ( 0 < p ? 1 ) - norm based robust measure for the representation loss , preventing large errors from dominating our objective .",Introduction,Contributions .,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'B', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",38,0.40860215053763443,48,0.17712177121771217,26,0.14689265536723164,1,1,approach
51,"Then , based on the observation that data size is generally much larger than feature length , i.e. N L , we propose a speedup solver .",Introduction,Contributions .,named-entity-recognition,0,"['O', 'O', 'B', 'I', 'O', 'B', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'O']","['O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O']","['O', 'O', 'B-p', 'I-p', 'O', 'B-b', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-ob', 'I-ob', 'O']",40,0.43010752688172044,50,0.18450184501845018,28,0.15819209039548024,1,1,approach
52,The main acceleration is owing to the Augmented Lagrange Multiplier ( ALM ) and an equivalent derivation .,Introduction,Contributions .,named-entity-recognition,0,"['O', 'B', 'I', 'B', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'B-n', 'I-n', 'B-p', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'B-ob', 'I-ob', 'B-p', 'B-p', 'I-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O']",41,0.44086021505376344,51,0.1881918819188192,29,0.1638418079096045,1,1,approach
203,"All experiments are conducted on a server with 64 - core Intel Xeon E7-4820 @ 2.00 GHz , 18 Mb Cache and 0.986 TB RAM , using Matlab 2012 .",Experiments Experimental Settings,Experiments Experimental Settings,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'B', 'O', 'B', 'I', 'B', 'O', 'B', 'B', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-p', 'B-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'B-p', 'O', 'B-ob', 'I-ob', 'B-p', 'O', 'B-p', 'B-ob', 'I-ob', 'O']",2,0.034482758620689655,202,0.7453874538745388,2,0.25,1,1,results
204,"Brief descriptions often benchmark datasets are summarized in , where ' Total ( N * ) ' denotes the total set of samples in each data .",Experiments Experimental Settings,Experiments Experimental Settings,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.05172413793103448,203,0.7490774907749077,3,0.375,1,1,results
221,"Compared with TED and RRSS Nie , the curve of ARSS is surprisingly lower and highly stable against increasing N ; there is almost no rise of selection time over growing N .",Experiments Experimental Settings,Speed Comparisons,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'B', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'B-p', 'I-p', 'I-p', 'O']",20,0.3448275862068966,220,0.8118081180811808,11,0.4782608695652174,1,1,results
228,"First , ARSS is the fastest algorithm , and RRSS our is the second fastest algorithm .",Experiments Experimental Settings,Speed Comparisons,named-entity-recognition,0,"['O', 'O', 'O', 'B', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'O', 'B-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.46551724137931033,227,0.8376383763837638,18,0.782608695652174,1,1,results
249,"Third , compared with TED , both RRSS and ARSS achieve an appreciable advantage .",Experiments Experimental Settings,Accuracy comparison,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-ob', 'I-ob', 'O']",48,0.8275862068965517,248,0.915129151291513,8,0.4444444444444444,1,1,results
256,"As we shall see , the prediction accuracies generally increase as K increases .",Experiments Experimental Settings,Accuracy comparison,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'O', 'B', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'B-p', 'I-p', 'O']",55,0.9482758620689655,255,0.940959409594096,15,0.8333333333333334,1,1,results
258,"For each sub-figure , ARSS is generally among the best .",Experiments Experimental Settings,Accuracy comparison,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'B', 'O', 'O', 'B', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O']","['O', 'O', 'O', 'O', 'B-ob', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O']",57,0.9827586206896551,257,0.948339483394834,17,0.9444444444444444,1,1,results
2,Neural Architectures for Named Entity Recognition,title,,named-entity-recognition,1,"['B', 'I', 'I', 'I', 'I', 'I']",,,1,0.0,1,0.004830917874396135,1,0.0,1,1,research-problem
4,"State - of - the - art named entity recognition systems rely heavily on hand - crafted features and domain - specific knowledge in order to learn effectively from the small , supervised training corpora that are available .",abstract,abstract,named-entity-recognition,1,"['B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.2,3,0.014492753623188406,1,0.2,1,1,research-problem
7,Our models obtain state - of - the - art performance in NER in four languages without resorting to any language - specific knowledge or resources such as gazetteers .,abstract,abstract,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.8,6,0.028985507246376812,4,0.8,1,1,research-problem
20,"We compare two models here , ( i ) a bidirectional LSTM with a sequential conditional random layer above it ( LSTM - CRF ; 2 ) , and ( ii ) anew model that constructs and labels chunks of input sentences using an algorithm inspired by transition - based parsing with states represented by stack LSTMs ( S - LSTM ; 3 ) .",Introduction,Introduction,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'B', 'I', 'I', 'I', 'B', 'I', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'B', 'I', 'I', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'B', 'I', 'B', 'I', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-b', 'I-b', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'B-p', 'I-p', 'O', 'B-b', 'I-b', 'I-b', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'B-ob', 'I-ob', 'O', 'B-b', 'I-b', 'I-b', 'O', 'O', 'O', 'O']",11,0.6470588235294118,19,0.09178743961352658,11,0.55,1,1,model
22,"To capture orthographic sensitivity , we use character - based word representation model to capture distributional sensitivity , we combine these representations with distributional representations .",Introduction,Introduction,named-entity-recognition,1,"['B', 'I', 'B', 'I', 'O', 'O', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'I', 'O']","['B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O']","['B-p', 'I-p', 'B-b', 'I-b', 'O', 'O', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-ob', 'I-ob', 'O']",13,0.7647058823529411,21,0.10144927536231885,13,0.65,1,1,model
157,"For both models presented , we train our networks using the back - propagation algorithm updating our parameters on every training example , one at a time , using stochastic gradient descent ( SGD ) with a learning rate of 0.01 and a gradient clipping of 5.0 .",Training,Training,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'B', 'O', 'B', 'I', 'I', 'I', 'B', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'B', 'I', 'O', 'B', 'O', 'O', 'B', 'I', 'O', 'B', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-b', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'B-p', 'O', 'B-b', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O', 'O', 'B-p', 'I-p', 'O', 'B-ob', 'O', 'O', 'B-p', 'I-p', 'O', 'B-ob', 'O']",1,0.03333333333333333,156,0.7536231884057971,1,0.08333333333333333,1,1,hyperparameters
160,Our LSTM - CRF model uses a single layer for the forward and backward LSTMs whose dimensions are set to 100 .,Training,Training,named-entity-recognition,1,"['O', 'B', 'I', 'I', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'B', 'O', 'O', 'O', 'B', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'O', 'O', 'B-n', 'O']","['O', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O', 'B-p', 'O', 'O', 'O', 'B-ob', 'O']",4,0.13333333333333333,159,0.7681159420289855,4,0.3333333333333333,1,1,hyperparameters
162,We set the dropout rate to 0.5 .,Training,Training,named-entity-recognition,1,"['O', 'O', 'O', 'B', 'I', 'O', 'B', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-ob', 'O']",6,0.2,161,0.7777777777777778,6,0.5,1,1,hyperparameters
164,The stack - LSTM model uses two layers each of dimension 100 for each stack .,Training,Training,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'B', 'B', 'I', 'O', 'O', 'B', 'B', 'B', 'B', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-b', 'I-b', 'O', 'O', 'B-p', 'B-b', 'B-p', 'B-ob', 'I-ob', 'O']",8,0.26666666666666666,163,0.7874396135265701,8,0.6666666666666666,1,1,hyperparameters
165,"The embeddings of the actions used in the composition functions have 16 dimensions each , and the output embedding is of dimension 20 .",Training,Training,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'B', 'I', 'B', 'B', 'I', 'I', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'B', 'B', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-ob', 'I-ob', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'B-b', 'I-b', 'O', 'O', 'B-p', 'B-ob', 'O']",9,0.3,164,0.7922705314009661,9,0.75,1,1,hyperparameters
181,"Our LSTM - CRF model outperforms all other systems , including the ones using external labeled data like gazetteers .",Training,Data Sets,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'B', 'B', 'I', 'I', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-b', 'I-b', 'I-b', 'O', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",25,0.8333333333333334,180,0.8695652173913043,12,0.7058823529411765,1,1,results
182,"Our Stack - LSTM model also outperforms all previous models that do not incorporate external features , apart from the one presented by .",Training,Data Sets,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'I', 'I', 'O', 'B', 'I', 'I', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-b', 'I-b', 'I-b', 'O', 'B-p', 'I-p', 'I-p', 'B-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.8666666666666667,181,0.8743961352657005,13,0.7647058823529411,1,1,results
183,"and 4 present our results on NER for German , Dutch and Spanish respectively in comparison to other models .",Training,Data Sets,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'I', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",,,27,0.9,182,0.8792270531400966,14,0.8235294117647058,1,1,results
184,"On these three languages , the LSTM - CRF model significantly outperforms all previous methods , including the ones using external labeled data .",Training,Data Sets,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'B', 'I', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'B-b', 'I-b', 'I-b', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",28,0.9333333333333333,183,0.8840579710144928,15,0.8823529411764706,1,1,results
185,"The only exception is Dutch , where the model of can perform better by leveraging the information from other NER datasets .",Training,Data Sets,named-entity-recognition,1,"['O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",29,0.9666666666666667,184,0.8888888888888888,16,0.9411764705882353,1,1,results
186,The Stack - LSTM also consistently presents statethe - art ( or close to ) results compared to systems that do not use external data .,Training,Data Sets,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'B', 'I', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'B-p', 'I-p', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O']",30,1.0,185,0.893719806763285,17,1.0,1,1,results
2,Fast and Accurate Entity Recognition with Iterated Dilated Convolutions,title,,named-entity-recognition,2,"['B', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O']",,,1,0.0,1,0.004694835680751174,1,0.0,1,1,research-problem
7,"This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction .",abstract,abstract,named-entity-recognition,2,"['O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.5714285714285714,6,0.028169014084507043,4,0.5714285714285714,1,1,research-problem
12,"In order to democratize large - scale NLP and information extraction while minimizing our environmental footprint , we require fast , resource - efficient methods for sequence tagging tasks such as part - of - speech tagging and named entity recognition ( NER ) .",Introduction,Introduction,named-entity-recognition,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.029411764705882353,11,0.051643192488262914,1,0.017543859649122806,1,1,research-problem
27,"In response , this paper presents an application of dilated convolutions for sequence labeling ) .",Introduction,Introduction,named-entity-recognition,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'B', 'I', 'B', 'B', 'I', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'O', 'O']",16,0.47058823529411764,26,0.12206572769953052,16,0.2807017543859649,1,1,approach
29,"Like typical CNN layers , dilated convolutions operate on a sliding window of context over the sequence , but unlike conventional convolutions , the context need not be consecutive ; the dilated window skips over every dilation width d inputs .",Introduction,Introduction,named-entity-recognition,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'B', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'O', 'B-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.5294117647058824,28,0.13145539906103287,18,0.3157894736842105,1,1,approach
30,"By stacking layers of dilated convolutions of exponentially increasing dilation width , we can expand the size of the effective input width to cover the entire length of most sequences using only a few layers :",Introduction,Introduction,named-entity-recognition,2,"['O', 'B', 'B', 'B', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-b', 'B-p', 'B-b', 'I-b', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'I-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.5588235294117647,29,0.13615023474178403,19,0.3333333333333333,1,1,approach
33,Our overall iterated dilated CNN architecture ( ID - CNN ) repeatedly applies the same block of dilated convolutions to token - wise representations .,Introduction,Introduction,named-entity-recognition,2,"['O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'B', 'B', 'I', 'I', 'I', 'O']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'O']",22,0.6470588235294118,32,0.15023474178403756,22,0.38596491228070173,1,1,approach
34,This parameter sharing prevents overfitting and also provides opportunities to inject supervision on intermediate activations of the network .,Introduction,Introduction,named-entity-recognition,2,"['O', 'O', 'O', 'B', 'B', 'O', 'O', 'B', 'I', 'I', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'B-p', 'B-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'B-p', 'B-ob', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",23,0.6764705882352942,33,0.15492957746478872,23,0.40350877192982454,1,1,approach
35,"Similar to models that use logits produced by an RNN , the ID - CNN provides two methods for performing prediction : we can predict each token 's label independently , or by running Viterbi inference in a chain structured graphical model .",Introduction,Introduction,named-entity-recognition,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'B', 'B', 'I', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'B', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-b', 'I-b', 'B-p', 'B-b', 'I-b', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",24,0.7058823529411765,34,0.1596244131455399,24,0.42105263157894735,1,1,approach
162,"We compare our ID - CNN against strong LSTM and CNN baselines : a Bi - LSTM with local decoding , and one with CRF decoding ( Bi - LSTM - CRF ) .",Baselines,Baselines,named-entity-recognition,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.02857142857142857,161,0.755868544600939,1,0.03125,1,1,baselines
163,We also compare against a non-dilated CNN architecture with the same number of convolutional layers as our dilated network ( 4 - layer CNN ) and one with enough layers to incorporate an effective input width of the same size as that of the dilated network ( 5 - layer CNN ) to demonstrate that the dilated convolutions more effectively aggregate contextual information than simple convolutions ( i.e. using fewer parameters ) .,Baselines,Baselines,named-entity-recognition,2,"['O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.05714285714285714,162,0.7605633802816901,2,0.0625,1,1,baselines
171,"The Viterbi - decoding Bi - LSTM - CRF and ID - CNN - CRF and greedy ID - CNN obtain the highest average scores , with the ID - CNN - CRF outperforming the Bi - LSTM - CRF by 0.11 points of F1 on average , and the Bi - LSTM - CRF out - performing the greedy ID - CNN by 0.11 as well .",Baselines,Baselines,named-entity-recognition,2,"['O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'B', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.2857142857142857,170,0.7981220657276995,10,0.3125,1,1,results
172,"Our greedy ID - CNN outperforms the Bi - LSTM and the 4 - layer CNN , which uses the same number of parameters as the ID - CNN , and performs similarly to the 5 - layer CNN which uses more parameters but covers the same effective input width .",Baselines,Baselines,named-entity-recognition,2,"['O', 'B', 'I', 'I', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.3142857142857143,171,0.8028169014084507,11,0.34375,1,1,results
174,"When paired with Viterbi decoding , our ID - CNN performs on par with the Bi - LSTM , showing that the ID - CNN is also an effective token encoder for structured inference .",Baselines,Baselines,named-entity-recognition,2,"['B', 'I', 'I', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'O', 'B', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'B', 'B', 'I', 'O']","['B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['B-p', 'I-p', 'I-p', 'B-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-b', 'I-b', 'I-b', 'O', 'B-p', 'O', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'O']",13,0.37142857142857144,173,0.812206572769953,13,0.40625,1,1,results
186,Document - level prediction,Baselines,Baselines,named-entity-recognition,2,"['B', 'I', 'I', 'I']","['B-n', 'I-n', 'I-n', 'I-n']","['B-b', 'I-b', 'I-b', 'I-b']",25,0.7142857142857143,185,0.8685446009389671,25,0.78125,1,1,results
187,In we show that adding document - level context improves every model on CoNLL - 2003 .,Baselines,Baselines,named-entity-recognition,2,"['O', 'O', 'O', 'O', 'B', 'B', 'I', 'I', 'I', 'B', 'B', 'I', 'O', 'B', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'O', 'B-b', 'I-b', 'I-b', 'O']",26,0.7428571428571429,186,0.8732394366197183,26,0.8125,1,1,results
200,"Our greedy model is out - performed by the Bi - LSTM - CRF reported in Chiu and Nichols ( 2016 ) as well as our own re-implementation , which appears to be the new state - of - the - art on this dataset .",Model,Model,named-entity-recognition,2,"['O', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-b', 'I-b', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.25,199,0.9342723004694836,3,0.25,1,1,results
2,Semi-supervised sequence tagging with bidirectional language models,title,,named-entity-recognition,3,"['B', 'I', 'I', 'O', 'O', 'O', 'O']",,,1,0.0,1,0.005405405405405406,1,0.0,1,1,research-problem
6,"In this paper , we demonstrate a general semi-supervised approach for adding pretrained context embeddings from bidirectional language models to NLP systems and apply it to sequence labeling tasks .",abstract,abstract,named-entity-recognition,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.75,5,0.02702702702702703,3,0.75,1,1,research-problem
16,"In this paper , we explore an alternate semisupervised approach which does not require additional labeled data .",Introduction,Introduction,named-entity-recognition,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'B', 'I', 'B', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-b', 'I-b', 'O', 'O', 'B-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'O']",8,0.4444444444444444,15,0.08108108108108109,8,0.4444444444444444,1,1,approach
17,"We use a neural language model ( LM ) , pre-trained on a large , unlabeled corpus to compute an encoding of the context at each position in the sequence ( hereafter an LM embedding ) and use it in the supervised sequence tagging model .",Introduction,Introduction,named-entity-recognition,3,"['O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'B', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O', 'B-p', 'I-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O', 'O', 'O', 'B-ob', 'I-ob', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",9,0.5,16,0.08648648648648649,9,0.5,1,1,approach
80,We use two bidirectional GRUs with 80 hidden units and 25 dimensional character embeddings for the token character encoder .,Experiments,CoNLL 2003 NER .,named-entity-recognition,3,"['O', 'O', 'B', 'I', 'I', 'B', 'B', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'B', 'O', 'B', 'I', 'I', 'O']","['O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['O', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'O']",10,0.5263157894736842,79,0.42702702702702705,10,0.23809523809523808,1,1,tasks
81,The sequence layer uses two bidirectional GRUs with 300 hidden units each .,Experiments,CoNLL 2003 NER .,named-entity-recognition,3,"['O', 'B', 'I', 'B', 'O', 'B', 'I', 'B', 'B', 'I', 'I', 'I', 'O']","['O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'B-b', 'I-b', 'B-p', 'O', 'B-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",11,0.5789473684210527,80,0.43243243243243246,11,0.2619047619047619,1,1,tasks
82,"For regularization , we add 25 % dropout to the input of each GRU , but not to the recurrent connections .",Experiments,CoNLL 2003 NER .,named-entity-recognition,3,"['O', 'B', 'O', 'O', 'B', 'B', 'I', 'I', 'B', 'I', 'I', 'I', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-b', 'O', 'O', 'B-p', 'B-b', 'I-b', 'I-b', 'B-p', 'I-p', 'I-p', 'I-p', 'B-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.631578947368421,81,0.43783783783783786,12,0.2857142857142857,1,1,tasks
87,The baseline sequence tagger uses 30 dimensional character embeddings and a CNN with 30 filters of width 3 characters followed by a tanh non-linearity for the token character encoder .,Experiments,CoNLL 2000 chunking .,named-entity-recognition,3,"['O', 'B', 'I', 'I', 'B', 'B', 'I', 'I', 'I', 'O', 'O', 'B', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'B-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'B-b', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",17,0.8947368421052632,86,0.4648648648648649,17,0.40476190476190477,1,1,tasks
88,The sequence layer uses two bidirectional LSTMs with 200 hidden units .,Experiments,CoNLL 2000 chunking .,named-entity-recognition,3,"['O', 'B', 'I', 'B', 'B', 'I', 'I', 'O', 'B', 'B', 'I', 'O']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'B-p', 'I-p', 'O']","['O', 'B-b', 'I-b', 'B-p', 'B-b', 'I-b', 'I-b', 'O', 'B-ob', 'B-p', 'I-p', 'O']",18,0.9473684210526315,87,0.4702702702702703,18,0.42857142857142855,1,1,tasks
89,"Following we added 50 % dropout to the character embeddings , the input to each LSTM layer ( but not recurrent connections ) and to the output of the final LSTM layer .",Experiments,CoNLL 2000 chunking .,named-entity-recognition,3,"['O', 'O', 'O', 'B', 'I', 'I', 'B', 'O', 'B', 'I', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",19,1.0,88,0.4756756756756757,19,0.4523809523809524,1,1,tasks
104,"All experiments use the Adam optimizer ( Kingma and Ba , 2015 ) with gradient norms clipped at 5.0 .",Pre-trained language models .,Pre-trained language models .,named-entity-recognition,3,"['O', 'O', 'B', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'I', 'B', 'I', 'B', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O']","['O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-b', 'I-b', 'B-p', 'I-p', 'B-ob', 'O']",14,0.6363636363636364,103,0.5567567567567567,34,0.8095238095238095,1,1,hyperparameters
105,"In all experiments , we fine tune the pre-trained Senna word embeddings but fix all weights in the pre-trained language models .",Pre-trained language models .,Pre-trained language models .,named-entity-recognition,3,"['O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'O', 'B', 'B', 'I', 'B', 'O', 'B', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'B-p', 'B-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'O']",15,0.6818181818181818,104,0.5621621621621622,35,0.8333333333333334,1,1,hyperparameters
106,"In addition to explicit dropout regularization , we also use early stopping to prevent over-fitting and use the following process to determine when to stop training .",Pre-trained language models .,Pre-trained language models .,named-entity-recognition,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'B', 'I', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-b', 'I-b', 'B-p', 'I-p', 'B-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.7272727272727273,105,0.5675675675675675,36,0.8571428571428571,1,1,hyperparameters
107,We first train with a constant learning rate ? = 0.001 on the training data and monitor the development set performance at each epoch .,Pre-trained language models .,Pre-trained language models .,named-entity-recognition,3,"['O', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'I-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.7727272727272727,106,0.572972972972973,37,0.8809523809523809,1,1,hyperparameters
118,"In the CoNLL 2003 NER task , our model scores 91.93 mean F 1 , which is a statistically significant increase over the previous best result of 91.62 0.33 from that used gazetteers ( at 95 % , two - sided Welch t- test , p = 0.021 ) .",Overall system results,Overall system results,named-entity-recognition,3,"['O', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'B', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'O', 'O', 'O', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.13157894736842105,117,0.6324324324324324,3,0.2727272727272727,1,1,results
119,"In the CoNLL 2000 Chunking task , Tag LM achieves 96.37 mean F 1 , exceeding all previously published results without additional labeled data by more then 1 % absolute F 1 .",Overall system results,Overall system results,named-entity-recognition,3,"['O', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'B', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'O', 'O', 'O', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.15789473684210525,118,0.6378378378378379,4,0.36363636363636365,1,1,results
2,Deep contextualized word representations,title,,named-entity-recognition,4,"['B', 'I', 'I', 'I']",,,1,0.0,1,0.003676470588235294,1,0.0,1,1,research-problem
13,Our representations differ from traditional word type embeddings in that each token is assigned a representation that is a function of the entire input sentence .,Introduction,Introduction,named-entity-recognition,4,"['O', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'B', 'I', 'O', 'B', 'O', 'B', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-p', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'B-b', 'I-b', 'O', 'B-p', 'O', 'B-p', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",5,0.1282051282051282,12,0.04411764705882353,5,0.13157894736842105,1,1,approach
14,We use vectors derived from a bidirectional LSTM that is trained with a coupled lan - guage model ( LM ) objective on a large text corpus .,Introduction,Introduction,named-entity-recognition,4,"['O', 'B', 'B', 'B', 'I', 'O', 'B', 'I', 'O', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'B', 'O', 'B', 'I', 'I', 'O']","['O', 'B-p', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['O', 'B-p', 'B-b', 'B-p', 'I-p', 'O', 'B-ob', 'I-ob', 'O', 'O', 'B-p', 'I-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'O']",6,0.15384615384615385,13,0.04779411764705882,6,0.15789473684210525,1,1,approach
15,"For this reason , we call them ELMo ( Embeddings from Language Models ) representations .",Introduction,Introduction,named-entity-recognition,4,"['O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.1794871794871795,14,0.051470588235294115,7,0.18421052631578946,1,1,approach
17,"More specifically , we learn a linear combination of the vectors stacked above each input word for each end task , which markedly improves performance over just using the top LSTM layer .",Introduction,Introduction,named-entity-recognition,4,"['O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'B', 'I', 'B', 'I', 'I', 'B', 'B', 'I', 'I', 'O', 'O', 'B', 'I', 'B', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'B-b', 'I-b', 'I-b', 'B-p', 'B-b', 'I-b', 'I-b', 'O', 'O', 'B-p', 'I-p', 'B-b', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",9,0.23076923076923078,16,0.058823529411764705,9,0.23684210526315788,1,1,approach
109,The Stanford Natural Language Inference ( SNLI ) corpus provides approximately 550K hypothesis / premise pairs .,Evaluation,Question Textual entailment,named-entity-recognition,4,"['O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.02608695652173913,108,0.39705882352941174,3,0.21428571428571427,1,1,tasks
111,"Overall , adding ELMo to the ESIM model improves accuracy by an average of 0.7 % across five random seeds .",Evaluation,Question Textual entailment,named-entity-recognition,4,"['O', 'O', 'B', 'B', 'B', 'O', 'B', 'I', 'B', 'B', 'B', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'I', 'I', 'O']","['O', 'O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['O', 'O', 'B-p', 'B-b', 'B-p', 'O', 'B-ob', 'I-ob', 'B-p', 'B-b', 'B-p', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'O']",5,0.043478260869565216,110,0.40441176470588236,5,0.35714285714285715,1,1,tasks
120,"In our experiments with the OntoNotes coreference annotations from the CoNLL 2012 shared task , adding ELMo improved the average F 1 by 3.2 % from 67.2 to 70.4 , establishing anew state of the art , again improving over the previous best ensemble result by 1.6 % F 1 .",Evaluation,He et al .,named-entity-recognition,4,"['O', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'O', 'B', 'B', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'B-p', 'B-ob', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.12173913043478261,119,0.4375,14,1.0,1,1,tasks
2,Sentence - State LSTM for Text Representation,title,,named-entity-recognition,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0,1,0.004784688995215311,1,0.0,1,1,research-problem
7,"We investigate an alternative LSTM structure for encoding text , which consists of a parallel state for each word .",abstract,Bi-directional,named-entity-recognition,5,"['O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.6666666666666666,6,0.028708133971291867,4,0.6666666666666666,1,1,research-problem
18,We investigate an alternative recurrent neural network structure for addressing these issues .,Introduction,Introduction,named-entity-recognition,5,"['O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O']",8,0.38095238095238093,17,0.08133971291866028,8,0.38095238095238093,1,1,research-problem
19,"As shown in , the main idea is to model the hidden states of all words simultaneously at each recurrent step , rather than one word at a time .",Introduction,Introduction,named-entity-recognition,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'B', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.42857142857142855,18,0.0861244019138756,9,0.42857142857142855,1,1,model
20,"In particular , we view the whole sentence as a single state , which consists of sub-states for individual words and an overall sentence - level state .",Introduction,Introduction,named-entity-recognition,5,"['O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'B', 'O', 'B', 'I', 'O', 'O', 'B', 'I', 'B', 'B', 'B', 'I', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'B-p', 'O', 'B-b', 'I-b', 'O', 'O', 'B-p', 'I-p', 'B-b', 'B-p', 'B-ob', 'I-ob', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",10,0.47619047619047616,19,0.09090909090909091,10,0.47619047619047616,1,1,model
21,"To capture local and non-local contexts , states are updated recurrently by exchanging information between each other .",Introduction,Introduction,named-entity-recognition,5,"['B', 'I', 'B', 'I', 'I', 'I', 'O', 'B', 'O', 'B', 'I', 'B', 'B', 'I', 'I', 'I', 'I', 'O']","['B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'B-p', 'I-p', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['B-p', 'I-p', 'B-b', 'I-b', 'I-b', 'I-b', 'O', 'B-b', 'O', 'B-p', 'I-p', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",11,0.5238095238095238,20,0.09569377990430622,11,0.5238095238095238,1,1,model
25,"At each recurrent step , information exchange is conducted between consecutive words in the sentence , and between the sentence - level state and each word .",Introduction,Introduction,named-entity-recognition,5,"['O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'B', 'I', 'B', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-b', 'I-b', 'O', 'B-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.7142857142857143,24,0.11483253588516747,15,0.7142857142857143,1,1,model
26,"In particular , each word receives information from its predecessor and successor simultaneously .",Introduction,Introduction,named-entity-recognition,5,"['O', 'O', 'O', 'B', 'I', 'B', 'B', 'B', 'O', 'B', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'B-b', 'I-b', 'B-p', 'B-b', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",16,0.7619047619047619,25,0.11961722488038277,16,0.7619047619047619,1,1,model
31,"We release our code and models at https://github.com/ leuchine /S - LSTM , which include all baselines and the final model .",Introduction,Introduction,named-entity-recognition,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,1.0,30,0.14354066985645933,21,1.0,1,1,code
148,All experiments are conducted using a GeForce GTX 1080 GPU with 8 GB memory .,Experiments,Experiments,named-entity-recognition,5,"['O', 'B', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'B', 'B', 'I', 'I', 'O']","['O', 'B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['O', 'B-b', 'O', 'B-p', 'I-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'O']",2,1.0,147,0.7033492822966507,2,1.0,1,1,experimental-setup
165,Final Results for Classification,,,named-entity-recognition,5,"['O', 'O', 'B', 'B']","['O', 'O', 'B-p', 'B-n']","['O', 'O', 'B-p', 'B-b']",0,0.0,164,0.784688995215311,0,0.0,1,1,results
170,"As shown in , the final results on the movie review dataset are consistent with the development results , where S - LSTM outperforms BiL - STM significantly , with a faster speed .",Final Results for Classification,Final Results for Classification,named-entity-recognition,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'B', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'B', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-b', 'I-b', 'I-b', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'B-b', 'I-b', 'I-b', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'O']",5,0.13157894736842105,169,0.8086124401913876,5,0.4166666666666667,1,1,results
173,"As shown in , among the 16 datasets of , S - LSTM gives the best results on 12 , compared with BiLSTM and 2 layered BiL - STM models .",Final Results for Classification,Final Results for Classification,named-entity-recognition,5,"['O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'B', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'B-p', 'B-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.21052631578947367,172,0.8229665071770335,8,0.6666666666666666,1,1,results
174,"The average accuracy of S - LSTM is 85.6 % , significantly higher compared with 84.9 % by 2 - layer stacked BiLSTM .",Final Results for Classification,Final Results for Classification,named-entity-recognition,5,"['O', 'B', 'I', 'B', 'B', 'I', 'I', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'I-p', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'I-p', 'B-p', 'B-b', 'I-b', 'I-b', 'O', 'B-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.23684210526315788,173,0.8277511961722488,9,0.75,1,1,results
187,"For NER , S - LSTM gives an F1 - score of 91.57 % on the CoNLL test set , which is significantly better compared with BiLSTMs .",Final Results for Classification,Bi-directional,named-entity-recognition,5,"['B', 'B', 'O', 'B', 'I', 'I', 'B', 'O', 'B', 'I', 'I', 'O', 'B', 'I', 'B', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-b', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-ob', 'I-ob', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.5789473684210527,186,0.8899521531100478,9,0.6,1,1,results
2,Robust Lexical Features for Improved Neural Network Named - Entity Recognition,title,title,named-entity-recognition,6,"['O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I']",,,1,0.0,1,0.0047169811320754715,1,0.0,1,1,research-problem
4,Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features .,abstract,abstract,named-entity-recognition,6,"['B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.1111111111111111,3,0.014150943396226415,1,0.1111111111111111,1,1,research-problem
14,Named - Entity Recognition ( NER ) is the task of identifying textual mentions and classifying them into a predefined set of types .,Introduction,Introduction,named-entity-recognition,6,"['B', 'I', 'I', 'I', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'B-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.02631578947368421,13,0.06132075471698113,1,0.05555555555555555,1,1,research-problem
24,"In this work , we discuss some of the limitations of gazetteer features and propose an alternative lexical representation which is trained offline and that can be added to any neural NER system .",Introduction,Introduction,named-entity-recognition,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'O', 'O', 'B', 'B', 'O', 'O', 'O', 'O', 'B', 'I', 'B', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'O', 'O', 'B-p', 'B-ob', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",11,0.2894736842105263,23,0.10849056603773585,11,0.6111111111111112,1,1,model
25,"Ina nutshell , we embed words and entity types into a joint vector space by leveraging WiFiNE , a ressource which automatically annotates mentions in Wikipedia with 120 entity types .",Introduction,Introduction,named-entity-recognition,6,"['O', 'O', 'O', 'O', 'B', 'B', 'I', 'I', 'I', 'B', 'O', 'B', 'I', 'I', 'B', 'I', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'I-p', 'B-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.3157894736842105,24,0.11320754716981132,12,0.6666666666666666,1,1,model
139,Training is carried out by mini-batch stochastic gradient descent ( SGD ) with a momentum of 0.9 and a gradient clipping of 5.0 .,Training and Implementation,Training and Implementation,named-entity-recognition,6,"['B', 'O', 'B', 'I', 'I', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'B', 'O', 'B', 'I', 'I', 'O', 'O', 'B', 'I', 'I', 'I', 'O']","['B-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['B-b', 'O', 'B-p', 'I-p', 'I-p', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",1,0.03125,138,0.6509433962264151,1,0.03333333333333333,1,1,experimental-setup
140,"The mini-batch is 10 for both datasets , and learning rates are 0.009 and 0.013 for CONLL and ONTONOTES respectively .",Training and Implementation,Training and Implementation,named-entity-recognition,6,"['O', 'B', 'O', 'B', 'B', 'B', 'I', 'O', 'O', 'B', 'I', 'O', 'B', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-ob', 'B-p', 'B-b', 'I-b', 'O', 'O', 'B-p', 'I-p', 'O', 'B-ob', 'O', 'B-ob', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.0625,139,0.6556603773584906,2,0.06666666666666667,1,1,experimental-setup
147,"We varied dropout . 65 ] ) , hidden units ) , capitalization ) and char ) embedding dimensions , learning rate ( [ 0.001 , 0.015 ] by step 0.002 ) , and optimization algorithms and fixed the other hyper - parameters .",Training and Implementation,Training and Implementation,named-entity-recognition,6,"['O', 'B', 'B', 'I', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'B-ob', 'I-ob', 'I-ob', 'O', 'B-ob', 'I-ob', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.28125,146,0.6886792452830188,9,0.3,1,1,experimental-setup
148,"We implemented our system using the Tensorflow library , and ran our models on a GeForce GTX TITAN Xp GPU .",Training and Implementation,Training and Implementation,named-entity-recognition,6,"['O', 'B', 'O', 'B', 'B', 'O', 'B', 'I', 'O', 'O', 'B', 'O', 'B', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'O']","['O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'B-p', 'O', 'B-b', 'B-p', 'O', 'B-ob', 'I-ob', 'O', 'O', 'B-p', 'O', 'B-b', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",10,0.3125,147,0.6933962264150944,10,0.3333333333333333,1,1,experimental-setup
155,"First , we observe that our model significantly outperforms models that use extensive sets of handcrafted features ) as well as the system of Standard deviation on the test set is reported in 2015 ) that uses NE and Entity Linking annotations to jointly optimize the performance on both tasks .",Training and Implementation,Training and Implementation,named-entity-recognition,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'B', 'O', 'B', 'B', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-b', 'O', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.53125,154,0.7264150943396226,17,0.5666666666666667,1,1,results
156,"Second , our model outperforms as well other NN models that only use standard word embeddings , which indicates that our lexical feature vector is complementary to standard word embeddings .",Training and Implementation,Training and Implementation,named-entity-recognition,6,"['O', 'O', 'O', 'O', 'B', 'O', 'O', 'B', 'I', 'I', 'B', 'I', 'I', 'B', 'I', 'I', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'I-p', 'I-p', 'B-b', 'I-b', 'I-b', 'O', 'O', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",18,0.5625,155,0.7311320754716981,18,0.6,1,1,results
157,"Third , our system matches state - of - the - art performances of models that use either more complex architectures or more elaborate features .",Training and Implementation,Training and Implementation,named-entity-recognition,6,"['O', 'O', 'O', 'O', 'B', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'I', 'O', 'B', 'I', 'I', 'O', 'B', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'B-b', 'B-p', 'I-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'O', 'B-ob', 'I-ob', 'I-ob', 'O']",19,0.59375,156,0.7358490566037735,19,0.6333333333333333,1,1,results
167,"In particular , our system significantly outperforms the Bi - LSTM - CNN - CRF models of ( Chiu and Nichols , 2016 ) and by an absolute gain of 1.68 and 0.96 points respectively .",Training and Implementation,Training and Implementation,named-entity-recognition,6,"['O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'B', 'B', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'O']",,,29,0.90625,166,0.7830188679245284,29,0.9666666666666667,1,1,results
168,"Less surprisingly , it surpasses systems with hand - crafted features , including that use gazetteers , and the system of which uses coreference annotation in ONTONOTES to jointly model NER , entity linking , and coreference resolution tasks .",Training and Implementation,Training and Implementation,named-entity-recognition,6,"['O', 'O', 'O', 'O', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'O', 'B', 'O', 'B', 'I', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'B', 'B', 'B', 'I', 'I', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O', 'B-p', 'O', 'B-ob', 'I-ob', 'O', 'O', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'B-b', 'B-p', 'I-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",30,0.9375,167,0.7877358490566038,30,1.0,1,1,results
174,We also observe that models that use both feature sets significantly outperform other configurations .,Model,Model,named-entity-recognition,6,"['O', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'B', 'I', 'B', 'I', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'B-ob', 'I-ob', 'O']",1,0.25,173,0.8160377358490566,1,0.25,1,1,ablation-analysis
183,"We observe that on both CONLL and ONTONOTES , the SSKIP model outperforms our feature vector approach by 0.65 F1 points on average .",Ablation Results,Ablation Results,named-entity-recognition,6,"['O', 'B', 'O', 'B', 'B', 'I', 'I', 'I', 'O', 'O', 'B', 'I', 'B', 'B', 'I', 'I', 'I', 'B', 'B', 'I', 'I', 'I', 'I', 'O']","['O', 'B-p', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'B-p', 'O', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'O', 'O', 'B-b', 'I-b', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",5,0.625,182,0.8584905660377359,5,0.625,1,1,ablation-analysis
2,A Neural Transition - based Model for Nested Mention Recognition,title,title,named-entity-recognition,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I']",,,1,0.0,1,0.006329113924050633,1,0.0,1,1,research-problem
5,This paper introduces a scalable transition - based method to model the nested structure of mentions .,abstract,abstract,named-entity-recognition,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",2,0.2857142857142857,4,0.02531645569620253,2,0.2857142857142857,1,1,research-problem
12,"There has been an increasing interest in named entity recognition or more generally recognizing entity mentions 2 ) that the nested hierarchical structure of entity mentions should betaken into account to better facilitate downstream tasks like question answering , relation extraction , event extraction , and coreference resolution .",Introduction,Introduction,named-entity-recognition,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.058823529411764705,11,0.06962025316455696,1,0.058823529411764705,1,1,research-problem
17,"To achieve a scalable and effective solution for recognizing nested mentions , we design a transition - based system which is inspired by the recent success of employing transition - based methods for constituent parsing ) and named entity recognition , especially when they are paired with neural networks .",Introduction,Introduction,named-entity-recognition,7,"['B', 'I', 'O', 'B', 'I', 'I', 'I', 'B', 'I', 'B', 'I', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'I-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'B-ob', 'I-ob', 'O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.35294117647058826,16,0.10126582278481013,6,0.35294117647058826,1,1,model
18,"Generally , each sentence with nested mentions is mapped to a forest where each outermost mention forms a tree consisting of its inner mentions .",Introduction,Introduction,named-entity-recognition,7,"['O', 'O', 'B', 'I', 'I', 'I', 'I', 'O', 'B', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O', 'B-p', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",7,0.4117647058823529,17,0.10759493670886076,7,0.4117647058823529,1,1,model
19,Then our transition - based system learns to construct this forest through a sequence of shift - reduce actions .,Introduction,Introduction,named-entity-recognition,7,"['O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'B', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",8,0.47058823529411764,18,0.11392405063291139,8,0.47058823529411764,1,1,model
23,"Following , we employ Stack - LSTM to represent the system 's state , which consists of the states of input , stack and action history , in a continuous space incrementally .",Introduction,Introduction,named-entity-recognition,7,"['O', 'O', 'O', 'B', 'B', 'I', 'I', 'B', 'I', 'O', 'B', 'I', 'I', 'O', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'B', 'O', 'B', 'I', 'I', 'O']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'B-p', 'B-b', 'I-b', 'I-b', 'B-p', 'I-p', 'O', 'B-b', 'I-b', 'I-b', 'O', 'O', 'B-p', 'I-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'O']",12,0.7058823529411765,22,0.13924050632911392,12,0.7058823529411765,1,1,model
24,The ( partially ) processed nested mentions in the stack are encoded with recursive neural networks where composition functions are used to capture dependencies between nested mentions .,Introduction,Introduction,named-entity-recognition,7,"['O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'B', 'I', 'B', 'I', 'I', 'B', 'B', 'I', 'B', 'I', 'I', 'I', 'B', 'B', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'B-p', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-b', 'I-b', 'I-b', 'B-p', 'B-b', 'I-b', 'B-p', 'I-p', 'I-p', 'I-p', 'B-b', 'B-p', 'O', 'O', 'O']",13,0.7647058823529411,23,0.14556962025316456,13,0.7647058823529411,1,1,model
25,"Based on the observation that letter - level patterns such as capitalization and prefix can be beneficial in detecting mentions , we incorporate a characterlevel LSTM to capture such morphological information .",Introduction,Introduction,named-entity-recognition,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'B', 'I', 'O', 'B', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'B-p', 'I-p', 'O', 'B-ob', 'I-ob', 'O']",14,0.8235294117647058,24,0.1518987341772152,14,0.8235294117647058,1,1,model
26,"Meanwhile , this character - level component can also help deal with the out - of - vocabulary problem of neural models .",Introduction,Introduction,named-entity-recognition,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'B', 'I', 'B', 'I', 'I', 'I', 'O', 'B', 'I', 'O']",,,15,0.8823529411764706,25,0.15822784810126583,15,0.8823529411764706,1,1,model
123,Pre-trained embeddings,Experiments,Setup,named-entity-recognition,7,"['B', 'I']","['B-p', 'I-p']","['B-p', 'I-p']",6,0.46153846153846156,122,0.7721518987341772,1,0.125,1,1,hyperparameters
124,Glo Ve of dimension 100 are used to initialize the word vectors for all three datasets .,Experiments,Setup,named-entity-recognition,7,"['B', 'I', 'B', 'B', 'I', 'O', 'B', 'I', 'I', 'O', 'B', 'I', 'B', 'B', 'I', 'I', 'O']","['B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['B-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'O']",7,0.5384615384615384,123,0.7784810126582279,2,0.25,1,1,hyperparameters
125,9 The embeddings of POS tags are initialized randomly with dimension 32 .,Experiments,Setup,named-entity-recognition,7,"['O', 'O', 'B', 'I', 'I', 'I', 'O', 'B', 'I', 'B', 'B', 'I', 'O']","['O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'O']",8,0.6153846153846154,124,0.7848101265822784,3,0.375,1,1,hyperparameters
126,The model is trained using Adam and a gradient clipping of 3.0 .,Experiments,Setup,named-entity-recognition,7,"['O', 'O', 'O', 'B', 'I', 'B', 'O', 'O', 'B', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'B-ob', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",9,0.6923076923076923,125,0.7911392405063291,4,0.5,1,1,hyperparameters
133,Our neural transition - based model achieves the best results in ACE datasets and comparable results in GENIA dataset in terms of F 1 measure .,Results,Results,named-entity-recognition,7,"['O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'B', 'B', 'I', 'O', 'B', 'I', 'O', 'B', 'I', 'B', 'I', 'I', 'B', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-ob', 'I-ob', 'B-p', 'B-b', 'I-b', 'O', 'B-ob', 'I-ob', 'O', 'B-b', 'I-b', 'B-p', 'I-p', 'I-p', 'B-b', 'I-b', 'I-b', 'O']",2,0.13333333333333333,132,0.8354430379746836,2,0.5,1,1,results
139,"We can observe that the margin of improvement is more significant in the portion of nested mentions , revealing our model 's effectiveness in handling nested mentions .",Results,Handling Nested Mentions,named-entity-recognition,7,"['O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'B', 'I', 'B', 'O', 'B', 'I', 'B', 'I', 'O', 'B', 'O', 'B', 'I', 'I', 'B', 'I', 'O', 'O', 'O']",,,8,0.5333333333333333,138,0.8734177215189873,3,0.6,1,1,results
148,"To evaluate the contribution of neural components including pre-trained embeddings , the characterlevel LSTM and dropout layers , we test the performances of ablated models .",Ablation Study,Ablation Study,named-entity-recognition,7,"['O', 'B', 'O', 'B', 'I', 'I', 'I', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.3333333333333333,147,0.930379746835443,1,0.3333333333333333,1,1,ablation-analysis
150,"From the performance gap , we can conclude that these components contribute significantly to the effectiveness of our model in all three datasets .",Ablation Study,Ablation Study,named-entity-recognition,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'O', 'B-ob', 'I-ob', 'I-ob', 'O']",3,1.0,149,0.9430379746835443,3,1.0,1,1,ablation-analysis
2,BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding,title,title,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I']",,,1,0.0,1,0.002583979328165375,1,0.0,1,1,research-problem
4,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers .",abstract,abstract,named-entity-recognition,8,"['O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.1111111111111111,3,0.007751937984496124,1,0.1111111111111111,1,1,research-problem
14,Language model pre-training has been shown to be effective for improving many natural language processing tasks .,Introduction,Introduction,named-entity-recognition,8,"['B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.041666666666666664,13,0.03359173126614987,1,0.041666666666666664,1,1,research-problem
24,"In this paper , we improve the fine - tuning based approaches by proposing BERT : Bidirectional Encoder Representations from Transformers .",Introduction,Introduction,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",11,0.4583333333333333,23,0.059431524547803614,11,0.4583333333333333,1,1,approach
25,"BERT alleviates the previously mentioned unidirectionality constraint by using a "" masked language model "" ( MLM ) pre-training objective , inspired by the Cloze task .",Introduction,Introduction,named-entity-recognition,8,"['B', 'B', 'O', 'O', 'O', 'B', 'I', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",,,12,0.5,24,0.06201550387596899,12,0.5,1,1,approach
26,"The masked language model randomly masks some of the tokens from the input , and the objective is to predict the original vocabulary id of the masked word based only on its context .",Introduction,Introduction,named-entity-recognition,8,"['O', 'B', 'I', 'I', 'B', 'I', 'B', 'B', 'I', 'I', 'B', 'O', 'B', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'B', 'I', 'I', 'O', 'B', 'O']",,,13,0.5416666666666666,25,0.06459948320413436,13,0.5416666666666666,1,1,approach
27,"Unlike left - toright language model pre-training , the MLM objective enables the representation to fuse the left and the right context , which allows us to pretrain a deep bidirectional Transformer .",Introduction,Introduction,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'B', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-b', 'I-b', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'O']",14,0.5833333333333334,26,0.06718346253229975,14,0.5833333333333334,1,1,approach
28,"In addition to the masked language model , we also use a "" next sentence prediction "" task that jointly pretrains text - pair representations .",Introduction,Introduction,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O']",,,15,0.625,27,0.06976744186046512,15,0.625,1,1,approach
156,"The General Language Understanding Evaluation ( GLUE ) benchmark ( Wang et al. , 2018 a ) is a collection of diverse natural language understanding tasks .",Experiments,GLUE,named-entity-recognition,8,"['O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.04285714285714286,155,0.4005167958656331,1,0.14285714285714285,1,1,tasks
170,We use a batch size of 32 and fine - tune for 3 epochs over the data for all GLUE tasks .,Experiments,GLUE,named-entity-recognition,8,"['O', 'B', 'O', 'B', 'I', 'B', 'B', 'O', 'B', 'I', 'I', 'B', 'I', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'B-p', 'O', 'B-b', 'I-b', 'B-p', 'B-ob', 'O', 'B-p', 'I-p', 'I-p', 'B-b', 'I-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",17,0.24285714285714285,169,0.43669250645994834,7,0.1346153846153846,1,1,tasks
171,"For each task , we selected the best fine - tuning learning rate ( among 5 e - 5 , 4 e - 5 , 3 e - 5 , and 2 e - 5 ) on the Dev set .",Experiments,GLUE,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'O', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'B', 'O', 'B', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'B-p', 'O', 'B-ob', 'I-ob', 'O']",18,0.2571428571428571,170,0.4392764857881137,8,0.15384615384615385,1,1,tasks
172,"Additionally , for BERT LARGE we found that finetuning was sometimes unstable on small datasets , so we ran several random restarts and selected the best model on the Dev set .",Experiments,GLUE,named-entity-recognition,8,"['O', 'O', 'B', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'I', 'I', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'B-p', 'B-b', 'I-b', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-b', 'I-b', 'I-b', 'O', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",19,0.2714285714285714,171,0.4418604651162791,9,0.17307692307692307,1,1,tasks
175,"Both BERT BASE and BERT LARGE outperform all systems on all tasks by a substantial margin , obtaining 4.5 % and 7.0 % respective average accuracy improvement over the prior state of the art .",Experiments,GLUE,named-entity-recognition,8,"['O', 'B', 'I', 'I', 'I', 'I', 'B', 'B', 'I', 'I', 'I', 'I', 'B', 'O', 'B', 'I', 'O', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'O', 'B-b', 'I-b', 'O', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",22,0.3142857142857143,174,0.4496124031007752,12,0.23076923076923078,1,1,tasks
179,"We find that BERT LARGE significantly outperforms BERT BASE across all tasks , especially those with very little training data .",Experiments,GLUE,named-entity-recognition,8,"['O', 'O', 'O', 'B', 'I', 'B', 'I', 'B', 'I', 'B', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-b', 'I-b', 'B-p', 'I-p', 'B-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.37142857142857144,178,0.4599483204134367,16,0.3076923076923077,1,1,tasks
182,The Stanford Question Answering Dataset ( SQuAD v1.1 ) is a collection of 100 k crowdsourced question / answer pairs .,Experiments,GLUE,named-entity-recognition,8,"['O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",29,0.4142857142857143,181,0.46770025839793283,19,0.36538461538461536,1,1,tasks
195,We fine - tune for 3 epochs with a learning rate of 5 e - 5 and a batch size of 32 .,Experiments,GLUE,named-entity-recognition,8,"['O', 'B', 'I', 'I', 'O', 'B', 'I', 'O', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'B', 'I', 'O', 'B', 'O']","['O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O']","['O', 'B-p', 'I-p', 'I-p', 'O', 'B-b', 'I-b', 'O', 'O', 'B-p', 'I-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'B-p', 'I-p', 'O', 'B-ob', 'O']",42,0.6,194,0.5012919896640827,32,0.6153846153846154,1,1,tasks
199,Our best performing system outperforms the top leaderboard system by + 1.5 F1 in ensembling and + 1.3 F1 as a single system .,Experiments,GLUE,named-entity-recognition,8,"['O', 'B', 'I', 'I', 'B', 'O', 'B', 'I', 'I', 'B', 'B', 'I', 'I', 'B', 'B', 'O', 'B', 'I', 'I', 'B', 'O', 'B', 'I', 'O']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['O', 'B-b', 'I-b', 'I-b', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'B-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'O']",46,0.6571428571428571,198,0.5116279069767442,36,0.6923076923076923,1,1,tasks
213,We fine - tuned for 2 epochs with a learning rate of 5 e - 5 and a batch size of 48 .,Experiments,GLUE,named-entity-recognition,8,"['O', 'B', 'I', 'I', 'O', 'B', 'I', 'O', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'B', 'I', 'O', 'B', 'O']","['O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O']","['O', 'B-p', 'I-p', 'I-p', 'O', 'B-b', 'I-b', 'O', 'O', 'B-p', 'I-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'B-p', 'I-p', 'O', 'B-ob', 'O']",60,0.8571428571428571,212,0.5478036175710594,50,0.9615384615384616,1,1,tasks
215,We observe a + 5.1 F1 improvement over the previous best system .,Experiments,GLUE,named-entity-recognition,8,"['O', 'B', 'O', 'B', 'I', 'I', 'I', 'B', 'O', 'B', 'I', 'I', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'O']",62,0.8857142857142857,214,0.5529715762273901,52,1.0,1,1,tasks
217,The Situations With Adversarial Generations ( SWAG ) dataset contains 113 k sentence - pair completion examples that evaluate grounded commonsense inference .,Experiments,SWAG,named-entity-recognition,8,"['O', 'B', 'I', 'I', 'I', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'B-b', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",64,0.9142857142857143,216,0.5581395348837209,1,0.14285714285714285,1,1,tasks
221,We fine - tune the model for 3 epochs with a learning rate of 2 e - 5 and a batch size of 16 .,Experiments,SWAG,named-entity-recognition,8,"['O', 'B', 'I', 'I', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'B', 'I', 'O', 'B', 'O']",,,68,0.9714285714285714,220,0.5684754521963824,5,0.7142857142857143,1,1,tasks
223,BERT LARGE outperforms the authors ' baseline ESIM + ELMo system by + 27.1 % and OpenAI GPT by 8.3 % .,Experiments,SWAG,named-entity-recognition,8,"['B', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'B', 'B', 'I', 'I', 'O', 'B', 'I', 'O', 'B', 'I', 'O']","['B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O']","['B-s', 'I-s', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'O', 'B-b', 'I-b', 'O', 'B-ob', 'I-ob', 'O']",70,1.0,222,0.5736434108527132,7,1.0,1,1,tasks
260,"However , we believe that this is the first work to demonstrate convincingly that scaling to extreme model sizes also leads to large improvements on very small scale tasks , provided that the model has been sufficiently pre-trained .",Effect of Model Size,Effect of Model Size,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'B', 'I', 'O', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'B', 'I', 'B', 'I', 'B', 'B', 'I', 'I', 'I', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'B-p', 'I-p', 'O', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'O', 'O', 'B-p', 'I-p', 'B-b', 'I-b', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'O', 'B-p', 'I-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",10,0.37037037037037035,259,0.6692506459948321,10,0.37037037037037035,1,1,ablation-analysis
275,BERT LARGE performs competitively with state - of - the - art methods .,Effect of Model Size,Effect of Model Size,named-entity-recognition,8,"['B', 'I', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['B-b', 'I-b', 'B-p', 'I-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",25,0.9259259259259259,274,0.7080103359173127,25,0.9259259259259259,1,1,ablation-analysis
277,This demonstrates that BERT is effective for both finetuning and feature - based approaches .,Effect of Model Size,Effect of Model Size,named-entity-recognition,8,"['O', 'B', 'O', 'B', 'I', 'I', 'B', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'I-p', 'B-ob', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",27,1.0,276,0.7131782945736435,27,1.0,1,1,ablation-analysis
2,Data and text mining BioBERT : a pre-trained biomedical language representation model for biomedical text mining,title,title,named-entity-recognition,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O']",,,1,0.0,1,0.005025125628140704,1,0.0,1,1,research-problem
6,"With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models .",abstract,abstract,named-entity-recognition,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.25,5,0.02512562814070352,3,0.25,1,1,research-problem
7,"However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora .",abstract,abstract,named-entity-recognition,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",4,0.3333333333333333,6,0.03015075376884422,4,0.3333333333333333,1,1,research-problem
8,"In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",abstract,abstract,named-entity-recognition,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",5,0.4166666666666667,7,0.035175879396984924,5,0.4166666666666667,1,1,research-problem
15,"We make the pre-trained weights of BioBERT freely available at https://github. com/naver/biobert-pretrained , and the source code for fine - tuning",abstract,abstract,named-entity-recognition,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,1.0,14,0.07035175879396985,12,1.0,1,1,code
27,"Also , the word distributions of general and biomedical corpora are quite different , which can often be a problem for biomedical text mining models .",Introduction,Introduction,named-entity-recognition,9,"['O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.6875,26,0.1306532663316583,11,0.6875,1,1,research-problem
34,"In this article , we introduce BioBERT , which is a pre-trained language representation model for the biomedical domain .",Approach,Approach,named-entity-recognition,9,"['O', 'O', 'O', 'O', 'O', 'B', 'B', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'B', 'O', 'B', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-b', 'O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'O']",1,0.07692307692307693,33,0.1658291457286432,1,0.07692307692307693,1,1,approach
36,"First , we initialize BioBERT with weights from BERT , which was pretrained on general domain corpora ( English Wikipedia and Books Corpus ) .",Approach,Approach,named-entity-recognition,9,"['O', 'O', 'O', 'B', 'B', 'B', 'B', 'B', 'B', 'O', 'O', 'O', 'B', 'I', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'B-p', 'B-b', 'B-p', 'B-b', 'B-p', 'B-b', 'O', 'O', 'O', 'B-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",3,0.23076923076923078,35,0.17587939698492464,3,0.23076923076923078,1,1,approach
37,"Then , BioBERT is pre-trained on biomedical domain corpora ( PubMed abstracts and PMC full - text articles ) .",Approach,Approach,named-entity-recognition,9,"['O', 'O', 'B', 'O', 'B', 'I', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'B-b', 'O', 'B-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",4,0.3076923076923077,36,0.18090452261306533,4,0.3076923076923077,1,1,approach
38,"To show the effectiveness of our approach in biomedical text mining , BioBERT is fine - tuned and evaluated on three popular biomedical text mining tasks .",Approach,Approach,named-entity-recognition,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'B', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",5,0.38461538461538464,37,0.18592964824120603,5,0.38461538461538464,1,1,approach
119,We used the BERT BASE model pre-trained on English Wikipedia and Books Corpus for 1 M steps .,Experimental setups,Experimental setups,named-entity-recognition,9,"['O', 'B', 'O', 'B', 'I', 'I', 'B', 'I', 'B', 'I', 'I', 'I', 'I', 'B', 'B', 'I', 'I', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'I-p', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'O']",1,0.0625,118,0.592964824120603,1,0.0625,1,1,experimental-setup
120,BioBERT v1.0 ( PubMed PMC ) is the version of BioBERT ( PubMed PMC ) trained for 470 K steps .,Experimental setups,Experimental setups,named-entity-recognition,9,"['B', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'B', 'I', 'B', 'I', 'I', 'I', 'I', 'B', 'I', 'B', 'I', 'I', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O', 'O', 'B-p', 'I-p', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'O']",2,0.125,119,0.5979899497487438,2,0.125,1,1,experimental-setup
126,We used eight NVIDIA V100 ( 32GB ) GPUs for the pre-training .,Experimental setups,Experimental setups,named-entity-recognition,9,"['O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'B', 'O', 'B', 'O']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O']","['O', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'O', 'B-ob', 'O']",8,0.5,125,0.628140703517588,8,0.5,1,1,experimental-setup
127,"The maximum sequence length was fixed to 512 and the mini-batch size was set to 192 , resulting in 98 304 words per iteration .",Experimental setups,Experimental setups,named-entity-recognition,9,"['O', 'B', 'I', 'I', 'O', 'B', 'I', 'B', 'O', 'O', 'B', 'I', 'O', 'B', 'I', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-b', 'I-b', 'I-b', 'O', 'B-p', 'I-p', 'B-ob', 'O', 'O', 'B-b', 'I-b', 'O', 'B-p', 'I-p', 'B-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.5625,126,0.6331658291457286,9,0.5625,1,1,experimental-setup
130,We used a single NVIDIA Titan Xp ( 12GB ) GPU to fine - tune BioBERT on each task .,Experimental setups,Experimental setups,named-entity-recognition,9,"['O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'B', 'I', 'I', 'I', 'B', 'B', 'B', 'I', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['O', 'O', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'I-p', 'I-p', 'B-b', 'B-p', 'B-ob', 'I-ob', 'O']",12,0.75,129,0.6482412060301508,12,0.75,1,1,experimental-setup
136,The results of NER are shown in .,Experimental results,Experimental results,named-entity-recognition,9,"['O', 'O', 'B', 'B', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'B-n', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'B-b', 'O', 'O', 'O', 'O']",1,0.03225806451612903,135,0.678391959798995,1,0.07142857142857142,1,1,results
138,"On the other hand , BioBERT achieves higher scores than BERT on all the datasets .",Experimental results,Experimental results,named-entity-recognition,9,"['O', 'O', 'O', 'O', 'O', 'B', 'B', 'B', 'I', 'B', 'B', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-b', 'B-p', 'B-b', 'I-b', 'B-p', 'B-ob', 'O', 'O', 'O', 'O', 'O']",3,0.0967741935483871,137,0.6884422110552764,3,0.21428571428571427,1,1,results
139,"BioBERT outperformed the state - of - the - art models on six out of nine datasets , and BioBERT v 1.1 ( PubMed ) outperformed the state - of - the - art models by 0.62 in terms of micro averaged F1 score .",Experimental results,Experimental results,named-entity-recognition,9,"['O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'B', 'B', 'I', 'I', 'I', 'I', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'B', 'I', 'I', 'B', 'I', 'I', 'I', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-b', 'B-p', 'I-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",4,0.12903225806451613,138,0.6934673366834171,4,0.2857142857142857,1,1,results
141,The RE results of each model are shown in .,Experimental results,Experimental results,named-entity-recognition,9,"['O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-b', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.1935483870967742,140,0.7035175879396985,6,0.42857142857142855,1,1,results
143,"On average ( micro ) , BioBERT v1.0 ( PubMed ) obtained a higher F1 score ( 2.80 higher ) than the state - of - the - art models .",Experimental results,Experimental results,named-entity-recognition,9,"['O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",8,0.25806451612903225,142,0.7135678391959799,8,0.5714285714285714,1,1,results
144,"Also , BioBERT achieved the highest F 1 scores on 2 out of 3 biomedical datasets .",Experimental results,Experimental results,named-entity-recognition,9,"['O', 'O', 'B', 'B', 'O', 'B', 'I', 'I', 'I', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'B-b', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",9,0.2903225806451613,143,0.7185929648241206,9,0.6428571428571429,1,1,results
149,"On all the biomedical QA datasets , BioBERT achieved new state - of - the - art performance in terms of MRR .",Experimental results,Experimental results,named-entity-recognition,9,"['O', 'O', 'O', 'B', 'I', 'I', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'B', 'I', 'I', 'B', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'O']","['O', 'O', 'O', 'B-b', 'I-b', 'I-b', 'O', 'B-b', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'I-p', 'B-ob', 'O']",14,0.45161290322580644,148,0.7437185929648241,14,1.0,1,1,results
2,Open Question Answering with Weakly Supervised Embedding Models,title,title,question-answering,0,"['B', 'I', 'I', 'O', 'O', 'O', 'O', 'O']",,,1,0.0,1,0.003875968992248062,1,0.0,1,1,research-problem
4,Building computers able to answer questions on any subject is along standing goal of artificial intelligence .,abstract,abstract,question-answering,0,"['B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.14285714285714285,3,0.011627906976744186,1,0.14285714285714285,1,1,research-problem
12,"This paper addresses the challenging problem of open - domain question answering , which consists of building systems able to answer questions from any domain .",Introduction,Introduction,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.03333333333333333,11,0.04263565891472868,1,0.03333333333333333,1,1,research-problem
23,"In this paper , we instead take the approach of converting questions to ( uninterpretable ) vectorial representations which require no pre-defined grammars or lexicons and can query any KB independent of its schema .",Introduction,Introduction,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'B', 'I', 'I', 'I', 'I', 'I', 'B', 'I', 'B', 'I', 'I', 'I', 'I', 'O', 'B', 'I', 'B', 'I', 'B', 'I', 'O', 'B', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'B-p', 'I-p', 'B-b', 'I-b', 'B-p', 'I-p', 'O', 'B-ob', 'O']",12,0.4,22,0.08527131782945736,12,0.4,1,1,approach
31,Our approach is based on learning low - dimensional vector embeddings of words and of KB triples so that representations of questions and corresponding answers end up being similar in the embedding space .,Introduction,Introduction,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'B', 'B', 'I', 'I', 'I', 'I', 'B', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'B', 'I', 'I', 'I', 'I', 'I', 'B', 'I', 'O', 'B', 'B', 'O', 'B', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'O', 'B-b', 'B-p', 'O', 'B-ob', 'I-ob', 'O']",20,0.6666666666666666,30,0.11627906976744186,20,0.6666666666666666,1,1,approach
33,"In order to avoid transferring the cost of manual intervention to the one of labeling large amounts of data , we make use of weak supervision .",Introduction,Introduction,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'B', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-ob', 'I-ob', 'O']",22,0.7333333333333333,32,0.12403100775193798,22,0.7333333333333333,1,1,approach
35,We end up learning meaningful vectorial representations for questions involving up to 800 k words and for triples of an mostly automatically created KB with 2.4 M entities and 600 k relationships .,Introduction,Introduction,question-answering,0,"['O', 'B', 'I', 'I', 'B', 'I', 'I', 'B', 'B', 'B', 'B', 'I', 'I', 'I', 'I', 'O', 'O', 'B', 'B', 'O', 'B', 'I', 'I', 'I', 'B', 'B', 'I', 'I', 'O', 'B', 'I', 'I', 'O']","['O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O']","['O', 'B-p', 'I-p', 'I-p', 'B-b', 'I-b', 'I-b', 'B-p', 'B-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'B-b', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'O', 'B-ob', 'I-ob', 'I-ob', 'O']",24,0.8,34,0.13178294573643412,24,0.8,1,1,approach
38,"Thus , we propose a method to fine - tune embedding - based models by carefully optimizing a matrix parameterizing the similarity used in the embedding space , leading to a consistent improvement in performance .",Introduction,Introduction,question-answering,0,"['O', 'O', 'O', 'B', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'B', 'B', 'I', 'B', 'O', 'B', 'B', 'I', 'O', 'B', 'I', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O', 'O', 'B-p', 'B-b', 'I-b', 'B-p', 'O', 'B-b', 'B-p', 'I-p', 'O', 'B-b', 'I-b', 'O', 'B-p', 'I-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",27,0.9,37,0.1434108527131783,27,0.9,1,1,approach
201,"First , we can see that multitasking with paraphrase data is essential since it improves F1 from 0.60 to 0.68 .",Results,This section now discusses our empirical performance .,question-answering,0,"['O', 'O', 'O', 'O', 'B', 'I', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'B', 'I', 'B', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-b', 'I-b', 'I-b', 'I-b', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",4,0.17391304347826086,200,0.7751937984496124,4,0.2,1,1,results
207,Fine - tuning the embedding model is very beneficial to optimize the top of the list and grants a bump of 5 points of F1 : carefully tuning the similarity makes a clear difference .,Results,This section now discusses our empirical performance .,question-answering,0,"['B', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'B', 'I', 'O', 'B', 'B', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'B', 'I', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",,,10,0.43478260869565216,206,0.7984496124031008,10,0.5,1,1,results
208,"All versions of our system greatly outperform paralex : the fine - tuned model improves the F1 - score by almost 20 points and , according to , is better in precision for all levels of recall .",Results,This section now discusses our empirical performance .,question-answering,0,"['B', 'I', 'I', 'I', 'I', 'B', 'I', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'B-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.4782608695652174,207,0.8023255813953488,11,0.55,1,1,results
2,Convolutional Neural Network Architectures for Matching Natural Language Sentences,title,,question-answering,1,"['O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I']",,,1,0.0,1,0.0051813471502590676,1,0.0,1,1,research-problem
4,"Semantic matching is of central importance to many natural language tasks [ 2,28 ] .",abstract,abstract,question-answering,1,"['B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.16666666666666666,3,0.015544041450777202,1,0.16666666666666666,1,1,research-problem
11,Matching two potentially heterogenous language objects is central to many natural language applications .,Introduction,Introduction,question-answering,1,"['B', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0625,10,0.05181347150259067,1,0.0625,1,1,research-problem
15,A successful sentence - matching algorithm therefore needs to capture not only the internal structures of sentences but also the rich patterns in their interactions .,Introduction,Introduction,question-answering,1,"['O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.3125,14,0.07253886010362694,5,0.3125,1,1,research-problem
16,"Towards this end , we propose deep neural network models , which adapt the convolutional strategy ( proven successful on image and speech ) to natural language .",Introduction,Introduction,question-answering,1,"['O', 'O', 'O', 'O', 'O', 'B', 'B', 'I', 'I', 'I', 'O', 'O', 'B', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-ob', 'I-ob', 'O']",6,0.375,15,0.07772020725388601,6,0.375,1,1,model
17,"To further explore the relation between representing sentences and matching them , we devise a novel model that can naturally host both the hierarchical composition for sentences and the simple - to - comprehensive fusion of matching patterns with the same convolutional architecture .",Introduction,Introduction,question-answering,1,"['B', 'I', 'I', 'O', 'B', 'B', 'B', 'I', 'I', 'I', 'I', 'O', 'O', 'B', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'O', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'B', 'O', 'B', 'I', 'I', 'O']","['B-p', 'I-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['B-p', 'I-p', 'I-p', 'O', 'B-b', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'O', 'B-p', 'I-p', 'I-p', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'O']",7,0.4375,16,0.08290155440414508,7,0.4375,1,1,model
127,"In other words , We use stochastic gradient descent for the optimization of models .",Training,Training,question-answering,1,"['O', 'O', 'O', 'O', 'O', 'B', 'B', 'I', 'I', 'B', 'O', 'B', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-b', 'I-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'O']",9,0.5,126,0.6528497409326425,9,0.5,1,1,hyperparameters
128,All the proposed models perform better with mini-batch ( 100 ? 200 in sizes ) which can be easily parallelized on single machine with multi-cores .,Training,Training,question-answering,1,"['O', 'O', 'O', 'O', 'B', 'I', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'B', 'I', 'O', 'B', 'B', 'B', 'I', 'O', 'B', 'O']",,,10,0.5555555555555556,127,0.6580310880829016,10,0.5555555555555556,1,1,hyperparameters
129,"For regularization , we find that for both architectures , early stopping is enough for models with medium size and large training sets ( with over 500K instances ) .",Training,Training,question-answering,1,"['B', 'I', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'B', 'B', 'B', 'I', 'O', 'B', 'I', 'I', 'O', 'O', 'B', 'I', 'I', 'O', 'O']","['B-p', 'I-p', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O']","['B-p', 'I-p', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'B-b', 'I-b', 'O', 'O', 'O', 'B-b', 'B-p', 'B-ob', 'I-ob', 'O', 'B-b', 'I-b', 'I-b', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'O', 'O']",11,0.6111111111111112,128,0.6632124352331606,11,0.6111111111111112,1,1,hyperparameters
131,"We use 50 - dimensional word embedding trained with the Word2 Vec : the embedding for English words ( Section 5.2 & 5.4 ) is learnt on Wikipedia ( ?1B words ) , while that for Chinese words ( Section 5.3 ) is learnt on Weibo data (? 300 M words ) .",Training,Training,question-answering,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",,,13,0.7222222222222222,130,0.6735751295336787,13,0.7222222222222222,1,1,hyperparameters
134,"We use 3 - word window throughout all experiments 2 , but test various numbers of feature maps ( typically from 200 to 500 ) , for optimal performance .",Training,Training,question-answering,1,"['O', 'O', 'B', 'I', 'I', 'I', 'B', 'B', 'I', 'O', 'O', 'O', 'B', 'B', 'I', 'I', 'I', 'I', 'O', 'B', 'I', 'B', 'I', 'I', 'O', 'O', 'B', 'B', 'I', 'O']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O']","['O', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'O', 'O', 'O', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O', 'B-p', 'I-p', 'B-b', 'I-b', 'I-b', 'O', 'O', 'B-p', 'B-ob', 'I-ob', 'O']",16,0.8888888888888888,133,0.689119170984456,16,0.8888888888888888,1,1,hyperparameters
136,"We use ReLu as the activation function for all of models ( convolution and MLP ) , which yields comparable or better results to sigmoid - like functions , but converges faster .",Training,Training,question-answering,1,"['O', 'O', 'B', 'B', 'O', 'B', 'I', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'B', 'B', 'I', 'I', 'I', 'B', 'B', 'I', 'I', 'I', 'O', 'O', 'B', 'B', 'O']","['O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'O']","['O', 'O', 'B-b', 'B-p', 'O', 'B-b', 'I-b', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O', 'O', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'B-p', 'B-ob', 'O']",18,1.0,135,0.6994818652849741,18,1.0,1,1,hyperparameters
142,WORDEMBED : We first represent each short - text as the sum of the embedding of the words it contains .,Competitor Methods,Competitor Methods,question-answering,1,"['B', 'O', 'O', 'O', 'B', 'B', 'I', 'I', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['B-n', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['B-b', 'O', 'O', 'O', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",1,0.16666666666666666,141,0.7305699481865285,1,0.03333333333333333,1,1,baselines
143,"The matching score of two short - texts are calculated with an MLP with the embedding of the two documents as input ; DEEPMATCH : We take the matching model in and train it on our datasets with 3 hidden layers and 1,000 hidden nodes in the first hidden layer ; URAE+ MLP :",Competitor Methods,Competitor Methods,question-answering,1,"['O', 'B', 'I', 'B', 'B', 'I', 'I', 'I', 'O', 'B', 'B', 'B', 'I', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'O', 'O', 'B', 'I', 'I', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'B', 'I', 'O']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O']","['O', 'B-b', 'I-b', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'O', 'B-p', 'B-p', 'B-b', 'I-b', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-b', 'I-b', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'B-b', 'I-b', 'O']",2,0.3333333333333333,142,0.7357512953367875,2,0.06666666666666667,1,1,baselines
144,"We use the Unfolding Recursive Autoencoder to get a 100 dimensional vector representation of each sentence , and put an MLP on the top as in WORDEMBED ; SENNA + MLP / SIM :",Competitor Methods,Competitor Methods,question-answering,1,"['O', 'B', 'O', 'B', 'I', 'I', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'O', 'B-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'B-ob', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O']",3,0.5,143,0.7409326424870466,3,0.1,1,1,baselines
145,We use the SENNA - type sentence model for sentence representation ;,Competitor Methods,Competitor Methods,question-answering,1,"['O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'B', 'B', 'I', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['O', 'O', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'O']",4,0.6666666666666666,144,0.7461139896373057,4,0.13333333333333333,1,1,baselines
146,"SENMLP : We take the whole sentence as input ( with word embedding aligned sequentially ) , and use an MLP to obtain the score of coherence .",Competitor Methods,Competitor Methods,question-answering,1,"['B', 'O', 'O', 'B', 'O', 'B', 'I', 'B', 'B', 'O', 'B', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'B', 'O', 'B', 'B', 'I', 'O', 'B', 'I', 'I', 'O']","['B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['B-b', 'O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'B-p', 'B-b', 'O', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'B-p', 'O', 'B-b', 'B-p', 'I-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'O']",5,0.8333333333333334,145,0.7512953367875648,5,0.16666666666666666,1,1,baselines
173,ARC - II outperforms others significantly when the training instances are relatively abundant ( as in Experiment I & II ) .,Experiment I : Sentence,Discussions,question-answering,1,"['B', 'I', 'I', 'B', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-b', 'I-b', 'I-b', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.7575757575757576,172,0.8911917098445595,1,0.1111111111111111,1,1,results
176,"As another important observation , convolutional models ( ARC - I & II , SENNA + MLP ) perform favorably over bag - of - words models , indicating the importance of utilizing sequential structures in understanding and matching sentences .",Experiment I : Sentence,Discussions,question-answering,1,"['O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'B', 'I', 'I', 'B', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",28,0.8484848484848485,175,0.9067357512953368,4,0.4444444444444444,1,1,results
177,"Quite interestingly , as shown by our other experiments , ARC - I and ARC - II trained purely with random negatives automatically gain some ability in telling whether the words in a given sentence are in right sequential order ( with around 60 % accuracy for both ) .",Experiment I : Sentence,Discussions,question-answering,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'B', 'I', 'I', 'B', 'I', 'B', 'I', 'I', 'I', 'B', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",,,29,0.8787878787878788,176,0.9119170984455959,5,0.5555555555555556,1,1,results
179,We noticed that simple sum of embedding learned via Word2 Vec yields reasonably good results on all three tasks .,Experiment I : Sentence,Discussions,question-answering,1,"['O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'B', 'B', 'I', 'I', 'B', 'B', 'I', 'I', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'B-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'O']",31,0.9393939393939394,178,0.9222797927461139,7,0.7777777777777778,1,1,results
2,Large - scale Simple Question Answering with Memory Networks,title,,question-answering,2,"['B', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O']",,,1,0.0,1,0.003663003663003663,1,0.0,1,1,research-problem
4,Training large - scale question answering systems is complicated because training sources usually cover a small portion of the range of possible questions .,abstract,abstract,question-answering,2,"['B', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.25,3,0.01098901098901099,1,0.25,1,1,research-problem
5,"This paper studies the impact of multitask and transfer learning for simple question answering ; a setting for which the reasoning required to answer is quite easy , as long as one can retrieve the correct evidence given a question , which can be difficult in large - scale conditions .",abstract,abstract,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.5,4,0.014652014652014652,2,0.5,1,1,research-problem
19,"First , as an effort to study the coverage of existing systems and the possibility to train jointly on different data sources via multitasking , we collected the first large - scale dataset of questions and answers based on a KB , called SimpleQuestions .",Introduction,Introduction,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'B', 'B', 'B', 'I', 'O', 'O', 'O', 'B', 'I', 'B', 'B', 'B', 'I', 'I', 'B', 'B', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'B', 'B', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-b', 'B-p', 'B-ob', 'I-ob', 'O', 'O', 'O', 'B-p', 'I-p', 'B-b', 'B-p', 'B-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O', 'B-p', 'B-ob', 'O']",11,0.07333333333333333,18,0.06593406593406594,11,0.4583333333333333,1,1,dataset
24,"Second , in sections 3 and 4 , we present an embedding - based QA system developed under the framework of Memory Networks ( Mem NNs ) .",Introduction,Introduction,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",16,0.10666666666666667,23,0.08424908424908426,16,0.6666666666666666,1,1,model
26,The setting of the simple QA corresponds to the elementary operation of performing a single lookup in the memory .,Introduction,Introduction,question-answering,2,"['O', 'B', 'I', 'O', 'B', 'I', 'B', 'I', 'O', 'B', 'I', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'O']","['O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'B-p', 'I-p', 'O', 'B-b', 'I-b', 'B-p', 'I-p', 'O', 'B-b', 'I-b', 'O', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",18,0.12,25,0.09157509157509157,18,0.75,1,1,model
229,"The embedding dimension and the learning rate were chosen among { 64 , 128 , 256 } and { 1 , 0.1 , ... , 1.0e ? 4 } respectively , and the margin ?",Experimental setup,Experimental setup,question-answering,2,"['O', 'B', 'I', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'B', 'O']",,,3,0.25,228,0.8351648351648352,3,0.25,1,1,hyperparameters
230,was set to 0.1 .,Experimental setup,Experimental setup,question-answering,2,"['O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O']",4,0.3333333333333333,229,0.8388278388278388,4,0.3333333333333333,1,1,hyperparameters
242,"On the main benchmark WebQuestions , our best results use all data sources , the bigger extract from Freebase and the CANDS AS NEGS setting .",Comparative results,Comparative results,question-answering,2,"['B', 'O', 'B', 'I', 'I', 'O', 'O', 'B', 'I', 'B', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-b', 'I-b', 'I-b', 'O', 'O', 'B-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.07142857142857142,241,0.8827838827838828,2,0.6666666666666666,1,1,results
257,"In this set of experiments , all Reverb facts are added to the memory , without any retraining , and we test our ability to rerank answers on the companion QA set .",Comparative results,Transfer learning on Reverb,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'I', 'B', 'I', 'B', 'B', 'O', 'B', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-b', 'I-b', 'B-p', 'I-p', 'B-b', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'O']",17,0.6071428571428571,256,0.9377289377289377,1,0.08333333333333333,1,1,results
259,"Our best results are 67 % accuracy ( and 68 % for the ensemble of 5 models ) , which are better than the 54 % of the original paper and close to the stateof - the - art 73 % of .",Comparative results,Transfer learning on Reverb,question-answering,2,"['O', 'B', 'I', 'O', 'B', 'I', 'I', 'O', 'O', 'B', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'I-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'O', 'O', 'B-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.6785714285714286,258,0.945054945054945,3,0.25,1,1,results
262,"We first notice that models trained on a single QA dataset perform poorly on the other datasets ( e.g. 46.6 % accuracy on SimpleQuestions for the model trained on WebQuestions only ) , which shows that the performance on We-bQuestions does not necessarily guarantee high coverage for simple QA .",Comparative results,Transfer learning on Reverb,question-answering,2,"['O', 'O', 'B', 'O', 'B', 'B', 'B', 'O', 'B', 'I', 'I', 'B', 'B', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",,,22,0.7857142857142857,261,0.9560439560439561,6,0.5,1,1,results
263,"On the other hand , training on both datasets only improves performance ; in particular , the model is able to capture all question patterns of the two datasets ; there is no "" negative interaction "" .",Comparative results,Transfer learning on Reverb,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'B', 'I', 'B', 'I', 'O', 'B', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'B', 'I', 'I', 'B', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-b', 'I-b', 'O', 'B-p', 'B-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-b', 'I-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",23,0.8214285714285714,262,0.9597069597069597,7,0.5833333333333334,1,1,results
2,Sentence Similarity Learning by Lexical Decomposition and Composition,title,,question-answering,3,"['B', 'I', 'I', 'O', 'O', 'O', 'O', 'O']",,,1,0.0,1,0.003937007874015748,1,0.0,1,1,research-problem
4,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences .",abstract,abstract,question-answering,3,"['O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.02,3,0.011811023622047244,1,0.02,1,1,research-problem
45,"In this paper , we propose a novel model to tackle all these challenges jointly by decomposing and composing lexical semantics over sentences .",abstract,E5,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'B', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'B', 'I', 'B', 'B', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-b', 'I-b', 'I-b', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-b', 'I-b', 'B-p', 'B-ob', 'O']",42,0.84,44,0.1732283464566929,42,0.84,1,1,model
46,"Given a sentence pair , the model represents each word as a low -dimensional vector ( challenge 1 ) , and calculates a semantic matching vector for each word based on all words in the other sentence ( challenge 2 ) .",abstract,E5,question-answering,3,"['B', 'O', 'B', 'I', 'O', 'O', 'O', 'B', 'B', 'I', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'B', 'O', 'O', 'B', 'I', 'B', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-b', 'I-b', 'O', 'O', 'O', 'B-p', 'B-b', 'I-b', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'O', 'O', 'B-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O']",43,0.86,45,0.17716535433070865,43,0.86,1,1,model
47,"Then based on the semantic matching vector , each word vector is decomposed into two components : a similar component and a dissimilar component ( challenge 3 ) .",abstract,E5,question-answering,3,"['O', 'B', 'I', 'O', 'B', 'I', 'I', 'O', 'B', 'I', 'I', 'O', 'B', 'B', 'B', 'I', 'O', 'O', 'B', 'I', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'I-p', 'O', 'B-b', 'I-b', 'I-b', 'O', 'B-b', 'I-b', 'I-b', 'O', 'B-p', 'B-p', 'B-b', 'I-b', 'O', 'O', 'B-ob', 'I-ob', 'O', 'O', 'B-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O']",44,0.88,46,0.18110236220472442,44,0.88,1,1,model
48,"We use similar components of all the words to represent the similar parts of the sentence pair , and dissimilar components of every word to model the dissimilar parts explicitly .",abstract,E5,question-answering,3,"['O', 'B', 'B', 'I', 'B', 'O', 'O', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'B', 'I', 'O', 'B', 'I', 'B', 'I', 'O', 'B', 'I', 'I', 'O']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['O', 'B-p', 'B-b', 'I-b', 'B-p', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'B-b', 'I-b', 'O', 'B-b', 'I-b', 'B-p', 'I-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'O']",45,0.9,47,0.18503937007874016,45,0.9,1,1,model
49,"After this , a two - channel CNN operation is performed to compose the similar and dissimilar components into a feature vector ( challenge 2 and 3 ) .",abstract,E5,question-answering,3,"['O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'O', 'B', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'B', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O', 'B-p', 'B-p', 'I-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",46,0.92,48,0.1889763779527559,46,0.92,1,1,model
50,"Finally , the composed feature vector is utilized to predict the sentence similarity .",abstract,E5,question-answering,3,"['O', 'O', 'O', 'B', 'I', 'I', 'O', 'B', 'B', 'I', 'O', 'B', 'I', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['O', 'O', 'O', 'B-b', 'I-b', 'I-b', 'O', 'B-p', 'B-p', 'I-p', 'O', 'B-ob', 'I-ob', 'O']",47,0.94,49,0.19291338582677164,47,0.94,1,1,model
177,"We switched the semantic matching functions among { max , global , local - l} , where l ? { 1 , 2 , 3 , 4 } , and fixed the other options as : the linear decomposition , the filter types including {unigram , bigram , trigram } , and 500 filters for each type .",Model Properties,Model Properties,question-answering,3,"['O', 'B', 'O', 'B', 'I', 'I', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'B', 'O', 'O', 'B', 'I', 'O', 'O', 'B', 'I', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'B', 'B', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'B-p', 'O', 'O', 'B-ob', 'I-ob', 'O', 'O', 'B-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'B-ob', 'B-p', 'O', 'O', 'O', 'O']",6,0.08695652173913043,176,0.6929133858267716,6,0.08695652173913043,1,1,experimental-setup
179,We found that the max function worked better than the global function on both MAP and MRR .,Model Properties,Model Properties,question-answering,3,"['O', 'B', 'O', 'O', 'B', 'I', 'B', 'I', 'I', 'O', 'B', 'I', 'B', 'I', 'B', 'I', 'I', 'O']","['O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['O', 'B-p', 'O', 'O', 'B-b', 'I-b', 'B-p', 'I-p', 'I-p', 'O', 'B-b', 'I-b', 'B-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'O']",8,0.11594202898550725,178,0.7007874015748031,8,0.11594202898550725,1,1,experimental-setup
185,"We varied the decomposition operation among { rigid , linear , orthogonal } , and kept the other options unchanged .",Model Properties,Model Properties,question-answering,3,"['O', 'B', 'O', 'B', 'I', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.2028985507246377,184,0.7244094488188977,14,0.2028985507246377,1,1,experimental-setup
191,"Third , we tested the influence of various filter types .",Model Properties,Model Properties,question-answering,3,"['O', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O']",20,0.2898550724637681,190,0.7480314960629921,20,0.2898550724637681,1,1,experimental-setup
192,"We constructed 5 groups of filters : win - 1 contains only the unigram filters , win - 2 contains both unigram and bigram filters , win - 3 contains all the filters in win - 2 plus trigram filters , win - 4 extends filters in win - 3 with 4 - gram filters , and win - 5 adds 5 - gram filters into win - 4 .",Model Properties,Model Properties,question-answering,3,"['O', 'B', 'B', 'I', 'I', 'B', 'O', 'O', 'O', 'O', 'B', 'B', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'B', 'I', 'I', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'B', 'I', 'I', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'B-p', 'B-b', 'I-b', 'I-b', 'B-p', 'O', 'O', 'O', 'O', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'B-b', 'I-b', 'I-b', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'B-b', 'I-b', 'I-b', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'I-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",21,0.30434782608695654,191,0.7519685039370079,21,0.30434782608695654,1,1,experimental-setup
214,"We can see that our model ( the last row of ) got the best MAP among all previous work , and a comparable MRR than dos .",Model Properties,Model Properties,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'B', 'B', 'I', 'I', 'O', 'O', 'O', 'B', 'I', 'B', 'B', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'B-b', 'I-b', 'B-p', 'B-ob', 'O']",43,0.6231884057971014,213,0.8385826771653543,43,0.6231884057971014,1,1,results
224,The last row of shows that our model is more effective than the other models .,Model Properties,Model Properties,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'B', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-ob', 'I-ob', 'O']",53,0.7681159420289855,223,0.8779527559055118,53,0.7681159420289855,1,1,results
237,"Comparing to these neural network based models , our model obtained a comparable performance ( the last row of ) without using any sparse features , extra annotated resources and specific training strategies .",Model Properties,Model Properties,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'B', 'I', 'I', 'O', 'B', 'I', 'I', 'O', 'B', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'O', 'B-ob', 'I-ob', 'I-ob', 'O', 'B-ob', 'I-ob', 'I-ob', 'O']",66,0.9565217391304348,236,0.9291338582677166,66,0.9565217391304348,1,1,results
2,A Parallel - Hierarchical Model for Machine Comprehension on Sparse Data,title,title,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I']",,,1,0.0,1,0.003436426116838488,1,0.0,1,1,research-problem
4,Understanding unstructured text is a major goal within natural language processing .,abstract,abstract,question-answering,4,"['B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.125,3,0.010309278350515464,1,0.125,1,1,research-problem
6,"In this work , we investigate machine comprehension on the challenging MCTest benchmark .",abstract,abstract,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",3,0.375,5,0.01718213058419244,3,0.375,1,1,research-problem
14,"Comprehension of unstructured text by machines , at a near- human level , is a major goal for natural language processing .",Introduction,Introduction,question-answering,4,"['B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.043478260869565216,13,0.044673539518900345,2,0.05555555555555555,1,1,research-problem
16,Machine comprehension ( MC ) is evaluated by posing a set of questions based on a text passage ( akin to the reading tests we all took in school ) .,Introduction,Introduction,question-answering,4,"['B', 'I', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-ob', 'I-ob', 'O', 'B-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.08695652173913043,15,0.05154639175257732,4,0.1111111111111111,1,1,research-problem
27,The key to our model is that it compares the question and answer candidates to the text using several distinct perspectives .,Introduction,Introduction,question-answering,4,"['O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'B', 'B', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'O', 'O', 'B-b', 'B-p', 'O', 'O', 'O', 'O']",15,0.32608695652173914,26,0.08934707903780069,15,0.4166666666666667,1,1,model
29,"The semantic perspective compares the hypothesis to sentences in the text viewed as single , self - contained thoughts ; these are represented using a sum and transformation of word embedding vectors , similarly to in .",Introduction,Introduction,question-answering,4,"['O', 'B', 'I', 'B', 'O', 'B', 'B', 'B', 'B', 'O', 'B', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['O', 'B-b', 'I-b', 'B-p', 'O', 'B-b', 'B-p', 'B-b', 'B-p', 'O', 'B-b', 'B-p', 'I-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O']",17,0.3695652173913043,28,0.09621993127147767,17,0.4722222222222222,1,1,model
30,"The word - by - word perspective focuses on similarity matches between individual words from hypothesis and text , at various scales .",Introduction,Introduction,question-answering,4,"['O', 'B', 'I', 'I', 'I', 'I', 'I', 'B', 'I', 'B', 'I', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'B-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",18,0.391304347826087,29,0.09965635738831616,18,0.5,1,1,model
32,"We also use a sliding window acting on a subsentential scale ( inspired by the work of ) , which implicitly considers the linear distance between matched words .",Introduction,Introduction,question-answering,4,"['O', 'O', 'B', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",20,0.43478260869565216,31,0.10652920962199312,20,0.5555555555555556,1,1,model
33,"Finally , this word - level sliding window operates on two different views of text sentences : the sequential view , where words appear in their natural order , and the dependency view , where words are reordered based on a linearization of the sentence 's dependency graph .",Introduction,Introduction,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'B', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'B', 'I', 'O', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'B', 'I', 'I', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O', 'O', 'B-b', 'I-b', 'O', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'B-b', 'I-b', 'O', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'I-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",21,0.45652173913043476,32,0.10996563573883161,21,0.5833333333333334,1,1,model
218,"For word vectors we use Google 's publicly available embeddings , trained with word2vec on the 100 - billion - word News corpus .",Training and Model Details,Training and Model Details,question-answering,4,"['B', 'B', 'I', 'O', 'B', 'B', 'I', 'I', 'I', 'I', 'O', 'B', 'I', 'B', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['B-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['B-p', 'B-b', 'I-b', 'O', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O', 'B-p', 'I-p', 'B-b', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",3,0.12,217,0.7457044673539519,3,0.12,1,1,hyperparameters
231,"We found dropout to be particularly effective at improving generalization from the training to the test set , and used 0.5 as the dropout probability .",Training and Model Details,Training and Model Details,question-answering,4,"['O', 'B', 'B', 'O', 'O', 'O', 'B', 'I', 'I', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'B', 'B', 'B', 'O', 'B', 'I', 'O']","['O', 'B-p', 'B-n', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['O', 'B-p', 'B-b', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'B-p', 'B-b', 'B-p', 'O', 'B-ob', 'I-ob', 'O']",16,0.64,230,0.7903780068728522,16,0.64,1,1,hyperparameters
235,"We used the Adam optimizer with the standard settings ( Kingma and Ba , 2014 ) and a learning rate of 0.003 .",Training and Model Details,Training and Model Details,question-answering,4,"['O', 'B', 'O', 'B', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'B', 'I', 'O', 'B', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O']","['O', 'B-p', 'O', 'B-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'B-p', 'I-p', 'O', 'B-ob', 'O']",20,0.8,234,0.8041237113402062,20,0.8,1,1,hyperparameters
236,To determine the best hyperparameters we performed a grid search over 150 settings based on validation - set accuracy .,Training and Model Details,Training and Model Details,question-answering,4,"['B', 'I', 'O', 'B', 'I', 'O', 'B', 'O', 'B', 'I', 'B', 'B', 'I', 'B', 'I', 'B', 'I', 'I', 'I', 'O']","['B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['B-p', 'I-p', 'O', 'B-b', 'I-b', 'O', 'B-p', 'O', 'B-b', 'I-b', 'B-p', 'B-b', 'I-b', 'B-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",21,0.84,235,0.8075601374570447,21,0.84,1,1,hyperparameters
247,"On MCTest - 500 , the Parallel Hierarchical model significantly outperforms these methods on single questions ( > 2 % ) and slightly outperforms the latter two on multi questions ( ? 0.3 % ) and overall ( ? 1 % ) .",Results,Results,question-answering,4,"['B', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'B', 'I', 'O', 'O', 'B', 'B', 'I', 'B', 'I', 'I', 'I', 'I', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'O']","['B-p', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'O', 'B-p', 'I-p', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['B-p', 'O', 'O', 'O', 'O', 'O', 'B-b', 'I-b', 'I-b', 'B-ob', 'I-ob', 'O', 'O', 'B-p', 'B-b', 'I-b', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'O', 'B-p', 'I-p', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",6,0.14285714285714285,246,0.845360824742268,6,0.42857142857142855,1,1,results
259,"Not surprisingly , the n-gram functionality is important , contributing almost 5 % accuracy improvement .",Results,Analysis and Discussion,question-answering,4,"['O', 'O', 'O', 'O', 'B', 'I', 'O', 'B', 'O', 'B', 'B', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'B-b', 'I-b', 'O', 'B-p', 'O', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",18,0.42857142857142855,258,0.8865979381443299,3,0.1111111111111111,1,1,ablation-analysis
262,"The top N function contributes very little to the overall performance , suggesting that most multi questions have their evidence distributed across contiguous sentences .",Results,Analysis and Discussion,question-answering,4,"['O', 'B', 'I', 'I', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'B-b', 'I-b', 'I-b', 'B-p', 'I-p', 'I-p', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",21,0.5,261,0.8969072164948454,6,0.2222222222222222,1,1,ablation-analysis
263,"Ablating the sentential component made the most significant difference , reducing performance by more than 5 % .",Results,Analysis and Discussion,question-answering,4,"['O', 'O', 'B', 'I', 'B', 'O', 'B', 'I', 'I', 'O', 'B', 'I', 'B', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'B-b', 'I-b', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'O', 'B-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",22,0.5238095238095238,262,0.9003436426116839,7,0.25925925925925924,1,1,ablation-analysis
264,Simple word - by - word matching is obviously useful on MCTest .,Results,Analysis and Discussion,question-answering,4,"['B', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'B', 'I', 'B', 'B', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-p', 'B-n', 'O']","['B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'B-p', 'I-p', 'B-p', 'B-b', 'O']",23,0.5476190476190477,263,0.9037800687285223,8,0.2962962962962963,1,1,ablation-analysis
265,"The sequential sliding window makes a 3 % contribution , highlighting the importance of word - distance measures .",Results,Analysis and Discussion,question-answering,4,"['O', 'B', 'I', 'I', 'B', 'O', 'B', 'I', 'I', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'B-b', 'I-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'O', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",24,0.5714285714285714,264,0.9072164948453608,9,0.3333333333333333,1,1,ablation-analysis
266,"On the other hand , the dependency - based sliding window makes only a minor contribution .",Results,Analysis and Discussion,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'O']",25,0.5952380952380952,265,0.9106529209621993,10,0.37037037037037035,1,1,ablation-analysis
269,"Finally , the exogenous word weights make a significant contribution of almost 5 % .",Results,Analysis and Discussion,question-answering,4,"['O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'B', 'B', 'I', 'B', 'I', 'I', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'B-b', 'I-b', 'I-b', 'O', 'O', 'B-p', 'B-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'O']",28,0.6666666666666666,268,0.9209621993127147,13,0.48148148148148145,1,1,ablation-analysis
2,Iterative Alternating Neural Attention for Machine Reading,title,,question-answering,5,"['O', 'O', 'O', 'O', 'O', 'B', 'I']",,,1,0.0,1,0.004524886877828055,1,0.0,1,1,research-problem
4,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document .",abstract,abstract,question-answering,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",1,0.3333333333333333,3,0.013574660633484163,1,0.3333333333333333,1,1,research-problem
19,"Encouraged by the recent success of deep learning attention architectures , we propose a novel neural attention - based inference model designed to perform machine reading comprehension tasks .",Introduction,Introduction,question-answering,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'B', 'I', 'I', 'B', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",12,0.13186813186813187,18,0.08144796380090498,12,0.5454545454545454,1,1,model
20,The model first reads the document and the query using a recurrent neural network .,Introduction,Introduction,question-answering,5,"['O', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'B', 'O', 'B', 'I', 'I', 'O']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'O']",13,0.14285714285714285,19,0.08597285067873303,13,0.5909090909090909,1,1,model
21,"Then , it deploys an iterative inference process to uncover the inferential links that exist between the missing query word , the query , and the document .",Introduction,Introduction,question-answering,5,"['O', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'B', 'I', 'O', 'B', 'I', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'I-p', 'O', 'B-b', 'I-b', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.15384615384615385,20,0.09049773755656108,14,0.6363636363636364,1,1,model
22,"This phase involves a novel alternating attention mechanism ; it first attends to some parts of the query , then finds their corresponding matches by attending to the document .",Introduction,Introduction,question-answering,5,"['O', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'B', 'I', 'B', 'I', 'I', 'I', 'I', 'O', 'O', 'B', 'O', 'B', 'I', 'B', 'I', 'I', 'O', 'B', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'O']","['O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'O', 'O', 'O', 'B-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'B-p', 'I-p', 'I-p', 'O', 'B-ob', 'O']",15,0.16483516483516483,21,0.09502262443438914,15,0.6818181818181818,1,1,model
23,The result of this alternating search is fed back into the iterative inference process to seed the next search step .,Introduction,Introduction,question-answering,5,"['O', 'B', 'B', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'O', 'B', 'I', 'I', 'B', 'I', 'O', 'B', 'I', 'I', 'O']","['O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['O', 'B-b', 'B-p', 'O', 'B-ob', 'I-ob', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'I-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'O']",16,0.17582417582417584,22,0.09954751131221719,16,0.7272727272727273,1,1,model
25,"After a fixed number of iterations , the model uses a summary of its inference process to predict the answer .",Introduction,Introduction,question-answering,5,"['B', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'B', 'I', 'O', 'B', 'O']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'O']","['B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'O', 'O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'O', 'B-ob', 'O']",18,0.1978021978021978,24,0.1085972850678733,18,0.8181818181818182,1,1,model
118,"To train our model , we used stochastic gradient descent with the ADAM optimizer ( Kingma and Ba , 2014 ) , with an initial learning rate of 0.001 .",Training Details,Training Details,question-answering,5,"['B', 'I', 'B', 'I', 'O', 'O', 'B', 'B', 'I', 'I', 'B', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'B', 'B', 'O']","['B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['B-p', 'I-p', 'B-b', 'I-b', 'O', 'O', 'B-p', 'B-b', 'I-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'O']",1,0.08333333333333333,117,0.5294117647058824,1,0.08333333333333333,1,1,experimental-setup
119,"We set the batch size to 32 and we decay the learning rate by 0.8 if the accuracy on the validation set does not increase after a half - epoch , i.e. 2000 batches ( for CBT ) and 5000 batches for ( CNN ) .",Training Details,Training Details,question-answering,5,"['O', 'B', 'O', 'B', 'I', 'B', 'B', 'O', 'O', 'B', 'O', 'B', 'I', 'B', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-b', 'I-b', 'B-p', 'B-ob', 'O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'B-p', 'B-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.16666666666666666,118,0.5339366515837104,2,0.16666666666666666,1,1,experimental-setup
120,"We initialize all weights of our model by sampling from the normal distribution N ( 0 , 0.05 ) .",Training Details,Training Details,question-answering,5,"['O', 'B', 'O', 'B', 'I', 'I', 'I', 'B', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'I-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",3,0.25,119,0.5384615384615384,3,0.25,1,1,experimental-setup
121,"Following , the GRU recurrent weights are initialized to be orthogonal and biases are initialized to zero .",Training Details,Training Details,question-answering,5,"['O', 'O', 'O', 'B', 'I', 'I', 'B', 'I', 'B', 'I', 'B', 'O', 'B', 'O', 'O', 'O', 'B', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-p', 'I-p', 'B-n', 'O', 'B-n', 'O', 'O', 'O', 'B-n', 'O']","['O', 'O', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'I-p', 'B-p', 'I-p', 'B-ob', 'O', 'B-b', 'O', 'O', 'O', 'B-ob', 'O']",4,0.3333333333333333,120,0.5429864253393665,4,0.3333333333333333,1,1,experimental-setup
122,"In order to stabilize the learning , we clip the gradients if their norm is greater than 5 and those marked with 2 are from .",Training Details,Training Details,question-answering,5,"['O', 'O', 'B', 'I', 'O', 'B', 'O', 'O', 'B', 'O', 'B', 'B', 'O', 'B', 'O', 'B', 'I', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'I-p', 'O', 'B-b', 'O', 'O', 'B-p', 'O', 'B-b', 'B-p', 'O', 'B-b', 'O', 'B-p', 'I-p', 'B-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.4166666666666667,121,0.5475113122171946,5,0.4166666666666667,1,1,experimental-setup
125,"Our model is implemented in Theano , using the Keras library .",Training Details,Training Details,question-answering,5,"['O', 'O', 'O', 'B', 'I', 'B', 'O', 'B', 'O', 'B', 'I', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'B-b', 'O', 'B-p', 'O', 'B-ob', 'I-ob', 'O']",8,0.6666666666666666,124,0.5610859728506787,8,0.6666666666666666,1,1,experimental-setup
135,Our model ( line 7 ) sets anew stateof - the - art on the common noun category by gaining 3.6 and 5.6 points in validation and test over the best baseline AS Reader ( line 5 ) .,Main result,Main result,question-answering,5,"['O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'B', 'O', 'B', 'I', 'I', 'B', 'I', 'B', 'I', 'I', 'I', 'B', 'B', 'I', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'B-p', 'I-p', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'B-b', 'I-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O']",1,0.06666666666666667,134,0.6063348416289592,1,0.07142857142857142,1,1,results
151,The results show that our model ( line 8 ) improves state - of - the - art accuracy by 4 percent absolute on validation and 3.4 on test with respect to the most recent published result ( AS Reader ) ( line 7 ) .,Main result,Main result,question-answering,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'I-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.023255813953488372,150,0.6787330316742082,1,0.05,1,1,results
159,"Similarly to the single model case , our ensembles achieve state - of - the - art test performance of 75.2 and 76.1 on validation and test respectively , outperforming previously published results .",Main result,Ensembles,question-answering,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'I', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'B', 'B', 'I', 'I', 'O']",,,9,0.20930232558139536,158,0.7149321266968326,9,0.45,1,1,results
2,Published as a conference paper at ICLR 2017 QUERY - REDUCTION NETWORKS FOR QUESTION ANSWERING,title,title,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I']",,,1,0.0,1,0.0030303030303030303,1,0.0,1,1,research-problem
4,"In this paper , we study the problem of question answering when reasoning over multiple facts is required .",abstract,abstract,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",1,0.2,3,0.00909090909090909,1,0.2,1,1,research-problem
24,"Our proposed model , Query - Reduction Network 1 ( QRN ) , is a single recurrent unit that addresses the long - term dependency problem of most RNN - based models by simplifying the recurrent update , while taking the advantage of RNN 's capability to model sequential data ) .",INTRODUCTION,INTRODUCTION,question-answering,6,"['O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'B', 'I', 'I', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'B', 'I', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'B', 'I', 'B', 'I', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O']","['O', 'O', 'O', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-b', 'I-b', 'I-b', 'O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'O', 'B-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'I-p', 'B-ob', 'I-ob', 'O', 'O']",15,0.4838709677419355,23,0.0696969696969697,15,0.4838709677419355,1,1,model
25,"QRN considers the context sentences as a sequence of state - changing triggers , and transforms ( reduces ) the original query to a more informed query as it observes each trigger through time .",INTRODUCTION,INTRODUCTION,question-answering,6,"['B', 'B', 'O', 'B', 'I', 'B', 'I', 'I', 'I', 'B', 'I', 'I', 'I', 'O', 'O', 'B', 'I', 'I', 'I', 'O', 'B', 'I', 'B', 'O', 'B', 'I', 'I', 'B', 'I', 'I', 'O', 'B', 'B', 'B', 'O']","['B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'O']","['B-b', 'B-p', 'O', 'B-b', 'I-b', 'B-p', 'I-p', 'I-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-b', 'I-b', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'I-p', 'I-p', 'O', 'B-b', 'B-p', 'B-ob', 'O']",16,0.5161290322580645,24,0.07272727272727272,16,0.5161290322580645,1,1,model
204,We withhold 10 % of the training for development .,Training .,Training .,question-answering,6,"['O', 'B', 'B', 'I', 'I', 'I', 'I', 'B', 'B', 'O']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['O', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'O']",1,0.09090909090909091,203,0.6151515151515151,23,0.2948717948717949,1,1,hyperparameters
205,We use the hidden state size of 50 by deafult .,Training .,Training .,question-answering,6,"['O', 'B', 'O', 'B', 'I', 'I', 'B', 'B', 'B', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'O']","['O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'B-b', 'B-p', 'O', 'O']",2,0.18181818181818182,204,0.6181818181818182,24,0.3076923076923077,1,1,hyperparameters
206,"Batch sizes of 32 for bAbI story - based QA 1k , bAb I dialog and DSTC2 dialog , and 128 for bAbI QA 10 k are used .",Training .,Training .,question-answering,6,"['B', 'I', 'B', 'B', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'O', 'B', 'I', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'O', 'B', 'O']","['B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O']","['B-b', 'I-b', 'B-p', 'B-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'B-ob', 'I-ob', 'I-ob', 'O', 'B-ob', 'I-ob', 'O', 'O', 'B-b', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'B-p', 'O']",3,0.2727272727272727,205,0.6212121212121212,25,0.32051282051282054,1,1,hyperparameters
210,L2 weight decay of 0.001 ( 0.0005 for QA 10 k ) is used for all weights .,Training .,Training .,question-answering,6,"['B', 'I', 'I', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']","['B-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.6363636363636364,209,0.6333333333333333,29,0.3717948717948718,1,1,hyperparameters
211,The loss function is the cross entropy betweenv and the one - hot vector of the true answer .,Training .,Training .,question-answering,6,"['O', 'B', 'I', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-b', 'I-b', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.7272727272727273,210,0.6363636363636364,30,0.38461538461538464,1,1,hyperparameters
212,"The loss is minimized by stochastic gradient descent for maximally 500 epochs , but training is early stopped if the loss on the development data does not decrease for 50 epochs .",Training .,Training .,question-answering,6,"['O', 'B', 'B', 'I', 'I', 'B', 'I', 'I', 'B', 'B', 'I', 'I', 'O', 'O', 'B', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'B', 'I', 'O']","['O', 'B-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'O', 'B-p', 'I-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O']","['O', 'B-b', 'B-p', 'I-p', 'I-p', 'B-b', 'I-b', 'I-b', 'B-p', 'B-b', 'I-b', 'I-b', 'O', 'O', 'B-b', 'O', 'B-p', 'I-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-b', 'I-b', 'I-b', 'O', 'B-ob', 'I-ob', 'O']",9,0.8181818181818182,211,0.6393939393939394,31,0.3974358974358974,1,1,hyperparameters
213,The learning rate is controlled by AdaGrad with the initial learning rate of 0.5 ( 0.1 for QA 10 k ) .,Training .,Training .,question-answering,6,"['O', 'B', 'I', 'O', 'B', 'B', 'B', 'O', 'O', 'B', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'B-n', 'I-n', 'O', 'B-p', 'B-p', 'B-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'B-b', 'I-b', 'O', 'B-p', 'B-p', 'B-b', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",10,0.9090909090909091,212,0.6424242424242425,32,0.41025641025641024,1,1,hyperparameters
220,"In 1 k data , QRN 's ' 2 r' ( 2 layers + reset gate + d = 50 ) outperforms all other models by a large margin ( 2.8 + % ) .",RESULTS .,Story - based QA .,question-answering,6,"['B', 'B', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'B', 'B', 'I', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['B-p', 'B-b', 'I-b', 'I-b', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'B-b', 'I-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",5,0.5,219,0.6636363636363637,39,0.5,1,1,results
221,"In 10 k dataset , the average accuracy of QRN 's ' 6r200 ' ( 6 layers + reset gate + d = 200 ) model outperforms all previous models by a large margin ( 2.5 + % ) , achieving a nearly perfect score of 99.7 % .",RESULTS .,Story - based QA .,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'B', 'B', 'I', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'B-b', 'I-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.6,220,0.6666666666666666,40,0.5128205128205128,1,1,results
225,QRN outperforms previous work by a large margin ( 2.0 + % ) in every comparison .,RESULTS .,Story - based QA .,question-answering,6,"['O', 'B', 'B', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O']",10,1.0,224,0.6787878787878788,44,0.5641025641025641,1,1,results
229,"According to the ablation results , we infer that : ( a ) When the number of layers is only one , the model lacks reasoning capability .",Ablations .,Ablations .,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'B', 'O', 'B', 'O', 'O', 'B', 'B', 'B', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'O', 'B-ob', 'O', 'O', 'B-b', 'B-p', 'B-b', 'I-b', 'O']",3,0.09090909090909091,228,0.6909090909090909,48,0.6153846153846154,1,1,ablation-analysis
232,( b ) Adding the reset gate helps .,Ablations .,Ablations .,question-answering,6,"['O', 'O', 'O', 'B', 'I', 'I', 'I', 'B', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O']","['O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'B-p', 'O']",6,0.18181818181818182,231,0.7,51,0.6538461538461539,1,1,ablation-analysis
233,"( c ) Including vector gates hurts in 1 k datasets , as the model either overfits to the training data or converges to local minima .",Ablations .,Ablations .,question-answering,6,"['O', 'O', 'O', 'B', 'I', 'I', 'B', 'B', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.21212121212121213,232,0.703030303030303,52,0.6666666666666666,1,1,ablation-analysis
234,"On the other hand , vector gates in bAbI story - based QA 10 k dataset sometimes help .",Ablations .,Ablations .,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'B', 'I', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'B', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O']","['O', 'O', 'O', 'O', 'O', 'B-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'B-p', 'I-p', 'O']",8,0.24242424242424243,233,0.706060606060606,53,0.6794871794871795,1,1,ablation-analysis
4,We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders .,abstract,abstract,question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O']",1,0.03571428571428571,3,0.01090909090909091,1,0.014492753623188406,1,1,research-problem
18,In this paper we propose a novel class of memory augmented neural networks called Neural Semantic Encoders ( NSE ) for natural language understanding .,abstract,abstract,question-answering,7,"['O', 'O', 'O', 'O', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'B', 'B', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'O']",15,0.5357142857142857,17,0.06181818181818182,15,0.21739130434782608,1,1,model
20,NSE has a variable sized encoding memory which allows the model to access entire input sequence during the reading process ; therefore efficiently delivering long - term dependencies overtime .,abstract,NSE offers several desirable properties .,question-answering,7,"['B', 'O', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'B', 'I', 'B', 'I', 'I', 'B', 'O', 'B', 'I', 'O', 'O', 'B', 'I', 'B', 'I', 'I', 'I', 'I', 'O']","['B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['B-b', 'O', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-b', 'I-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'O', 'O', 'B-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",17,0.6071428571428571,19,0.06909090909090909,17,0.2463768115942029,1,1,model
21,"The encoding memory evolves overtime and maintains the memory of the input sequence through read , compose and write operations .",abstract,NSE offers several desirable properties .,question-answering,7,"['O', 'B', 'B', 'B', 'B', 'O', 'B', 'O', 'O', 'B', 'O', 'B', 'I', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'O']",,,18,0.6428571428571429,20,0.07272727272727272,18,0.2608695652173913,1,1,model
22,NSE sequentially processes the input and supports word compositionality inheriting both temporal and hierarchical nature of human language .,abstract,NSE offers several desirable properties .,question-answering,7,"['O', 'B', 'I', 'B', 'I', 'O', 'B', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'I-p', 'B-ob', 'I-ob', 'O', 'B-p', 'B-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.6785714285714286,21,0.07636363636363637,19,0.2753623188405797,1,1,model
23,NSE can read from and write to a set of relevant encoding memories simultaneously or multiple NSEs can access a shared encoding memory effectively supporting knowledge and representation sharing .,abstract,NSE offers several desirable properties .,question-answering,7,"['O', 'O', 'B', 'I', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'O', 'B', 'B', 'I', 'I', 'I', 'O']","['O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'O', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",20,0.7142857142857143,22,0.08,20,0.2898550724637681,1,1,model
24,"NSE is flexible , robust and suitable for practical NLU tasks and can be trained easily by any gradient descent optimizer .",abstract,NSE offers several desirable properties .,question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.75,23,0.08363636363636363,21,0.30434782608695654,1,1,research-problem
127,The models are trained using Adam with hyperparameters selected on development set .,Experiments,Experiments,question-answering,7,"['O', 'O', 'O', 'B', 'I', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'B-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.02857142857142857,126,0.4581818181818182,4,0.36363636363636365,1,1,experimental-setup
128,We chose two one - layer LSTM for read / write modules on the tasks other than QA on which we used two - layer LSTM .,Experiments,Experiments,question-answering,7,"['O', 'B', 'B', 'I', 'I', 'I', 'I', 'B', 'B', 'I', 'I', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.03571428571428571,127,0.4618181818181818,5,0.45454545454545453,1,1,experimental-setup
129,The pre-trained 300 - D Glove 840B vectors and 100 - D Glove 6B vectors were obtained for the word embeddings .,Experiments,Experiments,question-answering,7,"['O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'O', 'B', 'I', 'O', 'B', 'I', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'B-p', 'I-p', 'O', 'B-b', 'I-b', 'O']",6,0.04285714285714286,128,0.46545454545454545,6,0.5454545454545454,1,1,experimental-setup
132,We crop or pad the input sequence to a fixed length .,Experiments,The word embeddings are fixed during training .,question-answering,7,"['O', 'B', 'I', 'I', 'O', 'B', 'I', 'B', 'O', 'B', 'I', 'O']","['O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['O', 'B-p', 'I-p', 'I-p', 'O', 'B-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'O']",9,0.06428571428571428,131,0.4763636363636364,9,0.8181818181818182,1,1,experimental-setup
134,The models were regularized by using dropouts and an l 2 weight decay .,Experiments,A padding vector was inserted when padding .,question-answering,7,"['O', 'O', 'O', 'B', 'I', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.07857142857142857,133,0.48363636363636364,11,1.0,1,1,experimental-setup
142,"In addition , the MLP has a hidden layer with 1024 units with ReLU activation and a sof tmax layer .",Experiments,A padding vector was inserted when padding .,question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'B', 'B', 'I', 'O', 'B', 'I', 'O', 'O', 'B', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-p', 'B-b', 'I-b', 'O', 'B-ob', 'I-ob', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'O']",19,0.1357142857142857,141,0.5127272727272727,7,0.2692307692307692,1,1,tasks
143,"We set the batch size to 128 , the initial learning rate to 3e - 4 and l 2 regularizer strength to 3 e - 5 , and train each model for 40 epochs .",Experiments,A padding vector was inserted when padding .,question-answering,7,"['O', 'O', 'O', 'B', 'I', 'O', 'B', 'O', 'O', 'B', 'I', 'I', 'O', 'B', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'B', 'O', 'O', 'O', 'B', 'I', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'O', 'O', 'B-n', 'I-n', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-ob', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'B-p', 'O', 'O', 'O', 'B-ob', 'I-ob', 'O']",20,0.14285714285714285,142,0.5163636363636364,8,0.3076923076923077,1,1,tasks
158,Our MMA - NSE attention model is similar to the LSTM attention model .,Experiments,A padding vector was inserted when padding .,question-answering,7,"['O', 'B', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",35,0.25,157,0.5709090909090909,23,0.8846153846153846,1,1,tasks
160,This model obtained 85.4 % accuracy score .,Experiments,,question-answering,7,"['O', 'O', 'B', 'B', 'I', 'I', 'I', 'O']","['O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",37,0.2642857142857143,159,0.5781818181818181,25,0.9615384615384616,1,1,tasks
173,"We set the batch size to 4 and the initial learning rate to 1 e - 5 , and train the model for 10 epochs .",Experiments,We experiment on WikiQA dataset constructed from Wikipedia .,question-answering,7,"['O', 'O', 'O', 'B', 'I', 'O', 'B', 'O', 'O', 'B', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'B', 'I', 'I', 'I', 'B', 'I', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-ob', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-ob', 'I-ob', 'O']",50,0.35714285714285715,172,0.6254545454545455,11,0.4782608695652174,1,1,tasks
174,We used 40 % dropouts afterword embeddings and no l 2 weight decay .,Experiments,We experiment on WikiQA dataset constructed from Wikipedia .,question-answering,7,"['O', 'B', 'B', 'I', 'I', 'B', 'B', 'O', 'B', 'B', 'I', 'I', 'I', 'O']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'B-p', 'B-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'O', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",51,0.36428571428571427,173,0.6290909090909091,12,0.5217391304347826,1,1,tasks
175,The word embeddings are pre-trained 300 - D Glove 840B vectors .,Experiments,We experiment on WikiQA dataset constructed from Wikipedia .,question-answering,7,"['O', 'B', 'I', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'B-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",52,0.37142857142857144,174,0.6327272727272727,13,0.5652173913043478,1,1,tasks
176,"For this task , a linear mapping layer transforms the 300 - D word embeddings to the 512- D LSTM inputs .",Experiments,We experiment on WikiQA dataset constructed from Wikipedia .,question-answering,7,"['O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'O', 'O', 'B', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",53,0.37857142857142856,175,0.6363636363636364,14,0.6086956521739131,1,1,tasks
181,Our MMA - NSE attention model exceeds the NASM by approximately 1 % on MAP and 0.8 % on MRR for this task .,Experiments,We experiment on WikiQA dataset constructed from Wikipedia .,question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'B', 'I', 'O', 'O', 'B', 'B', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'O', 'O', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-b', 'B-p', 'I-p', 'O', 'O', 'B-p', 'B-ob', 'O', 'O', 'O', 'O', 'B-ob', 'O', 'O', 'O', 'O']",58,0.4142857142857143,180,0.6545454545454545,19,0.8260869565217391,1,1,tasks
191,The first layer of the MLP has ReLU activation and 1024 or 300 units for binary or fine - grained setting .,Experiments,Sentence Classification,question-answering,7,"['O', 'B', 'I', 'I', 'I', 'I', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'O', 'B-ob', 'I-ob', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",68,0.4857142857142857,190,0.6909090909090909,5,0.38461538461538464,1,1,tasks
192,The second layer is a sof tmax layer .,Experiments,,question-answering,7,"['O', 'B', 'I', 'O', 'O', 'B', 'I', 'I', 'O']","['O', 'B-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']","['O', 'B-p', 'I-p', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'O']",69,0.4928571428571429,191,0.6945454545454546,6,0.46153846153846156,1,1,tasks
193,The read / write modules are two one - layer LSTM with 300 hidden units and the word embeddings are the pre-trained 300 - D Glove 840B vectors .,Experiments,The second layer is a sof tmax layer .,question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'I', 'I', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'O', 'O', 'B-p', 'I-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",70,0.5,192,0.6981818181818182,7,0.5384615384615384,1,1,tasks
194,"We set the batch size to 64 , the initial learning rate to 3e - 4 and l 2 regularizer strength to 3 e - 5 , and train each model for 25 epochs .",Experiments,The second layer is a sof tmax layer .,question-answering,7,"['O', 'O', 'O', 'B', 'I', 'O', 'B', 'O', 'O', 'B', 'I', 'I', 'O', 'B', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'B', 'O', 'O', 'O', 'B', 'I', 'O']",,,71,0.5071428571428571,193,0.7018181818181818,8,0.6153846153846154,1,1,tasks
199,Our model outperformed the DMN and set the state - of - the - art results on both subtasks .,Experiments,The second layer is a sof tmax layer .,question-answering,7,"['B', 'I', 'B', 'O', 'B', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'B', 'B', 'I', 'O']","['B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['B-b', 'I-b', 'B-p', 'O', 'B-ob', 'O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'O']",76,0.5428571428571428,198,0.72,13,1.0,1,1,tasks
204,We stack a NSE or LSTM on the top of another NSE for document modeling .,Experiments,"Particularly , we used the pre-split datasets of .",question-answering,7,"['O', 'B', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'B', 'I', 'B', 'B', 'I', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'O', 'O', 'O', 'O', 'B-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'O']",81,0.5785714285714286,203,0.7381818181818182,4,0.2,1,1,tasks
207,The whole network is trained jointly by backpropagating the cross entropy loss .,Experiments,"Particularly , we used the pre-split datasets of .",question-answering,7,"['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'O']","['B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['B-b', 'I-b', 'I-b', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'O']",84,0.6,206,0.7490909090909091,7,0.35,1,1,tasks
208,We used one - layer LSTM with 100 hidden units for the read / write modules and the pre-trained 100 - D Glove 6B vectors for this task .,Experiments,"Particularly , we used the pre-split datasets of .",question-answering,7,"['O', 'B', 'B', 'I', 'I', 'I', 'B', 'B', 'I', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'B-b', 'I-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O']",85,0.6071428571428571,207,0.7527272727272727,8,0.4,1,1,tasks
209,"We set the batch size to 32 , the initial learning rate to 3e - 4 and l 2 regularizer strength to 1 e - 5 , and trained each model for 50 epochs .",Experiments,"Particularly , we used the pre-split datasets of .",question-answering,7,"['O', 'O', 'O', 'B', 'I', 'O', 'B', 'O', 'O', 'B', 'I', 'I', 'O', 'B', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'B', 'O', 'O', 'O', 'B', 'I', 'O']",,,86,0.6142857142857143,208,0.7563636363636363,9,0.45,1,1,tasks
241,The models were trained to minimize word - level cross entropy loss and were regularized by 20 % input dropouts and the 30 % output dropouts .,Experiments,"For NTM , we implemented three different models .",question-answering,7,"['O', 'O', 'O', 'B', 'I', 'I', 'B', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'B', 'I', 'B', 'I', 'I', 'I', 'O', 'O', 'B', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'B-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",118,0.8428571428571429,240,0.8727272727272727,20,0.8695652173913043,1,1,tasks
242,"We set the batch size to 128 , the initial learning rate to 1e - 3 for LSTM - LSTM and 3e - 4 for the other models and l 2 regularizer strength to 3 e - 5 , and train each model for 40 epochs .",Experiments,"For NTM , we implemented three different models .",question-answering,7,"['O', 'O', 'O', 'B', 'I', 'O', 'B', 'O', 'O', 'B', 'I', 'I', 'O', 'B', 'I', 'I', 'B', 'B', 'I', 'I', 'O', 'B', 'I', 'I', 'O', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'B', 'O', 'O', 'O', 'B', 'I', 'O']",,,119,0.85,241,0.8763636363636363,21,0.9130434782608695,1,1,tasks
2,MACHINE COMPREHENSION USING MATCH - LSTM AND ANSWER POINTER,,,question-answering,8,"['B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",,,2,0.2,1,0.004016064257028112,1,0.0,1,1,research-problem
4,Machine comprehension of text is an important problem in natural language processing .,,MACHINE COMPREHENSION USING MATCH - LSTM AND ANSWER POINTER,question-answering,8,"['B', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.4,3,0.012048192771084338,1,0.14285714285714285,1,1,research-problem
38,"In this paper , we propose anew end - to - end neural architecture to address the machine comprehension problem as defined in the SQuAD dataset .",INTRODUCTION,INTRODUCTION,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'B', 'I', 'O', 'B', 'I', 'I', 'B', 'I', 'I', 'O', 'B', 'I', 'O']",,,27,0.6923076923076923,37,0.14859437751004015,27,0.6923076923076923,1,1,
39,"Specifically , observing that in the SQuAD dataset many questions are paraphrases of sentences from the original text , we adopt a match - LSTM model that we developed earlier for textual entailment .",INTRODUCTION,INTRODUCTION,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",,,28,0.717948717948718,38,0.15261044176706828,28,0.717948717948718,1,1,
40,"We further adopt the Pointer Net ( Ptr - Net ) model developed by , which enables the predictions of tokens from the input sequence only rather than from a larger fixed vocabulary and thus allows us to generate answers that consist of multiple tokens from the original text .",INTRODUCTION,INTRODUCTION,question-answering,8,"['O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'O', 'B', 'I', 'B', 'I', 'O', 'O', 'O', 'O', 'O']",,,29,0.7435897435897436,39,0.1566265060240964,29,0.7435897435897436,1,1,
41,We propose two ways to apply the Ptr - Net model for our task : a sequence model and a boundary model .,INTRODUCTION,INTRODUCTION,question-answering,8,"['O', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'B', 'I', 'O']",,,30,0.7692307692307693,40,0.1606425702811245,30,0.7692307692307693,1,1,
42,We also further extend the boundary model with a search mechanism .,INTRODUCTION,INTRODUCTION,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'O']",,,31,0.7948717948717948,41,0.1646586345381526,31,0.7948717948717948,1,1,
176,We use word embeddings from GloVe to initialize the model .,EXPERIMENT SETTINGS,EXPERIMENT SETTINGS,question-answering,8,"['O', 'B', 'B', 'I', 'O', 'B', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-b', 'I-b', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O']",3,0.25,175,0.7028112449799196,3,0.25,1,1,hyperparameters
179,The dimensionality l of the hidden layers is set to be 150 or 300 .,EXPERIMENT SETTINGS,EXPERIMENT SETTINGS,question-answering,8,"['O', 'B', 'I', 'O', 'O', 'B', 'I', 'O', 'B', 'O', 'O', 'B', 'I', 'I', 'O']","['O', 'B-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']","['O', 'B-p', 'I-p', 'O', 'O', 'B-b', 'I-b', 'O', 'B-p', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'O']",6,0.5,178,0.714859437751004,6,0.5,1,1,hyperparameters
180,We use ADAMAX with the coefficients ? 1 = 0.9 and ? 2 = 0.999 to optimize the model .,EXPERIMENT SETTINGS,EXPERIMENT SETTINGS,question-answering,8,"['O', 'O', 'B', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'B', 'I', 'O', 'B', 'O']","['O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'O']","['O', 'O', 'B-b', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'O', 'B-ob', 'O']",7,0.5833333333333334,179,0.7188755020080321,7,0.5833333333333334,1,1,hyperparameters
181,Each update is computed through a minibatch of 30 instances .,EXPERIMENT SETTINGS,EXPERIMENT SETTINGS,question-answering,8,"['O', 'B', 'O', 'B', 'I', 'O', 'B', 'B', 'B', 'I', 'O']","['O', 'B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['O', 'B-b', 'O', 'B-p', 'I-p', 'O', 'B-b', 'B-p', 'B-ob', 'I-ob', 'O']",8,0.6666666666666666,180,0.7228915662650602,8,0.6666666666666666,1,1,hyperparameters
191,"outperformed the logistic regression model by , which relies on carefully designed features .",RESULTS,RESULTS,question-answering,8,"['B', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'B', 'I', 'B', 'I', 'I', 'O']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['B-p', 'O', 'B-b', 'I-b', 'I-b', 'O', 'O', 'O', 'B-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'O']",5,0.1282051282051282,190,0.7630522088353414,5,0.3125,1,1,results
192,"Furthermore , our boundary model has outperformed the sequence model , achieving an exact match score of 61.1 % and an F1 score of 71.2 % .",RESULTS,RESULTS,question-answering,8,"['O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'B', 'I', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-b', 'I-b', 'O', 'O', 'O', 'B-b', 'I-b', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.15384615384615385,191,0.7670682730923695,6,0.375,1,1,results
200,"While by adding Bi - Ans - Ptr with bi-directional pre-processing LSTM , we can get 1.2 % improvement in F1 .",RESULTS,RESULTS,question-answering,8,"['O', 'B', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'B', 'B', 'I', 'I', 'I', 'I', 'O']",,,14,0.358974358974359,199,0.7991967871485943,14,0.875,1,1,results
2,LEARNING RECURRENT SPAN REPRESENTATIONS FOR EXTRACTIVE QUESTION ANSWERING,,,question-answering,9,"['O', 'O', 'O', 'O', 'O', 'B', 'I', 'I']",,,2,0.13333333333333333,1,0.0056179775280898875,1,0.0,1,1,research-problem
4,"The reading comprehension task , that asks questions about a given evidence document , is a central problem in natural language understanding .",,LEARNING RECURRENT SPAN REPRESENTATIONS FOR EXTRACTIVE QUESTION ANSWERING,question-answering,9,"['O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.26666666666666666,3,0.016853932584269662,1,0.08333333333333333,1,1,research-problem
17,A primary goal of natural language processing is to develop systems that can answer questions about the contents of documents .,INTRODUCTION,INTRODUCTION,question-answering,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",1,0.0625,16,0.0898876404494382,1,0.09090909090909091,1,1,research-problem
24,"To overcome this , we present a novel neural architecture called RASOR that builds fixed - length span representations , reusing recurrent computations for shared substructures .",INTRODUCTION,INTRODUCTION,question-answering,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'O', 'B', 'B', 'I', 'I', 'I', 'I', 'O', 'B', 'B', 'I', 'B', 'B', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-b', 'O', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O', 'B-p', 'B-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'O']",8,0.5,23,0.12921348314606743,8,0.7272727272727273,1,1,model
25,"We demonstrate that directly classifying each of the competing spans , and training with global normalization overall possible spans , leads to a significant increase in performance .",INTRODUCTION,INTRODUCTION,question-answering,9,"['O', 'B', 'O', 'B', 'I', 'B', 'I', 'I', 'I', 'I', 'O', 'O', 'B', 'I', 'B', 'I', 'B', 'B', 'I', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'O']","['O', 'B-p', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'B-p', 'O', 'B-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'B-p', 'I-p', 'B-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'O', 'O', 'O', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'O']",9,0.5625,24,0.1348314606741573,9,0.8181818181818182,1,1,model
106,We represent each of the words in the question and document using 300 dimensional GloVe embeddings trained on a corpus of 840 bn words .,EXPERIMENTAL SETUP,EXPERIMENTAL SETUP,question-answering,9,"['O', 'B', 'B', 'I', 'I', 'I', 'B', 'O', 'B', 'I', 'I', 'B', 'B', 'I', 'I', 'I', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'O']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'O', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",1,0.125,105,0.5898876404494382,1,0.125,1,1,experimental-setup
108,"We couple the input and forget gates in our LSTMs , as described in , and we use a single dropout mask to apply dropout across all LSTM time - steps as proposed by .",EXPERIMENTAL SETUP,EXPERIMENTAL SETUP,question-answering,9,"['O', 'B', 'O', 'B', 'I', 'I', 'I', 'B', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'O', 'B', 'I', 'B', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'O', 'B-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'O', 'B-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O']",3,0.375,107,0.601123595505618,3,0.375,1,1,experimental-setup
109,Hidden layers in the feed forward neural networks use rectified linear units .,EXPERIMENTAL SETUP,EXPERIMENTAL SETUP,question-answering,9,"['B', 'I', 'O', 'O', 'B', 'I', 'I', 'I', 'B', 'B', 'I', 'I', 'O']","['B-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['B-p', 'I-p', 'O', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'O']",4,0.5,108,0.6067415730337079,4,0.5,1,1,experimental-setup
111,"To choose the final model configuration , we ran grid searches over : the dimensionality of the LSTM hidden states ; the width and depth of the feed forward neural networks ; dropout for the LSTMs ; the number of stacked LSTM layers ; and the decay multiplier [ 0.9 , 0.95 , 1.0 ] with which we multiply the learning rate every 10 k steps .",EXPERIMENTAL SETUP,EXPERIMENTAL SETUP,question-answering,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'I', 'O', 'O', 'O', 'B', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'B', 'I', 'I', 'I', 'O', 'B', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'B', 'B', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-b', 'I-b', 'O', 'O', 'O', 'B-p', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'B-p', 'O', 'O', 'B-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O', 'O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'O']",6,0.75,110,0.6179775280898876,6,0.75,1,1,experimental-setup
112,The best model uses 50d LSTM states ; two - layer BiLSTMs for the span encoder and the passage - independent question representation ; dropout of 0.1 throughout ; and a learning rate decay of 5 % every 10 k steps .,EXPERIMENTAL SETUP,EXPERIMENTAL SETUP,question-answering,9,"['O', 'B', 'I', 'B', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'B', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O']","['O', 'B-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-b', 'B-p', 'B-ob', 'O', 'O', 'O', 'O', 'B-b', 'I-b', 'I-b', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O']",7,0.875,111,0.6235955056179775,7,0.875,1,1,experimental-setup
113,All models are implemented using TensorFlow 3 and trained on the SQUAD training set using the ADAM optimizer with a mini-batch size of 4 and trained using 10 asynchronous training threads on a single machine .,EXPERIMENTAL SETUP,EXPERIMENTAL SETUP,question-answering,9,"['O', 'O', 'O', 'B', 'B', 'B', 'I', 'O', 'B', 'B', 'O', 'B', 'I', 'I', 'O', 'O', 'B', 'I', 'B', 'O', 'B', 'I', 'B', 'B', 'O', 'B', 'I', 'B', 'I', 'I', 'I', 'O', 'O', 'B', 'I', 'O']",,,8,1.0,112,0.6292134831460674,8,1.0,1,1,experimental-setup
121,"Despite not having access to any external representation of linguistic structure , RASOR achieves an error reduction of more than 50 % over this baseline , both in terms of exact match and F1 , relative to the human performance upper bound .",RESULTS,COMPARISONS TO OTHER WORK,question-answering,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'O', 'B', 'I', 'B', 'I', 'I', 'B', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-b', 'B-p', 'O', 'B-b', 'I-b', 'B-p', 'I-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.6363636363636364,120,0.6741573033707865,4,0.5,1,1,results
125,"In contrast , RASOR can efficiently and explicitly model the quadratic number of possible answers , which leads to a 14 % error reduction over the best performing Match - LSTM model .",RESULTS,COMPARISONS TO OTHER WORK,question-answering,9,"['O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'O', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O', 'O', 'B-p', 'I-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",11,1.0,124,0.6966292134831461,8,1.0,1,1,results
131,"The passage - aligned question representation is crucial , since lexically similar regions of the passage provide strong signal for relevant answer spans .",MODEL VARIATIONS,MODEL VARIATIONS,question-answering,9,"['O', 'B', 'I', 'I', 'I', 'I', 'B', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.10869565217391304,130,0.7303370786516854,5,0.625,1,1,ablation-analysis
157,"First , we observe general improvements when using labels that closely align with the task .",MODEL VARIATIONS,MODEL VARIATIONS,question-answering,9,"['O', 'O', 'O', 'B', 'B', 'I', 'B', 'I', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'B-p', 'B-b', 'I-b', 'B-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",31,0.6739130434782609,156,0.8764044943820225,19,0.5757575757575758,1,1,ablation-analysis
162,"Second , we observe the importance of allowing interactions between the endpoints using the spanlevel FFNN .",MODEL VARIATIONS,MODEL VARIATIONS,question-answering,9,"['O', 'O', 'O', 'B', 'I', 'I', 'I', 'B', 'I', 'B', 'B', 'I', 'B', 'O', 'B', 'I', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-b', 'I-b', 'B-p', 'B-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'O']",36,0.782608695652174,161,0.9044943820224719,24,0.7272727272727273,1,1,ablation-analysis
163,"RASOR outperforms the endpoint prediction model by 1.1 inexact match , The interaction between endpoints enables RASOR to enforce consistency across its two substructures .",MODEL VARIATIONS,MODEL VARIATIONS,question-answering,9,"['B', 'B', 'O', 'B', 'I', 'I', 'B', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-b', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",37,0.8043478260869565,162,0.9101123595505618,25,0.7575757575757576,1,1,ablation-analysis
2,End - to - End Relation Extraction using LSTMs on Sequences and Tree Structures,title,title,relation-classification,0,"['B', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",,,1,0.0,1,0.004424778761061947,1,0.0,1,1,research-problem
4,We present a novel end - to - end neural model to extract entities and relations between them .,abstract,abstract,relation-classification,0,"['O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",1,0.14285714285714285,3,0.01327433628318584,1,0.14285714285714285,1,1,research-problem
6,This allows our model to jointly represent both entities and relations with shared parameters in a single model .,abstract,abstract,relation-classification,0,"['O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",3,0.42857142857142855,5,0.022123893805309734,3,0.42857142857142855,1,1,research-problem
12,Extracting semantic relations between entities in text is an important and well - studied task in information extraction and natural language processing ( NLP ) .,Introduction,Introduction,relation-classification,0,"['B', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.043478260869565216,11,0.048672566371681415,1,0.043478260869565216,1,1,research-problem
13,"Traditional systems treat this task as a pipeline of two separated tasks , i.e. , named entity recognition ( NER ) ) and relation extraction , but recent studies show that end - to - end ( joint ) modeling of entity and relation is important for high performance since relations interact closely with entity information .",Introduction,Introduction,relation-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.08695652173913043,12,0.05309734513274336,2,0.08695652173913043,1,1,research-problem
27,We present a novel end - to - end model to extract relations between entities on both word sequence and dependency tree structures .,Introduction,Introduction,relation-classification,0,"['O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'B', 'I', 'B', 'I', 'I', 'B', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O']","['O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'B-b', 'I-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'O', 'B-ob', 'I-ob', 'I-ob', 'O']",16,0.6956521739130435,26,0.11504424778761062,16,0.6956521739130435,1,1,model
28,Our model allows joint modeling of entities and relations in a single model by using both bidirectional sequential ( left - to - right and right - to - left ) and bidirectional tree - structured ( bottom - up and top - down ) LSTM - RNNs .,Introduction,Introduction,relation-classification,0,"['O', 'O', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.7391304347826086,27,0.11946902654867257,17,0.7391304347826086,1,1,model
29,"Our model first detects entities and then extracts relations between the detected entities using a single incrementally - decoded NN structure , and the NN parameters are jointly updated using both entity and relation labels .",Introduction,Introduction,relation-classification,0,"['O', 'O', 'O', 'B', 'B', 'O', 'B', 'I', 'B', 'B', 'O', 'B', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'B', 'I', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'B-p', 'B-b', 'O', 'B-p', 'I-p', 'B-b', 'B-p', 'O', 'B-ob', 'I-ob', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O', 'O', 'O', 'B-b', 'I-b', 'O', 'B-p', 'I-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",18,0.782608695652174,28,0.12389380530973451,18,0.782608695652174,1,1,model
30,"Unlike traditional incremental end - to - end relation extraction models , our model further incorporates two enhancements into training : entity pretraining , which pretrains the entity model , and scheduled sampling , which replaces ( unreliable ) predicted labels with gold labels in a certain probability .",Introduction,Introduction,relation-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'I', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-b', 'I-b', 'O', 'O', 'O', 'B-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.8260869565217391,29,0.12831858407079647,19,0.8260869565217391,1,1,model
31,"These enhancements alleviate the problem of low - performance entity detection in early stages of training , as well as allow entity information to further help downstream relation classification .",Introduction,Introduction,relation-classification,0,"['O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",20,0.8695652173913043,30,0.13274336283185842,20,0.8695652173913043,1,1,model
154,We implemented our model using the cnn library .,Experimental Settings,Experimental Settings,relation-classification,0,"['O', 'B', 'B', 'I', 'B', 'O', 'B', 'I', 'O']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['O', 'B-p', 'B-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'O']",1,0.023809523809523808,153,0.6769911504424779,1,0.023809523809523808,1,1,experimental-setup
155,"We parsed the texts using the Stanford neural dependency parser 7 ( Chen and Manning , 2014 ) with the original Stanford Dependencies .",Experimental Settings,Experimental Settings,relation-classification,0,"['O', 'B', 'O', 'B', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'B', 'O', 'B', 'I', 'I', 'O']","['O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['O', 'B-p', 'O', 'B-b', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'O']",2,0.047619047619047616,154,0.6814159292035398,2,0.047619047619047616,1,1,experimental-setup
156,"Based on preliminary tuning , we fixed embedding dimensions n w to 200 , n p , n d , n e to 25 , and dimensions of intermediate layers ( n ls , n lt of LSTM - RNNs and n he , n hr of hidden layers ) to 100 .",Experimental Settings,Experimental Settings,relation-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'I', 'B', 'I', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'B', 'O', 'O', 'B', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'B', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'O', 'O', 'B-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-b', 'I-b', 'B-p', 'I-p', 'O', 'B-ob', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'O', 'B-ob', 'O', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'O', 'O', 'B-ob', 'O']",3,0.07142857142857142,155,0.6858407079646017,3,0.07142857142857142,1,1,experimental-setup
157,We initialized word vectors via word2 vec trained on Wikipedia 8 and randomly initialized all other parameters .,Experimental Settings,Experimental Settings,relation-classification,0,"['O', 'B', 'B', 'I', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-b', 'I-b', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.09523809523809523,156,0.6902654867256637,4,0.09523809523809523,1,1,experimental-setup
165,"Table 1 compares our model with the state - of - theart feature - based model of on final test sets , and shows that our model performs better than the state - of - the - art model .",Experimental Settings,For SemEval-2010,relation-classification,0,"['O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'B-b', 'I-b', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",12,0.2857142857142857,164,0.7256637168141593,12,0.2857142857142857,1,1,results
166,"To analyze the contributions and effects of the various components of our end - to - end relation extraction model , we perform ablation tests on the ACE05 development set ( ) .",Experimental Settings,For SemEval-2010,relation-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'O', 'O', 'O']",13,0.30952380952380953,165,0.7300884955752213,13,0.30952380952380953,1,1,ablation-analysis
167,"The performance slightly degraded without scheduled sampling , and the performance significantly degraded when we removed entity pretraining or removed both ( p < 0.05 ) .",Experimental Settings,For SemEval-2010,relation-classification,0,"['O', 'B', 'I', 'I', 'B', 'I', 'I', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'I-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.3333333333333333,166,0.7345132743362832,14,0.3333333333333333,1,1,ablation-analysis
180,"When we removed all the enhancements , i.e. , scheduled sampling , entity pretraining , label embedding , and shared parameters , the performance is significantly worse than SP - Tree ( p < 0.01 ) , showing that these enhancements provide complementary benefits to end - to - end relation extraction .",Experimental Settings,For SemEval-2010,relation-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'B', 'I', 'O', 'B', 'I', 'O', 'O', 'B', 'I', 'O', 'O', 'B', 'I', 'I', 'I', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'O', 'B-ob', 'I-ob', 'O', 'B-ob', 'I-ob', 'O', 'O', 'B-ob', 'I-ob', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.6428571428571429,179,0.7920353982300885,27,0.6428571428571429,1,1,ablation-analysis
2,Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme,title,title,relation-classification,1,"['B', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'B', 'I']",,,1,0.0,1,0.0040650406504065045,1,0.0,1,1,research-problem
10,"Joint extraction of entities and relations is to detect entity mentions and recognize their semantic relations simultaneously from unstructured text , as shows .",Introduction,Introduction,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O']",1,0.025,9,0.036585365853658534,1,0.025,1,1,research-problem
17,"Different from the pipelined methods , joint learning framework is to extract entities together with relations using a single model .",Introduction,Introduction,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",8,0.2,16,0.06504065040650407,8,0.2,1,1,research-problem
67,We propose a novel tagging scheme and an end -toend model with biased objective function to jointly extract entities and their relations .,Method,Method,relation-classification,1,"['O', 'B', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'I', 'I', 'B', 'I', 'I', 'B', 'I', 'I', 'I', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-b', 'I-b', 'I-b', 'B-p', 'I-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",1,0.014492753623188406,66,0.2682926829268293,1,0.07142857142857142,1,1,approach
72,"Tag "" O "" represents the "" Other "" tag , which means that the corresponding word is independent of the extracted results .",Method,Method,relation-classification,1,"['O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'B', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']",,,6,0.08695652173913043,71,0.2886178861788618,6,0.42857142857142855,1,1,approach
74,"We use the "" BIES "" ( Begin , Inside , End , Single ) signs to represent the position information of a word in the entity .",Method,Method,relation-classification,1,"['O', 'B', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'B', 'I', 'B', 'B', 'O', 'B', 'O']","['O', 'B-p', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'O']","['O', 'B-p', 'O', 'O', 'B-b', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'B-p', 'I-p', 'B-b', 'B-p', 'O', 'B-ob', 'O']",8,0.11594202898550725,73,0.2967479674796748,8,0.5714285714285714,1,1,approach
99,"In this paper , we investigate an end - toend model to produce the tags sequence as shows .",Method,From Tag Sequence To Extracted Results,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-ob', 'I-ob', 'O', 'O', 'O']",33,0.4782608695652174,98,0.3983739837398374,12,0.25,1,1,approach
100,It contains a bi-directional Long Short Term Memory ( Bi - LSTM ) layer to encode the input sentence and a LSTM - based decoding layer with biased loss .,Method,From Tag Sequence To Extracted Results,relation-classification,1,"['O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'B', 'I', 'O', 'B', 'I', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'B', 'B', 'I', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'O', 'B-ob', 'I-ob', 'O', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'B-b', 'I-b', 'O']",34,0.4927536231884058,99,0.4024390243902439,13,0.2708333333333333,1,1,approach
101,The biased loss can enhance the relevance of entity tags .,Method,From Tag Sequence To Extracted Results,relation-classification,1,"['O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",35,0.5072463768115942,100,0.4065040650406504,14,0.2916666666666667,1,1,approach
154,The word embeddings used in the encoding part are initialed by running word2vec 3 on NYT training corpus .,Hyperparameters,Hyperparameters,relation-classification,1,"['O', 'B', 'I', 'B', 'I', 'O', 'B', 'I', 'O', 'B', 'I', 'B', 'I', 'I', 'B', 'B', 'I', 'I', 'O']","['O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['O', 'B-b', 'I-b', 'B-p', 'I-p', 'O', 'B-b', 'I-b', 'O', 'B-p', 'I-p', 'B-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'O']",2,0.13333333333333333,153,0.6219512195121951,17,0.40476190476190477,1,1,hyperparameters
155,The dimension of the word embeddings is d = 300 .,Hyperparameters,Hyperparameters,relation-classification,1,"['O', 'B', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O']","['O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']","['O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'O']",3,0.2,154,0.6260162601626016,18,0.42857142857142855,1,1,hyperparameters
156,We regularize our network using dropout on embedding layer and the dropout ratio is 0.5 .,Hyperparameters,Hyperparameters,relation-classification,1,"['O', 'B', 'O', 'B', 'B', 'B', 'B', 'B', 'I', 'O', 'O', 'O', 'B', 'I', 'B', 'O']","['O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O']","['O', 'B-p', 'O', 'B-b', 'B-p', 'B-b', 'B-p', 'B-ob', 'I-ob', 'O', 'O', 'O', 'B-p', 'I-p', 'B-ob', 'O']",4,0.26666666666666666,155,0.6300813008130082,19,0.4523809523809524,1,1,hyperparameters
157,The number of lstm units in encoding layer is 300 and the number in decoding layer is 600 .,Hyperparameters,Hyperparameters,relation-classification,1,"['O', 'B', 'I', 'B', 'I', 'B', 'I', 'I', 'O', 'B', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'B', 'O']","['O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'O']","['O', 'B-p', 'I-p', 'B-b', 'I-b', 'B-p', 'I-p', 'I-p', 'O', 'B-ob', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-ob', 'O']",5,0.3333333333333333,156,0.6341463414634146,20,0.47619047619047616,1,1,hyperparameters
158,The bias parameter ?,Hyperparameters,Hyperparameters,relation-classification,1,"['O', 'B', 'I', 'I']","['O', 'B-p', 'I-p', 'I-p']","['O', 'B-p', 'I-p', 'I-p']",6,0.4,157,0.6382113821138211,21,0.5,1,1,hyperparameters
159,corresponding to the results in is 10 .,Hyperparameters,Hyperparameters,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'B', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'O']",7,0.4666666666666667,158,0.6422764227642277,22,0.5238095238095238,1,1,hyperparameters
169,"We compare our method with several classical triplet extraction methods , which can be divided into the following categories : the pipelined methods , the jointly extracting methods and the end - to - end methods based our tagging scheme .",Baselines,Baselines,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",,,1,0.09090909090909091,168,0.6829268292682927,32,0.7619047619047619,1,1,
170,"For the pipelined methods , we follow ) 's settings :",Baselines,Baselines,relation-classification,1,"['O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",,,2,0.18181818181818182,169,0.6869918699186992,33,0.7857142857142857,1,1,
171,The NER results are obtained by CoType then several classical relation classification methods are applied to detect the relations .,Baselines,Baselines,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.2727272727272727,170,0.6910569105691057,34,0.8095238095238095,1,1,
172,These methods are :,Baselines,Baselines,relation-classification,1,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",4,0.36363636363636365,171,0.6951219512195121,35,0.8333333333333334,1,1,
173,"( 1 ) DS-logistic ) is a distant supervised and feature based method , which combines the advantages of supervised IE and unsupervised IE features ; ( 2 ) LINE is a network embedding method , which is suitable for arbitrary types of information networks ;",Baselines,Baselines,relation-classification,1,"['O', 'O', 'O', 'B', 'O', 'B', 'I', 'B', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'B', 'B', 'B', 'I', 'I', 'I', 'I', 'O']",,,5,0.45454545454545453,172,0.6991869918699187,36,0.8571428571428571,1,1,
174,( 3 ) FCM ) is a compositional model that combines lexicalized linguistic context and word embeddings for relation extraction .,Baselines,Baselines,relation-classification,1,"['O', 'O', 'O', 'B', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'O', 'B', 'I', 'O']",,,6,0.5454545454545454,173,0.7032520325203252,37,0.8809523809523809,1,1,
175,"The jointly extracting methods used in this paper are listed as follows : ( 4 ) DS - Joint ) is a supervised method , which jointly extracts entities and relations using structured perceptron on human - annotated dataset ; ( 5 ) MultiR is a typical distant supervised method based on multi-instance learning algorithms to combat the noisy training data ; ( 6 ) CoType ) is a domain independent framework by jointly embedding entity mentions , relation mentions , text features and type labels into meaningful representations .",Baselines,Baselines,relation-classification,1,"['O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'B', 'I', 'B', 'I', 'O', 'O', 'B', 'I', 'B', 'I', 'I', 'B', 'B', 'I', 'B', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'B', 'I', 'I', 'I', 'B', 'I', 'B', 'I', 'I', 'B', 'I', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'B', 'I', 'I', 'B', 'I', 'I', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']",,,7,0.6363636363636364,174,0.7073170731707317,38,0.9047619047619048,1,1,
176,"In addition , we also compare our method with two classical end - to - end tagging models : LSTM- CRF and LSTM - LSTM .",Baselines,Baselines,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'O']",,,8,0.7272727272727273,175,0.7113821138211383,39,0.9285714285714286,1,1,
177,LSTM - CRF is proposed for entity recognition by using a bidirectional LSTM to encode input sentence and a conditional random fields to predict the entity tag sequence .,Baselines,Baselines,relation-classification,1,"['O', 'O', 'O', 'O', 'B', 'I', 'B', 'I', 'B', 'I', 'O', 'B', 'I', 'B', 'I', 'B', 'I', 'O', 'O', 'B', 'I', 'I', 'B', 'I', 'O', 'B', 'I', 'I', 'O']",,,9,0.8181818181818182,176,0.7154471544715447,40,0.9523809523809523,1,1,
178,"Different from LSTM - CRF , LSTM - LSTM uses a LSTM layer to decode the tag sequence instead of CRF .",Baselines,Baselines,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'O']",,,10,0.9090909090909091,177,0.7195121951219512,41,0.9761904761904762,1,1,
182,"It can be seen that our method , LSTM - LSTM - Bias , outperforms all other methods in F 1 score and achieves a 3 % improvement in F 1 over the best method CoType .",Experimental Results,Experimental Results,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'O', 'B', 'B', 'I', 'I', 'B', 'B', 'I', 'I', 'O', 'B', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O', 'B-p', 'B-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'O', 'O', 'O', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'O']",2,0.03389830508474576,181,0.7357723577235772,2,0.11764705882352941,1,1,results
186,"When compared with the traditional methods , the precisions of the end - to - end models are significantly improved .",Experimental Results,Experimental Results,relation-classification,1,"['O', 'B', 'I', 'O', 'B', 'I', 'O', 'O', 'B', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'O', 'B', 'I', 'O']",,,6,0.1016949152542373,185,0.7520325203252033,6,0.35294117647058826,1,1,results
191,We also find that the LSTM - LSTM model is better than LSTM - CRF model based on our tagging scheme .,Experimental Results,Experimental Results,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'B', 'I', 'I', 'B', 'I', 'I', 'I', 'B', 'I', 'B', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'I-p', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'O']",11,0.1864406779661017,190,0.7723577235772358,11,0.6470588235294118,1,1,results
2,Joint entity recognition and relation extraction as a multi-head selection problem,title,title,relation-classification,2,"['B', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O']",,,1,0.0,1,0.003389830508474576,1,0.0,1,1,research-problem
7,"In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool .",abstract,abstract,relation-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.5714285714285714,6,0.020338983050847456,4,0.5714285714285714,1,1,research-problem
16,"On the other hand , more recent studies propose to use joint models to detect entities and their relations overcoming the aforementioned issues and achieving state - of - the - art performance .",Introduction,Introduction,relation-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.16129032258064516,15,0.05084745762711865,5,0.16129032258064516,1,1,research-problem
28,"In this work , we focus on anew general purpose joint model that performs the two tasks of entity recognition and relation extraction simultaneously , and that can handle multiple relations together .",Introduction,Introduction,relation-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.5483870967741935,27,0.09152542372881356,17,0.5483870967741935,1,1,research-problem
107,"In this section , we present our multi-head joint model illustrated in .",Joint model,Joint model,relation-classification,2,"['O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'O', 'O', 'O']",1,0.015625,106,0.3593220338983051,1,0.08333333333333333,1,1,model
111,"The input of our model is a sequence of tokens ( i.e. , words of the sentence ) which are then represented as word vectors ( i.e. , word embeddings ) .",Joint model,Joint model,relation-classification,2,"['O', 'B', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'B', 'I', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O', 'O', 'O', 'B-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",5,0.078125,110,0.3728813559322034,5,0.4166666666666667,1,1,model
114,"The outputs for each token ( e.g. , Smith ) are twofold : ( i ) an entity recognition label ( e.g. , I - PER , denoting the token is inside a named entity of type PER ) and ( ii ) a set of tuples comprising the head tokens of the entity and the types of relations between them ( e.g. , {( Center , Works for ) , ( Atlanta , Lives in ) } ) .",Joint model,Joint model,relation-classification,2,"['O', 'B', 'I', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'I-p', 'B-b', 'I-b', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.125,113,0.38305084745762713,8,0.6666666666666666,1,1,model
208,We have developed our joint model by using Python with the TensorFlow machine learning library .,Hyperparameters and implementation details,Hyperparameters and implementation details,relation-classification,2,"['O', 'O', 'B', 'B', 'I', 'I', 'B', 'I', 'B', 'B', 'O', 'B', 'I', 'I', 'I', 'O']","['O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'B-p', 'B-b', 'I-b', 'I-b', 'B-p', 'I-p', 'B-ob', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",1,0.07692307692307693,207,0.7016949152542373,1,0.07692307692307693,1,1,experimental-setup
209,"Training is performed using the Adam optimizer ( Kingma & Ba , 2015 ) with a learning rate of 10 ?3 .",Hyperparameters and implementation details,Hyperparameters and implementation details,relation-classification,2,"['B', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'B', 'O', 'B', 'I', 'B', 'B', 'I', 'O']","['B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['B-b', 'O', 'B-p', 'I-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'O', 'B-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'O']",2,0.15384615384615385,208,0.7050847457627119,2,0.15384615384615385,1,1,experimental-setup
210,We fix the size of the LSTM to d = 64 and the layer width of the neural network to l = 64 ( both for the entity and the relation scoring layers ) .,Hyperparameters and implementation details,Hyperparameters and implementation details,relation-classification,2,"['O', 'B', 'O', 'B', 'B', 'I', 'I', 'B', 'B', 'I', 'I', 'O', 'O', 'B', 'I', 'O', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",,,3,0.23076923076923078,209,0.7084745762711865,3,0.23076923076923078,1,1,experimental-setup
211,We use dropout to regularize our network .,Hyperparameters and implementation details,,relation-classification,2,"['O', 'B', 'B', 'B', 'I', 'B', 'I', 'O']","['O', 'B-p', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['O', 'B-p', 'B-b', 'B-p', 'I-p', 'B-ob', 'I-ob', 'O']",4,0.3076923076923077,210,0.711864406779661,4,0.3076923076923077,1,1,experimental-setup
214,The hidden dimension for the characterbased LSTMs is 25 ( for each direction ) .,Hyperparameters and implementation details,We use dropout to regularize our network .,relation-classification,2,"['O', 'B', 'I', 'B', 'O', 'B', 'I', 'B', 'B', 'O', 'O', 'B', 'I', 'O', 'O']","['O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'O', 'O']","['O', 'B-b', 'I-b', 'B-p', 'O', 'B-b', 'I-b', 'B-p', 'B-b', 'O', 'O', 'B-ob', 'I-ob', 'O', 'O']",7,0.5384615384615384,213,0.7220338983050848,7,0.5384615384615384,1,1,experimental-setup
217,We employ the technique of early stopping based on the validation set .,Hyperparameters and implementation details,We use dropout to regularize our network .,relation-classification,2,"['O', 'B', 'O', 'O', 'O', 'B', 'I', 'B', 'I', 'O', 'B', 'I', 'O']","['O', 'B-p', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['O', 'B-p', 'O', 'O', 'O', 'B-b', 'I-b', 'B-p', 'I-p', 'O', 'B-ob', 'I-ob', 'O']",10,0.7692307692307693,216,0.7322033898305085,10,0.7692307692307693,1,1,experimental-setup
218,"In all the datasets examined in this study , we obtain the best hyperparameters after 60 to 200 epochs depending on the size of the dataset .",Hyperparameters and implementation details,We use dropout to regularize our network .,relation-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'B', 'B', 'I', 'I', 'I', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",11,0.8461538461538461,217,0.735593220338983,11,0.8461538461538461,1,1,experimental-setup
239,We observe that our model outperforms all previous models that do not rely on complex hand - crafted features by a large margin ( > 4 % for both tasks ) .,Results,The first column indicates the considered dataset .,relation-classification,2,"['O', 'B', 'O', 'B', 'I', 'B', 'B', 'I', 'I', 'B', 'I', 'I', 'I', 'I', 'B', 'I', 'I', 'I', 'I', 'B', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O']","['O', 'B-p', 'O', 'B-b', 'I-b', 'B-p', 'B-b', 'I-b', 'I-b', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O']",17,0.3695652173913043,238,0.8067796610169492,17,0.4594594594594595,1,1,results
249,"We also report results for the DREC dataset , with two different evaluation settings .",Results,The first column indicates the considered dataset .,relation-classification,2,"['O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.5869565217391305,248,0.8406779661016949,27,0.7297297297297297,1,1,results
250,"Specifically , we use the boundaries and the strict settings .",Results,The first column indicates the considered dataset .,relation-classification,2,"['O', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",28,0.6086956521739131,249,0.8440677966101695,28,0.7567567567567568,1,1,results
253,"In the boundaries evaluation , we achieve ? 3 % improvement for both tasks .",Results,The first column indicates the considered dataset .,relation-classification,2,"['O', 'O', 'B', 'I', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'O', 'B', 'I', 'O']","['O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O']","['O', 'O', 'B-b', 'I-b', 'O', 'O', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'O', 'B-ob', 'I-ob', 'O']",31,0.6739130434782609,252,0.8542372881355932,31,0.8378378378378378,1,1,results
261,We conduct ablation tests on the ACE04 dataset reported in to analyze the effectiveness of the various parts of our joint model .,Results,Analysis of feature contribution,relation-classification,2,"['O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'O', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'O', 'O', 'B-p', 'I-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",39,0.8478260869565217,260,0.8813559322033898,1,0.125,1,1,ablation-analysis
262,The performance of the RE task decreases ( ? 1 % in terms of F 1 score ) when we remove the label embeddings layer and only use the LSTM hidden states as inputs for the RE task .,Results,Analysis of feature contribution,relation-classification,2,"['O', 'B', 'I', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'I-p', 'O', 'B-b', 'I-b', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-ob', 'O', 'O', 'O', 'O', 'O']",40,0.8695652173913043,261,0.8847457627118644,2,0.25,1,1,ablation-analysis
264,Removing character embeddings also degrades the performance of both NER ( ? 1 % ) and RE ( ? 2 % ) tasks by a relatively large margin .,Results,Analysis of feature contribution,relation-classification,2,"['B', 'B', 'I', 'B', 'I', 'I', 'I', 'I', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'B', 'O', 'B', 'I', 'I', 'O']","['B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['B-p', 'B-b', 'I-b', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'O']",42,0.9130434782608695,263,0.8915254237288136,4,0.5,1,1,ablation-analysis
266,"Finally , we conduct experiments for the NER task by removing the CRF loss layer and substituting it with a softmax .",Results,Analysis of feature contribution,relation-classification,2,"['O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'B', 'I', 'O', 'B', 'I', 'I', 'O', 'B', 'I', 'I', 'O', 'B', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'B-p', 'I-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-b', 'O']",44,0.9565217391304348,265,0.8983050847457628,6,0.75,1,1,ablation-analysis
267,"Assuming independent distribution of labels ( i.e. , softmax ) leads to a slight decrease in the F 1 performance of the NER module and a ? 2 % decrease in the performance of the RE task .",Results,Analysis of feature contribution,relation-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'B', 'I', 'I', 'O', 'O', 'B', 'I', 'B', 'O', 'O', 'B', 'I', 'O', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O']",,,45,0.9782608695652174,266,0.9016949152542373,7,0.875,1,1,ablation-analysis
2,Adversarial training for multi-context joint entity and relation extraction,title,title,relation-classification,3,"['O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I']",,,1,0.0,1,0.0072992700729927005,1,0.0,1,1,research-problem
5,We show how to use AT for the tasks of entity recognition and relation extraction .,abstract,abstract,relation-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",2,0.6666666666666666,4,0.029197080291970802,2,0.6666666666666666,1,1,research-problem
6,"In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",abstract,abstract,relation-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,1.0,5,0.0364963503649635,3,1.0,1,1,research-problem
36,"The baseline model , described in detail in , is illustrated in .",Model,Model,relation-classification,3,"['O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-b', 'I-b', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.044444444444444446,35,0.25547445255474455,1,0.022727272727272728,1,1,model
37,It aims to detect ( i ) the type and the boundaries of the entities and ( ii ) the relations between them .,Model,Model,relation-classification,3,"['O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O']","['O', 'B-p', 'I-p', 'I-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']","['O', 'B-p', 'I-p', 'I-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'O']",3,0.06666666666666667,36,0.26277372262773724,2,0.045454545454545456,1,1,model
38,"The input is a sequence of tokens ( i.e. , sentence ) w = w 1 , ... , w n .",Model,Model,relation-classification,3,"['O', 'B', 'B', 'I', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'B-b', 'B-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",4,0.08888888888888889,37,0.27007299270072993,3,0.06818181818181818,1,1,model
39,"We use character level embeddings to implicitly capture morphological features ( e.g. , prefixes and suffixes ) , representing each character by a vector ( embedding ) .",Model,Model,relation-classification,3,"['O', 'B', 'B', 'I', 'I', 'B', 'I', 'I', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'B', 'I', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'B-p', 'B-b', 'I-b', 'I-b', 'B-p', 'I-p', 'I-p', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O', 'B-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",5,0.1111111111111111,38,0.2773722627737226,4,0.09090909090909091,1,1,model
40,The character embeddings are fed to a bidirectional LSTM ( BiLSTM ) to obtain the character - based representation of the word .,Model,Model,relation-classification,3,"['O', 'B', 'I', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'B-b', 'I-b', 'O', 'B-p', 'I-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",6,0.13333333333333333,39,0.2846715328467153,5,0.11363636363636363,1,1,model
41,We also use pre-trained word embeddings .,Model,Model,relation-classification,3,"['O', 'O', 'O', 'B', 'I', 'I', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'O']",7,0.15555555555555556,40,0.291970802919708,6,0.13636363636363635,1,1,model
42,"Word and character embeddings are concatenated to form the final token representation , which is then fed to a BiLSTM layer to extract sequential information .",Model,Model,relation-classification,3,"['B', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'B', 'I', 'B', 'I', 'B', 'I', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['B-b', 'I-b', 'I-b', 'I-b', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-b', 'I-b', 'I-b', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-b', 'I-b', 'B-p', 'I-p', 'B-ob', 'I-ob', 'O']",8,0.17777777777777778,41,0.29927007299270075,7,0.1590909090909091,1,1,model
43,"For the NER task , we adopt the BIO ( Beginning , Inside , Outside ) encoding scheme .",Model,Model,relation-classification,3,"['B', 'O', 'B', 'I', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['B-p', 'O', 'B-b', 'I-b', 'O', 'O', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",9,0.2,42,0.30656934306569344,8,0.18181818181818182,1,1,model
53,We model the relation extraction task as a multi-label head selection problem .,Model,Model,relation-classification,3,"['O', 'B', 'O', 'B', 'I', 'I', 'B', 'I', 'B', 'I', 'I', 'I', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",19,0.4222222222222222,52,0.3795620437956204,18,0.4090909090909091,1,1,model
68,We exploit the idea of AT as a regularization method to make our model robust to input perturbations .,Model,Model,relation-classification,3,"['O', 'B', 'O', 'B', 'I', 'I', 'B', 'I', 'B', 'I', 'B', 'I', 'B', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'I-p', 'B-b', 'I-b', 'B-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",34,0.7555555555555555,67,0.48905109489051096,33,0.75,1,1,model
81,"We evaluate our models on four datasets , using the code as available from our github codebase .",Experimental setup,Experimental setup,relation-classification,3,"['O', 'B', 'B', 'I', 'B', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.021739130434782608,80,0.583941605839416,1,0.021739130434782608,1,1,experimental-setup
85,We also evaluate our models on the NER task similar to in the same dataset using 10 - fold cross validation .,Experimental setup,Experimental setup,relation-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-b', 'I-b', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",5,0.10869565217391304,84,0.6131386861313869,5,0.10869565217391304,1,1,experimental-setup
89,We employ early stopping in all of the experiments .,Experimental setup,Experimental setup,relation-classification,3,"['O', 'B', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.1956521739130435,88,0.6423357664233577,9,0.1956521739130435,1,1,experimental-setup
90,"We use the Adam optimizer and we fix the hyperparameters ( i.e. , ? , dropout values , best epoch , learning rate ) on the validation sets .",Experimental setup,Experimental setup,relation-classification,3,"['O', 'B', 'O', 'B', 'I', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'B', 'O', 'B', 'I', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['O', 'B-p', 'O', 'B-ob', 'I-ob', 'O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'O']",10,0.21739130434782608,89,0.6496350364963503,10,0.21739130434782608,1,1,experimental-setup
95,"We use three types of evaluation , namely : ( i ) S( trict ) : we score an entity as correct if both the entity boundaries and the entity type are correct ( ACE04 , ADE , CoNLL04 , DREC ) , ( ii ) B ( oundaries ) : we score an entity as correct if only the entity boundaries are correct while the entity type is not taken into account ( DREC ) and ( iii ) R( elaxed ) : a multi-token entity is considered correct if at least one correct type is assigned to the tokens comprising the entity , assuming that the : Comparison of our method with the stateof - the - art in terms of F 1 score .",Experimental setup,Experimental setup,relation-classification,3,"['O', 'O', 'B', 'I', 'I', 'I', 'O', 'B', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'B', 'B', 'I', 'B', 'B', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'O', 'B-p', 'O', 'O', 'O', 'O', 'B-b', 'I-b', 'I-b', 'O', 'O', 'B-p', 'B-b', 'I-b', 'B-p', 'B-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-b', 'I-b', 'I-b', 'O', 'O', 'B-b', 'I-b', 'O', 'B-p', 'I-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.32608695652173914,94,0.6861313868613139,15,0.32608695652173914,1,1,experimental-setup
106,"For ACE04 , the baseline outperforms by ? 2 % in both tasks .",Experimental setup,Experimental setup,relation-classification,3,"['B', 'B', 'O', 'O', 'B', 'B', 'I', 'B', 'I', 'I', 'B', 'B', 'I', 'O']","['B-p', 'B-n', 'O', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['B-p', 'B-b', 'O', 'O', 'B-b', 'B-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'B-p', 'B-ob', 'I-ob', 'O']",26,0.5652173913043478,105,0.7664233576642335,26,0.5652173913043478,1,1,results
111,"For the CoNLL04 dataset , we use two evaluation settings .",Experimental setup,Experimental setup,relation-classification,3,"['O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-b', 'I-b', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",31,0.6739130434782609,110,0.8029197080291971,31,0.6739130434782609,1,1,results
113,"The baseline model outperforms the state - of - the - art models that do not rely on manually extracted features ( > 4 % improvement for both tasks ) , since we directly model the whole sentence , instead of just considering pairs of entities .",Experimental setup,Experimental setup,relation-classification,3,"['O', 'B', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'B', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-b', 'I-b', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",33,0.717391304347826,112,0.8175182481751825,33,0.717391304347826,1,1,results
117,"For the DREC dataset , we use two evaluation methods .",Experimental setup,Experimental setup,relation-classification,3,"['O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-b', 'I-b', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",37,0.8043478260869565,116,0.8467153284671532,37,0.8043478260869565,1,1,results
118,"In the boundaries evaluation , the baseline has an improvement of ? 3 % on both tasks compared to , whose quadratic scoring layer complicates NER .",Experimental setup,Experimental setup,relation-classification,3,"['B', 'O', 'B', 'I', 'O', 'O', 'B', 'B', 'I', 'I', 'I', 'B', 'I', 'I', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-b', 'I-b', 'O', 'O', 'B-b', 'B-p', 'I-p', 'I-p', 'I-p', 'B-b', 'I-b', 'I-b', 'O', 'B-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",38,0.8260869565217391,117,0.8540145985401459,38,0.8260869565217391,1,1,results
119,and show the effectiveness of the adversarial training on top of the baseline model .,Experimental setup,Experimental setup,relation-classification,3,"['O', 'B', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'B-p', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'B-p', 'O', 'O', 'O', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O']",39,0.8478260869565217,118,0.8613138686131386,39,0.8478260869565217,1,1,results
120,"In all of the experiments , AT improves the predictive performance of the baseline model in the joint setting .",Experimental setup,Experimental setup,relation-classification,3,"['B', 'B', 'B', 'I', 'I', 'O', 'B', 'B', 'O', 'B', 'I', 'O', 'O', 'B', 'I', 'B', 'O', 'B', 'I', 'O']",,,40,0.8695652173913043,119,0.8686131386861314,40,0.8695652173913043,1,1,results
122,"Specifically , for ACE04 , there is an improvement in both tasks as well as in the overall F 1 performance ( 0.4 % ) .",Experimental setup,Experimental setup,relation-classification,3,"['O', 'O', 'B', 'B', 'O', 'O', 'O', 'O', 'B', 'I', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'B-p', 'B-b', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",42,0.9130434782608695,121,0.8832116788321168,42,0.9130434782608695,1,1,results
123,"For CoNLL04 , we note an improvement in the overall F 1 of 0.4 % for the EC and 0.8 % for the NER tasks , respectively .",Experimental setup,Experimental setup,relation-classification,3,"['B', 'B', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'B', 'O', 'B', 'O', 'B', 'I', 'O', 'O', 'B', 'I', 'O', 'O', 'O']","['B-p', 'B-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O']","['B-p', 'B-b', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'O', 'B-ob', 'O', 'B-b', 'I-b', 'O', 'O', 'B-ob', 'I-ob', 'O', 'O', 'O']",43,0.9347826086956522,122,0.8905109489051095,43,0.9347826086956522,1,1,results
124,"For the DREC dataset , in both settings , there is an overall improvement of ? 1 % .",Experimental setup,Experimental setup,relation-classification,3,"['O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'B-b', 'I-b', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",44,0.9565217391304348,123,0.8978102189781022,44,0.9565217391304348,1,1,results
126,"Finally , for ADE , our AT model beats the baseline F 1 by 0.7 % .",Experimental setup,Experimental setup,relation-classification,3,"['O', 'O', 'O', 'B', 'O', 'O', 'B', 'I', 'B', 'O', 'B', 'I', 'I', 'B', 'B', 'I', 'O']","['O', 'O', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['O', 'O', 'O', 'B-b', 'O', 'O', 'B-b', 'I-b', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'O']",46,1.0,125,0.9124087591240876,46,1.0,1,1,results
2,Graph Convolution over Pruned Dependency Trees Improves Relation Extraction,title,,relation-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I']",,,1,0.0,1,0.0038022813688212928,1,0.0,1,1,research-problem
4,Dependency trees help relation extraction models capture long - range relations between words .,abstract,abstract,relation-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",1,0.125,3,0.011406844106463879,1,0.125,1,1,research-problem
18,"have proven to be very effective in relation extraction , because they capture long - range syntactic relations that are obscure from the surface form alone ( e.g. , when long clauses or complex scoping are present ) .",Introduction,Introduction,relation-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.2608695652173913,17,0.06463878326996197,6,0.2608695652173913,1,1,research-problem
29,"Our model encodes the dependency structure over the input sentence with efficient graph convolution operations , then extracts entity - centric representations to make robust relation predictions .",Introduction,Introduction,relation-classification,4,"['O', 'O', 'B', 'O', 'B', 'I', 'B', 'O', 'B', 'I', 'B', 'B', 'I', 'I', 'I', 'O', 'B', 'I', 'B', 'I', 'I', 'I', 'B', 'I', 'B', 'I', 'I', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'B-p', 'O', 'B-b', 'I-b', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'O', 'B-p', 'I-p', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'O']",17,0.7391304347826086,28,0.10646387832699619,17,0.7391304347826086,1,1,model
30,"We also apply a novel path - centric pruning technique to remove irrelevant information from the tree while maximally keeping relevant content , which further improves the performance of several dependencybased models including ours .",Introduction,Introduction,relation-classification,4,"['O', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'B', 'I', 'B', 'I', 'I', 'I', 'I', 'B', 'I', 'I', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'I-p', 'B-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.782608695652174,29,0.11026615969581749,18,0.782608695652174,1,1,model
126,( 1 ) A logistic regression ( LR ) classifier which combines dependencybased features with other lexical features .,Baseline Models,Baseline Models,relation-classification,4,"['O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'O', 'B', 'B', 'I', 'B', 'B', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O', 'B-p', 'B-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'O']",4,0.3076923076923077,125,0.4752851711026616,4,0.3076923076923077,1,1,baselines
127,"( 2 ) Shortest Dependency Path LSTM ( SDP - LSTM ) , which applies a neural sequence model on the shortest path between the subject and object entities in the dependency tree .",Baseline Models,Baseline Models,relation-classification,4,"['O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'B', 'O', 'B', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'B', 'O', 'B', 'I', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['O', 'O', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'O', 'B-b', 'I-b', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'O']",5,0.38461538461538464,126,0.4790874524714829,5,0.38461538461538464,1,1,baselines
128,"Tree - LSTM , which is a recursive model that generalizes the LSTM to arbitrary tree structures .",Baseline Models,Baseline Models,relation-classification,4,"['B', 'I', 'I', 'O', 'O', 'B', 'I', 'B', 'I', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'O']","['B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['B-b', 'I-b', 'I-b', 'O', 'O', 'B-p', 'I-p', 'B-b', 'I-b', 'B-p', 'I-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",6,0.46153846153846156,127,0.4828897338403042,6,0.46153846153846156,1,1,baselines
133,"Our group presented a competitive sequence model that employs a position - aware attention mechanism over LSTM outputs ( PA - LSTM ) , and showed that it outperforms several CNN and dependency - based models by a substantial margin .",Baseline Models,Baseline Models,relation-classification,4,"['O', 'O', 'B', 'O', 'B', 'I', 'I', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'B', 'I', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'B', 'O', 'B', 'I', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'I-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'O']",11,0.8461538461538461,132,0.5019011406844106,11,0.8461538461538461,1,1,baselines
149,Results on the TACRED Dataset,Experimental Setup,,relation-classification,4,"['O', 'B', 'O', 'B', 'I']","['O', 'B-p', 'O', 'B-n', 'I-n']","['O', 'B-p', 'O', 'B-b', 'I-b']",13,0.30952380952380953,148,0.5627376425855514,0,0.0,1,1,results
151,We observe that our GCN model Our Model ( C - GCN ) 84.8 * 76.5 * outperforms all dependency - based models by at least 1.6 F 1 .,Experimental Setup,Results on the TACRED Dataset,relation-classification,4,"['O', 'B', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'I', 'I', 'I', 'I', 'B', 'B', 'I', 'I', 'I', 'I', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",15,0.35714285714285715,150,0.5703422053231939,2,0.15384615384615385,1,1,results
152,"By using contextualized word representations , the C - GCN model further outperforms the strong PA - LSTM model by 1.3 F 1 , and achieves anew state of the art .",Experimental Setup,Results on the TACRED Dataset,relation-classification,4,"['O', 'B', 'B', 'I', 'I', 'O', 'O', 'B', 'I', 'I', 'I', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'B', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-b', 'I-b', 'I-b', 'O', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.38095238095238093,151,0.5741444866920152,3,0.23076923076923078,1,1,results
153,"In addition , we find our model improves upon other dependencybased models in both precision and recall .",Experimental Setup,Results on the TACRED Dataset,relation-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'I', 'I', 'I', 'B', 'I', 'B', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'O']",17,0.40476190476190477,152,0.5779467680608364,4,0.3076923076923077,1,1,results
154,"Comparing the C - GCN model with the GCN model , we find that the gain mainly comes from improved recall .",Experimental Setup,Results on the TACRED Dataset,relation-classification,4,"['B', 'O', 'B', 'I', 'B', 'I', 'B', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'B', 'B', 'I', 'I', 'O', 'O', 'O']",,,18,0.42857142857142855,153,0.5817490494296578,5,0.38461538461538464,1,1,results
156,"As we will show in Section 6.2 , we find that our GCN models have complementary strengths when compared to the PA - LSTM .",Experimental Setup,Results on the TACRED Dataset,relation-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'B', 'I', 'I', 'B', 'B', 'I', 'B', 'I', 'I', 'O', 'B', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-b', 'I-b', 'I-b', 'B-p', 'B-b', 'I-b', 'B-p', 'I-p', 'I-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'O']",20,0.47619047619047616,155,0.5893536121673004,7,0.5384615384615384,1,1,results
165,"We find that under the conventional with- entity evaluation , our C - GCN model outperforms all existing dependency - based neural models on this sep - arate dataset .",Experimental Setup,Results on the SemEval Dataset,relation-classification,4,"['O', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'B', 'I', 'I', 'I', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'O', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",29,0.6904761904761905,164,0.623574144486692,2,0.14285714285714285,1,1,results
166,"Notably , by properly incorporating off - path information , our model outperforms the previous shortest dependency path - based model ( SDP - LSTM ) .",Experimental Setup,Results on the SemEval Dataset,relation-classification,4,"['O', 'O', 'B', 'I', 'I', 'B', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'B-p', 'I-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O']",30,0.7142857142857143,165,0.6273764258555133,3,0.21428571428571427,1,1,results
167,"Under the mask - entity evaluation , our C - GCN model also outperforms PA - LSTM by a substantial margin , suggesting its generalizability even when entities are not seen .",Experimental Setup,Results on the SemEval Dataset,relation-classification,4,"['B', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'B', 'I', 'I', 'I', 'B', 'I', 'B', 'I', 'I', 'B', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'O', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'B-b', 'I-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",31,0.7380952380952381,166,0.6311787072243346,4,0.2857142857142857,1,1,results
170,"To show the effectiveness of path - centric pruning , we compare the two GCN models and the Tree - LSTM when the pruning distance K is varied .",Experimental Setup,Pruning,relation-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'O', 'O', 'B', 'I', 'I', 'B', 'O', 'B', 'I', 'I', 'B', 'B', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'O', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'O']",34,0.8095238095238095,169,0.6425855513307985,7,0.5,1,1,results
172,"As shown in , the performance of all three models peaks when K = 1 , outperforming their respective dependency path - based counterpart ( K = 0 ) .",Experimental Setup,Pruning,relation-classification,4,"['O', 'O', 'O', 'O', 'O', 'B', 'I', 'B', 'I', 'I', 'B', 'I', 'B', 'I', 'I', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-b', 'I-b', 'I-b', 'B-p', 'I-p', 'B-b', 'I-b', 'I-b', 'O', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",36,0.8571428571428571,171,0.6501901140684411,9,0.6428571428571429,1,1,results
176,"We find that all three models are less effective when the entire dependency tree is present , indicating that including extra information hurts performance .",Experimental Setup,Pruning,relation-classification,4,"['O', 'B', 'O', 'B', 'I', 'I', 'B', 'B', 'I', 'B', 'O', 'B', 'I', 'I', 'B', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'B-b', 'I-b', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",40,0.9523809523809523,175,0.6653992395437263,13,0.9285714285714286,1,1,results
177,"Finally , we note that contextualizing the GCN makes it less sensitive to changes in the tree structures provided , presumably because the model can use word sequence information in the LSTM layer to recover any off - path information that it needs for correct relation extraction .",Experimental Setup,Pruning,relation-classification,4,"['O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'B', 'I', 'B', 'I', 'B', 'I', 'I', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-b', 'B-p', 'I-p', 'B-b', 'I-b', 'B-p', 'I-p', 'I-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",41,0.9761904761904762,176,0.6692015209125475,14,1.0,1,1,results
180,"To study the contribution of each component in the C - GCN model , we ran an ablation study on the TACRED dev set ) .",Ablation Study,Ablation Study,relation-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'O', 'O']",,,1,0.05263157894736842,179,0.6806083650190115,1,0.05263157894736842,1,1,
181,We find that : The entity representations and feedforward layers contribute 1.0 F 1 .,Ablation Study,Ablation Study,relation-classification,4,"['O', 'B', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'B', 'B', 'I', 'I', 'O']",,,2,0.10526315789473684,180,0.6844106463878327,2,0.10526315789473684,1,1,
182,"( 2 ) When we remove the dependency structure ( i.e. , setting to I ) , the score drops by 3.2 F 1 .",Ablation Study,Ablation Study,relation-classification,4,"['O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'B', 'I', 'I', 'O']",,,3,0.15789473684210525,181,0.688212927756654,3,0.15789473684210525,1,1,
183,"( 3 ) F 1 drops by 10.3 when we remove the feedforward layers , the LSTM component and the dependency structure altogether .",Ablation Study,Ablation Study,relation-classification,4,"['O', 'O', 'O', 'B', 'I', 'I', 'I', 'B', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']",,,4,0.21052631578947367,182,0.6920152091254753,4,0.21052631578947367,1,1,
184,"( 4 ) Removing the pruning ( i.e. , using full trees as input ) further hurts the result by another 9.7 F 1 .",Ablation Study,Ablation Study,relation-classification,4,"['O', 'O', 'O', 'B', 'O', 'B', 'O', 'O', 'O', 'B', 'B', 'I', 'I', 'I', 'O', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O']",,,5,0.2631578947368421,183,0.6958174904942965,5,0.2631578947368421,1,1,
2,End - to - end neural relation extraction using deep biaffine attention,title,title,relation-classification,5,"['B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O']",,,1,0.0,1,0.008849557522123894,1,0.0,1,1,research-problem
4,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features .",abstract,abstract,relation-classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.3333333333333333,3,0.02654867256637168,1,0.3333333333333333,1,1,research-problem
8,Extracting entities and their semantic relations from raw text is a key information extraction task .,Introduction,Introduction,relation-classification,5,"['B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.037037037037037035,7,0.061946902654867256,1,0.037037037037037035,1,1,research-problem
13,"More recently , end - to - end systems which jointly learn to extract entities and relations have been proposed with strong potential to obtain high performance .",Introduction,Introduction,relation-classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.2222222222222222,12,0.10619469026548672,6,0.2222222222222222,1,1,research-problem
25,"In this paper , we present a novel end - to - end neural model for joint entity and relation extraction .",Introduction,Their approach relies on various manually extracted features .,relation-classification,5,"['O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'B', 'B', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",18,0.6666666666666666,24,0.21238938053097345,18,0.6666666666666666,1,1,model
26,"As illustrated in , our model architecture can be viewed as a mixture of a named entity recognition ( NER ) component and a relation classification ( RC ) component .",Introduction,Their approach relies on various manually extracted features .,relation-classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",19,0.7037037037037037,25,0.22123893805309736,19,0.7037037037037037,1,1,model
27,Our NER component employs a BiLSTM - CRF architecture to predict entities from input word tokens .,Introduction,Their approach relies on various manually extracted features .,relation-classification,5,"['O', 'B', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'B', 'I', 'B', 'B', 'B', 'I', 'I', 'O']","['O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['O', 'B-b', 'I-b', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'B-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'O']",20,0.7407407407407407,26,0.23008849557522124,20,0.7407407407407407,1,1,model
28,"Based on both the input words and the predicted NER labels , the RC component uses another BiLSTM to learn latent features relevant for relation classification .",Introduction,Their approach relies on various manually extracted features .,relation-classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'B', 'B', 'I', 'B', 'I', 'B', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-b', 'I-b', 'B-p', 'B-b', 'I-b', 'B-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",21,0.7777777777777778,27,0.23893805309734514,21,0.7777777777777778,1,1,model
30,"In contrast , our RC component takes into account second - order interactions over the latent features via a tensor .",Introduction,Their approach relies on various manually extracted features .,relation-classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'B', 'I', 'I', 'I', 'B', 'O', 'B', 'I', 'B', 'O', 'B', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'O', 'B-b', 'I-b', 'B-p', 'O', 'B-ob', 'O']",23,0.8518518518518519,29,0.25663716814159293,23,0.8518518518518519,1,1,model
31,"In particular , for relation classification we propose a novel use of the deep biaffine attention mechanism which was first introduced in dependency parsing .",Introduction,Their approach relies on various manually extracted features .,relation-classification,5,"['O', 'O', 'O', 'B', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'B-b', 'I-b', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.8888888888888888,30,0.26548672566371684,24,0.8888888888888888,1,1,model
72,Our model is implemented using DYNET v 2.0 .,Experimental setup,,relation-classification,5,"['O', 'O', 'O', 'O', 'B', 'B', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'O']",13,0.6190476190476191,71,0.6283185840707964,13,0.6190476190476191,1,1,experimental-setup
73,"We optimize the objective loss using Adam , no mini-batches and run for 100 epochs .",Experimental setup,Our model is implemented using DYNET v 2.0 .,relation-classification,5,"['O', 'B', 'O', 'B', 'I', 'B', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-b', 'I-b', 'B-p', 'B-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.6666666666666666,72,0.6371681415929203,14,0.6666666666666666,1,1,experimental-setup
77,Our code is available at : https : //github.com/datquocnguyen/jointRE,Experimental setup,,relation-classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.8571428571428571,76,0.672566371681416,18,0.8571428571428571,1,1,code
94,"We provide in the results of a pipeline approach where we treat our two NER and RC components as independent networks , and train them separately .",Main results,Main results,relation-classification,5,"['O', 'B', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'B', 'I', 'I', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'B', 'I', 'B', 'O']","['O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O']","['O', 'B-p', 'O', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'I-p', 'O', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O', 'O', 'B-p', 'I-p', 'B-ob', 'O']",13,0.4642857142857143,93,0.8230088495575221,13,0.4642857142857143,1,1,ablation-analysis
96,"We find that the joint approach does slightly better than the pipeline approach in relation classification , although the .",Main results,Main results,relation-classification,5,"['O', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'B', 'O', 'B', 'I', 'B', 'B', 'I', 'O', 'O', 'O', 'O']","['O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O']","['O', 'B-p', 'I-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'O', 'B-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'O', 'O', 'O', 'O']",15,0.5357142857142857,95,0.8407079646017699,15,0.5357142857142857,1,1,ablation-analysis
2,Semantic Relation Classification via Bidirectional LSTM Networks with Entity - aware Attention using Latent Entity Typing,title,title,relation-classification,6,"['B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",,,1,0.0,1,0.0055248618784530384,1,0.0,1,1,research-problem
4,Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) .,abstract,abstract,relation-classification,6,"['B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.16666666666666666,3,0.016574585635359115,1,0.16666666666666666,1,1,research-problem
12,A task of relation classification is defined as predicting a semantic relationship between two tagged entities in a sentence .,Introduction,Introduction,relation-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",2,0.125,11,0.06077348066298342,2,0.125,1,1,research-problem
48,"In this section , we introduce a novel recurrent neural model that incorporate an entity - aware attention mechanism with a LET method in detail .",Model,Model,relation-classification,6,"['O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'B', 'O', 'B', 'I', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'O', 'O', 'O']",1,0.013157894736842105,47,0.2596685082872928,1,0.1,1,1,model
49,"As shown inure 2 , our model consists of four main components : Word Representation that maps each word in a sentence into vector representations ; ( 2 ) Self Attention that captures the meaning of the correlation between words based on multi-head attention ; ( 3 ) BLSTM which sequentially encodes the representations of self attention layer ; ( 4 ) Entity - aware Attention that calculates attention weights with respect to the entity pairs , word positions relative to these pairs , and their latent types obtained by LET .",Model,Model,relation-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'B', 'I', 'I', 'O', 'B', 'I', 'O', 'B', 'B', 'I', 'I', 'I', 'I', 'B', 'B', 'I', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'B', 'I', 'B', 'I', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'O', 'B', 'B', 'I', 'B', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-b', 'I-b', 'I-b', 'O', 'B-b', 'I-b', 'O', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'O', 'O', 'O', 'O', 'B-b', 'I-b', 'O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'B-ob', 'I-ob', 'O', 'O', 'O', 'O', 'B-b', 'O', 'B-p', 'I-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'O', 'B-p', 'B-b', 'I-b', 'B-p', 'I-p', 'I-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",2,0.02631578947368421,48,0.26519337016574585,2,0.2,1,1,model
160,"Non Our proposed model achieves an F1-score of 85.2 % which outperforms all competing state - of - theart approaches except depLCNN + NS , DRNNs , and Attention - CNN .",Model F1,Model F1,relation-classification,6,"['O', 'O', 'B', 'I', 'B', 'O', 'B', 'B', 'B', 'I', 'O', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'B', 'B', 'I', 'I', 'O', 'B', 'O', 'O', 'B', 'I', 'I', 'O']","['O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']","['O', 'O', 'B-b', 'I-b', 'B-p', 'O', 'B-b', 'B-p', 'B-ob', 'I-ob', 'O', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'O', 'B-ob', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'O']",1,0.07142857142857142,159,0.8784530386740331,1,0.2,1,1,results
2,Data and text mining BioBERT : a pre-trained biomedical language representation model for biomedical text mining,title,title,relation-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O']",,,1,0.0,1,0.005025125628140704,1,0.0,1,1,research-problem
6,"With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models .",abstract,abstract,relation-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.25,5,0.02512562814070352,3,0.25,1,1,research-problem
7,"However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora .",abstract,abstract,relation-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",4,0.3333333333333333,6,0.03015075376884422,4,0.3333333333333333,1,1,research-problem
8,"In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",abstract,abstract,relation-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",5,0.4166666666666667,7,0.035175879396984924,5,0.4166666666666667,1,1,research-problem
15,"We make the pre-trained weights of BioBERT freely available at https://github. com/naver/biobert-pretrained , and the source code for fine - tuning",abstract,abstract,relation-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,1.0,14,0.07035175879396985,12,1.0,1,1,code
27,"Also , the word distributions of general and biomedical corpora are quite different , which can often be a problem for biomedical text mining models .",Introduction,Introduction,relation-classification,7,"['O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.6875,26,0.1306532663316583,11,0.6875,1,1,research-problem
34,"In this article , we introduce BioBERT , which is a pre-trained language representation model for the biomedical domain .",Approach,Approach,relation-classification,7,"['O', 'O', 'O', 'O', 'O', 'B', 'B', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'B', 'O', 'B', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-b', 'O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'O']",1,0.07692307692307693,33,0.1658291457286432,1,0.07692307692307693,1,1,approach
36,"First , we initialize BioBERT with weights from BERT , which was pretrained on general domain corpora ( English Wikipedia and Books Corpus ) .",Approach,Approach,relation-classification,7,"['O', 'O', 'O', 'B', 'B', 'B', 'B', 'B', 'B', 'O', 'O', 'O', 'B', 'I', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'B-p', 'B-b', 'B-p', 'B-b', 'B-p', 'B-b', 'O', 'O', 'O', 'B-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",3,0.23076923076923078,35,0.17587939698492464,3,0.23076923076923078,1,1,approach
37,"Then , BioBERT is pre-trained on biomedical domain corpora ( PubMed abstracts and PMC full - text articles ) .",Approach,Approach,relation-classification,7,"['O', 'O', 'B', 'O', 'B', 'I', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'B-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'B-b', 'O', 'B-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",4,0.3076923076923077,36,0.18090452261306533,4,0.3076923076923077,1,1,approach
38,"To show the effectiveness of our approach in biomedical text mining , BioBERT is fine - tuned and evaluated on three popular biomedical text mining tasks .",Approach,Approach,relation-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'B', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",5,0.38461538461538464,37,0.18592964824120603,5,0.38461538461538464,1,1,approach
119,We used the BERT BASE model pre-trained on English Wikipedia and Books Corpus for 1 M steps .,Experimental setups,Experimental setups,relation-classification,7,"['O', 'B', 'O', 'B', 'I', 'I', 'B', 'I', 'B', 'I', 'I', 'I', 'I', 'B', 'B', 'I', 'I', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'I-p', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'O']",1,0.0625,118,0.592964824120603,1,0.0625,1,1,experimental-setup
120,BioBERT v1.0 ( PubMed PMC ) is the version of BioBERT ( PubMed PMC ) trained for 470 K steps .,Experimental setups,Experimental setups,relation-classification,7,"['B', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'B', 'I', 'B', 'I', 'I', 'I', 'I', 'B', 'I', 'B', 'I', 'I', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O', 'O', 'B-p', 'I-p', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'O']",2,0.125,119,0.5979899497487438,2,0.125,1,1,experimental-setup
126,We used eight NVIDIA V100 ( 32GB ) GPUs for the pre-training .,Experimental setups,Experimental setups,relation-classification,7,"['O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'B', 'O', 'B', 'O']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O']","['O', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'O', 'B-ob', 'O']",8,0.5,125,0.628140703517588,8,0.5,1,1,experimental-setup
127,"The maximum sequence length was fixed to 512 and the mini-batch size was set to 192 , resulting in 98 304 words per iteration .",Experimental setups,Experimental setups,relation-classification,7,"['O', 'B', 'I', 'I', 'O', 'B', 'I', 'B', 'O', 'O', 'B', 'I', 'O', 'B', 'I', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-b', 'I-b', 'I-b', 'O', 'B-p', 'I-p', 'B-ob', 'O', 'O', 'B-b', 'I-b', 'O', 'B-p', 'I-p', 'B-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.5625,126,0.6331658291457286,9,0.5625,1,1,experimental-setup
130,We used a single NVIDIA Titan Xp ( 12GB ) GPU to fine - tune BioBERT on each task .,Experimental setups,Experimental setups,relation-classification,7,"['O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'B', 'I', 'I', 'I', 'B', 'B', 'B', 'I', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['O', 'O', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'I-p', 'I-p', 'B-b', 'B-p', 'B-ob', 'I-ob', 'O']",12,0.75,129,0.6482412060301508,12,0.75,1,1,experimental-setup
136,The results of NER are shown in .,Experimental results,Experimental results,relation-classification,7,"['O', 'O', 'B', 'B', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'B-n', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'B-b', 'O', 'O', 'O', 'O']",1,0.03225806451612903,135,0.678391959798995,1,0.07142857142857142,1,1,results
138,"On the other hand , BioBERT achieves higher scores than BERT on all the datasets .",Experimental results,Experimental results,relation-classification,7,"['O', 'O', 'O', 'O', 'O', 'B', 'B', 'B', 'I', 'B', 'B', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-b', 'B-p', 'B-b', 'I-b', 'B-p', 'B-ob', 'O', 'O', 'O', 'O', 'O']",3,0.0967741935483871,137,0.6884422110552764,3,0.21428571428571427,1,1,results
139,"BioBERT outperformed the state - of - the - art models on six out of nine datasets , and BioBERT v 1.1 ( PubMed ) outperformed the state - of - the - art models by 0.62 in terms of micro averaged F1 score .",Experimental results,Experimental results,relation-classification,7,"['O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'B', 'B', 'I', 'I', 'I', 'I', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'B', 'I', 'I', 'B', 'I', 'I', 'I', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-b', 'B-p', 'I-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",4,0.12903225806451613,138,0.6934673366834171,4,0.2857142857142857,1,1,results
141,The RE results of each model are shown in .,Experimental results,Experimental results,relation-classification,7,"['O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-b', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.1935483870967742,140,0.7035175879396985,6,0.42857142857142855,1,1,results
143,"On average ( micro ) , BioBERT v1.0 ( PubMed ) obtained a higher F1 score ( 2.80 higher ) than the state - of - the - art models .",Experimental results,Experimental results,relation-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",8,0.25806451612903225,142,0.7135678391959799,8,0.5714285714285714,1,1,results
144,"Also , BioBERT achieved the highest F 1 scores on 2 out of 3 biomedical datasets .",Experimental results,Experimental results,relation-classification,7,"['O', 'O', 'B', 'B', 'O', 'B', 'I', 'I', 'I', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'B-b', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",9,0.2903225806451613,143,0.7185929648241206,9,0.6428571428571429,1,1,results
149,"On all the biomedical QA datasets , BioBERT achieved new state - of - the - art performance in terms of MRR .",Experimental results,Experimental results,relation-classification,7,"['O', 'O', 'O', 'B', 'I', 'I', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'B', 'I', 'I', 'B', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'O']","['O', 'O', 'O', 'B-b', 'I-b', 'I-b', 'O', 'B-b', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'I-p', 'B-ob', 'O']",14,0.45161290322580644,148,0.7437185929648241,14,1.0,1,1,results
2,Extracting Multiple - Relations in One - Pass with Pre-Trained Transformers,title,title,relation-classification,8,"['B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O']",,,1,0.0,1,0.0072992700729927005,1,0.0,1,1,research-problem
4,The state - of - the - art solutions for extracting multiple entity - relations from an input paragraph always require a multiple - pass encoding on the input .,abstract,abstract,relation-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.2,3,0.021897810218978103,1,0.2,1,1,research-problem
5,"This paper proposes anew solution that can complete the multiple entityrelations extraction task with only one - pass encoding on the input corpus , and achieve anew state - of - the - art accuracy performance , as demonstrated in the ACE 2005 benchmark .",abstract,abstract,relation-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.4,4,0.029197080291970802,2,0.4,1,1,research-problem
10,Relation extraction ( RE ) aims to find the semantic relation between a pair of entity mentions from an input paragraph .,Introduction,Introduction,relation-classification,8,"['B', 'I', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-ob', 'I-ob', 'O', 'B-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.09090909090909091,9,0.06569343065693431,1,0.09090909090909091,1,1,research-problem
12,One particular type of the RE task is multiplerelations extraction ( MRE ) that aims to recognize relations of multiple pairs of entity mentions from an input paragraph .,Introduction,Introduction,relation-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'B', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'O', 'B-ob', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",3,0.2727272727272727,11,0.08029197080291971,3,0.2727272727272727,1,1,research-problem
29,This section describes the proposed one - pass encoding MRE solution .,Proposed Approach,Proposed Approach,relation-classification,8,"['O', 'O', 'O', 'O', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",1,0.029411764705882353,28,0.20437956204379562,1,0.25,1,1,approach
30,"The solution is built upon BERT with a structured prediction layer to enable BERT to predict multiple relations with onepass encoding , and an entity - aware self - attention mechanism to infuse the relational information with regard to multiple entities at each layer of hidden states .",Proposed Approach,Proposed Approach,relation-classification,8,"['O', 'O', 'O', 'B', 'I', 'B', 'B', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'B', 'I', 'B', 'I', 'O', 'B', 'I', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'B', 'I', 'I', 'I', 'B', 'I', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'B-b', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'O', 'O', 'O', 'B-p', 'I-p', 'B-b', 'I-b', 'O', 'B-ob', 'I-ob', 'O', 'O', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'I-p', 'I-p', 'B-ob', 'I-ob', 'O']",2,0.058823529411764705,29,0.2116788321167883,2,0.5,1,1,approach
76,"We compare our solution with previous works that predict a single relation per pass , our model that predicts single relation per pass for MRE , and with the following naive modifications of BERT that could achieve MRE in one - pass .",Methods,Methods,relation-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.125,75,0.5474452554744526,1,0.125,1,1,baselines
77,"BERT SP : BERT with structured prediction only , which includes proposed improvement in 3.1 .",Methods,Methods,relation-classification,8,"['B', 'I', 'O', 'O', 'B', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-b', 'I-b', 'O', 'O', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.25,76,0.5547445255474452,2,0.25,1,1,baselines
78,"Entity - Aware BERT SP : our full model , which includes both improvements in 3.1 and 3.2 .",Methods,Methods,relation-classification,8,"['B', 'I', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O', 'B-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.375,77,0.5620437956204379,3,0.375,1,1,baselines
79,BERT SP with position embedding on the final attention layer .,Methods,Methods,relation-classification,8,"['B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O']",4,0.5,78,0.5693430656934306,4,0.5,1,1,baselines
81,"In this method , the BERT model encode the paragraph to the last attention - layer .",Methods,Methods,relation-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'B', 'O', 'B', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-b', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",6,0.75,80,0.583941605839416,6,0.75,1,1,baselines
82,"Then , for each entity pair , it takes the hidden states , adds the relative position embeddings corresponding to the target entities , and finally makes the relation prediction for this pair .",Methods,Methods,relation-classification,8,"['O', 'O', 'B', 'B', 'I', 'I', 'O', 'O', 'B', 'O', 'B', 'I', 'O', 'B', 'O', 'B', 'I', 'I', 'B', 'I', 'O', 'B', 'I', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'B-b', 'I-b', 'I-b', 'O', 'O', 'B-p', 'O', 'B-ob', 'I-ob', 'O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'I-p', 'O', 'B-ob', 'I-ob', 'O', 'O', 'O', 'B-p', 'O', 'B-ob', 'I-ob', 'O', 'O', 'O', 'O']",7,0.875,81,0.5912408759124088,7,0.875,1,1,baselines
86,The first observation is that our model architecture achieves much better results compared to the previous state - of - the - art methods .,Results on ACE 2005,Results on ACE 2005,relation-classification,8,"['O', 'B', 'I', 'O', 'O', 'B', 'I', 'I', 'B', 'B', 'I', 'I', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'B-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'B-p', 'I-p', 'O', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'B-b', 'I-b', 'I-b', 'B-p', 'I-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",2,0.10526315789473684,85,0.6204379562043796,2,0.08,1,1,results
94,"Our full model , with the structured fine - tuning of attention layers , brings further improvement of about 5.5 % , in the MRE one - pass setting , and achieves anew state - of - the - art performance when compared to the methods with domain adaptation .",Results on ACE 2005,Summing up the vectors confuses this information .,relation-classification,8,"['B', 'I', 'I', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'B', 'I', 'I', 'O', 'B', 'B', 'I', 'O', 'B', 'I', 'I', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",,,10,0.5263157894736842,93,0.6788321167883211,10,0.4,1,1,results
116,The results on SemEval 2018 Task 7 are shown in .,Results on SemEval 2018,Task 7,relation-classification,8,"['O', 'O', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'O', 'O', 'O', 'O']",2,0.2857142857142857,115,0.8394160583941606,6,0.46153846153846156,1,1,results
117,"Our Entity - Aware BERT SP gives comparable results to the top - ranked system in the shared task , with slightly lower Macro - F1 , which is the official metric of the task , and slightly higher Micro - F1 .",Results on SemEval 2018,Task 7,relation-classification,8,"['B', 'I', 'I', 'I', 'I', 'I', 'B', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'B', 'B', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.42857142857142855,116,0.8467153284671532,7,0.5384615384615384,1,1,results
118,"When predicting multiple relations in one - pass , we have 0.9 % drop on Macro - F1 , but a further 0.8 % improvement on Micro - F1 .",Results on SemEval 2018,Task 7,relation-classification,8,"['B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'B', 'I', 'I', 'B', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'O', 'O', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.5714285714285714,117,0.8540145985401459,8,0.6153846153846154,1,1,results
120,"On the other hand , compared to the top singlemodel result , which makes use of additional word and entity embeddings pretrained on in - domain data , our methods demonstrate clear advantage as a single model .",Results on SemEval 2018,Task 7,relation-classification,8,"['O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'B', 'I', 'I', 'I', 'I', 'B', 'I', 'B', 'I', 'I', 'I', 'O', 'B', 'I', 'B', 'B', 'I', 'I', 'I', 'B', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-b', 'I-b', 'I-b', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'B-b', 'I-b', 'B-p', 'B-p', 'I-p', 'I-p', 'I-p', 'B-ob', 'I-ob', 'O']",6,0.8571428571428571,119,0.8686131386861314,10,0.7692307692307693,1,1,results
2,SCIBERT : A Pretrained Language Model for Scientific Text,title,,relation-classification,9,"['O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O']",,,1,0.0,1,0.006802721088435374,1,0.0,1,1,research-problem
4,Obtaining large - scale annotated data for NLP tasks in the scientific domain is challenging and expensive .,abstract,abstract,relation-classification,9,"['B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O']",1,0.16666666666666666,3,0.02040816326530612,1,0.16666666666666666,1,1,research-problem
5,"We release SCIBERT , a pretrained language model based on BERT ( Devlin et al. , 2019 ) to address the lack of high - quality , large - scale labeled scientific data .",abstract,abstract,relation-classification,9,"['O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",2,0.3333333333333333,4,0.027210884353741496,2,0.3333333333333333,1,1,research-problem
9,The code and pretrained models are available at https://github.com/allenai/scibert/.,abstract,abstract,relation-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,1.0,8,0.05442176870748299,6,1.0,1,1,code
13,"In general domains , large - scale training data is often possible to obtain through crowdsourcing , but in scientific domains , annotated data is difficult and expensive to collect due to the expertise required for quality annotation .",Introduction,Introduction,relation-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",3,0.25,12,0.08163265306122448,3,0.25,1,1,research-problem
27,SCIB - ERT follows the same architecture as BERT but is instead pretrained on scientific text .,Background,Background,relation-classification,9,"['B', 'I', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'B', 'I', 'B', 'I', 'O']","['B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['B-b', 'I-b', 'I-b', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'O', 'O', 'O', 'B-p', 'I-p', 'B-ob', 'I-ob', 'O']",3,0.0375,26,0.17687074829931973,4,0.4,0,1,approach
31,"We construct SCIVOCAB , anew WordPiece vocabulary on our scientific corpus using the Sen - tencePiece 1 library .",Background,Background,relation-classification,9,"['O', 'B', 'B', 'O', 'B', 'I', 'I', 'B', 'B', 'I', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'O']","['O', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'B-p', 'B-b', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'B-b', 'I-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",7,0.0875,30,0.20408163265306123,8,0.8,0,1,approach
35,We train SCIBERT on a random sample of 1.14 M papers from Semantic Scholar .,Background,Corpus,relation-classification,9,"['O', 'B', 'B', 'B', 'O', 'B', 'I', 'B', 'B', 'I', 'I', 'B', 'B', 'I', 'O']","['O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['O', 'B-p', 'B-b', 'B-p', 'O', 'B-b', 'I-b', 'B-p', 'B-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'O']",11,0.1375,34,0.23129251700680273,1,0.16666666666666666,0,1,approach
36,This corpus consists of 18 % papers from the computer science domain and 82 % from the broad biomedical domain .,Background,Corpus,relation-classification,9,"['O', 'O', 'B', 'O', 'B', 'I', 'I', 'B', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'B', 'I', 'I', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']","['O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'O', 'O', 'O', 'O', 'O', 'B-b', 'I-b', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'O']",12,0.15,35,0.23809523809523808,2,0.3333333333333333,0,1,approach
44,Named Entity Recognition ( NER ),Background,Tasks,relation-classification,9,"['B', 'I', 'I', 'O', 'B', 'O']","['B-n', 'I-n', 'I-n', 'O', 'B-n', 'O']","['B-b', 'I-b', 'I-b', 'O', 'B-ob', 'O']",20,0.25,43,0.2925170068027211,3,0.3333333333333333,0,1,tasks
45,2 . PICO Extraction ( PICO ),Background,Tasks,relation-classification,9,"['O', 'O', 'B', 'I', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'O', 'O', 'O']","['O', 'O', 'B-b', 'I-b', 'O', 'O', 'O']",21,0.2625,44,0.29931972789115646,4,0.4444444444444444,0,1,tasks
46,3 . Text Classification ( CLS ),Background,Tasks,relation-classification,9,"['O', 'O', 'B', 'I', 'O', 'B', 'O']","['O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'O']","['O', 'O', 'B-b', 'I-b', 'O', 'B-ob', 'O']",22,0.275,45,0.30612244897959184,5,0.5555555555555556,0,1,tasks
47,4 . Relation Classification ( REL ),Background,Tasks,relation-classification,9,"['O', 'O', 'B', 'I', 'O', 'B', 'O']","['O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'O']","['O', 'O', 'B-b', 'I-b', 'O', 'B-ob', 'O']",23,0.2875,46,0.3129251700680272,6,0.6666666666666666,0,1,tasks
48,5 . Dependency Parsing ( DEP ) ,Background,Tasks,relation-classification,9,"['O', 'O', 'B', 'I', 'O', 'B', 'O', 'O']",,,24,0.3,47,0.3197278911564626,7,0.7777777777777778,0,1,
103,We observe that SCIBERT outperforms BERT - Base on scientific tasks ( + 2.11 F1 with finetuning and + 2.43 F1 without ),Background,Embeddings,relation-classification,9,"['O', 'B', 'O', 'B', 'B', 'B', 'I', 'I', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I']","['O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']","['O', 'B-p', 'O', 'B-b', 'B-p', 'B-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob']",79,0.9875,102,0.6938775510204082,22,0.9565217391304348,0,1,results
107,We observe that SCIBERT outperforms BERT - Base on biomedical tasks ( + 1.92 F1 with finetuning and + 3.59 F1 without ) .,Results,Biomedical Domain,relation-classification,9,"['O', 'B', 'O', 'B', 'B', 'B', 'I', 'I', 'B', 'B', 'I', 'O', 'B', 'I', 'I', 'B', 'I', 'O', 'B', 'I', 'I', 'B', 'O', 'O']",,,2,0.06451612903225806,106,0.7210884353741497,1,0.09090909090909091,1,1,results
108,"In addition , SCIB - ERT achieves new SOTA results on BC5 CDR and ChemProt , and EBM - NLP .",Results,Biomedical Domain,relation-classification,9,"['O', 'O', 'O', 'B', 'I', 'I', 'B', 'B', 'I', 'I', 'B', 'B', 'I', 'I', 'I', 'O', 'O', 'B', 'I', 'I', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'B-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'O']",3,0.0967741935483871,107,0.7278911564625851,2,0.18181818181818182,1,1,results
119,We observe that SCIBERT outperforms BERT - Base on computer science tasks ( + 3.55 F1 with finetuning and + 1.13 F1 without ) .,Results,Computer Science Domain,relation-classification,9,"['O', 'B', 'O', 'B', 'B', 'B', 'I', 'I', 'B', 'B', 'I', 'I', 'O', 'B', 'I', 'I', 'B', 'I', 'O', 'B', 'I', 'I', 'B', 'O', 'O']",,,14,0.45161290322580644,118,0.8027210884353742,1,0.3333333333333333,1,1,results
120,"In addition , SCIBERT achieves new SOTA results on ACL - ARC , and the NER part of SciERC .",Results,Computer Science Domain,relation-classification,9,"['O', 'O', 'O', 'B', 'B', 'B', 'I', 'I', 'B', 'B', 'I', 'I', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'B-b', 'B-p', 'B-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",15,0.4838709677419355,119,0.8095238095238095,2,0.6666666666666666,1,1,results
123,We observe that SCIBERT outperforms BERT - Base on the multidomain tasks ( + 0.49 F1 with finetuning and + 0.93 F1 without ) .,Results,Multiple Domains,relation-classification,9,"['O', 'B', 'O', 'B', 'B', 'B', 'I', 'I', 'B', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'B', 'I', 'O', 'B', 'I', 'I', 'B', 'O', 'O']",,,18,0.5806451612903226,122,0.8299319727891157,1,0.3333333333333333,1,1,results
124,"In addition , SCIBERT outperforms the SOTA on Sci - Cite .",Results,Multiple Domains,relation-classification,9,"['O', 'O', 'O', 'B', 'B', 'O', 'B', 'B', 'B', 'I', 'I', 'O']","['O', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'B-b', 'B-p', 'O', 'B-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'O']",19,0.6129032258064516,123,0.8367346938775511,2,0.6666666666666666,1,1,results
2,Character - level Convolutional Networks for Text Classification,title,,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'B', 'I']",,,1,0.5,1,0.004405286343612335,1,0.5,1,1,research-problem
13,"Text classification is a classic topic for natural language processing , in which one needs to assign predefined categories to free - text documents .",Introduction,Introduction,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",1,0.02040816326530612,12,0.05286343612334802,1,0.05555555555555555,1,1,research-problem
18,"In this article we explore treating text as a kind of raw signal at character level , and applying temporal ( one-dimensional ) ConvNets to it .",Introduction,Introduction,text-classification,0,"['O', 'O', 'O', 'O', 'B', 'B', 'I', 'B', 'I', 'I', 'I', 'B', 'I', 'B', 'B', 'I', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'B-b', 'I-b', 'B-p', 'I-p', 'I-p', 'I-p', 'B-b', 'I-b', 'B-p', 'B-b', 'I-b', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.12244897959183673,17,0.07488986784140969,6,0.3333333333333333,1,1,model
19,For this article we only used a classification task as away to exemplify ConvNets ' ability to understand texts .,Introduction,Introduction,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'O', 'O', 'B', 'I', 'B', 'I', 'I', 'B', 'I', 'B', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'O', 'O', 'B-p', 'I-p', 'B-b', 'I-b', 'I-b', 'B-p', 'I-p', 'B-ob', 'O']",7,0.14285714285714285,18,0.07929515418502203,7,0.3888888888888889,1,1,model
195,The most important conclusion from our experiments is that character - level ConvNets could work for text classification without the need for words .,Deep Learning Methods,We obtained Yahoo!,text-classification,0,"['O', 'B', 'I', 'I', 'B', 'B', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'B', 'I', 'I', 'B', 'I', 'B', 'I', 'I', 'I', 'B', 'O']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'O']","['O', 'B-b', 'I-b', 'I-b', 'B-p', 'B-b', 'I-b', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'I-p', 'B-b', 'I-b', 'B-p', 'I-p', 'I-p', 'I-p', 'B-ob', 'O']",74,0.74,194,0.8546255506607929,56,0.6829268292682927,1,1,results
199,The most obvious trend coming from all the plots in is that the larger datasets tend to perform better .,Deep Learning Methods,We obtained Yahoo!,text-classification,0,"['O', 'B', 'I', 'I', 'B', 'I', 'B', 'I', 'I', 'O', 'B', 'I', 'O', 'B', 'I', 'B', 'I', 'I', 'B', 'O']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'O']","['O', 'B-b', 'I-b', 'I-b', 'B-p', 'I-p', 'B-b', 'I-b', 'I-b', 'O', 'B-p', 'I-p', 'O', 'B-b', 'I-b', 'B-p', 'I-p', 'I-p', 'B-ob', 'O']",78,0.78,198,0.8722466960352423,60,0.7317073170731707,1,1,results
207,"However , further analysis is needed to validate the hypothesis that ConvNets are truly good at identifying exotic character combinations such as misspellings and emoticons , as our experiments alone do not show any explicit evidence .",Deep Learning Methods,Answers .,text-classification,0,"['O', 'O', 'B', 'I', 'O', 'B', 'B', 'I', 'O', 'B', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'B', 'I', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",,,86,0.86,206,0.9074889867841409,68,0.8292682926829268,1,1,results
217,"Comparing with traditional models , this suggests such a simple use of a distributed word representation may not give us an advantage to text classification .",Deep Learning Methods,Answers .,text-classification,0,"['B', 'I', 'B', 'I', 'O', 'O', 'B', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'B', 'I', 'I', 'I', 'B', 'I', 'I', 'I', 'I', 'O']","['B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['B-p', 'I-p', 'B-b', 'I-b', 'O', 'O', 'B-p', 'O', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'I-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",96,0.96,216,0.9515418502202643,78,0.9512195121951219,1,1,results
2,Supervised and Semi- Supervised Text Categorization using LSTM for Region Embeddings,title,title,text-classification,1,"['B', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O']",,,1,0.0,1,0.00390625,1,0.0,1,1,research-problem
4,"One - hot CNN ( convolutional neural network ) has been shown to be effective for text categorization ( Johnson & Zhang , 2015a ; b ) .",abstract,abstract,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.125,3,0.01171875,1,0.125,1,1,research-problem
21,"In this work , we consider a more general framework ( subsuming one - hot CNN ) which jointly trains a feature generator and a linear model , where the feature generator consists of ' region embedding + pooling ' .",Introduction,Introduction,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'O', 'B', 'O', 'O', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O', 'B-p', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O']",9,0.08108108108108109,20,0.078125,9,0.2647058823529412,1,1,model
31,"In this work , we build on the general framework of ' region embedding + pooling ' and explore a more sophisticated region embedding via Long Short - Term Memory ( LSTM ) , seeking to overcome the shortcomings above , in the supervised and semi-supervised settings .",Introduction,JZ15 proposed variations to alleviate these issues .,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.17117117117117117,30,0.1171875,19,0.5588235294117647,1,1,model
37,"We pursue the best use of LSTM for our purpose , and then compare the resulting model with the previous best methods including one - hot CNN and previous LSTM .",Introduction,LSTM ) is a recurrent neural network .,text-classification,1,"['O', 'B', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'I', 'I', 'B', 'O', 'B', 'I', 'I', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-b', 'I-b', 'I-b', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",25,0.22522522522522523,36,0.140625,25,0.7352941176470589,1,1,model
38,"Our strategy is to simplify the model as much as possible , including elimination of a word embedding layer routinely used to produce input to LSTM .",Introduction,LSTM ) is a recurrent neural network .,text-classification,1,"['O', 'B', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'O', 'B', 'I', 'I', 'B', 'I', 'I', 'I', 'B', 'I', 'I', 'O']","['O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['O', 'B-p', 'O', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'I-p', 'I-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'O']",26,0.23423423423423423,37,0.14453125,26,0.7647058823529411,1,1,model
139,"Comparing the two types of LSTM in , we see that our one - hot bidirectional LSTM with pooling ( oh - 2 LSTMp ) outperforms word - vector LSTM ( wv - LSTM ) on all the datasets , confirming the effectiveness of our approach .",Experiments ( supervised ),The datasets are summarized in .,text-classification,1,"['B', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'O', 'B', 'B', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'O', 'B', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'O', 'B-ob', 'I-ob', 'I-ob', 'O', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.21428571428571427,138,0.5390625,48,0.676056338028169,1,1,results
143,"In , on three out of the four datasets , oh - 2 LSTMp outperforms SVM and the CNN .",Experiments ( supervised ),Now we review the non -LSTM baseline methods .,text-classification,1,"['O', 'O', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'B', 'B', 'I', 'I', 'I', 'O']","['O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",19,0.2714285714285714,142,0.5546875,52,0.7323943661971831,1,1,results
212,"Therefore , we tested wv - 2 LSTMp ( word - vector bidirectional LSTM with pooling ) , whose only difference from oh - 2 LSTMp is that the input to the LSTM layers is the pre-trained word vectors .",Semi-supervised experiments,Other details followed the supervised experiments .,text-classification,1,"['O', 'O', 'O', 'B', 'B', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'B', 'I', 'B', 'I', 'I', 'I', 'B', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'O', 'O', 'B', 'I', 'I', 'O']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'O', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'O']",17,0.3333333333333333,211,0.82421875,17,0.6071428571428571,1,1,results
220,"Now we review the performance of one - hot CNN with one 200 - dim CNN tv-embedding row # 5 ) , which is comparable with our LSTM with two 100 - dim LSTM tv-embeddings ( row # 4 ) in terms of the dimensionality of tv-embeddings .",Semi-supervised experiments,Two types of word vectors were tested .,text-classification,1,"['O', 'O', 'B', 'O', 'B', 'B', 'B', 'I', 'I', 'I', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'B', 'I', 'I', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['O', 'O', 'B-p', 'O', 'B-b', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-b', 'I-b', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'O']",25,0.49019607843137253,219,0.85546875,25,0.8928571428571429,1,1,results
2,Bag of Tricks for Efficient Text Classification,title,title,text-classification,2,"['O', 'O', 'O', 'O', 'B', 'I', 'I']",,,1,0.0,1,0.010752688172043012,1,0.0,1,1,research-problem
4,This paper explores a simple and efficient baseline for text classification .,abstract,abstract,text-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'O']",1,0.3333333333333333,3,0.03225806451612903,1,0.3333333333333333,1,1,research-problem
22,shows a simple linear model with rank constraint .,Model architecture,Model architecture,text-classification,2,"['B', 'O', 'B', 'I', 'I', 'B', 'B', 'I', 'O']",,,5,0.16666666666666666,21,0.22580645161290322,5,0.38461538461538464,1,1,
23,The first weight matrix A is a look - up table over the words .,Model architecture,Model architecture,text-classification,2,"['O', 'O', 'B', 'I', 'I', 'B', 'I', 'B', 'I', 'I', 'I', 'B', 'O', 'B', 'O']",,,6,0.2,22,0.23655913978494625,6,0.46153846153846156,1,1,
24,"The word representations are then averaged into a text representation , which is in turn fed to a linear classifier .",Model architecture,Model architecture,text-classification,2,"['O', 'B', 'I', 'B', 'I', 'I', 'I', 'O', 'B', 'I', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'O', 'B', 'I', 'O']",,,7,0.23333333333333334,23,0.24731182795698925,7,0.5384615384615384,1,1,
27,We use the softmax function f to compute the probability distribution over the predefined classes .,Model architecture,Model architecture,text-classification,2,"['O', 'B', 'O', 'B', 'I', 'I', 'B', 'I', 'O', 'B', 'I', 'B', 'O', 'B', 'I', 'O']",,,10,0.3333333333333333,26,0.27956989247311825,10,0.7692307692307693,1,1,
34,"In order to improve our running time , we use a hierarchical softmax ) based on the Huffman coding tree .",Model architecture,Hierarchical softmax,text-classification,2,"['O', 'O', 'B', 'I', 'O', 'B', 'I', 'O', 'O', 'B', 'O', 'B', 'I', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'O']",,,17,0.5666666666666667,33,0.3548387096774194,3,0.1875,1,1,
45,"Instead , we use a bag of n-grams as additional features to capture some partial information about the local word order .",Model architecture,N - gram features,text-classification,2,"['O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'B', 'B', 'I', 'B', 'I', 'B', 'I', 'I', 'B', 'O', 'B', 'I', 'B', 'O']",,,28,0.9333333333333333,44,0.4731182795698925,14,0.875,1,1,
46,This is very efficient in practice while achieving comparable results to methods that explicitly use the order .,Model architecture,N - gram features,text-classification,2,"['O', 'O', 'B', 'I', 'B', 'I', 'O', 'B', 'B', 'I', 'B', 'I', 'I', 'I', 'I', 'O', 'O', 'O']",,,29,0.9666666666666667,45,0.4838709677419355,15,0.9375,1,1,
47,"We maintain a fast and memory efficient mapping of the n-grams by using the hashing trick with the same hashing function as in and 10M bins if we only used bigrams , and 100M otherwise .",Model architecture,N - gram features,text-classification,2,"['O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'B', 'O', 'B', 'B', 'I', 'O', 'B', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'O', 'B', 'I', 'O', 'O', 'B', 'I', 'B', 'O', 'O', 'O', 'O', 'O']",,,30,1.0,46,0.4946236559139785,16,1.0,1,1,
60,"We use 10 hidden units and run fastText for 5 epochs with a learning rate selected on a validation set from { 0.05 , 0.1 , 0.25 , 0.5 } .",Results .,We present the results in .,text-classification,2,"['O', 'O', 'B', 'B', 'I', 'O', 'B', 'B', 'B', 'B', 'I', 'B', 'O', 'B', 'I', 'B', 'I', 'O', 'B', 'I', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'B-ob', 'B-p', 'I-p', 'O', 'B-p', 'B-b', 'B-p', 'B-b', 'I-b', 'B-p', 'O', 'B-b', 'I-b', 'B-p', 'I-p', 'O', 'B-ob', 'I-ob', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",2,0.18181818181818182,59,0.6344086021505376,7,0.4666666666666667,1,1,tasks
61,"On this task , adding bigram information improves the performance by 1 - 4 % .",Results .,We present the results in .,text-classification,2,"['O', 'O', 'O', 'O', 'B', 'B', 'I', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'B-p', 'B-b', 'I-b', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",3,0.2727272727272727,60,0.6451612903225806,8,0.5333333333333333,1,1,tasks
62,"Overall our accuracy is slightly better than char - CNN and char - CRNN and , a bit worse than VDCNN .",Results .,We present the results in .,text-classification,2,"['O', 'B', 'I', 'O', 'B', 'I', 'I', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'B', 'I', 'I', 'B', 'O']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'O']","['O', 'B-b', 'I-b', 'O', 'B-p', 'I-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-ob', 'O']",4,0.36363636363636365,61,0.6559139784946236,9,0.6,1,1,tasks
63,"Note that we can increase the accuracy slightly by using more n-grams , for example with trigrams , the performance on Sogou goes up to 97.1 % .",Results .,We present the results in .,text-classification,2,"['O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'O', 'B', 'B', 'I', 'O', 'B', 'I', 'I', 'B', 'O', 'O', 'B', 'I', 'B', 'B', 'I', 'I', 'B', 'I', 'O']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'O', 'B-p', 'B-b', 'I-b', 'O', 'B-p', 'I-p', 'I-p', 'B-b', 'O', 'O', 'B-p', 'I-p', 'B-b', 'B-p', 'I-p', 'I-p', 'B-ob', 'I-ob', 'O']",5,0.45454545454545453,62,0.6666666666666666,10,0.6666666666666666,1,1,tasks
84,"At test time , Tagspace needs to compute the scores for all the classes which makes it relatively slow , while our fast inference gives a significant speed - up when the number of classes is large ( more than 300 K here ) .",Dataset and baselines .,Results and training time . and 200 .,text-classification,2,"['B', 'I', 'I', 'O', 'B', 'B', 'I', 'I', 'O', 'B', 'B', 'B', 'I', 'I', 'O', 'B', 'I', 'B', 'I', 'O', 'O', 'B', 'I', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['B-p', 'I-p', 'I-p', 'O', 'B-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['B-p', 'I-p', 'I-p', 'O', 'B-b', 'B-p', 'I-p', 'I-p', 'O', 'B-b', 'B-p', 'B-b', 'I-b', 'I-b', 'O', 'B-p', 'I-p', 'B-ob', 'I-ob', 'O', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",14,0.8235294117647058,83,0.8924731182795699,15,0.8333333333333334,1,1,tasks
85,"Overall , we are more than an order of magnitude faster to obtain model with a better quality .",Dataset and baselines .,Results and training time . and 200 .,text-classification,2,"['O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'B', 'B', 'I', 'B', 'B', 'O', 'B', 'I', 'O']","['O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'B-n', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'B-b', 'B-p', 'I-p', 'B-b', 'B-p', 'O', 'B-ob', 'I-ob', 'O']",15,0.8823529411764706,84,0.9032258064516129,16,0.8888888888888888,1,1,tasks
2,On the Role of Text Preprocessing in Neural Network Architectures : An Evaluation Study on Text Categorization and Sentiment Analysis,title,title,text-classification,3,"['O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",,,1,0.0,1,0.008130081300813009,1,0.0,1,1,research-problem
4,"Text preprocessing is often the first step in the pipeline of a Natural Language Processing ( NLP ) system , with potential impact in its final performance .",abstract,abstract,text-classification,3,"['B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.14285714285714285,3,0.024390243902439025,1,0.14285714285714285,1,1,research-problem
23,"In this paper we focus on the role of preprocessing the input text , particularly in how it is split into individual ( meaning - bearing ) tokens and how it affects the performance of standard neural text classification models based on .",Introduction,Introduction,text-classification,3,"['O', 'O', 'O', 'O', 'B', 'I', 'I', 'B', 'I', 'I', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'O', 'O', 'B', 'I', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'B', 'I', 'I', 'I', 'I', 'O', 'O', 'O']",,,12,0.24,22,0.17886178861788618,12,0.5714285714285714,1,1,research-problem
32,The accompanying materials of this submission can be downloaded at the following repository : github.com/pedrada88/preproc-textclassification .,Introduction,Introduction,text-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'O']",21,0.42,31,0.25203252032520324,21,1.0,1,1,code
63,We tried with two classification models .,Experimental setup,,text-classification,3,"['O', 'B', 'O', 'B', 'I', 'I', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'O']",1,0.14285714285714285,62,0.5040650406504065,1,0.02,1,1,experimental-setup
64,"The first one is a standard CNN model similar to that of , using ReLU as non-linear activation function .",Experimental setup,We tried with two classification models .,text-classification,3,"['O', 'B', 'I', 'I', 'O', 'B', 'I', 'I', 'B', 'I', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'B-p', 'I-p', 'I-p', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'I-p', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",2,0.2857142857142857,63,0.5121951219512195,2,0.04,1,1,experimental-setup
65,"In the second model , we add a recurrent layer ( specifically an LSTM ) before passing the pooled features directly to the fully connected softmax layer .",Experimental setup,We tried with two classification models .,text-classification,3,"['O', 'O', 'B', 'B', 'O', 'O', 'B', 'O', 'B', 'I', 'O', 'B', 'I', 'B', 'O', 'B', 'I', 'O', 'B', 'I', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'B-b', 'O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'O', 'B-p', 'I-p', 'B-ob', 'O', 'B-p', 'I-p', 'O', 'B-b', 'I-b', 'B-p', 'I-p', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.42857142857142855,64,0.5203252032520326,3,0.06,1,1,experimental-setup
68,The embedding layer was initialized using 300 - dimensional CBOW Word2vec embeddings trained on the 3B - word UMBC WebBase corpus with standard hyperparameters,Experimental setup,We tried with two classification models .,text-classification,3,"['O', 'B', 'I', 'O', 'B', 'I', 'B', 'I', 'I', 'I', 'I', 'I', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'B', 'B', 'I']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n']","['O', 'B-b', 'I-b', 'O', 'B-p', 'I-p', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'I-ob']",6,0.8571428571428571,67,0.5447154471544715,6,0.12,1,1,experimental-setup
95,"Nevertheless , the use of more complex preprocessing techniques such as lemmatization and multiword grouping does not help in general .",Evaluation datasets .,Experiment 1 : Preprocessing effect,text-classification,3,"['O', 'O', 'O', 'B', 'I', 'B', 'I', 'I', 'I', 'B', 'I', 'B', 'I', 'I', 'I', 'B', 'I', 'B', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'B-ob', 'O', 'O', 'O']",25,0.9615384615384616,94,0.7642276422764228,33,0.66,1,1,results
100,"In this experiment we observe a different trend , with multiwordenhanced vectors exhibiting a better performance both on the single CNN model ( best overall performance in seven of the nine datasets ) and on the CNN + LSTM model ( best performance in four datasets and in the same ballpark as the best results in four of the remaining five datasets ) .",Experiment 2 : Cross-preprocessing,Experiment 2 : Cross-preprocessing,text-classification,3,"['O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'B', 'O', 'B', 'I', 'B', 'I', 'O', 'B', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'B', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'B', 'I', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'B', 'I', 'B', 'I', 'I', 'O', 'B', 'I', 'I', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'O', 'B-b', 'I-b', 'B-p', 'I-p', 'O', 'B-b', 'I-b', 'I-b', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'O', 'B-p', 'I-p', 'I-p', 'B-ob', 'I-ob', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-b', 'I-b', 'B-p', 'I-p', 'I-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'O', 'O']",3,0.2,99,0.8048780487804879,38,0.76,1,1,results
109,"Interestingly , using multiword - wise embeddings on the vanilla setting leads to consistently better results than using them on the same multiwordgrouped preprocessed dataset in eight of the nine datasets .",Experiment 2 : Cross-preprocessing,Apple when part of this multiword expression .,text-classification,3,"['O', 'O', 'B', 'B', 'I', 'I', 'I', 'B', 'O', 'B', 'I', 'B', 'I', 'B', 'I', 'I', 'B', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'B', 'B', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'O', 'B-b', 'I-b', 'B-p', 'I-p', 'B-b', 'I-b', 'I-b', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",12,0.8,108,0.8780487804878049,47,0.94,1,1,results
111,"Apart from this somewhat surprising finding , the use of the embeddings trained on a simple tokenized corpus ( i.e. vanilla ) proved again competitive , as different preprocessing techniques such as lowercasing and lemmatizing do not seem to help .",Experiment 2 : Cross-preprocessing,Apple when part of this multiword expression .,text-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'B', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-b', 'B-p', 'I-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.9333333333333333,110,0.8943089430894309,49,0.98,1,1,results
2,Learning Context - Sensitive Convolutional Filters for Text Processing,title,,text-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I']",,,1,0.0,1,0.004524886877828055,1,0.0,1,1,research-problem
6,"In this paper , we consider an approach of using a small meta network to learn contextsensitive convolutional filters for text processing .",abstract,abstract,text-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",3,0.42857142857142855,5,0.02262443438914027,3,0.42857142857142855,1,1,research-problem
26,"In this paper , we propose a generic approach to learn context - sensitive convolutional filters for natural language understanding .",Introduction,Introduction,text-classification,4,"['O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'B', 'I', 'B', 'I', 'I', 'I', 'I', 'B', 'B', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'B-p', 'I-p', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'O']",15,0.6521739130434783,25,0.11312217194570136,15,0.6521739130434783,1,1,approach
27,"In contrast to traditional CNNs , the convolution operation in our framework does not have a fixed set of filters , and thus provides the network with stronger modeling flexibility and capacity .",Introduction,Introduction,text-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'B', 'B', 'I', 'B', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'B', 'O', 'B', 'B', 'B', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-b', 'I-b', 'B-p', 'B-b', 'I-b', 'B-p', 'I-p', 'I-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'O', 'O', 'O', 'B-p', 'O', 'B-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",16,0.6956521739130435,26,0.11764705882352941,16,0.6956521739130435,1,1,approach
28,"Specifically , we introduce a meta network to generate a set of contextsensitive filters , conditioned on specific input sentences ; these filters are adaptively applied to either the same ( Section 3.2 ) or different ( Section 3.3 ) text sequences .",Introduction,Introduction,text-classification,4,"['O', 'O', 'O', 'B', 'O', 'B', 'I', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'O', 'B', 'I', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'B-p', 'I-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'O', 'B-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.7391304347826086,27,0.12217194570135746,17,0.7391304347826086,1,1,approach
29,"In this manner , the learned filters vary from sentence to sentence and allow for more fine - grained feature abstraction .",Introduction,Introduction,text-classification,4,"['O', 'O', 'O', 'O', 'O', 'B', 'I', 'B', 'I', 'B', 'I', 'I', 'O', 'B', 'I', 'B', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'B-b', 'I-b', 'B-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'O', 'B-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",18,0.782608695652174,28,0.12669683257918551,18,0.782608695652174,1,1,approach
31,"In this regard , we propose a novel bidirectional filter generation mechanism to allow interactions between sentence pairs while constructing context - sensitive representations .",Introduction,Introduction,text-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'B', 'I', 'B', 'I', 'I', 'I', 'B', 'I', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'O', 'O', 'O', 'O', 'O']",20,0.8695652173913043,30,0.13574660633484162,20,0.8695652173913043,1,1,approach
152,"For the document classification experiments , we randomly initialize the word embeddings uniformly within [ ? 0.001 , 0.001 ] and update them during training .",Training Details,Training Details,text-classification,4,"['B', 'O', 'B', 'I', 'I', 'O', 'O', 'B', 'I', 'O', 'B', 'I', 'B', 'I', 'B', 'I', 'I', 'I', 'I', 'I', 'O', 'B', 'I', 'B', 'I', 'O']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['B-p', 'O', 'B-b', 'I-b', 'I-b', 'O', 'O', 'B-p', 'I-p', 'O', 'B-b', 'I-b', 'B-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'B-p', 'I-p', 'B-ob', 'I-ob', 'O']",1,0.07692307692307693,151,0.6832579185520362,14,0.42424242424242425,1,1,experimental-setup
153,"For the generated filters , we set the window size as h = 5 , with K = 100 feature maps ( the dimension of z is set as 100 ) .",Training Details,Training Details,text-classification,4,"['O', 'O', 'B', 'I', 'O', 'O', 'B', 'O', 'B', 'I', 'B', 'B', 'I', 'I', 'O', 'B', 'B', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-b', 'I-b', 'O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'O', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.15384615384615385,152,0.6877828054298643,15,0.45454545454545453,1,1,experimental-setup
155,"A one - layer architec - ture is utilized for both the CNN baseline and the ACNN model , since we did not observe significant performance gains with a multilayer architecture .",Training Details,Training Details,text-classification,4,"['O', 'B', 'I', 'I', 'I', 'I', 'I', 'O', 'B', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O', 'B-p', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.3076923076923077,154,0.6968325791855203,17,0.5151515151515151,1,1,experimental-setup
156,"The minibatch size is set as 128 , and a dropout rate of 0.2 is utilized on the embedding layer .",Training Details,Training Details,text-classification,4,"['O', 'B', 'I', 'B', 'I', 'I', 'B', 'O', 'O', 'O', 'B', 'I', 'B', 'B', 'B', 'I', 'I', 'O', 'B', 'I', 'O']","['O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['O', 'B-b', 'I-b', 'B-p', 'I-p', 'I-p', 'B-ob', 'O', 'O', 'O', 'B-b', 'I-b', 'B-p', 'B-b', 'B-p', 'I-p', 'I-p', 'O', 'B-ob', 'I-ob', 'O']",5,0.38461538461538464,155,0.7013574660633484,18,0.5454545454545454,1,1,experimental-setup
158,"For the sentence matching tasks , we initialized the word embeddings with 50 - dimensional Glove word vectors pretrained from Wikipedia 2014 and Gigaword 5 for all model variants .",Training Details,Training Details,text-classification,4,"['O', 'O', 'B', 'I', 'I', 'O', 'O', 'B', 'O', 'B', 'I', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'B', 'I', 'B', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-b', 'I-b', 'I-b', 'O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O']",7,0.5384615384615384,157,0.7104072398190046,20,0.6060606060606061,1,1,experimental-setup
159,"As for the filters , we set the window size as h = 5 , with K = 300 feature maps .",Training Details,Training Details,text-classification,4,"['O', 'B', 'O', 'B', 'O', 'O', 'B', 'O', 'B', 'I', 'B', 'B', 'I', 'I', 'O', 'B', 'B', 'I', 'I', 'I', 'I', 'O']",,,8,0.6153846153846154,158,0.7149321266968326,21,0.6363636363636364,1,1,experimental-setup
161,"We use Adam to train the models , with a learning rate of 3 10 ?4 .",Training Details,Training Details,text-classification,4,"['O', 'B', 'B', 'B', 'I', 'O', 'B', 'O', 'B', 'O', 'B', 'I', 'B', 'B', 'I', 'I', 'O']","['O', 'B-p', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['O', 'B-p', 'B-b', 'B-p', 'I-p', 'O', 'B-ob', 'O', 'B-p', 'O', 'B-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'O']",10,0.7692307692307693,160,0.7239819004524887,23,0.696969696969697,1,1,experimental-setup
162,"Dropout , with a rate of 0.5 , is employed on the word embedding layer .",Training Details,Training Details,text-classification,4,"['B', 'O', 'B', 'I', 'I', 'I', 'B', 'O', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'O']","['B-n', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['B-b', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-b', 'O', 'O', 'B-p', 'I-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'O']",11,0.8461538461538461,161,0.7285067873303167,24,0.7272727272727273,1,1,experimental-setup
164,All models are implemented with TensorFlow and are trained using one NVIDIA GeForce GTX TITAN X GPU with 12 GB memory .,Training Details,Training Details,text-classification,4,"['B', 'I', 'O', 'B', 'B', 'B', 'O', 'O', 'B', 'I', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'O']",,,13,1.0,163,0.7375565610859729,26,0.7878787878787878,1,1,experimental-setup
166,"For document classification , we consider several baseline models : ( i ) ngrams , a bag - of - means method based on TFIDF representations built by choosing the 500,000 most frequent n-grams ( up to 5 - grams ) from the training set and use their corresponding counts as features ; ( ii ) small / large word CNN : 6 layer word - based convolutional networks , with 256/1024 features at each layer , denoted as small / large , respectively ; ( iii ) deep CNN : deep convolutional neural networks with 9/17 /29 layers .",Baselines,Baselines,text-classification,4,"['B', 'B', 'I', 'O', 'O', 'B', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'B', 'I', 'B', 'I', 'B', 'I', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'B', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'B', 'B', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'B', 'B', 'I', 'B', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'O']",,,1,0.5,165,0.746606334841629,28,0.8484848484848485,1,1,baselines
175,"As illustrated in , S - ACNN significantly outperforms S - CNN on both datasets , demonstrating the advantage of the filtergeneration module in our ACNN framework .",Experimental Results,Document Classification,text-classification,4,"['O', 'O', 'B', 'O', 'B', 'I', 'I', 'B', 'I', 'B', 'I', 'I', 'B', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O']","['O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'I-p', 'B-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-b', 'I-b', 'O', 'B-ob', 'I-ob', 'I-ob', 'O']",3,0.061224489795918366,174,0.7873303167420814,2,0.14285714285714285,1,1,results
179,"Although we only use one convolution layer for our ACNN model , it already outperforms other CNN baseline methods with much deeper architectures .",Experimental Results,Document Classification,text-classification,4,"['O', 'O', 'O', 'B', 'B', 'I', 'I', 'B', 'O', 'B', 'I', 'O', 'O', 'B', 'I', 'B', 'I', 'I', 'I', 'B', 'B', 'I', 'I', 'O']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'B-p', 'B-b', 'I-b', 'I-b', 'B-p', 'O', 'B-b', 'I-b', 'O', 'O', 'B-p', 'I-p', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'O']",7,0.14285714285714285,178,0.8054298642533937,6,0.42857142857142855,1,1,results
182,"Moreover , our M - ACNN also achieves slightly better performance than self - attentive sentence embeddings proposed in , which requires significant more parameters than our method .",Experimental Results,Document Classification,text-classification,4,"['O', 'O', 'O', 'B', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'B', 'B', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'B', 'B', 'I', 'I', 'O', 'B', 'I', 'O']",,,10,0.20408163265306123,181,0.8190045248868778,9,0.6428571428571429,1,1,results
200,"Notably , our model yields significantly better results than an attentive pooling network and ABCNN ( attention - based CNN ) baselines .",Experimental Results,Answer Sentence Selection,text-classification,4,"['O', 'O', 'B', 'I', 'B', 'B', 'I', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'B-b', 'I-b', 'B-p', 'B-b', 'I-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",28,0.5714285714285714,199,0.9004524886877828,12,0.6666666666666666,1,1,results
2,Universal Language Model Fine - tuning for Text Classification,title,title,text-classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I']",,,1,0.0,1,0.003968253968253968,1,0.0,1,1,research-problem
31,"1 ) We propose Universal Language Model Fine - tuning ( ULMFiT ) , a method that can be used to achieve CV - like transfer learning for any task for NLP .",Introduction,Contributions,text-classification,5,"['O', 'O', 'O', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'O', 'B', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'B', 'I', 'I', 'I', 'I', 'B', 'B', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O', 'B-ob', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",22,0.8461538461538461,30,0.11904761904761904,22,0.8461538461538461,1,1,model
32,"2 ) We propose discriminative fine - tuning , slanted triangular learning rates , and gradual unfreezing , novel techniques to retain previous knowledge and avoid catastrophic forgetting during fine - tuning .",Introduction,Contributions,text-classification,5,"['O', 'O', 'O', 'B', 'B', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'B', 'I', 'B', 'I', 'O', 'B', 'B', 'I', 'B', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'B-ob', 'I-ob', 'O', 'O', 'O', 'B-p', 'I-p', 'B-b', 'I-b', 'O', 'B-p', 'B-b', 'I-b', 'B-p', 'O', 'O', 'O', 'O']",23,0.8846153846153846,31,0.12301587301587301,23,0.8846153846153846,1,1,model
162,"We use the AWD - LSTM language model with an embedding size of 400 , 3 layers , 1150 hidden activations per layer , and a BPTT batch size of 70 .",Hyperparameters,Hyperparameters,text-classification,5,"['O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'B', 'O', 'B', 'I', 'O', 'B', 'O', 'B', 'B', 'O', 'B', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'B', 'O']",,,3,0.375,161,0.6388888888888888,3,0.25,1,1,hyperparameters
163,"We apply dropout of 0.4 to layers , 0.3 to RNN layers , 0.4 to input embedding layers , 0.05 to embedding layers , and weight dropout of 0.5 to the RNN hidden - to - hidden matrix .",Hyperparameters,Hyperparameters,text-classification,5,"['O', 'B', 'B', 'O', 'B', 'B', 'I', 'O', 'B', 'B', 'I', 'I', 'O', 'O', 'B', 'I', 'I', 'I', 'O', 'B', 'B', 'I', 'I', 'O', 'O', 'B', 'I', 'O', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'B-p', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'B-p', 'B-b', 'O', 'B-p', 'B-ob', 'I-ob', 'O', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'O', 'O', 'B-b', 'I-b', 'O', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",4,0.5,162,0.6428571428571429,4,0.3333333333333333,1,1,hyperparameters
164,The classifier has a hidden layer of size 50 .,Hyperparameters,Hyperparameters,text-classification,5,"['O', 'B', 'O', 'O', 'B', 'I', 'O', 'B', 'B', 'O']","['O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'O']","['O', 'B-b', 'O', 'O', 'B-b', 'I-b', 'O', 'B-p', 'B-ob', 'O']",5,0.625,163,0.6468253968253969,5,0.4166666666666667,1,1,hyperparameters
165,"We use Adam with ? 1 = 0.7 instead of the default ? 1 = 0.9 and ? 2 = 0.99 , similar to .",Hyperparameters,Hyperparameters,text-classification,5,"['O', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']","['O', 'O', 'B-b', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O']",6,0.75,164,0.6507936507936508,6,0.5,1,1,hyperparameters
166,"We use a batch size of 64 , abase learning rate of 0.004 and 0.01 for finetuning the LM and the classifier respectively , and tune the number of epochs on the validation set of each task 7 .",Hyperparameters,Hyperparameters,text-classification,5,"['O', 'O', 'O', 'B', 'I', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'B', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O']",,,7,0.875,165,0.6547619047619048,7,0.5833333333333334,1,1,hyperparameters
175,"Our method outperforms both CoVe , a state - of - the - art transfer learning method based on hypercolumns , as well as the state - of - the - art on both datasets .",Results,Results,text-classification,5,"['O', 'B', 'B', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'B-n', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'B-b', 'B-p', 'O', 'B-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",3,0.13636363636363635,174,0.6904761904761905,3,0.2,1,1,results
176,"On IMDb , we reduce the error dramatically by 43.9 % and 22 % with regard to CoVe and the state - of - the - art respectively .",Results,Results,text-classification,5,"['B', 'B', 'O', 'O', 'B', 'O', 'B', 'O', 'B', 'B', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'O', 'O', 'B-p', 'O', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-b', 'O', 'O', 'B-p', 'O', 'B-b', 'O', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.18181818181818182,175,0.6944444444444444,4,0.26666666666666666,1,1,results
181,"On TREC - 6 , our improvement - similar as the improvements of state - of - the - art approaches - is not statistically significant , due to the small size of the 500 - examples test set .",Results,Results,text-classification,5,"['O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",,,9,0.4090909090909091,180,0.7142857142857143,9,0.6,1,1,results
186,"On AG , we observe a similarly dramatic error reduction by 23.7 % compared to the state - of - the - art .",Results,Results,text-classification,5,"['O', 'B', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'B', 'I', 'I', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O']",,,14,0.6363636363636364,185,0.7341269841269841,14,0.9333333333333333,1,1,results
187,"On DBpedia , Yelp - bi , and Yelp - full , we reduce the error by 4.8 % , 18.2 % , 2.0 % respectively .",Results,Results,text-classification,5,"['O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'B', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O']","['O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O']",15,0.6818181818181818,186,0.7380952380952381,15,1.0,1,1,results
202,"On IMDb and AG , supervised ULMFiT with only 100 labeled examples matches the performance of training from scratch with 10 and 20 more data respectively , clearly demonstrating the benefit of general - domain LM pretraining .",Pretraining,We show the results in .,text-classification,5,"['B', 'B', 'I', 'I', 'O', 'B', 'I', 'B', 'I', 'B', 'I', 'I', 'B', 'I', 'I', 'I', 'B', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-b', 'I-b', 'I-b', 'O', 'B-b', 'I-b', 'B-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'B-p', 'I-p', 'I-p', 'I-p', 'B-b', 'I-b', 'I-b', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.7777777777777778,201,0.7976190476190477,7,0.7777777777777778,1,1,ablation-analysis
204,"On TREC - 6 , ULMFiT significantly improves upon training from scratch ; as examples are shorter and fewer , supervised and semi-supervised ULMFiT achieve similar results .",Pretraining,We show the results in .,text-classification,5,"['O', 'B', 'I', 'I', 'O', 'B', 'B', 'I', 'I', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-b', 'I-b', 'I-b', 'O', 'B-b', 'B-p', 'I-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,1.0,203,0.8055555555555556,9,1.0,1,1,ablation-analysis
207,"Pretraining is most useful for small and medium - sized datasets , which are most common in commercial applications .",Impact of pretraining,Impact of pretraining,text-classification,5,"['O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.047619047619047616,206,0.8174603174603174,2,0.058823529411764705,1,1,ablation-analysis
211,"Using our fine - tuning techniques , even a regular LM reaches surprisingly good performance on the larger datasets .",Impact of pretraining,Impact of LM quality,text-classification,5,"['B', 'B', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'I', 'I', 'B', 'O', 'B', 'I', 'O']","['B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-b', 'I-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'O']",6,0.14285714285714285,210,0.8333333333333334,6,0.17647058823529413,1,1,ablation-analysis
212,"On the smaller TREC - 6 , a vanilla LM without dropout runs the risk of overfitting , which decreases performance .",Impact of pretraining,Impact of LM quality,text-classification,5,"['B', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'B', 'I', 'I', 'I', 'B', 'I', 'I', 'I', 'B', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'O', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'I-p', 'I-p', 'B-ob', 'O', 'O', 'O', 'O', 'O']",7,0.16666666666666666,211,0.8373015873015873,7,0.20588235294117646,1,1,ablation-analysis
215,Fine - tuning the LM is most beneficial for larger datasets .,Impact of pretraining,Impact of LM fine - tuning,text-classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'B', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-ob', 'I-ob', 'O']",10,0.23809523809523808,214,0.8492063492063492,10,0.29411764705882354,1,1,ablation-analysis
225,"Fine - tuning the classifier significantly improves over training from scratch , particularly on the small TREC - 6 . ' Last ' , the standard fine - tuning method in CV , severely underfits and is never able to lower the training error to 0 . ' Chainthaw ' achieves competitive performance on the smaller datasets , but is outperformed significantly on the large AG .",Impact of pretraining,We show the results in .,text-classification,5,"['O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'B', 'I', 'I', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-b', 'I-b', 'I-b', 'O', 'B-p', 'I-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.47619047619047616,224,0.8888888888888888,20,0.5882352941176471,1,1,ablation-analysis
238,"At the cost of training a second model , ensembling the predictions of a forward and backwards LM - classifier brings a performance boost of around 0.5 - 0.7 .",Impact of pretraining,Impact of bidirectionality,text-classification,5,"['O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'O', 'O', 'B', 'I', 'I', 'O']","['O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'O']",33,0.7857142857142857,237,0.9404761904761905,33,0.9705882352941176,1,1,ablation-analysis
239,On IMD b we lower the test error from 5.30 of a single model to 4.58 for the bidirectional model .,Impact of pretraining,Impact of bidirectionality,text-classification,5,"['B', 'B', 'I', 'O', 'B', 'O', 'B', 'I', 'B', 'B', 'B', 'O', 'B', 'I', 'B', 'B', 'B', 'O', 'B', 'I', 'O']","['B-p', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['B-p', 'B-b', 'I-b', 'O', 'B-p', 'O', 'B-b', 'I-b', 'B-p', 'B-b', 'B-p', 'O', 'B-ob', 'I-ob', 'B-p', 'B-b', 'B-p', 'O', 'B-ob', 'I-ob', 'O']",34,0.8095238095238095,238,0.9444444444444444,34,1.0,1,1,ablation-analysis
2,Universal Sentence Encoder,title,,text-classification,6,"['B', 'I', 'I']",,,1,0.0,1,0.006756756756756757,1,0.0,1,1,research-problem
4,We present models for encoding sentences into embedding vectors that specifically target transfer learning to other NLP tasks .,abstract,abstract,text-classification,6,"['O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O']",1,0.1111111111111111,3,0.02027027027027027,1,0.1111111111111111,1,1,research-problem
9,We find that transfer learning using sentence embeddings tends to outperform word level transfer .,abstract,abstract,text-classification,6,"['O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'B', 'I', 'I', 'O']",,,6,0.6666666666666666,8,0.05405405405405406,6,0.6666666666666666,1,1,research-problem
10,"With transfer learning via sentence embeddings , we observe surprisingly good performance with minimal amounts of supervised training data fora transfer task .",abstract,abstract,text-classification,6,"['O', 'B', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.7777777777777778,9,0.060810810810810814,7,0.7777777777777778,1,1,research-problem
32,Both models are implemented in TensorFlow and are available to download from TF Hub : 1 https://tfhub.dev/google/universal-sentence-encoder/1,Model Toolkit,Model Toolkit,text-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob']",3,0.0967741935483871,31,0.20945945945945946,3,0.375,1,1,code
44,The transformer based sentence encoding model constructs sentence embeddings using the encoding sub - graph of the transformer architecture .,Model Toolkit,Transformer,text-classification,6,"['O', 'O', 'B', 'B', 'I', 'I', 'B', 'B', 'I', 'B', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O']",,,15,0.4838709677419355,43,0.2905405405405405,1,0.058823529411764705,1,1,model
45,This sub - graph uses attention to compute context aware representations of words in a sentence that take into account both the ordering and identity of all the other words .,Model Toolkit,Transformer,text-classification,6,"['O', 'O', 'O', 'O', 'B', 'B', 'B', 'I', 'B', 'I', 'I', 'B', 'B', 'I', 'I', 'I', 'B', 'I', 'I', 'I', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'B-p', 'B-b', 'B-p', 'I-p', 'B-b', 'I-b', 'I-b', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",16,0.5161290322580645,44,0.2972972972972973,2,0.11764705882352941,1,1,model
46,The context aware word representations are converted to a fixed length sentence encoding vector by computing the element - wise sum of the representations at each word position .,Model Toolkit,Transformer,text-classification,6,"['O', 'B', 'I', 'I', 'I', 'O', 'B', 'B', 'I', 'B', 'I', 'I', 'I', 'I', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'B-b', 'I-b', 'I-b', 'I-b', 'O', 'B-p', 'B-p', 'I-p', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",17,0.5483870967741935,45,0.30405405405405406,3,0.17647058823529413,1,1,model
48,The encoding model is designed to be as general purpose as possible .,Model Toolkit,Transformer,text-classification,6,"['O', 'B', 'I', 'O', 'B', 'I', 'I', 'O', 'B', 'I', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O']","['O', 'B-b', 'I-b', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-b', 'I-b', 'O', 'O', 'O']",19,0.6129032258064516,47,0.31756756756756754,5,0.29411764705882354,1,1,model
49,This is accomplished by using multi-task learning whereby a single encoding model is used to feed multiple downstream tasks .,Model Toolkit,Transformer,text-classification,6,"['O', 'O', 'B', 'I', 'B', 'I', 'I', 'B', 'B', 'I', 'I', 'I', 'B', 'I', 'I', 'I', 'B', 'I', 'I', 'O']","['O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['O', 'O', 'B-p', 'I-p', 'B-b', 'I-b', 'I-b', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'I-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'O']",20,0.6451612903225806,48,0.32432432432432434,6,0.35294117647058826,1,1,model
55,The second encoding model makes use of a deep averaging network ( DAN ) whereby input embeddings for words and bi-grams are first averaged together and then passed through a feedforward deep neural network ( DNN ) to produce sentence embeddings .,Model Toolkit,Transformer,text-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'B', 'I', 'B', 'I', 'O', 'B', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'B', 'I', 'B', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'B-ob', 'I-ob', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'B-ob', 'I-ob', 'O']",26,0.8387096774193549,54,0.36486486486486486,12,0.7058823529411765,1,1,model
56,"Similar to the Transformer encoder , the DAN encoder takes as input a lowercased PTB tokenized string and outputs a 512 dimensional sentence embedding .",Model Toolkit,Transformer,text-classification,6,"['O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'O']",,,27,0.8709677419354839,55,0.3716216216216216,13,0.7647058823529411,1,1,model
58,We make use of mul-titask learning whereby a single DAN encoder is used to supply sentence embeddings for multiple downstream tasks .,Model Toolkit,Transformer,text-classification,6,"['O', 'O', 'B', 'O', 'B', 'I', 'B', 'O', 'B', 'I', 'I', 'O', 'B', 'I', 'I', 'B', 'I', 'B', 'B', 'I', 'I', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'O', 'B-p', 'I-p', 'I-p', 'B-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'O']",29,0.9354838709677419,57,0.38513513513513514,15,0.8823529411764706,1,1,model
59,The primary advantage of the DAN encoder is that compute time is linear in the length of the input sequence .,Model Toolkit,Transformer,text-classification,6,"['O', 'B', 'I', 'B', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O']","['O', 'B-p', 'I-p', 'B-p', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O']","['O', 'B-p', 'I-p', 'B-p', 'O', 'O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'O']",30,0.967741935483871,58,0.3918918918918919,16,0.9411764705882353,1,1,model
69,MR : Movie review snippet sentiment on a five star scale .,Encoder Training Data,Transfer Tasks,text-classification,6,"['B', 'O', 'B', 'I', 'I', 'I', 'B', 'O', 'B', 'I', 'I', 'O']","['B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['B-b', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'O']",8,0.4,68,0.4594594594594595,3,0.2,1,1,tasks
70,CR : Sentiment of sentences mined from customer reviews .,Encoder Training Data,Transfer Tasks,text-classification,6,"['B', 'O', 'B', 'I', 'I', 'B', 'I', 'B', 'I', 'O']","['B-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['B-b', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'I-p', 'B-ob', 'I-ob', 'O']",9,0.45,69,0.46621621621621623,4,0.26666666666666666,1,1,tasks
71,SUBJ : Subjectivity of sentences from movie reviews and plot summaries .,Encoder Training Data,Transfer Tasks,text-classification,6,"['B', 'O', 'B', 'I', 'I', 'B', 'B', 'I', 'I', 'I', 'I', 'O']","['B-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['B-b', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",10,0.5,70,0.47297297297297297,5,0.3333333333333333,1,1,tasks
72,MPQA : Phrase level opinion polarity from news data .,Encoder Training Data,Transfer Tasks,text-classification,6,"['B', 'O', 'B', 'I', 'I', 'I', 'B', 'B', 'I', 'O']","['B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['B-b', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'O']",11,0.55,71,0.4797297297297297,6,0.4,1,1,tasks
73,TREC : Fine grained question classification sourced from TREC .,Encoder Training Data,Transfer Tasks,text-classification,6,"['B', 'O', 'B', 'I', 'I', 'I', 'B', 'I', 'O', 'O']","['B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'O']","['B-b', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'O', 'O']",12,0.6,72,0.4864864864864865,7,0.4666666666666667,1,1,tasks
74,SST : Binary phrase level sentiment classification .,Encoder Training Data,Transfer Tasks,text-classification,6,"['B', 'O', 'B', 'I', 'I', 'I', 'I', 'O']","['B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['B-b', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",13,0.65,73,0.49324324324324326,8,0.5333333333333333,1,1,tasks
75,STS Benchmark : Semantic textual similarity ( STS ) between sentence pairs scored by Pearson correlation with human judgments .,Encoder Training Data,Transfer Tasks,text-classification,6,"['B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'I', 'B', 'I', 'B', 'I', 'I', 'I', 'I', 'O']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['B-b', 'I-b', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-b', 'I-b', 'B-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",14,0.7,74,0.5,9,0.6,1,1,tasks
76,WEAT : Word pairs from the psychology literature on implicit association tests ( IAT ) that are used to characterize model bias .,Encoder Training Data,Transfer Tasks,text-classification,6,"['B', 'O', 'B', 'I', 'B', 'O', 'B', 'I', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-b', 'O', 'B-b', 'I-b', 'B-p', 'O', 'B-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.75,75,0.5067567567567568,10,0.6666666666666666,1,1,tasks
87,Additional baseline CNN and DAN models are trained without using any pretrained word or sentence embeddings .,Baselines,Baselines,text-classification,6,"['B', 'I', 'B', 'I', 'I', 'I', 'B', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'O']","['B-n', 'I-n', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['B-b', 'I-b', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",5,1.0,86,0.581081081081081,5,1.0,1,1,baselines
112,We observe that transfer learning from the transformer based sentence encoder usually performs as good or better than transfer learning from the DAN encoder .,Results,Results,text-classification,6,"['O', 'B', 'O', 'B', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'B', 'I', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O']","['O', 'B-p', 'O', 'B-b', 'I-b', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'O']",2,0.06451612903225806,111,0.75,2,0.16666666666666666,1,1,results
114,Models that make use of sentence level transfer learning tend to perform better than models that only use word level transfer .,Results,Results,text-classification,6,"['B', 'O', 'B', 'I', 'I', 'B', 'I', 'I', 'I', 'B', 'I', 'I', 'I', 'I', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['B-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['B-b', 'O', 'B-p', 'I-p', 'I-p', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",4,0.12903225806451613,113,0.7635135135135135,4,0.3333333333333333,1,1,results
117,"We observe that , for smaller quantities of data , sentence level transfer learning can achieve surprisingly good task performance .",Results,Results,text-classification,6,"['O', 'O', 'O', 'O', 'B', 'B', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'B', 'I', 'B', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",7,0.22580645161290322,116,0.7837837837837838,7,0.5833333333333334,1,1,results
118,"As the training set size increases , models that do not make use of transfer learning approach the performance of the other models .",Results,Results,text-classification,6,"['B', 'O', 'B', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O']","['B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'O']",8,0.25806451612903225,117,0.7905405405405406,8,0.6666666666666666,1,1,results
2,Investigating Capsule Networks with Dynamic Routing for Text Classification,title,,text-classification,7,"['O', 'O', 'O', 'O', 'B', 'I', 'O', 'B', 'I']",,,1,0.0,1,0.00411522633744856,1,0.0,1,1,research-problem
10,1 Codes are publicly available at : https://github.com/andyweizhao/capsule_text_classification .,abstract,abstract,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'O']",7,1.0,9,0.037037037037037035,7,1.0,1,1,code
12,Modeling articles or sentences computationally is a fundamental topic in natural language processing .,Introduction,Introduction,text-classification,7,"['B', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.037037037037037035,11,0.04526748971193416,1,0.037037037037037035,1,1,research-problem
16,"Earlier efforts in modeling texts have achieved limited success on text categorization using a simple bag - of - words classifier , implying understanding the meaning of the individual word or n-gram is a necessary step towards more sophisticated models .",Introduction,Introduction,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.18518518518518517,15,0.06172839506172839,5,0.18518518518518517,1,1,research-problem
40,"Our capsule network , depicted in , is a variant of the capsule networks proposed in .",Our Model,Our Model,text-classification,7,"['O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.010638297872340425,39,0.16049382716049382,1,0.05555555555555555,1,1,model
41,"It consists of four layers : ngram convolutional layer , primary capsule layer , convolutional capsule layer , and fully connected capsule layer .",Our Model,Our Model,text-classification,7,"['O', 'B', 'I', 'B', 'I', 'O', 'B', 'I', 'I', 'O', 'B', 'I', 'I', 'O', 'B', 'I', 'I', 'O', 'O', 'B', 'I', 'I', 'I', 'O']","['O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'B-p', 'I-p', 'B-b', 'I-b', 'O', 'B-ob', 'I-ob', 'I-ob', 'O', 'B-ob', 'I-ob', 'I-ob', 'O', 'B-ob', 'I-ob', 'I-ob', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",2,0.02127659574468085,40,0.1646090534979424,2,0.1111111111111111,1,1,model
42,"In addition , we explore two capsule frameworks to integrate these four components in different ways .",Our Model,Our Model,text-classification,7,"['O', 'O', 'O', 'O', 'B', 'B', 'I', 'I', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'B-p', 'B-b', 'I-b', 'I-b', 'B-p', 'I-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",3,0.031914893617021274,41,0.16872427983539096,3,0.16666666666666666,1,1,model
45,This layer is a standard convolutional layer which extracts n-gram features at different positions of a sentence through various convolutional filters .,Our Model,Our Model,text-classification,7,"['O', 'O', 'B', 'I', 'B', 'I', 'I', 'O', 'B', 'B', 'I', 'B', 'I', 'I', 'I', 'O', 'B', 'B', 'B', 'I', 'I', 'O']","['O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['O', 'O', 'B-p', 'I-p', 'B-b', 'I-b', 'I-b', 'O', 'B-p', 'B-b', 'I-b', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'O']",6,0.06382978723404255,44,0.18106995884773663,6,0.3333333333333333,1,1,model
59,This is the first capsule layer in which the capsules replace the scalar - output feature detectors of CNNs with vector- output capsules to preserve the instantiated parameters such as the local order of words and semantic representations of words .,Our Model,Primary Capsule Layer,text-classification,7,"['O', 'B', 'O', 'B', 'I', 'I', 'B', 'I', 'O', 'B', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'B', 'B', 'I', 'I', 'B', 'I', 'O', 'B', 'I', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'I-p', 'O', 'B-b', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'B-b', 'I-b', 'I-b', 'B-p', 'I-p', 'O', 'B-b', 'I-b', 'B-p', 'I-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",20,0.2127659574468085,58,0.23868312757201646,1,0.14285714285714285,1,1,model
85,The basic idea of dynamic routing is to construct a non-linear map in an iterative manner ensuring that the output of each capsule gets sent to an appropriate parent in the subsequent layer :,Our Model,Dynamic Routing,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'B', 'I', 'B', 'O', 'B', 'I', 'B', 'O', 'O', 'B', 'I', 'I', 'I', 'B', 'I', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-b', 'I-b', 'B-p', 'O', 'B-b', 'I-b', 'B-p', 'O', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",46,0.48936170212765956,84,0.345679012345679,1,0.1111111111111111,1,1,model
108,"In this layer , each capsule is connected only to a local region K 2 C spatially in the layer below .",Our Model,Convolutional Capsule Layer,text-classification,7,"['O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'B', 'I', 'O', 'B', 'I', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'B-b', 'I-b', 'O', 'O', 'O', 'O', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'O', 'B-ob', 'I-ob', 'O']",69,0.7340425531914894,107,0.4403292181069959,1,0.1111111111111111,1,1,model
109,Those capsules in the region multiply transformation matrices to learn child - parent relationships followed by routing by agreement to produce parent capsules in the layer above .,Our Model,Convolutional Capsule Layer,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'B', 'B', 'I', 'B', 'I', 'B', 'I', 'I', 'I', 'O', 'O', 'B', 'I', 'I', 'B', 'I', 'B', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-b', 'I-b', 'B-p', 'I-p', 'B-b', 'I-b', 'I-b', 'I-b', 'O', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",70,0.7446808510638298,108,0.4444444444444444,2,0.2222222222222222,1,1,model
118,The capsules in the layer below are flattened into a list of capsules and fed into fully connected capsule layer in which capsules are multiplied by transformation matrix W d 1 ?,Our Model,Fully Connected Capsule Layer,text-classification,7,"['O', 'B', 'I', 'I', 'I', 'I', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'O', 'B', 'I', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O', 'B-p', 'I-p', 'O', 'B-b', 'I-b', 'I-b', 'O', 'B-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",79,0.8404255319148937,117,0.48148148148148145,1,0.16666666666666666,1,1,model
125,We explore two capsule architectures ( denoted as Capsule - A and Capsule - B ) to integrate these four,Our Model,The Architectures of Capsule Network,text-classification,7,"['O', 'B', 'B', 'I', 'I', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-b', 'I-b', 'I-b', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'O', 'B-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O']",86,0.9148936170212766,124,0.5102880658436214,1,0.1111111111111111,1,1,model
139,"In the experiments , we use 300 - dimensional word2vec vectors to initialize embedding vectors .",Implementation Details,Implementation Details,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'B', 'B', 'I', 'I', 'I', 'I', 'B', 'I', 'B', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'B-ob', 'I-ob', 'O']",1,0.25,138,0.5679012345679012,1,0.25,1,1,hyperparameters
140,We conduct mini-batch with size 50 for AG 's news and size 25 for other datasets .,Implementation Details,Implementation Details,text-classification,7,"['O', 'B', 'B', 'B', 'B', 'I', 'B', 'B', 'I', 'I', 'O', 'B', 'I', 'O', 'B', 'I', 'O']","['O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O']","['O', 'B-p', 'B-b', 'B-p', 'B-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'O', 'B-b', 'I-b', 'O', 'B-ob', 'I-ob', 'O']",2,0.5,139,0.5720164609053497,2,0.5,1,1,hyperparameters
141,We use Adam optimization algorithm with 1e - 3 learning rate to train the model .,Implementation Details,Implementation Details,text-classification,7,"['O', 'O', 'B', 'I', 'I', 'B', 'B', 'I', 'I', 'I', 'I', 'B', 'I', 'O', 'B', 'O']","['O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'O']","['O', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'O', 'B-ob', 'O']",3,0.75,140,0.5761316872427984,3,0.75,1,1,hyperparameters
142,We use 3 iteration of routing for all datasets since it optimizes the loss faster and converges to a lower loss at the end .,Implementation Details,Implementation Details,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,1.0,141,0.5802469135802469,4,1.0,1,1,hyperparameters
144,"In the experiments , we evaluate and compare our model with several strong baseline methods including : LSTM / Bi - LSTM , tree - structured LSTM ( Tree - LSTM ) , LSTM regularized by linguistic knowledge ( LR - LSTM ) , CNNrand / CNN - static / CNN - non-static ( Kim , 2014 ) , very deep convolutional network ( VD - CNN ) , and character - level convolutional network ( CL - CNN ) .",Baseline methods,Baseline methods,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']",,,1,0.0,143,0.588477366255144,1,0.0,1,1,baselines
149,"From the results , we observe that the capsule networks achieve best results on 4 out of 6 benchmarks , which verifies the effectiveness of the capsule networks .",Quantitative Evaluation,Quantitative Evaluation,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'B', 'I', 'B', 'I', 'I', 'I', 'B', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-b', 'I-b', 'B-p', 'I-p', 'I-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.75,148,0.6090534979423868,3,0.75,1,1,results
154,"Generally , all three proposed dynamic routing strategies contribute to the effectiveness of Capsule - B by alleviating the disturbance of some noise capsules which may contain "" background "" information such as stop words and the words that are unrelated to specific categories .",Ablation Study,Ablation Study,text-classification,7,"['O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.05454545454545454,153,0.6296296296296297,3,0.10714285714285714,1,1,ablation-analysis
2,Baseline Needs More Love : On Simple Word - Embedding - Based Models and Associated Pooling Mechanisms,title,title,text-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O']",,,1,0.0,1,0.0037174721189591076,1,0.0,1,1,research-problem
4,"Many deep learning architectures have been proposed to model the compositionality in text sequences , requiring a substantial number of parameters and expensive computations .",abstract,abstract,text-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.14285714285714285,3,0.011152416356877323,1,0.14285714285714285,1,1,research-problem
10,The source code and datasets can be obtained from https://github.com/dinghanshen/SWEM .,abstract,abstract,text-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'O']",7,1.0,9,0.03345724907063197,7,1.0,1,1,code
14,"Leveraging the word - embedding construct , many deep architectures have been proposed to model the compositionality in variable - length text sequences .",Introduction,Introduction,text-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",3,0.13636363636363635,13,0.048327137546468404,3,0.13636363636363635,1,1,research-problem
21,"In this paper , we conduct an extensive experimental investigation to understand when , and why , simple pooling strategies , operated over word embeddings alone , already carry sufficient information for natural language understanding .",Introduction,Introduction,text-classification,8,"['O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'B', 'I', 'B', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'B', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'I-p', 'B-b', 'I-b', 'I-b', 'I-b', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'O']",10,0.45454545454545453,20,0.07434944237918216,10,0.45454545454545453,1,1,approach
22,"To ac- count for the distinct nature of various NLP tasks that may require different semantic features , we compare SWEM - based models with existing recurrent and convolutional networks in a pointby - point manner .",Introduction,Introduction,text-classification,8,"['B', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'B', 'B', 'I', 'I', 'I', 'B', 'B', 'I', 'I', 'I', 'I', 'B', 'I', 'B', 'I', 'I', 'I', 'O']","['B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O', 'O', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",11,0.5,21,0.07806691449814127,11,0.5,1,1,approach
81,"Interestingly , for the sentiment analysis tasks , both CNN and LSTM compositional functions perform better than SWEM , suggesting that wordorder information maybe required for analyzing sentiment orientations .",Model,Model,text-classification,8,"['O', 'O', 'B', 'O', 'B', 'I', 'O', 'O', 'B', 'B', 'I', 'I', 'I', 'I', 'B', 'I', 'I', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'O', 'O', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'I-p', 'B-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.058823529411764705,80,0.29739776951672864,2,0.2857142857142857,1,1,results
117,We use Glo Ve word embeddings with K = 300 as initialization for all our models .,Experiments,Experiments,text-classification,8,"['O', 'B', 'B', 'I', 'I', 'I', 'B', 'B', 'I', 'I', 'B', 'B', 'B', 'B', 'I', 'I', 'O']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['O', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'B-b', 'I-b', 'I-b', 'B-p', 'B-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'O']",3,0.1875,116,0.4312267657992565,3,0.1875,1,1,hyperparameters
118,"Out - Of - Vocabulary ( OOV ) words are initialized from a uniform distribution with range [ ? 0.01 , 0.01 ] .",Experiments,Experiments,text-classification,8,"['B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'B', 'I', 'O', 'B', 'I', 'B', 'I', 'B', 'I', 'I', 'I', 'I', 'I', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O', 'B-p', 'I-p', 'O', 'B-b', 'I-b', 'B-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",4,0.25,117,0.4349442379182156,4,0.25,1,1,hyperparameters
119,"The Glo Ve embeddings are employed in two ways to learn refined word embeddings : ( i ) directly updating each word embedding during training ; and ( ii ) training a 300 dimensional Multilayer Perceptron ( MLP ) layer with ReLU activation , with Glo Ve embeddings as input to the MLP and with output defining the refined word embeddings .",Experiments,Experiments,text-classification,8,"['O', 'B', 'I', 'I', 'O', 'B', 'I', 'B', 'I', 'B', 'I', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'I', 'I', 'B', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'B', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O']",,,5,0.3125,118,0.43866171003717475,5,0.3125,1,1,hyperparameters
124,"Adam ) is used to optimize all models , with learning rate selected from .",Experiments,Experiments,text-classification,8,"['B', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.625,123,0.45724907063197023,10,0.625,1,1,hyperparameters
125,"Surprisingly , on topic prediction tasks , our SWEM model exhibits stronger performances , relative to both LSTM and CNN compositional architectures , this by leveraging both the average and max - pooling features from word embeddings .",Experiments,Experiments,text-classification,8,"['O', 'O', 'B', 'B', 'I', 'I', 'O', 'O', 'B', 'I', 'B', 'B', 'I', 'O', 'B', 'I', 'I', 'B', 'I', 'I', 'I', 'I', 'O', 'O', 'B', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'B', 'B', 'I', 'O']","['O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['O', 'O', 'B-p', 'B-b', 'I-b', 'I-b', 'O', 'O', 'B-b', 'I-b', 'B-p', 'B-b', 'I-b', 'O', 'B-p', 'I-p', 'I-p', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'O']",11,0.6875,124,0.46096654275092935,11,0.6875,1,1,results
127,"On the ontology classification problem ( DBpedia dataset ) , we observe the same trend , that SWEM exhibits comparable or even superior results , relative to CNN or LSTM models .",Experiments,Experiments,text-classification,8,"['B', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'B', 'I', 'I', 'I', 'I', 'O', 'B', 'I', 'B', 'I', 'I', 'I', 'O']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['B-p', 'O', 'B-b', 'I-b', 'I-b', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'O', 'O', 'O', 'O', 'B-b', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O', 'B-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",13,0.8125,126,0.4684014869888476,13,0.8125,1,1,results
152,"Surprisingly , on most of the datasets considered ( except WikiQA ) , SWEM demonstrates the best results compared with those with CNN or the LSTM encoder .",Interpreting model predictions,The corresponding performance metrics are shown in .,text-classification,8,"['O', 'O', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'B', 'B', 'O', 'B', 'I', 'B', 'I', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O', 'B-b', 'B-p', 'O', 'B-b', 'I-b', 'B-p', 'I-p', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",21,0.2916666666666667,151,0.5613382899628253,3,0.14285714285714285,1,1,results
153,"Notably , on SNLI dataset , we observe that SWEM - max performs the best among all SWEM variants , consistent with the findings in Nie and Bansal ( 2017 ) ; , that max - pooling over BiLSTM hidden units outperforms average pooling operation on SNLI dataset .",Interpreting model predictions,The corresponding performance metrics are shown in .,text-classification,8,"['O', 'O', 'O', 'B', 'I', 'O', 'O', 'B', 'I', 'B', 'I', 'I', 'B', 'I', 'I', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-b', 'I-b', 'O', 'O', 'B-p', 'I-p', 'B-b', 'I-b', 'I-b', 'B-p', 'I-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.3055555555555556,152,0.5650557620817844,4,0.19047619047619047,1,1,results
177,"As demonstrated in Section 4.2.1 , word - order information plays a vital role for sentiment analysis tasks .",Interpreting model predictions,SWEM - hier for sentiment analysis,text-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'B', 'I', 'I', 'I', 'I', 'B', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'O']",46,0.6388888888888888,176,0.654275092936803,6,0.3157894736842105,1,1,results
178,"However , according to the case study above , the most important features for sentiment prediction maybe some key n-gram phrase / words from Negative :",Interpreting model predictions,SWEM - hier for sentiment analysis,text-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'B', 'B', 'I', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O']",,,47,0.6527777777777778,177,0.6579925650557621,7,0.3684210526315789,1,1,results
189,"SWEM - hier greatly outperforms the other three SWEM variants , and the corresponding accuracies are comparable to the results of CNN or LSTM ) .",Interpreting model predictions,"Food is just okay , not great .",text-classification,8,"['O', 'O', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'B', 'I', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'O', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'O', 'O', 'O', 'B-b', 'I-b', 'O', 'B-p', 'I-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O']",58,0.8055555555555556,188,0.6988847583643123,18,0.9473684210526315,1,1,results
195,"Compared with CNN / LSTM compositional functions , SWEM yields inferior accuracies on sentiment analysis datasets , consistent with our observation in the case of document categorization .",Interpreting model predictions,The corresponding results are shown in .,text-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'B', 'I', 'B', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-b', 'B-p', 'B-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",64,0.8888888888888888,194,0.7211895910780669,4,0.36363636363636365,1,1,results
196,"However , SWEM exhibits comparable performance on the other two tasks , again with much less parameters and faster training .",Interpreting model predictions,The corresponding results are shown in .,text-classification,8,"['O', 'O', 'O', 'B', 'B', 'I', 'B', 'O', 'B', 'I', 'I', 'O', 'O', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'B-p', 'B-b', 'I-b', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'O', 'O', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",65,0.9027777777777778,195,0.724907063197026,5,0.45454545454545453,1,1,results
2,Translations as Additional Contexts for Sentence Classification,title,,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'B', 'I']",,,1,0.0,1,0.003968253968253968,1,0.0,1,1,research-problem
12,"One of the primary tasks in natural language processing ( NLP ) is sentence classification , where given a sentence ( e.g. a sentence of a review ) as input , we are tasked to classify it into one of multiple classes ( e.g. into positive or negative ) .",Introduction,Introduction,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",1,0.013513513513513514,11,0.04365079365079365,1,0.024390243902439025,1,1,research-problem
42,"Based on these observations , we present a neural attentionbased multiple context fixing attachment ( MCFA ) .",Introduction,Introduction,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'B', 'I', 'I', 'O', 'B', 'O', 'O']",,,31,0.4189189189189189,41,0.1626984126984127,31,0.7560975609756098,1,1,model
43,"MCFA is a series of modules that uses all the sentence vectors ( e.g. Arabic , English , Korean , etc. ) as context to fix a sentence vector ( e.g. Korean ) .",Introduction,Introduction,text-classification,9,"['O', 'B', 'O', 'B', 'I', 'I', 'O', 'B', 'B', 'I', 'I', 'I', 'O', 'B', 'B', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'O', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'O', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",32,0.43243243243243246,42,0.16666666666666666,32,0.7804878048780488,1,1,model
44,"Fixing the vectors is done by selectively moving the vectors to a location in the same vector space that better separates the class , as shown in .",Introduction,Introduction,text-classification,9,"['B', 'I', 'I', 'O', 'B', 'I', 'B', 'I', 'I', 'I', 'B', 'I', 'I', 'B', 'I', 'I', 'I', 'I', 'B', 'I', 'I', 'B', 'I', 'O', 'O', 'O', 'O', 'O']",,,33,0.44594594594594594,43,0.17063492063492064,33,0.8048780487804879,1,1,model
46,MCFA computes two sentence usability metrics to control the noise when fixing vectors : ( a ) self usability ? i ( a ) weighs the confidence of using sentence a in solving the task .,Introduction,Introduction,text-classification,9,"['B', 'B', 'B', 'I', 'I', 'I', 'B', 'I', 'O', 'B', 'B', 'B', 'I', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-b', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'O', 'B-b', 'B-p', 'B-ob', 'I-ob', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",35,0.47297297297297297,45,0.17857142857142858,35,0.8536585365853658,1,1,model
47,"( b ) relative usability ? r ( a , b ) weighs the confidence of using sentence a in fixing sentence b.",Introduction,Introduction,text-classification,9,"['O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",36,0.4864864864864865,46,0.18253968253968253,36,0.8780487804878049,1,1,model
153,"For our CNN , we use rectified linear units and three filters with different window sizes h = 3 , 4 , 5 with 100 feature maps each , following .",Experimental Setting,Experimental Setting,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'B', 'B', 'I', 'I', 'O', 'B', 'I', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'O', 'B-b', 'I-b', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O']",13,0.5,152,0.6031746031746031,13,0.5,1,1,hyperparameters
154,"For the final sentence vector , we concatenate the feature maps to get a 300 - dimension vector .",Experimental Setting,Experimental Setting,text-classification,9,"['B', 'O', 'B', 'I', 'I', 'O', 'O', 'B', 'O', 'B', 'I', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'O']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['B-p', 'O', 'B-b', 'I-b', 'I-b', 'O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'B-p', 'I-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",14,0.5384615384615384,153,0.6071428571428571,14,0.5384615384615384,1,1,hyperparameters
155,We use dropout on all nonlinear connections with a dropout rate of 0.5 .,Experimental Setting,Experimental Setting,text-classification,9,"['O', 'O', 'B', 'B', 'B', 'I', 'I', 'B', 'O', 'B', 'I', 'O', 'B', 'O']",,,15,0.5769230769230769,154,0.6111111111111112,15,0.5769230769230769,1,1,hyperparameters
158,"During training , we use mini-batch size of 50 .",Experimental Setting,Experimental Setting,text-classification,9,"['B', 'B', 'O', 'O', 'B', 'B', 'I', 'B', 'B', 'O']","['B-p', 'B-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'O']","['B-p', 'B-b', 'O', 'O', 'B-p', 'B-b', 'I-b', 'B-p', 'B-ob', 'O']",18,0.6923076923076923,157,0.623015873015873,18,0.6923076923076923,1,1,hyperparameters
159,Training is done via stochastic gradient descent over shuffled mini-batches with the Adadelta update rule .,Experimental Setting,Experimental Setting,text-classification,9,"['B', 'B', 'I', 'I', 'B', 'I', 'I', 'B', 'B', 'I', 'B', 'O', 'B', 'I', 'I', 'O']","['B-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['B-b', 'B-p', 'I-p', 'I-p', 'B-b', 'I-b', 'I-b', 'B-p', 'B-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'O']",19,0.7307692307692307,158,0.626984126984127,19,0.7307692307692307,1,1,hyperparameters
160,We perform early stopping using a random 10 % of the training set as the development set .,Experimental Setting,Experimental Setting,text-classification,9,"['O', 'B', 'B', 'I', 'B', 'O', 'B', 'I', 'I', 'B', 'O', 'B', 'I', 'B', 'O', 'B', 'I', 'O']","['O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['O', 'B-p', 'B-b', 'I-b', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'O', 'B-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'O']",20,0.7692307692307693,159,0.6309523809523809,20,0.7692307692307693,1,1,hyperparameters
169,We show that CNN + MCFA achieves state of the art performance on three of the four data sets and performs competitively on one data set .,Results and Discussion,Results and Discussion,text-classification,9,"['O', 'B', 'O', 'B', 'I', 'I', 'B', 'B', 'I', 'I', 'I', 'I', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'B', 'I', 'I', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'B-p', 'I-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'O']",2,0.08333333333333333,168,0.6666666666666666,2,0.08333333333333333,1,1,results
170,"When N = 1 , MCFA increases the performance of a normal CNN from 85.0 to 87.6 , beating the current state of the art on the CR data set .",Results and Discussion,Results and Discussion,text-classification,9,"['B', 'B', 'I', 'I', 'O', 'B', 'B', 'I', 'I', 'I', 'O', 'B', 'I', 'B', 'B', 'B', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-b', 'I-b', 'I-b', 'O', 'B-b', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-b', 'I-b', 'B-p', 'B-ob', 'B-p', 'B-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.125,169,0.6706349206349206,3,0.125,1,1,results
171,"When N = 10 , MCFA additionally beats the state of the art on the TREC data set .",Results and Discussion,Results and Discussion,text-classification,9,"['O', 'B', 'I', 'I', 'O', 'B', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'B', 'O', 'B', 'I', 'I', 'O']","['O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['O', 'B-b', 'I-b', 'I-b', 'O', 'B-b', 'B-p', 'I-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'O']",4,0.16666666666666666,170,0.6746031746031746,4,0.16666666666666666,1,1,results
172,"Finally , our ensemble classifier additionally outperforms all competing models on the MR data set .",Results and Discussion,Results and Discussion,text-classification,9,"['O', 'O', 'O', 'B', 'I', 'B', 'I', 'B', 'I', 'I', 'B', 'O', 'B', 'I', 'I', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'B-b', 'I-b', 'B-p', 'I-p', 'B-b', 'I-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'O']",5,0.20833333333333334,171,0.6785714285714286,5,0.20833333333333334,1,1,results
