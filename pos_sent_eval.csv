idx,text,main_heading,heading,topic,paper_idx,BIO,BIO_1,offset1,pro1,offset2,pro2,offset3,pro3,mask,labels,title
3,abstract,,,machine-translation,0,['O'],['O'],0,0.0,2,0.0091324200913242,0,0.0,1,0,
10,"Deep neural networks have shown great success in various applications such as objection recognition ( see , e.g. , ) and speech recognition ( see , e.g. , ) .",Introduction,Introduction,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0092592592592592,9,0.0410958904109589,1,0.0263157894736842,1,0,Introduction
11,"Furthermore , many recent works showed that neural networks can be successfully used in a number of tasks in natural language processing ( NLP ) .",Introduction,Introduction,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.0185185185185185,10,0.045662100456621,2,0.0526315789473684,1,0,Introduction
27,"A recurrent neural network ( RNN ) is a neural network that consists of a hidden state hand an optional output y which operates on a variablelength sequence x = ( x 1 , . . . , x T ) .",Introduction,Introduction,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.1666666666666666,26,0.1187214611872146,18,0.4736842105263157,1,0,Introduction
28,"At each time step t , the hidden state ht of the RNN is updated by",Introduction,Introduction,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.1759259259259259,27,0.1232876712328767,19,0.5,1,0,Introduction
29,where f is a non-linear activation function .,Introduction,Introduction,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.1851851851851851,28,0.1278538812785388,20,0.5263157894736842,1,0,Introduction
30,f maybe as simple as an elementwise logistic sigmoid function and as complex as along short - term memory ( LSTM ) unit ) .,Introduction,Introduction,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.1944444444444444,29,0.1324200913242009,21,0.5526315789473685,1,0,Introduction
31,An RNN can learn a probability distribution over a sequence by being trained to predict the next symbol in a sequence .,Introduction,Introduction,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.2037037037037037,30,0.136986301369863,22,0.5789473684210527,1,0,Introduction
32,"In that case , the output at each timestep t is the conditional distribution p ( x t | x t?1 , . . . , x 1 ) .",Introduction,Introduction,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",23,0.2129629629629629,31,0.1415525114155251,23,0.6052631578947368,1,0,Introduction
33,"For example , a multinomial distribution ( 1 - of - K coding ) can be output using a softmax activation function",Introduction,Introduction,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.2222222222222222,32,0.1461187214611872,24,0.631578947368421,1,0,Introduction
34,"for all possible symbols j = 1 , . . . , K , where w j are the rows of a weight matrix W. By combining these probabilities , we can compute the probability of the sequence x using",Introduction,Introduction,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.2314814814814814,33,0.1506849315068493,25,0.6578947368421053,1,0,Introduction
35,"From this learned distribution , it is straightforward to sample anew sequence by iteratively sampling a symbol at each time step .",Introduction,Introduction,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.2407407407407407,34,0.1552511415525114,26,0.6842105263157895,1,0,Introduction
36,RNN Encoder - Decoder,Introduction,Introduction,machine-translation,0,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",27,0.25,35,0.1598173515981735,27,0.7105263157894737,1,0,Introduction
38,"The input matrix between each input symbol x t and the hidden unit is approximated with two lower - rank matrices , and the output matrix is approximated similarly .",Introduction,Introduction,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",29,0.2685185185185185,37,0.1689497716894977,29,0.7631578947368421,1,0,Introduction
40,The activation function used forh in Eq. ( 8 ) is a hyperbolic tangent function .,Introduction,Introduction,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",31,0.287037037037037,39,0.1780821917808219,31,0.8157894736842105,1,0,Introduction
43,"For the recurrent weight matrices , we first sampled from a white Gaussian distribution and used its left singular vectors matrix , following .",Introduction,Introduction,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",34,0.3148148148148148,42,0.1917808219178082,34,0.8947368421052632,1,0,Introduction
48,Hidden Unit that Adaptively Remembers and Forgets,Introduction,,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O']",39,0.3611111111111111,47,0.2146118721461187,0,0.0,1,0,Introduction
49,"In addition to a novel model architecture , we also propose anew type of hidden unit ( f in Eq .",Introduction,Hidden Unit that Adaptively Remembers and Forgets,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",40,0.3703703703703703,48,0.2191780821917808,1,0.0454545454545454,1,0,Introduction: Hidden Unit that Adaptively Remembers and Forgets
50,( 1 ) ) that has been motivated by the LSTM unit but is much simpler to compute and implement .,Introduction,Hidden Unit that Adaptively Remembers and Forgets,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",41,0.3796296296296296,49,0.2237442922374429,2,0.0909090909090909,1,0,Introduction: Hidden Unit that Adaptively Remembers and Forgets
52,Let us describe how the activation of the j - th hidden unit is computed .,Introduction,Hidden Unit that Adaptively Remembers and Forgets,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",43,0.3981481481481481,51,0.2328767123287671,4,0.1818181818181818,1,0,Introduction: Hidden Unit that Adaptively Remembers and Forgets
53,"First , the reset gate r j is computed by",Introduction,Hidden Unit that Adaptively Remembers and Forgets,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",44,0.4074074074074074,52,0.2374429223744292,5,0.2272727272727272,1,0,Introduction: Hidden Unit that Adaptively Remembers and Forgets
54,where ?,Introduction,Hidden Unit that Adaptively Remembers and Forgets,machine-translation,0,"['O', 'O']","['O', 'O']",45,0.4166666666666667,53,0.2420091324200913,6,0.2727272727272727,1,0,Introduction: Hidden Unit that Adaptively Remembers and Forgets
55,"is the logistic sigmoid function , and [. ] j denotes the j - th element of a vector .",Introduction,Hidden Unit that Adaptively Remembers and Forgets,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",46,0.4259259259259259,54,0.2465753424657534,7,0.3181818181818182,1,0,Introduction: Hidden Unit that Adaptively Remembers and Forgets
56,"x and h t?1 are the input and the previous hidden state , respectively .",Introduction,Hidden Unit that Adaptively Remembers and Forgets,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",47,0.4351851851851852,55,0.2511415525114155,8,0.3636363636363636,1,0,Introduction: Hidden Unit that Adaptively Remembers and Forgets
57,W rand Ur are weight matrices which are learned .,Introduction,Hidden Unit that Adaptively Remembers and Forgets,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",48,0.4444444444444444,56,0.2557077625570776,9,0.4090909090909091,1,0,Introduction: Hidden Unit that Adaptively Remembers and Forgets
58,"Similarly , the update gate z j is computed by",Introduction,Hidden Unit that Adaptively Remembers and Forgets,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",49,0.4537037037037037,57,0.2602739726027397,10,0.4545454545454545,1,0,Introduction: Hidden Unit that Adaptively Remembers and Forgets
59,The actual activation of the proposed unit h j is then computed by,Introduction,Hidden Unit that Adaptively Remembers and Forgets,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",50,0.4629629629629629,58,0.2648401826484018,11,0.5,1,0,Introduction: Hidden Unit that Adaptively Remembers and Forgets
60,wher ?,Introduction,Hidden Unit that Adaptively Remembers and Forgets,machine-translation,0,"['O', 'O']","['O', 'O']",51,0.4722222222222222,59,0.2694063926940639,12,0.5454545454545454,1,0,Introduction: Hidden Unit that Adaptively Remembers and Forgets
61,h,Introduction,Hidden Unit that Adaptively Remembers and Forgets,machine-translation,0,['O'],['O'],52,0.4814814814814814,60,0.273972602739726,13,0.5909090909090909,1,0,Introduction: Hidden Unit that Adaptively Remembers and Forgets
62,"In this formulation , when the reset gate is close to 0 , the hidden state is forced to ignore the previous hidden state and reset with the current input only .",Introduction,Hidden Unit that Adaptively Remembers and Forgets,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",53,0.4907407407407407,61,0.2785388127853881,14,0.6363636363636364,1,0,Introduction: Hidden Unit that Adaptively Remembers and Forgets
64,"On the other hand , the update gate controls how much information from the previous hidden state will carryover to the current hidden state .",Introduction,Hidden Unit that Adaptively Remembers and Forgets,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",55,0.5092592592592593,63,0.2876712328767123,16,0.7272727272727273,1,0,Introduction: Hidden Unit that Adaptively Remembers and Forgets
66,"Furthermore , this maybe considered an adaptive variant of a leaky - integration unit .",Introduction,Hidden Unit that Adaptively Remembers and Forgets,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",57,0.5277777777777778,65,0.2968036529680365,18,0.8181818181818182,1,0,Introduction: Hidden Unit that Adaptively Remembers and Forgets
68,"Those units that learn to capture short - term dependencies will tend to have reset gates that are frequently active , but those that capture longer - term dependencies will have update gates that are mostly active .",Introduction,Hidden Unit that Adaptively Remembers and Forgets,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",59,0.5462962962962963,67,0.3059360730593607,20,0.9090909090909092,1,0,Introduction: Hidden Unit that Adaptively Remembers and Forgets
70,We were notable to get meaningful result with an oft - used tanh unit without any gating .,Introduction,Hidden Unit that Adaptively Remembers and Forgets,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",61,0.5648148148148148,69,0.3150684931506849,22,1.0,1,0,Introduction: Hidden Unit that Adaptively Remembers and Forgets
71,Statistical Machine Translation,Introduction,,machine-translation,0,"['O', 'O', 'O']","['O', 'O', 'O']",62,0.5740740740740741,70,0.319634703196347,0,0.0,1,0,Introduction
72,"Ina commonly used statistical machine translation system ( SMT ) , the goal of the system ( decoder , specifically ) is to find a translation f given a source sentence e , which maximizes",Introduction,Statistical Machine Translation,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",63,0.5833333333333334,71,0.3242009132420091,1,0.0217391304347826,1,0,Introduction: Statistical Machine Translation
73,"where the first term at the right hand side is called translation model and the latter language model ( see , e.g. , ) .",Introduction,Statistical Machine Translation,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",64,0.5925925925925926,72,0.3287671232876712,2,0.0434782608695652,1,0,Introduction: Statistical Machine Translation
74,"In practice , however , most SMT systems model log p ( f | e ) as a loglinear model with additional features and corre - sponding weights : where f n and w n are the n - th feature and weight , respectively .",Introduction,Statistical Machine Translation,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",65,0.6018518518518519,73,0.3333333333333333,3,0.0652173913043478,1,0,Introduction: Statistical Machine Translation
75,Z ( e ) is a normalization constant that does not depend on the weights .,Introduction,Statistical Machine Translation,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",66,0.6111111111111112,74,0.3378995433789954,4,0.0869565217391304,1,0,Introduction: Statistical Machine Translation
76,The weights are often optimized to maximize the BLEU score on a development set .,Introduction,Statistical Machine Translation,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",67,0.6203703703703703,75,0.3424657534246575,5,0.108695652173913,1,0,Introduction: Statistical Machine Translation
78,2,Introduction,Statistical Machine Translation,machine-translation,0,['O'],['O'],69,0.6388888888888888,77,0.3515981735159817,7,0.1521739130434782,1,0,Introduction: Statistical Machine Translation
79,These probabilities are once again considered additional features in the log - linear model ( see Eq. ) and are weighted accordingly to maximize the BLEU score .,Introduction,Statistical Machine Translation,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",70,0.6481481481481481,78,0.3561643835616438,8,0.1739130434782608,1,0,Introduction: Statistical Machine Translation
80,"Since the neural net language model was proposed in , neural networks have been used widely in SMT systems .",Introduction,Statistical Machine Translation,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",71,0.6574074074074074,79,0.3607305936073059,9,0.1956521739130435,1,0,Introduction: Statistical Machine Translation
81,"In many cases , neural networks have been used to rescore translation hypotheses ( n- best lists ) ( see , e.g. , ) .",Introduction,Statistical Machine Translation,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",72,0.6666666666666666,80,0.365296803652968,10,0.217391304347826,1,0,Introduction: Statistical Machine Translation
82,"Recently , however , there has been interest in training neural networks to score the translated sentence ( or phrase pairs ) using a representation of the source sentence as an additional input .",Introduction,Statistical Machine Translation,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",73,0.6759259259259259,81,0.3698630136986301,11,0.2391304347826087,1,0,Introduction: Statistical Machine Translation
83,"See , e.g. , , and .",Introduction,Statistical Machine Translation,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O']",74,0.6851851851851852,82,0.3744292237442922,12,0.2608695652173913,1,0,Introduction: Statistical Machine Translation
86,"When we train the RNN Encoder - Decoder , we ignore the ( normalized ) frequencies of each phrase pair in the original corpora .",Introduction,Scoring Phrase Pairs with RNN Encoder - Decoder,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",77,0.7129629629629629,85,0.3881278538812785,15,0.3260869565217391,1,0,Introduction: Scoring Phrase Pairs with RNN Encoder - Decoder
87,This measure was taken in order ( 1 ) to reduce the computational expense of randomly selecting phrase pairs from a large phrase table according to the normalized frequencies and ( 2 ) to ensure that the RNN Encoder - Decoder does not simply learn to rank the phrase pairs according to their numbers of occurrences .,Introduction,Scoring Phrase Pairs with RNN Encoder - Decoder,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",78,0.7222222222222222,86,0.3926940639269406,16,0.3478260869565217,1,0,Introduction: Scoring Phrase Pairs with RNN Encoder - Decoder
88,One underlying reason for this choice was that the existing translation probability in the phrase table already reflects the frequencies of the phrase pairs in the original corpus .,Introduction,Scoring Phrase Pairs with RNN Encoder - Decoder,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",79,0.7314814814814815,87,0.3972602739726027,17,0.3695652173913043,1,0,Introduction: Scoring Phrase Pairs with RNN Encoder - Decoder
92,"As Schwenk pointed out in , it is possible to completely replace the existing phrase table with the proposed RNN Encoder - Decoder .",Introduction,Scoring Phrase Pairs with RNN Encoder - Decoder,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",83,0.7685185185185185,91,0.4155251141552511,21,0.4565217391304347,1,0,Introduction: Scoring Phrase Pairs with RNN Encoder - Decoder
93,"In that case , fora given source phrase , the RNN Encoder - Decoder will need to generate a list of ( good ) target phrases .",Introduction,Scoring Phrase Pairs with RNN Encoder - Decoder,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",84,0.7777777777777778,92,0.4200913242009132,22,0.4782608695652174,1,0,Introduction: Scoring Phrase Pairs with RNN Encoder - Decoder
94,"This requires , however , an expensive sampling procedure to be performed repeatedly .",Introduction,Scoring Phrase Pairs with RNN Encoder - Decoder,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",85,0.7870370370370371,93,0.4246575342465753,23,0.5,1,0,Introduction: Scoring Phrase Pairs with RNN Encoder - Decoder
95,"In this paper , thus , we only consider rescoring the phrase pairs in the phrase table .",Introduction,Scoring Phrase Pairs with RNN Encoder - Decoder,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",86,0.7962962962962963,94,0.4292237442922374,24,0.5217391304347826,1,0,Introduction: Scoring Phrase Pairs with RNN Encoder - Decoder
98,Schwenk in proposed a similar approach of scoring phrase pairs .,Introduction,Related Approaches : Neural Networks in Machine Translation,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",89,0.8240740740740741,97,0.4429223744292237,27,0.5869565217391305,1,0,Introduction: Related Approaches : Neural Networks in Machine Translation
99,"Instead of the RNN - based neural network , he used a feedforward neural network that has fixed - size inputs ( 7 words in his case , with zero - padding for shorter phrases ) and fixed - size outputs ( 7 words in the target language ) .",Introduction,Related Approaches : Neural Networks in Machine Translation,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",90,0.8333333333333334,98,0.4474885844748858,28,0.6086956521739131,1,0,Introduction: Related Approaches : Neural Networks in Machine Translation
100,"When it is used specifically for scoring phrases for the SMT system , the maximum phrase length is often chosen to be small .",Introduction,Related Approaches : Neural Networks in Machine Translation,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",91,0.8425925925925926,99,0.4520547945205479,29,0.6304347826086957,1,0,Introduction: Related Approaches : Neural Networks in Machine Translation
101,"However , as the length of phrases increases or as we apply neural networks to other variable - length sequence data , it is important that the neural network can handle variable - length input and output .",Introduction,Related Approaches : Neural Networks in Machine Translation,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",92,0.8518518518518519,100,0.45662100456621,30,0.6521739130434783,1,0,Introduction: Related Approaches : Neural Networks in Machine Translation
103,"Similar to , Devlin et al. proposed to use a feedforward neural network to model a translation model , however , by predicting one word in a target phrase at a time .",Introduction,Related Approaches : Neural Networks in Machine Translation,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",94,0.8703703703703703,102,0.4657534246575342,32,0.6956521739130435,1,0,Introduction: Related Approaches : Neural Networks in Machine Translation
105,"Although it is not exactly a neural network they train , the authors of proposed to learn a bilingual embedding of words / phrases .",Introduction,Related Approaches : Neural Networks in Machine Translation,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",96,0.8888888888888888,104,0.4748858447488584,34,0.7391304347826086,1,0,Introduction: Related Approaches : Neural Networks in Machine Translation
108,"This is closely related to both the proposed RNN Encoder - Decoder and the model proposed in , except that their input representation of a phrase is a bag - of - words .",Introduction,Related Approaches : Neural Networks in Machine Translation,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",99,0.9166666666666666,107,0.4885844748858447,37,0.8043478260869565,1,0,Introduction: Related Approaches : Neural Networks in Machine Translation
109,A similar approach of using bag - of - words representations was proposed in as well .,Introduction,Related Approaches : Neural Networks in Machine Translation,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",100,0.925925925925926,108,0.4931506849315068,38,0.8260869565217391,1,0,Introduction: Related Approaches : Neural Networks in Machine Translation
110,"Earlier , a similar encoder - decoder model using two recursive neural networks was proposed in ) , but their model was restricted to a monolingual setting , i.e. the model reconstructs an input sentence .",Introduction,Related Approaches : Neural Networks in Machine Translation,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",101,0.9351851851851852,109,0.4977168949771689,39,0.8478260869565217,1,0,Introduction: Related Approaches : Neural Networks in Machine Translation
111,"More recently , another encoder - decoder model using an RNN was proposed in , where the decoder is conditioned on a representation of either a source sentence or a source context .",Introduction,Related Approaches : Neural Networks in Machine Translation,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",102,0.9444444444444444,110,0.502283105022831,40,0.8695652173913043,1,0,Introduction: Related Approaches : Neural Networks in Machine Translation
112,One important difference between the proposed RNN Encoder - Decoder and the approaches in and is that the order of the words in source and target phrases is taken into account .,Introduction,Related Approaches : Neural Networks in Machine Translation,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",103,0.9537037037037036,111,0.5068493150684932,41,0.8913043478260869,1,0,Introduction: Related Approaches : Neural Networks in Machine Translation
113,"The RNN Encoder - Decoder naturally distinguishes between sequences that have the same words but in a different order , whereas the aforementioned approaches effectively ignore order information .",Introduction,Related Approaches : Neural Networks in Machine Translation,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",104,0.9629629629629628,112,0.5114155251141552,42,0.9130434782608696,1,0,Introduction: Related Approaches : Neural Networks in Machine Translation
114,The closest approach related to the proposed RNN Encoder - Decoder is the Recurrent Continuous Translation Model ( Model 2 ) proposed in .,Introduction,Related Approaches : Neural Networks in Machine Translation,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",105,0.9722222222222222,113,0.5159817351598174,43,0.9347826086956522,1,0,Introduction: Related Approaches : Neural Networks in Machine Translation
115,"In their paper , they proposed a similar model that consists of an encoder and decoder .",Introduction,Related Approaches : Neural Networks in Machine Translation,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",106,0.9814814814814816,114,0.5205479452054794,44,0.9565217391304348,1,0,Introduction: Related Approaches : Neural Networks in Machine Translation
116,The difference with our model is that they used a convolutional n-gram model ( CGM ) for the encoder and the hybrid of an inverse CGM and a recurrent neural network for the decoder .,Introduction,Related Approaches : Neural Networks in Machine Translation,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",107,0.9907407407407408,115,0.5251141552511416,45,0.9782608695652174,1,0,Introduction: Related Approaches : Neural Networks in Machine Translation
117,"They , however , evaluated their model on rescoring the n-best list proposed by the conventional SMT system and computing the perplexity of the gold standard translations .",Introduction,Related Approaches : Neural Networks in Machine Translation,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",108,1.0,116,0.5296803652968036,46,1.0,1,0,Introduction: Related Approaches : Neural Networks in Machine Translation
118,Experiments,,,machine-translation,0,['O'],['O'],0,0.0,117,0.5342465753424658,0,0.0,1,0,
120,Data and Baseline System,,,machine-translation,0,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",0,0.0,119,0.54337899543379,0,0.0,1,0,
121,Large amounts of resources are available to build an English / French SMT system in the framework of the WMT ' 14 translation task .,Data and Baseline System,Data and Baseline System,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0625,120,0.547945205479452,1,0.0625,1,0,Data and Baseline System
123,The last two corpora are quite noisy .,Data and Baseline System,Data and Baseline System,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.1875,122,0.5570776255707762,3,0.1875,1,0,Data and Baseline System
124,"To train the French language model , about 712M words of crawled newspaper material is available in addition to the target side of the bitexts .",Data and Baseline System,Data and Baseline System,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.25,123,0.5616438356164384,4,0.25,1,0,Data and Baseline System
125,All the word counts refer to French words after tokenization .,Data and Baseline System,Data and Baseline System,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.3125,124,0.5662100456621004,5,0.3125,1,0,Data and Baseline System
126,"It is commonly acknowledged that training statistical models on the concatenation of all this data does not necessarily lead to optimal performance , and results in extremely large models which are difficult to handle .",Data and Baseline System,Data and Baseline System,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.375,125,0.5707762557077626,6,0.375,1,0,Data and Baseline System
127,"Instead , one should focus on the most relevant subset of the data fora given task .",Data and Baseline System,Data and Baseline System,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.4375,126,0.5753424657534246,7,0.4375,1,0,Data and Baseline System
129,By these means we selected a subset of 418 M words out of more than 2G words for language modeling and a subset of 348 M out of 850 M words for training the RNN Encoder - Decoder .,Data and Baseline System,Data and Baseline System,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.5625,128,0.5844748858447488,9,0.5625,1,0,Data and Baseline System
132,"For training the neural networks , including the proposed RNN Encoder - Decoder , we limited the source and target vocabulary to the most frequent 15,000 words for both English and French .",Data and Baseline System,Data and Baseline System,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.75,131,0.5981735159817352,12,0.75,1,0,Data and Baseline System
134,All the out - of - vocabulary words were mapped to a special token ( [ UNK ] ) .,Data and Baseline System,Data and Baseline System,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.875,133,0.6073059360730594,14,0.875,1,0,Data and Baseline System
137,Neural Language Model,,,machine-translation,0,"['O', 'O', 'O']","['O', 'O', 'O']",0,0.0,136,0.6210045662100456,0,0.0,1,0,
139,"Especially , the comparison between the SMT system using CSLM and that using the proposed approach of phrase scoring by RNN Encoder - Decoder will clarify whether the contributions from multiple neural networks in different parts of the SMT sys - tem add up or are redundant .",Neural Language Model,Neural Language Model,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.0289855072463768,138,0.6301369863013698,2,0.1538461538461538,1,0,Neural Language Model
146,The validation set was a random selection of 0.1 % of the corpus .,Neural Language Model,Neural Language Model,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.1304347826086956,145,0.6621004566210046,9,0.6923076923076923,1,0,Neural Language Model
148,To address the computational complexity of using a CSLM in the decoder a buffer was used to aggregate n-grams during the stacksearch performed by the decoder .,Neural Language Model,Neural Language Model,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.1594202898550724,147,0.6712328767123288,11,0.8461538461538461,1,0,Neural Language Model
149,"Only when the buffer is full , or a stack is about to be pruned , the n-grams are scored by the CSLM .",Neural Language Model,Neural Language Model,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.1739130434782608,148,0.6757990867579908,12,0.9230769230769232,1,0,Neural Language Model
155,?,Neural Language Model,Quantitative Analysis,machine-translation,0,['O'],['O'],18,0.2608695652173913,154,0.7031963470319634,4,0.2,1,0,Neural Language Model: Quantitative Analysis
157,r is a Cyrillic letter ghe .,Neural Language Model,Quantitative Analysis,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.2898550724637681,156,0.7123287671232876,6,0.3,1,0,Neural Language Model: Quantitative Analysis
158,The results are presented in .,Neural Language Model,Quantitative Analysis,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O']",21,0.3043478260869565,157,0.7168949771689498,7,0.35,1,0,Neural Language Model: Quantitative Analysis
162,"Furthermore , we tried penalizing the number of words that are unknown to the neural networks ( i.e. words which are not in the shortlist ) .",Neural Language Model,Quantitative Analysis,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.3623188405797101,161,0.7351598173515982,11,0.55,1,0,Neural Language Model: Quantitative Analysis
164,3,Neural Language Model,Quantitative Analysis,machine-translation,0,['O'],['O'],27,0.391304347826087,163,0.7442922374429224,13,0.65,1,0,Neural Language Model: Quantitative Analysis
165,"However , in this case we 3 To understand the effect of the penalty , consider the set of all words in the 15,000 large shortlist , SL .",Neural Language Model,Quantitative Analysis,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",28,0.4057971014492754,164,0.7488584474885844,14,0.7,1,0,Neural Language Model: Quantitative Analysis
166,All words x i / ?,Neural Language Model,Quantitative Analysis,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O']",29,0.4202898550724637,165,0.7534246575342466,15,0.75,1,0,Neural Language Model: Quantitative Analysis
167,SL are replaced by a special token [ UNK ] before being scored by the neural networks .,Neural Language Model,Quantitative Analysis,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",30,0.4347826086956521,166,0.7579908675799086,16,0.8,1,0,Neural Language Model: Quantitative Analysis
168,"Hence , the conditional probability of any xi t / ?",Neural Language Model,Quantitative Analysis,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",31,0.4492753623188406,167,0.7625570776255708,17,0.85,1,0,Neural Language Model: Quantitative Analysis
169,SL is actually given by the model as,Neural Language Model,Quantitative Analysis,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",32,0.463768115942029,168,0.7671232876712328,18,0.9,1,0,Neural Language Model: Quantitative Analysis
170,"where x <t is a shorthand notation for xt ? 1 , . . . , x 1 .",Neural Language Model,Quantitative Analysis,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",33,0.4782608695652174,169,0.771689497716895,19,0.95,1,0,Neural Language Model: Quantitative Analysis
171,"were notable to achieve better performance on the test set , but only on the development set .",Neural Language Model,Quantitative Analysis,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",34,0.4927536231884058,170,0.776255707762557,20,1.0,1,0,Neural Language Model: Quantitative Analysis
175,"Also , as we mentioned earlier in Sec. 3.1 , we further expect the RNN Encoder - Decoder which was trained without any frequency information to score the phrase pairs based rather on the linguistic regularities than on the statistics of their occurrences in the corpus .",Neural Language Model,Qualitative Analysis,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",38,0.5507246376811594,174,0.7945205479452054,3,0.15,1,0,Neural Language Model: Qualitative Analysis
176,We focus on those pairs whose source phrase is long ( more than 3 words per source phrase ) and,Neural Language Model,Qualitative Analysis,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",39,0.5652173913043478,175,0.7990867579908676,4,0.2,1,0,Neural Language Model: Qualitative Analysis
177,"As a result , the probability of words not in the shortlist is always overestimated .",Neural Language Model,Qualitative Analysis,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",40,0.5797101449275363,176,0.8036529680365296,5,0.25,1,0,Neural Language Model: Qualitative Analysis
178,"It is possible to address this issue by backing off to an existing model that contain non-shortlisted words ( see ) In this paper , however , we opt for introducing a word penalty instead , which counteracts the word probability overestimation .",Neural Language Model,Qualitative Analysis,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",41,0.5942028985507246,177,0.8082191780821918,6,0.3,1,0,Neural Language Model: Qualitative Analysis
179,frequent .,Neural Language Model,Qualitative Analysis,machine-translation,0,"['O', 'O']","['O', 'O']",42,0.6086956521739131,178,0.8127853881278538,7,0.35,1,0,Neural Language Model: Qualitative Analysis
180,"For each such source phrase , we look at the target phrases that have been scored high either by the translation probability p ( f | e ) or by the RNN Encoder - Decoder .",Neural Language Model,Qualitative Analysis,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",43,0.6231884057971014,179,0.817351598173516,8,0.4,1,0,Neural Language Model: Qualitative Analysis
183,The source phrases were randomly chosen among long ones having more than 4 or 5 words .,Neural Language Model,Qualitative Analysis,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",46,0.6666666666666666,182,0.8310502283105022,11,0.55,1,0,Neural Language Model: Qualitative Analysis
184,"In most cases , the choices of the target phrases by the RNN Encoder - Decoder are closer to actual or literal translations .",Neural Language Model,Qualitative Analysis,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",47,0.6811594202898551,183,0.8356164383561644,12,0.6,1,0,Neural Language Model: Qualitative Analysis
186,"Interestingly , many phrase pairs were scored similarly by both the translation model and the RNN Encoder - Decoder , but there were as many other phrase pairs that were scored radically different ( see ) .",Neural Language Model,Qualitative Analysis,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",49,0.7101449275362319,185,0.8447488584474886,14,0.7,1,0,Neural Language Model: Qualitative Analysis
191,This encourages us to further investigate the possibility of replacing the whole or apart of the phrase table with the proposed RNN Encoder - Decoder in the future .,Neural Language Model,Qualitative Analysis,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",54,0.782608695652174,190,0.867579908675799,19,0.95,1,0,Neural Language Model: Qualitative Analysis
192,"Furthermore , in",Neural Language Model,Qualitative Analysis,machine-translation,0,"['O', 'O', 'O']","['O', 'O', 'O']",55,0.7971014492753623,191,0.8721461187214612,20,1.0,1,0,Neural Language Model: Qualitative Analysis
195,"It has been known for sometime that continuous space language models using neural networks are able to learn semantically meaningful embeddings ( See , e.g. , ) .",Neural Language Model,Word and Phrase Representations,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",58,0.8405797101449275,194,0.8858447488584474,2,0.1538461538461538,1,0,Neural Language Model: Word and Phrase Representations
196,"Since the proposed RNN Encoder - Decoder also projects to and maps back from a sequence of words into a continuous space vector , we expect to see a similar property with the proposed model as well .",Neural Language Model,Word and Phrase Representations,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",59,0.855072463768116,195,0.8904109589041096,3,0.2307692307692307,1,0,Neural Language Model: Word and Phrase Representations
198,The projection was done by the recently proposed Barnes - Hut - SNE .,Neural Language Model,Word and Phrase Representations,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",61,0.8840579710144928,197,0.8995433789954338,5,0.3846153846153846,1,0,Neural Language Model: Word and Phrase Representations
201,The representation ( c in ) in this case is a 1000 - dimensional vector .,Neural Language Model,Word and Phrase Representations,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",64,0.927536231884058,200,0.91324200913242,8,0.6153846153846154,1,0,Neural Language Model: Word and Phrase Representations
204,"For instance , in the bottom - left plot , most of the phrases are about the duration of time , while those phrases that are syntactically similar are clustered together .",Neural Language Model,Word and Phrase Representations,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",67,0.9710144927536232,203,0.9269406392694064,11,0.8461538461538461,1,0,Neural Language Model: Word and Phrase Representations
206,"On the other hand , the top - right plot shows the phrases that are syntactically similar .",Neural Language Model,Word and Phrase Representations,machine-translation,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",69,1.0,205,0.9360730593607306,13,1.0,1,0,Neural Language Model: Word and Phrase Representations
207,Conclusion,,,machine-translation,0,['O'],['O'],0,0.0,206,0.9406392694063926,0,0.0,1,0,
2,Neural Machine Translation in Linear Time,title,,machine-translation,1,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O']",1,0.0,1,0.0049751243781094,1,0.0,1,0,title
3,abstract,,,machine-translation,1,['O'],['O'],0,0.0,2,0.0099502487562189,0,0.0,1,0,
16,The network can bethought of as composed of two parts : a source network ( the encoder ) that encodes the source sequence into a representation and a target network ( the decoder ) that uses the representation of the source encoder to generate the target sequence .,Introduction,Introduction,machine-translation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.081081081081081,15,0.0746268656716417,3,0.081081081081081,1,0,Introduction
17,"Recurrent neural networks ( RNN ) are powerful sequence models and are widely used in language modelling ) , yet they have a potential drawback .",Introduction,Introduction,machine-translation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.1081081081081081,16,0.0796019900497512,4,0.1081081081081081,1,0,Introduction
18,RNNs have an inherently serial structure that prevents them from being run in parallel along the sequence length during training and evaluation .,Introduction,Introduction,machine-translation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.1351351351351351,17,0.0845771144278607,5,0.1351351351351351,1,0,Introduction
19,Forward and backward signals in a RNN also need to traverse the full distance of the serial path to reach from one token in the sequence to another .,Introduction,Introduction,machine-translation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.1621621621621621,18,0.0895522388059701,6,0.1621621621621621,1,0,Introduction
20,"The larger the distance , the harder it is to learn the dependencies between the tokens .",Introduction,Introduction,machine-translation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.1891891891891892,19,0.0945273631840796,7,0.1891891891891892,1,0,Introduction
21,"A number of neural architectures have been proposed for modelling translation , such as encoder - decoder networks , networks with attentional pooling and twodimensional networks .",Introduction,Introduction,machine-translation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.2162162162162162,20,0.099502487562189,8,0.2162162162162162,1,0,Introduction
24,"At each step the decoder is conditioned on the source representation produced by the encoder for that step , or simply on no representation for steps beyond the extended length | t | .",Introduction,Introduction,machine-translation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.2972972972972973,23,0.1144278606965174,11,0.2972972972972973,1,0,Introduction
26,"either have running time that is super - linear in the length of the source and target sequences , or they process the source sequence into a constant size representation , burdening the model with a memorization step .",Introduction,Introduction,machine-translation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.3513513513513513,25,0.1243781094527363,13,0.3513513513513513,1,0,Introduction
31,The ByteNet is the instance within this family of models that uses one - dimensional convolutional neural networks ( CNN ) of fixed depth for both the encoder and the decoder ) .,Introduction,Introduction,machine-translation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.4864864864864865,30,0.1492537313432835,18,0.4864864864864865,1,0,Introduction
32,The two CNNs use increasing factors of dilation to rapidly grow the receptive fields ; a similar technique is also used in .,Introduction,Introduction,machine-translation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.5135135135135135,31,0.154228855721393,19,0.5135135135135135,1,0,Introduction
35,"From a computational perspective , the network has a running time that is linear in the length of the source and target sequences ( up to a constant c ?",Introduction,Introduction,machine-translation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.5945945945945946,34,0.1691542288557214,22,0.5945945945945946,1,0,Introduction
36,log d where dis the size of the desired dependency field ) .,Introduction,Introduction,machine-translation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",23,0.6216216216216216,35,0.1741293532338308,23,0.6216216216216216,1,0,Introduction
37,The computation in the encoder during training and decoding and in the decoder during training can also be run efficiently in parallel along the sequences ( Sect. 2 ) .,Introduction,Introduction,machine-translation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.6486486486486487,36,0.1791044776119403,24,0.6486486486486487,1,0,Introduction
39,"In addition , the distance traversed by forward and backward signals between any input and output tokens corresponds to the fixed depth of the networks and is largely independent of the dis - tance between the tokens .",Introduction,Introduction,machine-translation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.7027027027027027,38,0.1890547263681592,26,0.7027027027027027,1,0,Introduction
46,The paper is organized as follows .,Introduction,Introduction,machine-translation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O']",33,0.8918918918918919,45,0.2238805970149253,33,0.8918918918918919,1,0,Introduction
51,Neural Translation Model,,,machine-translation,1,"['O', 'O', 'O']","['O', 'O', 'O']",0,0.0,50,0.2487562189054726,0,0.0,1,0,
53,The distribution indicates the probability of a string t being a translation of s .,Neural Translation Model,Neural Translation Model,machine-translation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.0333333333333333,52,0.2587064676616915,2,0.2,1,0,Neural Translation Model
54,"A product of conditionals over the tokens in the target t = t 0 , ... , t N leads to a tractable formulation of the distribution :",Neural Translation Model,Neural Translation Model,machine-translation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.05,53,0.263681592039801,3,0.3,1,0,Neural Translation Model
56,"The strings are usually sentences of the respective languages ; the tokens are words or , as in the our case , characters .",Neural Translation Model,Neural Translation Model,machine-translation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.0833333333333333,55,0.2736318407960199,5,0.5,1,0,Neural Translation Model
61,It is also useful for the model to be able to assign a non-zero probability to any string in the target language and retain an open vocabulary .,Neural Translation Model,Neural Translation Model,machine-translation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.1666666666666666,60,0.2985074626865671,10,1.0,1,0,Neural Translation Model
62,Desiderata,Neural Translation Model,,machine-translation,1,['O'],['O'],11,0.1833333333333333,61,0.3034825870646766,0,0.0,1,0,Neural Translation Model
63,"Beyond these basic properties the definition of a neural translation model does not determine a unique neural architecture , so we aim at identifying some desiderata .",Neural Translation Model,Desiderata,machine-translation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.2,62,0.308457711442786,1,0.0526315789473684,1,0,Neural Translation Model: Desiderata
64,"First , the running time of the network should be linear in the length of the source and target strings .",Neural Translation Model,Desiderata,machine-translation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.2166666666666666,63,0.3134328358208955,2,0.1052631578947368,1,0,Neural Translation Model: Desiderata
66,The use of operations that run in parallel along the sequence length can also be beneficial for reducing computation time .,Neural Translation Model,Desiderata,machine-translation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.25,65,0.3233830845771144,4,0.2105263157894736,1,0,Neural Translation Model: Desiderata
67,"Second , the size of the source representation should be linear in the length of the source string , i.e. it should be resolution preserving , and not have constant size .",Neural Translation Model,Desiderata,machine-translation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.2666666666666666,66,0.3283582089552239,5,0.2631578947368421,1,0,Neural Translation Model: Desiderata
68,This is to avoid burdening the model with an additional memorization step before translation .,Neural Translation Model,Desiderata,machine-translation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.2833333333333333,67,0.3333333333333333,6,0.3157894736842105,1,0,Neural Translation Model: Desiderata
69,"In more general terms , the size of a representation should be proportional to the amount of information it represents or predicts .",Neural Translation Model,Desiderata,machine-translation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.3,68,0.3383084577114428,7,0.3684210526315789,1,0,Neural Translation Model: Desiderata
70,"Third , the path traversed by forward and backward signals in the network ( between input and ouput tokens ) should be short .",Neural Translation Model,Desiderata,machine-translation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.3166666666666666,69,0.3432835820895522,8,0.4210526315789473,1,0,Neural Translation Model: Desiderata
72,Byte Net,Neural Translation Model,,machine-translation,1,"['O', 'O']","['O', 'O']",21,0.35,71,0.3532338308457711,10,0.5263157894736842,1,0,Neural Translation Model
78,Encoder - Decoder Stacking,Neural Translation Model,,machine-translation,1,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",27,0.45,77,0.3830845771144278,16,0.8421052631578947,1,0,Neural Translation Model
81,This is in contrast to models that compress the source representation into a fixed - size vector or that pool over the source representation with a mechanism such as attentional pooling .,Neural Translation Model,Encoder - Decoder Stacking,machine-translation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",30,0.5,80,0.3980099502487562,19,1.0,1,0,Neural Translation Model: Encoder - Decoder Stacking
83,An encoder and a decoder network that process sequences of different lengths can not be directly connected due to the different sizes of the computed representations .,Neural Translation Model,Dynamic Unfolding,machine-translation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",32,0.5333333333333333,82,0.4079601990049751,1,0.1,1,0,Neural Translation Model: Dynamic Unfolding
85,"Given source and target sequences sand t with respective lengths | s | and | t| , one first chooses a sufficiently tight upper bound | t| on the target length | t | as a linear function of the source length | s | : |",Neural Translation Model,Dynamic Unfolding,machine-translation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",34,0.5666666666666667,84,0.417910447761194,3,0.3,1,0,Neural Translation Model: Dynamic Unfolding
86,"The tight upper bound | t| is chosen in such away that , on the one hand , it is greater than the actual length | t | in almost all cases and , on the other hand , it does not increase excessively the amount of computation that is required .",Neural Translation Model,Dynamic Unfolding,machine-translation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",35,0.5833333333333334,85,0.4228855721393035,4,0.4,1,0,Neural Translation Model: Dynamic Unfolding
87,"Once a linear relationship is chosen , one designs the source encoder so that , given a source sequence of length | s | , the encoder outputs a representation of the established lengt ?",Neural Translation Model,Dynamic Unfolding,machine-translation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",36,0.6,86,0.4278606965174129,5,0.5,1,0,Neural Translation Model: Dynamic Unfolding
89,"In our case , we let a = 1.20 and b = 0 when translating from English into German , as German sentences tend to be somewhat longer than their English counterparts .",Neural Translation Model,Dynamic Unfolding,machine-translation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",38,0.6333333333333333,88,0.4378109452736318,7,0.7,1,0,Neural Translation Model: Dynamic Unfolding
91,"Once the encoder representation is computed , we let the decoder unfold stepby - step over the encoder representation until the decoder itself outputs an end - of - sequence symbol ; the unfolding process may freely proceed beyond the estimated length | t| of the encoder representation .",Neural Translation Model,Dynamic Unfolding,machine-translation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",40,0.6666666666666666,90,0.4477611940298507,9,0.9,1,0,Neural Translation Model: Dynamic Unfolding
92,gives an example of dynamic unfolding .,Neural Translation Model,Dynamic Unfolding,machine-translation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O']",41,0.6833333333333333,91,0.4527363184079602,10,1.0,1,0,Neural Translation Model: Dynamic Unfolding
93,Input Embedding Tensor,Neural Translation Model,,machine-translation,1,"['O', 'O', 'O']","['O', 'O', 'O']",42,0.7,92,0.4577114427860697,0,0.0,1,0,Neural Translation Model
94,"Given the target sequence t = t 0 , ... , tn the ByteNet decoder embeds each of the first n tokens t 0 , ... , t n?1 via a look - up table ( the n tokens t 1 , ... , tn serve as targets for the predictions ) .",Neural Translation Model,Input Embedding Tensor,machine-translation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",43,0.7166666666666667,93,0.4626865671641791,1,0.125,1,0,Neural Translation Model: Input Embedding Tensor
96,Masked One-dimensional,Neural Translation Model,,machine-translation,1,"['O', 'O']","['O', 'O']",45,0.75,95,0.472636815920398,3,0.375,1,0,Neural Translation Model
97,Convolutions,Neural Translation Model,,machine-translation,1,['O'],['O'],46,0.7666666666666667,96,0.4776119402985074,4,0.5,1,0,Neural Translation Model
100,The operation can be implemented either by zeroing out some of the weights of a wider kernel of size 2 k ?,Neural Translation Model,Convolutions,machine-translation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",49,0.8166666666666667,99,0.4925373134328358,7,0.875,1,0,Neural Translation Model: Convolutions
101,1 or by padding the input map .,Neural Translation Model,Convolutions,machine-translation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",50,0.8333333333333334,100,0.4975124378109453,8,1.0,1,0,Neural Translation Model: Convolutions
102,Dilation,Neural Translation Model,,machine-translation,1,['O'],['O'],51,0.85,101,0.5024875621890548,0,0.0,1,0,Neural Translation Model
106,The scheme is repeated multiple times in the network always starting from a dilation rate of 1 ( van den .,Neural Translation Model,Dilation,machine-translation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",55,0.9166666666666666,105,0.5223880597014925,4,1.0,1,0,Neural Translation Model: Dilation
107,Residual Blocks,Neural Translation Model,,machine-translation,1,"['O', 'O']","['O', 'O']",56,0.9333333333333332,106,0.527363184079602,0,0.0,1,0,Neural Translation Model
109,diagrams the two variants of the blocks .,Neural Translation Model,Residual Blocks,machine-translation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",58,0.9666666666666668,108,0.5373134328358209,2,0.5,1,0,Neural Translation Model: Residual Blocks
112,Model Comparison,,,machine-translation,1,"['O', 'O']","['O', 'O']",0,0.0,111,0.5522388059701493,0,0.0,1,0,
114,"For the sake of a more complete analysis , we include two recurrent ByteNet variants ( which we do not evaluate in the experiments ) .",Model Comparison,Model Comparison,machine-translation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.0625,113,0.5621890547263682,2,1.0,1,0,Model Comparison
115,Recurrent ByteNets,Model Comparison,,machine-translation,1,"['O', 'O']","['O', 'O']",3,0.09375,114,0.5671641791044776,0,0.0,1,0,Model Comparison
117,This way of combining the networks is not tied to the networks being strictly convolutional .,Model Comparison,Recurrent ByteNets,machine-translation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.15625,116,0.5771144278606966,2,0.25,1,0,Model Comparison: Recurrent ByteNets
118,We may consider two variants of the ByteNet that use recurrent networks for one or both of the networks ( see ) .,Model Comparison,Recurrent ByteNets,machine-translation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.1875,117,0.582089552238806,3,0.375,1,0,Model Comparison: Recurrent ByteNets
120,"The second variant also replaces the convolutional encoder with a recurrent encoder , e.g. a bidirectional RNN .",Model Comparison,Recurrent ByteNets,machine-translation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.25,119,0.5920398009950248,5,0.625,1,0,Model Comparison: Recurrent ByteNets
121,The target RNN is then placed on top of the source RNN .,Model Comparison,Recurrent ByteNets,machine-translation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.28125,120,0.5970149253731343,6,0.75,1,0,Model Comparison: Recurrent ByteNets
122,"Considering the latter Recurrent ByteNet , we can see that the RNN Enc - Dec network ) is a Recurrent ByteNet where all connections between source and target - except for the first one that connects s 0 and t 0 - have been severed .",Model Comparison,Recurrent ByteNets,machine-translation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.3125,121,0.6019900497512438,7,0.875,1,0,Model Comparison: Recurrent ByteNets
123,"The Recurrent ByteNet is a generalization of the RNN Enc - Dec and , modulo the type of weight - sharing scheme , so is the convolutional ByteNet .",Model Comparison,Recurrent ByteNets,machine-translation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.34375,122,0.6069651741293532,8,1.0,1,0,Model Comparison: Recurrent ByteNets
124,Comparison of Properties,Model Comparison,,machine-translation,1,"['O', 'O', 'O']","['O', 'O', 'O']",12,0.375,123,0.6119402985074627,0,0.0,1,0,Model Comparison
127,We separate the first ( computation time ) desider - atum into three columns .,Model Comparison,Comparison of Properties,machine-translation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.46875,126,0.6268656716417911,3,0.1578947368421052,1,0,Model Comparison: Comparison of Properties
128,The first column indicates the time complexity of the network as a function of the length of the sequences and is denoted by Time .,Model Comparison,Comparison of Properties,machine-translation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.5,127,0.6318407960199005,4,0.2105263157894736,1,0,Model Comparison: Comparison of Properties
129,"The other two columns Net Sand Net T indicate , respectively , whether the source and the target network use a convolutional structure ( CNN ) or a recurrent one ( RNN ) ; a CNN structure has the advantage that it can be run in parallel along the length of the sequence .",Model Comparison,Comparison of Properties,machine-translation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.53125,128,0.6368159203980099,5,0.2631578947368421,1,0,Model Comparison: Comparison of Properties
130,"The second ( resolution preservation ) desideratum corresponds to the RP column , which indicates whether the source representation in the network is resolution preserving .",Model Comparison,Comparison of Properties,machine-translation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.5625,129,0.6417910447761194,6,0.3157894736842105,1,0,Model Comparison: Comparison of Properties
131,"Finally , the third desideratum ( short forward and backward flow paths ) is reflected by two columns .",Model Comparison,Comparison of Properties,machine-translation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.59375,130,0.6467661691542289,7,0.3684210526315789,1,0,Model Comparison: Comparison of Properties
132,The Path S column corresponds to the length in layer steps of the shortest path between a source token and any output target token .,Model Comparison,Comparison of Properties,machine-translation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.625,131,0.6517412935323383,8,0.4210526315789473,1,0,Model Comparison: Comparison of Properties
133,"Similarly , the Path T column corresponds to the length of the shortest path between an input target token and any output target token .",Model Comparison,Comparison of Properties,machine-translation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.65625,132,0.6567164179104478,9,0.4736842105263157,1,0,Model Comparison: Comparison of Properties
134,Shorter paths lead to better forward and backward signal propagation .,Model Comparison,Comparison of Properties,machine-translation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.6875,133,0.6616915422885572,10,0.5263157894736842,1,0,Model Comparison: Comparison of Properties
136,"The ByteNet , the Recurrent ByteNets and the RNN Enc - Dec are the only networks that have linear running time ( up to the constant c ) .",Model Comparison,Comparison of Properties,machine-translation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.75,135,0.6716417910447762,12,0.631578947368421,1,0,Model Comparison: Comparison of Properties
137,"The RNN Enc - Dec , however , does not preserve the source sequence resolution , a feature that aggravates learning for long sequences such as those that appear in character - to - character machine translation .",Model Comparison,Comparison of Properties,machine-translation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.78125,136,0.6766169154228856,13,0.6842105263157895,1,0,Model Comparison: Comparison of Properties
139,The ByteNet stands out also for its Path properties .,Model Comparison,Comparison of Properties,machine-translation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.84375,138,0.6865671641791045,15,0.7894736842105263,1,0,Model Comparison: Comparison of Properties
144,RNN,Model Comparison,,machine-translation,1,['O'],['O'],32,1.0,143,0.7114427860696517,0,0.0,1,0,Model Comparison
146,Stacked LSTM,Model Test,,machine-translation,1,"['O', 'O']","['O', 'O']",1,0.0208333333333333,145,0.7213930348258707,1,0.1666666666666666,1,0,Model Test
150,Norm HyperLSTM 1.34 Recurrent Highway Networks 1.32 Byte,Model Test,Stacked LSTM,machine-translation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.1041666666666666,149,0.7412935323383084,5,0.8333333333333334,1,0,Model Test: Stacked LSTM
152,Character Prediction,Model Test,,machine-translation,1,"['O', 'O']","['O', 'O']",7,0.1458333333333333,151,0.7512437810945274,0,0.0,1,0,Model Test
158,This gives a receptive field of 315 characters .,Model Test,Character Prediction,machine-translation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.2708333333333333,157,0.7810945273631841,6,0.1463414634146341,1,0,Model Test: Character Prediction
172,The outputs of the network are strings of characters in the target language .,Model Test,Character - Level Machine Translation,machine-translation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.5625,171,0.8507462686567164,20,0.4878048780487805,1,0,Model Test: Character - Level Machine Translation
173,We keep 323 characters in the German vocabulary and 296 in the English vocabulary .,Model Test,Character - Level Machine Translation,machine-translation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",28,0.5833333333333334,172,0.8557213930348259,21,0.5121951219512195,1,0,Model Test: Character - Level Machine Translation
175,"As in the ByteNet Decoder , the residual blocks are arranged in sets of five with corresponding dilation rates of 1 , 2 , 4 , 8 and 16 .",Model Test,Character - Level Machine Translation,machine-translation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",30,0.625,174,0.8656716417910447,23,0.5609756097560976,1,0,Model Test: Character - Level Machine Translation
178,"The size of the kernel in the source network is 3 , whereas the size of the masked kernel in the target network is 3 .",Model Test,Character - Level Machine Translation,machine-translation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",33,0.6875,177,0.8805970149253731,26,0.6341463414634146,1,0,Model Test: Character - Level Machine Translation
180,Each sentence is padded with special characters to the nearest greater multiple of 50 ; 20 % of further padding is ap - plied to each source sentence as apart of dynamic unfolding ( eq. 2 ) .,Model Test,Character - Level Machine Translation,machine-translation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",35,0.7291666666666666,179,0.8905472636815921,28,0.6829268292682927,1,0,Model Test: Character - Level Machine Translation
184,"We do not use length normalization , nor do we keep score of which parts of the source sentence have been translated .",Model Test,Character - Level Machine Translation,machine-translation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",39,0.8125,183,0.9104477611940298,32,0.7804878048780488,1,0,Model Test: Character - Level Machine Translation
188,contains some of the unaltered generated translations from the ByteNet that highlight reordering and other phenomena such as transliteration .,Model Test,Character - Level Machine Translation,machine-translation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",43,0.8958333333333334,187,0.9303482587064676,36,0.8780487804878049,1,0,Model Test: Character - Level Machine Translation
189,The character - level aspect of the model makes post -processing unnecessary in principle .,Model Test,Character - Level Machine Translation,machine-translation,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",44,0.9166666666666666,188,0.9353233830845772,37,0.902439024390244,1,0,Model Test: Character - Level Machine Translation
194,Conclusion,,,machine-translation,1,['O'],['O'],0,0.0,193,0.9601990049751244,0,0.0,1,0,
3,abstract,,,machine-translation,2,['O'],['O'],0,0.0,2,0.0088888888888888,0,0.0,1,0,
4,The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder .,abstract,abstract,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0625,3,0.0133333333333333,1,0.0625,1,0,abstract
11,* Equal contribution .,abstract,abstract,machine-translation,2,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",8,0.5,10,0.0444444444444444,8,0.5,1,0,abstract
12,Listing order is random .,abstract,abstract,machine-translation,2,"['O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O']",9,0.5625,11,0.0488888888888888,9,0.5625,1,0,abstract
13,Jakob proposed replacing RNNs with self - attention and started the effort to evaluate this idea .,abstract,abstract,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.625,12,0.0533333333333333,10,0.625,1,0,abstract
14,"Ashish , with Illia , designed and implemented the first Transformer models and has been crucially involved in every aspect of this work .",abstract,abstract,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.6875,13,0.0577777777777777,11,0.6875,1,0,abstract
15,"Noam proposed scaled dot-product attention , multi-head attention and the parameter - free position representation and became the other person involved in nearly every detail .",abstract,abstract,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.75,14,0.0622222222222222,12,0.75,1,0,abstract
17,"Llion also experimented with novel model variants , was responsible for our initial codebase , and efficient inference and visualizations .",abstract,abstract,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.875,16,0.0711111111111111,14,0.875,1,0,abstract
18,"Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor , replacing our earlier codebase , greatly improving results and massively accelerating our research .",abstract,abstract,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.9375,17,0.0755555555555555,15,0.9375,1,0,abstract
19,Work performed while at Google Brain .,abstract,abstract,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O']",16,1.0,18,0.08,16,1.0,1,0,abstract
21,"Recurrent neural networks , long short - term memory and gated recurrent neural networks in particular , have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation .",Introduction,Introduction,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0909090909090909,20,0.0888888888888888,1,0.0909090909090909,1,0,Introduction
22,Numerous efforts have since continued to push the boundaries of recurrent language models and encoder - decoder architectures .,Introduction,Introduction,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.1818181818181818,21,0.0933333333333333,2,0.1818181818181818,1,0,Introduction
23,Recurrent models typically factor computation along the symbol positions of the input and output sequences .,Introduction,Introduction,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.2727272727272727,22,0.0977777777777777,3,0.2727272727272727,1,0,Introduction
24,"Aligning the positions to steps in computation time , they generate a sequence of hidden states ht , as a function of the previous hidden state h t?1 and the input for position t.",Introduction,Introduction,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.3636363636363636,23,0.1022222222222222,4,0.3636363636363636,1,0,Introduction
25,"This inherently sequential nature precludes parallelization within training examples , which becomes critical at longer sequence lengths , as memory constraints limit batching across examples .",Introduction,Introduction,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.4545454545454545,24,0.1066666666666666,5,0.4545454545454545,1,0,Introduction
27,"The fundamental constraint of sequential computation , however , remains .",Introduction,Introduction,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.6363636363636364,26,0.1155555555555555,7,0.6363636363636364,1,0,Introduction
28,"Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences .",Introduction,Introduction,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.7272727272727273,27,0.12,8,0.7272727272727273,1,0,Introduction
29,"In all but a few cases , however , such attention mechanisms are used in conjunction with a recurrent network .",Introduction,Introduction,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.8181818181818182,28,0.1244444444444444,9,0.8181818181818182,1,0,Introduction
32,Background,,,machine-translation,2,['O'],['O'],0,0.0,31,0.1377777777777777,0,0.0,1,0,
42,Model Architecture,,,machine-translation,2,"['O', 'O']","['O', 'O']",0,0.0,41,0.1822222222222222,0,0.0,1,0,
43,Most competitive neural sequence transduction models have an encoder - decoder structure .,Model Architecture,Model Architecture,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0091743119266055,42,0.1866666666666666,1,0.2,1,0,Model Architecture
44,"Here , the encoder maps an input sequence of symbol representations ( x 1 , ... , x n ) to a sequence of continuous representations z = ( z 1 , ... , z n ) .",Model Architecture,Model Architecture,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.018348623853211,43,0.1911111111111111,2,0.4,1,0,Model Architecture
45,"Given z , the decoder then generates an output sequence ( y 1 , ... , y m ) of symbols one element at a time .",Model Architecture,Model Architecture,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.0275229357798165,44,0.1955555555555555,3,0.6,1,0,Model Architecture
46,"At each step the model is auto-regressive , consuming the previously generated symbols as additional input when generating the next .",Model Architecture,Model Architecture,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.036697247706422,45,0.2,4,0.8,1,0,Model Architecture
48,Encoder and Decoder Stacks,Model Architecture,,machine-translation,2,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",6,0.055045871559633,47,0.2088888888888889,0,0.0,1,0,Model Architecture
49,Encoder :,Model Architecture,Encoder and Decoder Stacks,machine-translation,2,"['O', 'O']","['O', 'O']",7,0.0642201834862385,48,0.2133333333333333,1,0.0769230769230769,1,0,Model Architecture: Encoder and Decoder Stacks
54,"That is , the output of each sub - layer is LayerNorm ( x + Sublayer ( x ) ) , where Sublayer ( x ) is the function implemented by the sub - layer itself .",Model Architecture,Encoder and Decoder Stacks,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.110091743119266,53,0.2355555555555555,6,0.4615384615384615,1,0,Model Architecture: Encoder and Decoder Stacks
55,"To facilitate these residual connections , all sub- layers in the model , as well as the embedding layers , produce outputs of dimension d model = 512 .",Model Architecture,Encoder and Decoder Stacks,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.1192660550458715,54,0.24,7,0.5384615384615384,1,0,Model Architecture: Encoder and Decoder Stacks
56,Decoder :,Model Architecture,Encoder and Decoder Stacks,machine-translation,2,"['O', 'O']","['O', 'O']",14,0.128440366972477,55,0.2444444444444444,8,0.6153846153846154,1,0,Model Architecture: Encoder and Decoder Stacks
61,"This masking , combined with fact that the output embeddings are offset by one position , ensures that the predictions for position i can depend only on the known outputs at positions less than i.",Model Architecture,Encoder and Decoder Stacks,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.1743119266055046,60,0.2666666666666666,13,1.0,1,0,Model Architecture: Encoder and Decoder Stacks
62,Attention,Model Architecture,,machine-translation,2,['O'],['O'],20,0.1834862385321101,61,0.2711111111111111,0,0.0,1,0,Model Architecture
63,"An attention function can be described as mapping a query and a set of key - value pairs to an output , where the query , keys , values , and output are all vectors .",Model Architecture,Attention,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.1926605504587156,62,0.2755555555555555,1,0.3333333333333333,1,0,Model Architecture: Attention
65,Scaled Dot - Product,Model Architecture,,machine-translation,2,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",23,0.2110091743119266,64,0.2844444444444444,3,1.0,1,0,Model Architecture
66,Attention,Model Architecture,,machine-translation,2,['O'],['O'],24,0.2201834862385321,65,0.2888888888888888,0,0.0,1,0,Model Architecture
67,"We call our particular attention "" Scaled Dot -Product Attention "" ) .",Model Architecture,Attention,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.2293577981651376,66,0.2933333333333333,1,0.0416666666666666,1,0,Model Architecture: Attention
70,"In practice , we compute the attention function on a set of queries simultaneously , packed together into a matrix Q .",Model Architecture,Attention,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",28,0.2568807339449541,69,0.3066666666666666,4,0.1666666666666666,1,0,Model Architecture: Attention
71,The keys and values are also packed together into matrices K and V .,Model Architecture,Attention,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",29,0.2660550458715596,70,0.3111111111111111,5,0.2083333333333333,1,0,Model Architecture: Attention
72,We compute the matrix of outputs as :,Model Architecture,Attention,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",30,0.2752293577981651,71,0.3155555555555555,6,0.25,1,0,Model Architecture: Attention
73,"The two most commonly used attention functions are additive attention , and dot-product ( multiplicative ) attention .",Model Architecture,Attention,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",31,0.2844036697247706,72,0.32,7,0.2916666666666667,1,0,Model Architecture: Attention
74,"Dot-product attention is identical to our algorithm , except for the scaling factor of 1",Model Architecture,Attention,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",32,0.2935779816513761,73,0.3244444444444444,8,0.3333333333333333,1,0,Model Architecture: Attention
76,"While the two are similar in theoretical complexity , dot-product attention is much faster and more space - efficient in practice , since it can be implemented using highly optimized matrix multiplication code .",Model Architecture,Attention,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",34,0.3119266055045872,75,0.3333333333333333,10,0.4166666666666667,1,0,Model Architecture: Attention
78,"We suspect that for large values of d k , the dot products grow large in magnitude , pushing the softmax function into regions where it has extremely small gradients",Model Architecture,Attention,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",36,0.3302752293577982,77,0.3422222222222222,12,0.5,1,0,Model Architecture: Attention
79,4 .,Model Architecture,Attention,machine-translation,2,"['O', 'O']","['O', 'O']",37,0.3394495412844037,78,0.3466666666666667,13,0.5416666666666666,1,0,Model Architecture: Attention
80,"To counteract this effect , we scale the dot products by 1",Model Architecture,Attention,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",38,0.3486238532110092,79,0.3511111111111111,14,0.5833333333333334,1,0,Model Architecture: Attention
84,"These are concatenated and once again projected , resulting in the final values , as depicted in .",Model Architecture,Multi - Head Attention,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",42,0.3853211009174312,83,0.3688888888888889,18,0.75,1,0,Model Architecture: Multi - Head Attention
87,Where the projections are parameter matrices,Model Architecture,Multi - Head Attention,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O']",45,0.4128440366972477,86,0.3822222222222222,21,0.875,1,0,Model Architecture: Multi - Head Attention
89,For each of these we use,Model Architecture,Multi - Head Attention,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O']",47,0.4311926605504587,88,0.3911111111111111,23,0.9583333333333334,1,0,Model Architecture: Multi - Head Attention
93,"In "" encoder - decoder attention "" layers , the queries come from the previous decoder layer , and the memory keys and values come from the output of the encoder .",Model Architecture,Applications of Attention in our Model,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",51,0.4678899082568807,92,0.4088888888888889,2,0.1176470588235294,1,0,Model Architecture: Applications of Attention in our Model
94,This allows every position in the decoder to attend overall positions in the input sequence .,Model Architecture,Applications of Attention in our Model,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",52,0.4770642201834862,93,0.4133333333333333,3,0.1764705882352941,1,0,Model Architecture: Applications of Attention in our Model
96,"Ina self - attention layer all of the keys , values and queries come from the same place , in this case , the output of the previous layer in the encoder .",Model Architecture,Applications of Attention in our Model,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",54,0.4954128440366973,95,0.4222222222222222,5,0.2941176470588235,1,0,Model Architecture: Applications of Attention in our Model
97,Each position in the encoder can attend to all positions in the previous layer of the encoder .,Model Architecture,Applications of Attention in our Model,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",55,0.5045871559633027,96,0.4266666666666667,6,0.3529411764705882,1,0,Model Architecture: Applications of Attention in our Model
99,We need to prevent leftward information flow in the decoder to preserve the auto - regressive property .,Model Architecture,Applications of Attention in our Model,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",57,0.5229357798165137,98,0.4355555555555555,8,0.4705882352941176,1,0,Model Architecture: Applications of Attention in our Model
100,We implement this inside of scaled dot-product attention by masking out ( setting to ?? ) all values in the input of the softmax which correspond to illegal connections .,Model Architecture,Applications of Attention in our Model,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",58,0.5321100917431193,99,0.44,9,0.5294117647058824,1,0,Model Architecture: Applications of Attention in our Model
101,See.,Model Architecture,,machine-translation,2,['O'],['O'],59,0.5412844036697247,100,0.4444444444444444,10,0.5882352941176471,1,0,Model Architecture
102,Position - wise Feed - Forward Networks,Model Architecture,See.,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O']",60,0.5504587155963303,101,0.4488888888888889,11,0.6470588235294118,1,0,Model Architecture: See.
106,Another way of describing this is as two convolutions with kernel size,Model Architecture,See.,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",64,0.5871559633027523,105,0.4666666666666667,15,0.8823529411764706,1,0,Model Architecture: See.
107,1 .,Model Architecture,See.,machine-translation,2,"['O', 'O']","['O', 'O']",65,0.5963302752293578,106,0.4711111111111111,16,0.9411764705882352,1,0,Model Architecture: See.
108,"The dimensionality of input and output is d model = 512 , and the inner-layer has dimensionality d ff = 2048 .",Model Architecture,See.,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",66,0.6055045871559633,107,0.4755555555555555,17,1.0,1,0,Model Architecture: See.
114,Positional Encoding,Model Architecture,,machine-translation,2,"['O', 'O']","['O', 'O']",72,0.6605504587155964,113,0.5022222222222222,0,0.0,1,0,Model Architecture
115,"Since our model contains no recurrence and no convolution , in order for the model to make use of the order of the sequence , we must inject some information about the relative or absolute position of the : Maximum path lengths , per-layer complexity and minimum number of sequential operations for different layer types .",Model Architecture,Positional Encoding,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",73,0.6697247706422018,114,0.5066666666666667,1,0.5,1,0,Model Architecture: Positional Encoding
116,"n is the sequence length , dis the representation dimension , k is the kernel size of convolutions and r the size of the neighborhood in restricted self - attention .",Model Architecture,Positional Encoding,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",74,0.6788990825688074,115,0.5111111111111111,2,1.0,1,0,Model Architecture: Positional Encoding
117,Layer Type Complexity per Layer Sequential Maximum Path Length Operations,Model Architecture,Positional Encoding,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",75,0.6880733944954128,116,0.5155555555555555,0,0.0,1,0,Model Architecture: Positional Encoding
118,tokens in the sequence .,Model Architecture,Positional Encoding,machine-translation,2,"['O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O']",76,0.6972477064220184,117,0.52,1,0.0294117647058823,1,0,Model Architecture: Positional Encoding
120,"The positional encodings have the same dimension d model as the embeddings , so that the two can be summed .",Model Architecture,Positional Encoding,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",78,0.7155963302752294,119,0.5288888888888889,3,0.088235294117647,1,0,Model Architecture: Positional Encoding
121,"There are many choices of positional encodings , learned and fixed .",Model Architecture,Positional Encoding,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",79,0.7247706422018348,120,0.5333333333333333,4,0.1176470588235294,1,0,Model Architecture: Positional Encoding
122,"In this work , we use sine and cosine functions of different frequencies : where pos is the position and i is the dimension .",Model Architecture,Positional Encoding,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",80,0.7339449541284404,121,0.5377777777777778,5,0.1470588235294117,1,0,Model Architecture: Positional Encoding
123,"That is , each dimension of the positional encoding corresponds to a sinusoid .",Model Architecture,Positional Encoding,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",81,0.7431192660550459,122,0.5422222222222223,6,0.1764705882352941,1,0,Model Architecture: Positional Encoding
124,The wavelengths form a geometric progression from 2 ? to 10000 2 ?.,Model Architecture,Positional Encoding,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",82,0.7522935779816514,123,0.5466666666666666,7,0.2058823529411764,1,0,Model Architecture: Positional Encoding
125,"We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions , since for any fixed offset k , PE pos+k can be represented as a linear function of PE pos .",Model Architecture,Positional Encoding,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",83,0.7614678899082569,124,0.5511111111111111,8,0.2352941176470588,1,0,Model Architecture: Positional Encoding
127,We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training .,Model Architecture,Positional Encoding,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",85,0.7798165137614679,126,0.56,10,0.2941176470588235,1,0,Model Architecture: Positional Encoding
129,"In this section we compare various aspects of self - attention layers to the recurrent and convolutional layers commonly used for mapping one variable - length sequence of symbol representations ( x 1 , ... , x n ) to another sequence of equal length ( z 1 , ... , z n ) , with x i , z i ?",Model Architecture,Why Self - Attention,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",87,0.7981651376146789,128,0.5688888888888889,12,0.3529411764705882,1,0,Model Architecture: Why Self - Attention
130,"Rd , such as a hidden layer in atypical sequence transduction encoder or decoder .",Model Architecture,Why Self - Attention,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",88,0.8073394495412844,129,0.5733333333333334,13,0.3823529411764705,1,0,Model Architecture: Why Self - Attention
132,One is the total computational complexity per layer .,Model Architecture,Why Self - Attention,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",90,0.8256880733944955,131,0.5822222222222222,15,0.4411764705882353,1,0,Model Architecture: Why Self - Attention
133,"Another is the amount of computation that can be parallelized , as measured by the minimum number of sequential operations required .",Model Architecture,Why Self - Attention,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",91,0.8348623853211009,132,0.5866666666666667,16,0.4705882352941176,1,0,Model Architecture: Why Self - Attention
134,The third is the path length between long - range dependencies in the network .,Model Architecture,Why Self - Attention,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",92,0.8440366972477065,133,0.5911111111111111,17,0.5,1,0,Model Architecture: Why Self - Attention
136,One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network .,Model Architecture,Why Self - Attention,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",94,0.8623853211009175,135,0.6,19,0.5588235294117647,1,0,Model Architecture: Why Self - Attention
137,"The shorter these paths between any combination of positions in the input and output sequences , the easier it is to learn long - range dependencies .",Model Architecture,Why Self - Attention,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",95,0.8715596330275229,136,0.6044444444444445,20,0.5882352941176471,1,0,Model Architecture: Why Self - Attention
138,Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types .,Model Architecture,Why Self - Attention,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",96,0.8807339449541285,137,0.6088888888888889,21,0.6176470588235294,1,0,Model Architecture: Why Self - Attention
139,"As noted in , a self - attention layer connects all positions with a constant number of sequentially executed operations , whereas a recurrent layer requires O ( n ) sequential operations .",Model Architecture,Why Self - Attention,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",97,0.8899082568807339,138,0.6133333333333333,22,0.6470588235294118,1,0,Model Architecture: Why Self - Attention
141,"To improve computational performance for tasks involving very long sequences , self - attention could be restricted to considering only a neighborhood of sizer in the input sequence centered around the respective output position .",Model Architecture,Why Self - Attention,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",99,0.908256880733945,140,0.6222222222222222,24,0.7058823529411765,1,0,Model Architecture: Why Self - Attention
142,This would increase the maximum path length to O ( n / r ) .,Model Architecture,Why Self - Attention,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",100,0.9174311926605504,141,0.6266666666666667,25,0.7352941176470589,1,0,Model Architecture: Why Self - Attention
143,We plan to investigate this approach further in future work .,Model Architecture,Why Self - Attention,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",101,0.926605504587156,142,0.6311111111111111,26,0.7647058823529411,1,0,Model Architecture: Why Self - Attention
144,A single convolutional layer with kernel width k < n does not connect all pairs of input and output positions .,Model Architecture,Why Self - Attention,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",102,0.9357798165137616,143,0.6355555555555555,27,0.7941176470588235,1,0,Model Architecture: Why Self - Attention
146,"Convolutional layers are generally more expensive than recurrent layers , by a factor of k.",Model Architecture,Why Self - Attention,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",104,0.9541284403669724,145,0.6444444444444445,29,0.8529411764705882,1,0,Model Architecture: Why Self - Attention
147,"Separable convolutions , however , decrease the complexity considerably , to O ( k n d + n d 2 ) .",Model Architecture,Why Self - Attention,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",105,0.963302752293578,146,0.6488888888888888,30,0.8823529411764706,1,0,Model Architecture: Why Self - Attention
148,"Even with k = n , however , the complexity of a separable convolution is equal to the combination of a self - attention layer and a point - wise feed - forward layer , the approach we take in our model .",Model Architecture,Why Self - Attention,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",106,0.9724770642201837,147,0.6533333333333333,31,0.9117647058823528,1,0,Model Architecture: Why Self - Attention
149,"As side benefit , self - attention could yield more interpretable models .",Model Architecture,Why Self - Attention,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",107,0.981651376146789,148,0.6577777777777778,32,0.9411764705882352,1,0,Model Architecture: Why Self - Attention
152,Training,,,machine-translation,2,['O'],['O'],0,0.0,151,0.6711111111111111,0,0.0,1,0,
154,Training Data and Batching,,,machine-translation,2,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",0,0.0,153,0.68,0,0.0,1,0,
166,Optimizer,Training Data and Batching,,machine-translation,2,['O'],['O'],12,0.48,165,0.7333333333333333,0,0.0,1,0,Training Data and Batching
168,"We varied the learning rate over the course of training , according to the formula :",Training Data and Batching,Optimizer,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.56,167,0.7422222222222222,2,0.5,1,0,Training Data and Batching: Optimizer
169,"This corresponds to increasing the learning rate linearly for the first warmup_steps training steps , and decreasing it thereafter proportionally to the inverse square root of the step number .",Training Data and Batching,Optimizer,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.6,168,0.7466666666666667,3,0.75,1,0,Training Data and Batching: Optimizer
171,Regularization,Training Data and Batching,,machine-translation,2,['O'],['O'],17,0.68,170,0.7555555555555555,0,0.0,1,0,Training Data and Batching
173,Residual Dropout,Training Data and Batching,,machine-translation,2,"['O', 'O']","['O', 'O']",19,0.76,172,0.7644444444444445,2,0.25,1,0,Training Data and Batching
180,Results,,,machine-translation,2,['O'],['O'],0,0.0,179,0.7955555555555556,0,0.0,1,0,
181,Machine Translation,Results,,machine-translation,2,"['O', 'O']","['O', 'O']",1,0.0714285714285714,180,0.8,0,0.0,1,0,Results
183,The configuration of this model is listed in the bottom line of .,Results,Machine Translation,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.2142857142857142,182,0.8088888888888889,2,0.1538461538461538,1,0,Results: Machine Translation
195,Model Variations,,,machine-translation,2,"['O', 'O']","['O', 'O']",0,0.0,194,0.8622222222222222,0,0.0,1,0,
202,This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product maybe beneficial .,Model Variations,Model Variations,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.3333333333333333,201,0.8933333333333333,7,0.7777777777777778,1,0,Model Variations
205,English Constituency Parsing,Model Variations,,machine-translation,2,"['O', 'O', 'O']","['O', 'O', 'O']",10,0.4761904761904761,204,0.9066666666666666,0,0.0,1,0,Model Variations
207,This task presents specific challenges : the output is subject to strong structural constraints and is significantly longer than the input .,Model Variations,English Constituency Parsing,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.5714285714285714,206,0.9155555555555556,2,0.1818181818181818,1,0,Model Variations: English Constituency Parsing
208,"Furthermore , RNN sequence - to - sequence models have not been able to attain state - of - the - art results in small - data regimes .",Model Variations,English Constituency Parsing,machine-translation,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.6190476190476191,207,0.92,3,0.2727272727272727,1,0,Model Variations: English Constituency Parsing
217,Conclusion,,,machine-translation,2,['O'],['O'],0,0.0,216,0.96,0,0.0,1,0,
3,abstract,,,machine-translation,3,['O'],['O'],0,0.0,2,0.0063897763578274,0,0.0,1,0,
4,Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years .,abstract,abstract,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.1111111111111111,3,0.0095846645367412,1,0.1111111111111111,1,0,abstract
5,"However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system .",abstract,abstract,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.2222222222222222,4,0.0127795527156549,2,0.2222222222222222,1,0,abstract
14,Neural machine translation ( NMT ) has attracted a lot of interest in solving the machine translation ( MT ) problem in recent years .,Introduction,Introduction,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0067114093959731,13,0.0415335463258785,1,0.0344827586206896,1,0,Introduction
17,"In general , there are two types of NMT topologies : the encoder - decoder network and the attention network .",Introduction,Introduction,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.0268456375838926,16,0.0511182108626198,4,0.1379310344827586,1,0,Introduction
21,"However , a single neural model of either of the above types has not been competitive with the best conventional system when evaluated on the WMT ' 14 English - to - French task .",Introduction,Introduction,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.0536912751677852,20,0.0638977635782747,8,0.2758620689655172,1,0,Introduction
24,Deep topology has been proven to outperform the shallow architecture in computer vision .,Introduction,Introduction,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.0738255033557047,23,0.0734824281150159,11,0.3793103448275862,1,0,Introduction
25,In the past two years the top positions of the ImageNet contest have always been occupied by systems with tensor even hundreds of layers .,Introduction,Introduction,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.0805369127516778,24,0.0766773162939297,12,0.4137931034482758,1,0,Introduction
26,"But in NMT , the biggest depth used successfully is only six .",Introduction,Introduction,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.087248322147651,25,0.0798722044728434,13,0.4482758620689655,1,0,Introduction
27,We attribute this problem to the properties of the Long Short - Term Memory ( LSTM ) which is widely used in NMT .,Introduction,Introduction,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.0939597315436241,26,0.0830670926517571,14,0.4827586206896552,1,0,Introduction
28,"In the LSTM , there are more non-linear activations than in convolution layers .",Introduction,Introduction,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.1006711409395973,27,0.0862619808306709,15,0.5172413793103449,1,0,Introduction
30,"There are also many efforts to increase the depth of the LSTM such as the work by , where the shortcuts do not avoid the nonlinear and recurrent computation .",Introduction,Introduction,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.1140939597315436,29,0.0926517571884984,17,0.5862068965517241,1,0,Introduction
34,This topology can be used for both the encoder - decoder network and the attention network .,Introduction,Introduction,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.1409395973154362,33,0.1054313099041533,21,0.7241379310344828,1,0,Introduction
41,"As a reference , previous work showed that oracle rescoring of the 1000 - best sequences generated by the SMT model can achieve the BLEU score of about 45 .",Introduction,Introduction,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",28,0.1879194630872483,40,0.1277955271565495,28,0.9655172413793104,1,0,Introduction
43,Neural Machine Translation,Introduction,,machine-translation,3,"['O', 'O', 'O']","['O', 'O', 'O']",30,0.2013422818791946,42,0.134185303514377,0,0.0,1,0,Introduction
44,"Neural machine translation aims at generating the target word sequence y = {y 1 , . . . , y n } given the source word sequence x = {x 1 , . . . , x m } with neural models .",Introduction,Neural Machine Translation,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",31,0.2080536912751678,43,0.1373801916932907,1,0.032258064516129,1,0,Introduction: Neural Machine Translation
45,"In this task , the likelihood p ( y | x , ? ) of the target sequence will be maximized with parameter ?",Introduction,Neural Machine Translation,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",32,0.2147651006711409,44,0.1405750798722044,2,0.064516129032258,1,0,Introduction: Neural Machine Translation
46,to learn :,Introduction,Neural Machine Translation,machine-translation,3,"['O', 'O', 'O']","['O', 'O', 'O']",33,0.2214765100671141,45,0.1437699680511182,3,0.0967741935483871,1,0,Introduction: Neural Machine Translation
47,where y 0:j?1 is the sub sequence from y 0 toy j?1 . y 0 and y m + 1 denote the start mark and end mark of target sequence respectively .,Introduction,Neural Machine Translation,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",34,0.2281879194630872,46,0.1469648562300319,4,0.1290322580645161,1,0,Introduction: Neural Machine Translation
49,"In the encoding part , the source sequence is processed and transformed into a group of vectors e = {e 1 , , em } for each time step .",Introduction,Neural Machine Translation,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",36,0.2416107382550335,48,0.1533546325878594,6,0.1935483870967742,1,0,Introduction: Neural Machine Translation
50,Further operations will be used at the interface part to extract the final representation c of the source sequence from e.,Introduction,Neural Machine Translation,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",37,0.2483221476510067,49,0.1565495207667731,7,0.2258064516129032,1,0,Introduction: Neural Machine Translation
51,"At the decoding step , the target sequence is generated from the representation c.",Introduction,Neural Machine Translation,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",38,0.2550335570469799,50,0.1597444089456869,8,0.2580645161290322,1,0,Introduction: Neural Machine Translation
52,"Recently , there have been two types of NMT models which are different in the interface part .",Introduction,Neural Machine Translation,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",39,0.261744966442953,51,0.1629392971246006,9,0.2903225806451613,1,0,Introduction: Neural Machine Translation
53,"In the encoder - decoder model , a single vector extracted from e is used as the representation .",Introduction,Neural Machine Translation,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",40,0.2684563758389262,52,0.1661341853035143,10,0.3225806451612903,1,0,Introduction: Neural Machine Translation
54,"In the attention model , c is dynamically obtained according to the relationship between the target sequence and the source sequence .",Introduction,Neural Machine Translation,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",41,0.2751677852348993,53,0.1693290734824281,11,0.3548387096774194,1,0,Introduction: Neural Machine Translation
55,"The recurrent neural network ( RNN ) , or its specific form the LSTM , is generally used as the basic unit of the encoding and decoding part .",Introduction,Neural Machine Translation,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",42,0.2818791946308724,54,0.1725239616613418,12,0.3870967741935484,1,0,Introduction: Neural Machine Translation
56,"However , the topology of most of the existing models is shallow .",Introduction,Neural Machine Translation,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",43,0.2885906040268456,55,0.1757188498402556,13,0.4193548387096774,1,0,Introduction: Neural Machine Translation
57,"In the attention network , the encoding part and the decoding part have only one LSTM layer respectively .",Introduction,Neural Machine Translation,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",44,0.2953020134228188,56,0.1789137380191693,14,0.4516129032258064,1,0,Introduction: Neural Machine Translation
58,"In the encoder - decoder network , researchers have used at most six LSTM layers .",Introduction,Neural Machine Translation,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",45,0.3020134228187919,57,0.182108626198083,15,0.4838709677419355,1,0,Introduction: Neural Machine Translation
59,"Because machine translation is a difficult problem , we believe more complex encoding and decoding architecture is needed for modeling the relationship between the source sequence and the target sequence .",Introduction,Neural Machine Translation,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",46,0.3087248322147651,58,0.1853035143769968,16,0.5161290322580645,1,0,Introduction: Neural Machine Translation
61,Deep neural models have been studied in a wide range of problems .,Introduction,Neural Machine Translation,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",48,0.3221476510067114,60,0.1916932907348242,18,0.5806451612903226,1,0,Introduction: Neural Machine Translation
64,"Training networks based on LSTM layers , which are widely used in language problems , is a much more challenging task .",Introduction,Neural Machine Translation,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",51,0.3422818791946309,63,0.2012779552715655,21,0.6774193548387096,1,0,Introduction: Neural Machine Translation
65,"Because of the existence of many more nonlinear activations and the recurrent computation , gradient values are not stable and are generally smaller .",Introduction,Neural Machine Translation,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",52,0.348993288590604,64,0.2044728434504792,22,0.7096774193548387,1,0,Introduction: Neural Machine Translation
66,"Following the same spirit for convolutional networks , a lot of effort has also been spent on training deep LSTM networks .",Introduction,Neural Machine Translation,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",53,0.3557046979865771,65,0.2076677316293929,23,0.7419354838709677,1,0,Introduction: Neural Machine Translation
70,"Similarly , proposed a two dimensional structure for the LSTM .",Introduction,Neural Machine Translation,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",57,0.3825503355704698,69,0.2204472843450479,27,0.8709677419354839,1,0,Introduction: Neural Machine Translation
72,"However , the gradient propagation still relies on the recurrent computation .",Introduction,Neural Machine Translation,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",59,0.3959731543624161,71,0.2268370607028754,29,0.935483870967742,1,0,Introduction: Neural Machine Translation
75,Deep Topology,Introduction,,machine-translation,3,"['O', 'O']","['O', 'O']",62,0.4161073825503356,74,0.2364217252396166,0,0.0,1,0,Introduction
78,We call these connections fastforward connections .,Introduction,Deep Topology,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O']",65,0.436241610738255,77,0.2460063897763578,3,0.75,1,0,Introduction: Deep Topology
80,Network,Introduction,,machine-translation,3,['O'],['O'],67,0.4496644295302013,79,0.2523961661341853,0,0.0,1,0,Introduction
81,Our entire deep neural network is shown in .,Introduction,Network,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",68,0.4563758389261745,80,0.255591054313099,1,0.0121951219512195,1,0,Introduction: Network
82,"This topology can be divided into three parts : the encoder part ( P -E ) on the left , the decoder part ( P - D ) on the right and the interface between these two parts ( P - I ) which extracts the representation of the source sequence .",Introduction,Network,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",69,0.4630872483221476,81,0.2587859424920127,2,0.024390243902439,1,0,Introduction: Network
86,Recurrent layer :,Introduction,Network,machine-translation,3,"['O', 'O', 'O']","['O', 'O', 'O']",73,0.4899328859060403,85,0.2715654952076677,6,0.073170731707317,1,0,Introduction: Network
87,"When an input sequence {x 1 , . . . , x m } is given to a recurrent layer , the output ht at each time step t can be computed as ( see )",Introduction,Network,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",74,0.4966442953020134,86,0.2747603833865815,7,0.0853658536585365,1,0,Introduction: Network
88,where the bias parameter is not included for simplicity .,Introduction,Network,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",75,0.5033557046979866,87,0.2779552715654952,8,0.0975609756097561,1,0,Introduction: Network
90,"A blue square with a "" - "" denotes the previous hidden state .",Introduction,Network,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",77,0.5167785234899329,89,0.2843450479233226,10,0.1219512195121951,1,0,Introduction: Network
91,A dotted line means that the hidden state is used recurrently .,Introduction,Network,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",78,0.5234899328859061,90,0.2875399361022364,11,0.1341463414634146,1,0,Introduction: Network
92,This computation can be equivalently split into two consecutive steps :,Introduction,Network,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",79,0.5302013422818792,91,0.2907348242811501,12,0.1463414634146341,1,0,Introduction: Network
93,"Feed-Forward computation : ft = W f x t . Left part in ) . "" f "" block .",Introduction,Network,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",80,0.5369127516778524,92,0.2939297124600639,13,0.1585365853658536,1,0,Introduction: Network
95,Right part and the sum operation ( + ) followed by activation in .,Introduction,Network,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",82,0.5503355704697986,94,0.3003194888178914,15,0.1829268292682926,1,0,Introduction: Network
96,""" r "" block .",Introduction,Network,machine-translation,3,"['O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O']",83,0.5570469798657718,95,0.3035143769968051,16,0.1951219512195122,1,0,Introduction: Network
97,"For a deep topology with stacked recurrent layers , the input of each block "" f "" at recurrent layer k ( denoted by f k ) is usually the output of block "" r "" at its previous recurrent layer k ?",Introduction,Network,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",84,0.5637583892617449,96,0.3067092651757188,17,0.2073170731707317,1,0,Introduction: Network
98,1 ( denoted by h k?1 ) .,Introduction,Network,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",85,0.5704697986577181,97,0.3099041533546325,18,0.2195121951219512,1,0,Introduction: Network
100,"It means that each block "" f "" at recurrent layer k takes both the outputs of block "" f "" and block "" r "" at its previous layer as input ( ) .",Introduction,Network,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",87,0.5838926174496645,99,0.31629392971246,20,0.2439024390243902,1,0,Introduction: Network
101,F - F connections are denoted by dashed red lines in and .,Introduction,Network,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",88,0.5906040268456376,100,0.3194888178913738,21,0.2560975609756097,1,0,Introduction: Network
102,The path of F - F connections contains neither nonlinear activations nor recurrent computation .,Introduction,Network,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",89,0.5973154362416108,101,0.3226837060702875,22,0.2682926829268293,1,0,Introduction: Network
103,"It provides a fast path for information to propagate , so we call this path fast - forward connections .",Introduction,Network,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",90,0.6040268456375839,102,0.3258785942492013,23,0.2804878048780488,1,0,Introduction: Network
105,This is quantitatively expressed in Eq. 3 :,Introduction,Network,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",92,0.6174496644295302,104,0.3322683706070287,25,0.3048780487804878,1,0,Introduction: Network
106,The opposite directions are marked by the direction term ( ? 1 ) k .,Introduction,Network,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",93,0.6241610738255033,105,0.3354632587859425,26,0.3170731707317073,1,0,Introduction: Network
107,"At the first recurrent layer , the block "" f "" takes x t as the input .",Introduction,Network,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",94,0.6308724832214765,106,0.3386581469648562,27,0.3292682926829268,1,0,Introduction: Network
108,"[ , ] denotes the concatenation of vectors .",Introduction,Network,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",95,0.6375838926174496,107,0.3418530351437699,28,0.3414634146341463,1,0,Introduction: Network
109,This is shown in .,Introduction,Network,machine-translation,3,"['O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O']",96,0.6442953020134228,108,0.3450479233226837,29,0.3536585365853658,1,0,Introduction: Network
111,We add a connection between f kt and f k ?1 t .,Introduction,Network,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",98,0.6577181208053692,110,0.3514376996805112,31,0.3780487804878049,1,0,Introduction: Network
112,"Without f k ?1 t , our model will be reduced to the traditional stacked model .",Introduction,Network,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",99,0.6644295302013423,111,0.3546325878594249,32,0.3902439024390244,1,0,Introduction: Network
113,We alternate the RNN direction at different layers k with the direction term ( ? 1 ) k .,Introduction,Network,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",100,0.6711409395973155,112,0.3578274760383386,33,0.4024390243902439,1,0,Introduction: Network
114,"If we fix the direction term to ? 1 , all layers work in the forward direction .",Introduction,Network,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",101,0.6778523489932886,113,0.3610223642172524,34,0.4146341463414634,1,0,Introduction: Network
115,LSTM layer :,Introduction,Network,machine-translation,3,"['O', 'O', 'O']","['O', 'O', 'O']",102,0.6845637583892618,114,0.3642172523961661,35,0.4268292682926829,1,0,Introduction: Network
116,"In our experiments , instead of an RNN , a specific type of recurrent layer called LSTM ) is used .",Introduction,Network,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",103,0.6912751677852349,115,0.3674121405750798,36,0.4390243902439024,1,0,Introduction: Network
117,The LSTM is structurally more complex than the basic RNN in Eq .,Introduction,Network,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",104,0.697986577181208,116,0.3706070287539936,37,0.4512195121951219,1,0,Introduction: Network
118,"2 . We define the computation of the LSTM as a function which maps the input f and its state - output pair ( h , s ) at the previous time step to the current stateoutput pair .",Introduction,Network,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",105,0.7046979865771812,117,0.3738019169329074,38,0.4634146341463415,1,0,Introduction: Network
120,"is the concatenation of four vectors of equal size , means element - wise multiplication , ?",Introduction,Network,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",107,0.7181208053691275,119,0.3801916932907348,40,0.4878048780487805,1,0,Introduction: Network
121,"i is the input activation function , ?",Introduction,Network,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",108,0.7248322147651006,120,0.3833865814696485,41,0.5,1,0,Introduction: Network
122,"o is the output activation function , ?",Introduction,Network,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",109,0.7315436241610739,121,0.3865814696485623,42,0.5121951219512195,1,0,Introduction: Network
123,"g is the activation function for gates , and W r , ? ? , ? ? , and ? ?",Introduction,Network,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",110,0.738255033557047,122,0.389776357827476,43,0.524390243902439,1,0,Introduction: Network
124,are the parameters of the LSTM .,Introduction,Network,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O']",111,0.7449664429530202,123,0.3929712460063898,44,0.5365853658536586,1,0,Introduction: Network
125,It is slightly different from the standard notation in that we do not have a matrix to multiply with the input fin our notation .,Introduction,Network,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",112,0.7516778523489933,124,0.3961661341853035,45,0.5487804878048781,1,0,Introduction: Network
126,"With this notation , we can write down the computations for our deep bi-directional LSTM model with F - F connections :",Introduction,Network,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",113,0.7583892617449665,125,0.3993610223642173,46,0.5609756097560976,1,0,Introduction: Network
127,where x t is the input to the deep bi-directional LSTM model .,Introduction,Network,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",114,0.7651006711409396,126,0.402555910543131,47,0.573170731707317,1,0,Introduction: Network
128,"For the encoder , x t is the embedding of the t th word in the source sentence .",Introduction,Network,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",115,0.7718120805369127,127,0.4057507987220447,48,0.5853658536585366,1,0,Introduction: Network
129,For the decoder x t is the concatenation of the embedding of the t th word in the target sentence and the encoder representation for step t.,Introduction,Network,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",116,0.7785234899328859,128,0.4089456869009584,49,0.5975609756097561,1,0,Introduction: Network
130,"In our final model two additional operations are used with Eq. 5 , which is shown in Eq .",Introduction,Network,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",117,0.785234899328859,129,0.4121405750798722,50,0.6097560975609756,1,0,Introduction: Network
131,"6 . Half ( f ) denotes the first half of the elements off , and Dr ( h ) is the dropout operation which randomly sets an element of h to zero with a certain probability .",Introduction,Network,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",118,0.7919463087248322,130,0.4153354632587859,51,0.6219512195121951,1,0,Introduction: Network
132,The use of Half ( ) is to reduce the parameter size and does not affect the performance .,Introduction,Network,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",119,0.7986577181208053,131,0.4185303514376997,52,0.6341463414634146,1,0,Introduction: Network
136,A similar idea was also used in .,Introduction,Network,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",123,0.825503355704698,135,0.4313099041533546,56,0.6829268292682927,1,0,Introduction: Network
138,We call this type of encoder interleaved bidirectional encoder .,Introduction,Network,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",125,0.8389261744966443,137,0.4376996805111821,58,0.7073170731707317,1,0,Introduction: Network
139,"In addition , there are two similar columns ( a 1 and a 2 ) in the encoder part .",Introduction,Network,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",126,0.8456375838926175,138,0.4408945686900958,59,0.7195121951219512,1,0,Introduction: Network
141,There is no connection between the two columns .,Introduction,Network,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",128,0.8590604026845637,140,0.4472843450479233,61,0.7439024390243902,1,0,Introduction: Network
143,"At the last LSTM layers , there are two groups of vectors representing the source sequence .",Introduction,Network,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",130,0.87248322147651,142,0.4536741214057508,63,0.7682926829268293,1,0,Introduction: Network
144,The group size is the same as the length of the input sequence .,Introduction,Network,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",131,0.8791946308724832,143,0.4568690095846645,64,0.7804878048780488,1,0,Introduction: Network
145,Interface : Prior encoder - decoder models and attention models are different in their method of extracting the representations of the source sequences .,Introduction,Network,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",132,0.8859060402684564,144,0.4600638977635782,65,0.7926829268292683,1,0,Introduction: Network
146,"In our work , as a consequence of the introduced F - F connections , we have 4 output vectors ( h For Deep - Att , we do not need the above two operations .",Introduction,Network,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",133,0.8926174496644296,145,0.463258785942492,66,0.8048780487804879,1,0,Introduction: Network
148,e t is summarized as :,Introduction,Network,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O']",135,0.906040268456376,147,0.4696485623003195,68,0.8292682926829268,1,0,Introduction: Network
149,Note that the vector dimensionality off is four times larger than that of h ( see Eq. 4 ) .,Introduction,Network,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",136,0.912751677852349,148,0.4728434504792332,69,0.8414634146341463,1,0,Introduction: Network
150,ct is summarized as :,Introduction,Network,machine-translation,3,"['O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O']",137,0.9194630872483222,149,0.476038338658147,70,0.8536585365853658,1,0,Introduction: Network
151,?,Introduction,Network,machine-translation,3,['O'],['O'],138,0.9261744966442952,150,0.4792332268370607,71,0.8658536585365854,1,0,Introduction: Network
152,"t,t is the normalized attention weight computed by :",Introduction,Network,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",139,0.9328859060402684,151,0.4824281150159744,72,0.8780487804878049,1,0,Introduction: Network
153,"the concatenated vector e t to a vector with 1 / 4 dimension size , denoted by the ( fully connected ) block "" fc "" in .",Introduction,Network,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",140,0.9395973154362416,152,0.4856230031948881,73,0.8902439024390244,1,0,Introduction: Network
154,Decoder :,Introduction,Network,machine-translation,3,"['O', 'O']","['O', 'O']",141,0.9463087248322148,153,0.4888178913738019,74,0.902439024390244,1,0,Introduction: Network
155,The decoder follows Eq. 5 and Eq. 6 with fixed direction term ? 1 .,Introduction,Network,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",142,0.953020134228188,154,0.4920127795527156,75,0.9146341463414634,1,0,Introduction: Network
156,"At the first layer , we use the following x t :",Introduction,Network,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",143,0.959731543624161,155,0.4952076677316294,76,0.926829268292683,1,0,Introduction: Network
157,y t?1 is the target word embedding at the previous time step and y 0 is zero .,Introduction,Network,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",144,0.9664429530201344,156,0.4984025559105431,77,0.9390243902439024,1,0,Introduction: Network
160,"Note that at the last LSTM layer , we only use ht to make the prediction with a softmax layer .",Introduction,Network,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",147,0.9865771812080536,159,0.5079872204472844,80,0.975609756097561,1,0,Introduction: Network
161,"Although the network is deep , the training technique is straightforward .",Introduction,Network,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",148,0.9932885906040269,160,0.5111821086261981,81,0.9878048780487804,1,0,Introduction: Network
163,Training technique,,,machine-translation,3,"['O', 'O']","['O', 'O']",0,0.0,162,0.5175718849840255,0,0.0,1,0,
165,"Because of the deep bi-directional structure , we do not need to reverse the sequence order as .",Training technique,Training technique,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.1,164,0.5239616613418531,2,0.1666666666666666,1,0,Training technique
166,"The deep topology brings difficulties for the model training , especially when first order methods such as stochastic gradient descent ( SGD ) are used .",Training technique,Training technique,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.15,165,0.5271565495207667,3,0.25,1,0,Training technique
168,"We tried several optimization techniques such as AdaDelta ( Zeiler , 2012 ) , RMSProp ( Tieleman and and .",Training technique,Training technique,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.25,167,0.5335463258785943,5,0.4166666666666667,1,0,Training technique
170,"In this work , we chose Adam for model training and do not present a detailed comparison with other optimization methods .",Training technique,Training technique,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.35,169,0.5399361022364217,7,0.5833333333333334,1,0,Training technique
172,It is utilized on the LSTM nodes h kt ( See Eq. 5 ) with a ratio of pd for both the encoder and decoder .,Training technique,Training technique,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.45,171,0.5463258785942492,9,0.75,1,0,Training technique
175,"For such a deep and large network , it is not easy to determine the tuning strategy and this will be considered in future work .",Training technique,Training technique,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.6,174,0.5559105431309904,12,1.0,1,0,Training technique
176,Generation,Training technique,,machine-translation,3,['O'],['O'],13,0.65,175,0.5591054313099042,0,0.0,1,0,Training technique
178,"At each time step t , the wordy t can be predicted by :",Training technique,Generation,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.75,177,0.5654952076677316,2,0.2857142857142857,1,0,Training technique: Generation
179,where ?,Training technique,Generation,machine-translation,3,"['O', 'O']","['O', 'O']",16,0.8,178,0.5686900958466453,3,0.4285714285714285,1,0,Training technique: Generation
180,t is the predicted target word .?,Training technique,Generation,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.85,179,0.5718849840255591,4,0.5714285714285714,1,0,Training technique: Generation
181,0:t ? 1 is the generated sequence from time step 0 tot ?,Training technique,Generation,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.9,180,0.5750798722044729,5,0.7142857142857143,1,0,Training technique: Generation
183,"The hypotheses are ranked by the total likelihood of the generated sequence , although normalized likelihood is used in some works .",Training technique,Generation,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,1.0,182,0.5814696485623003,7,1.0,1,0,Training technique: Generation
184,Experiments,,,machine-translation,3,['O'],['O'],0,0.0,183,0.5846645367412141,0,0.0,1,0,
188,Data sets,Experiments,,machine-translation,3,"['O', 'O']","['O', 'O']",4,0.25,187,0.597444089456869,0,0.0,1,0,Experiments
194,"The news - test - 2012 and news - test - 2013 are concatenated as our development set , and the news - test - 2014 is the test set .",Experiments,Data sets,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.625,193,0.6166134185303515,6,0.5,1,0,Experiments: Data sets
197,For the target language we select the most frequent 80 K French words and the most frequent 160K German words as the output vocabulary .,Experiments,Data sets,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.8125,196,0.6261980830670927,9,0.75,1,0,Experiments: Data sets
198,"The full vocabulary of the German corpus is larger , so we select more German words to build the target vocabulary .",Experiments,Data sets,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.875,197,0.6293929712460063,10,0.8333333333333334,1,0,Experiments: Data sets
199,Out - of - vocabulary words are replaced with the unknown symbol unk .,Experiments,Data sets,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.9375,198,0.6325878594249201,11,0.9166666666666666,1,0,Experiments: Data sets
201,Model settings,,,machine-translation,3,"['O', 'O']","['O', 'O']",0,0.0,200,0.6389776357827476,0,0.0,1,0,
203,Both models have exactly the same configuration and layer size except the interface part P - I.,Model settings,Model settings,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.064516129032258,202,0.645367412140575,2,0.2,1,0,Model settings
206,The output layer size is the same as the size of the target vocabulary .,Model settings,Model settings,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.1612903225806451,205,0.6549520766773163,5,0.5,1,0,Model settings
208,"For each LSTM layer , the activation functions for gates , inputs and outputs are sigmoid , tanh , and tanh respectively .",Model settings,Model settings,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.2258064516129032,207,0.6613418530351438,7,0.7,1,0,Model settings
210,"Note that in previous work , 1000 dimensional word embeddings and 1000 dimensional LSTM layers are used .",Model settings,Model settings,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.2903225806451613,209,0.6677316293929713,9,0.9,1,0,Model settings
212,Optimization,Model settings,,machine-translation,3,['O'],['O'],11,0.3548387096774194,211,0.6741214057507987,0,0.0,1,0,Model settings
213,"Note that each LSTM layer includes two parts as described in Eq. 3 , feed - forward computation and recurrent computation .",Model settings,Optimization,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.3870967741935484,212,0.6773162939297125,1,0.05,1,0,Model settings: Optimization
214,"Since there are non-linear activations in the recurrent computation , a larger learning rate l r = 5 10 ? 4 is used , while for the feed - forward computation a smaller learning rate l f = 4 10 ? 5 is used .",Model settings,Optimization,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.4193548387096774,213,0.6805111821086262,2,0.1,1,0,Model settings: Optimization
215,Word embeddings and the softmax layer also use this learning rate l f .,Model settings,Optimization,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.4516129032258064,214,0.6837060702875399,3,0.15,1,0,Model settings: Optimization
216,We refer all the parameters not used for recurrent computation as non-recurrent part of the model .,Model settings,Optimization,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.4838709677419355,215,0.6869009584664537,4,0.2,1,0,Model settings: Optimization
218,"Here r is the regularization strength , l is the corresponding learning rate , g stands for the gradients of v.",Model settings,Optimization,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.5483870967741935,217,0.6932907348242812,6,0.3,1,0,Model settings: Optimization
220,All the other layers have the same r = 2 .,Model settings,Optimization,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.6129032258064516,219,0.6996805111821086,8,0.4,1,0,Model settings: Optimization
225,"In each batch , there are 500 ? 800 sequences in our work .",Model settings,Optimization,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.7741935483870968,224,0.7156549520766773,13,0.65,1,0,Model settings: Optimization
226,The exact number depends on the sequence lengths and model size .,Model settings,Optimization,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.8064516129032258,225,0.7188498402555911,14,0.7,1,0,Model settings: Optimization
228,"However , the largest batch size is constrained by the GPU memory .",Model settings,Optimization,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.8709677419354839,227,0.7252396166134185,16,0.8,1,0,Model settings: Optimization
231,One thing we want to emphasize here is that our deep model is not sensitive to these settings .,Model settings,Optimization,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",30,0.967741935483871,230,0.7348242811501597,19,0.95,1,0,Model settings: Optimization
233,Results,,,machine-translation,3,['O'],['O'],0,0.0,232,0.7412140575079872,0,0.0,1,0,
237,Single models,,,machine-translation,3,"['O', 'O']","['O', 'O']",0,0.0,236,0.7539936102236422,0,0.0,1,0,
249,Note that RNNsearch - LV uses a large output vocabulary of 500K words based on the standard attention model RNNsearch .,Single models,Single models,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.8571428571428571,248,0.792332268370607,12,0.8571428571428571,1,0,Single models
252,Methods,,,machine-translation,3,['O'],['O'],0,0.0,251,0.8019169329073482,0,0.0,1,0,
260,It is shown in Tab .,Methods,Post processing,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O']",8,0.1666666666666666,259,0.8274760383386581,5,0.3125,1,0,Methods: Post processing
272,Analysis,Methods,,machine-translation,3,['O'],['O'],20,0.4166666666666667,271,0.865814696485623,0,0.0,1,0,Methods
273,Length,Methods,,machine-translation,3,['O'],['O'],21,0.4375,272,0.8690095846645367,0,0.0,1,0,Methods
279,Unknown words,Methods,,machine-translation,3,"['O', 'O']","['O', 'O']",27,0.5625,278,0.8881789137380192,0,0.0,1,0,Methods
286,This suggests that the difficulty on this subset is not much different from that on the full set .,Methods,Unknown words,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",34,0.7083333333333334,285,0.9105431309904152,7,0.7,1,0,Methods: Unknown words
287,We therefore attribute the larger gap for Deep - att to the existence of unknown words .,Methods,Unknown words,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",35,0.7291666666666666,286,0.9137380191693292,8,0.8,1,0,Methods: Unknown words
288,We also compute the BLEU score on the subset of the ensemble model and obtain 41.4 .,Methods,Unknown words,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",36,0.75,287,0.9169329073482428,9,0.9,1,0,Methods: Unknown words
289,"As a reference related to human performance , in , it has been tested that the BLEU score of oracle re-scoring the LIUM 1000 - best results is 45 .",Methods,Unknown words,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",37,0.7708333333333334,288,0.9201277955271564,10,1.0,1,0,Methods: Unknown words
290,Over-fitting,Methods,,machine-translation,3,['O'],['O'],38,0.7916666666666666,289,0.9233226837060704,0,0.0,1,0,Methods
291,"Deep models have more parameters , and thus have a stronger ability to fit the large data set .",Methods,Over-fitting,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",39,0.8125,290,0.926517571884984,1,0.1,1,0,Methods: Over-fitting
296,The curve with circle marks corresponds ton e = 5 and n d = 3 .,Methods,Over-fitting,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",44,0.9166666666666666,295,0.9424920127795527,6,0.6,1,0,Methods: Over-fitting
297,The curve with triangle marks corresponds ton e = 1 and n d = 1 .,Methods,Over-fitting,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",45,0.9375,296,0.9456869009584664,7,0.7,1,0,Methods: Over-fitting
300,"We only plot the early training stage curves because , during the late training stage , the curves are not smooth .",Methods,Over-fitting,machine-translation,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",48,1.0,299,0.9552715654952076,10,1.0,1,0,Methods: Over-fitting
301,Conclusion,,,machine-translation,3,['O'],['O'],0,0.0,300,0.9584664536741214,0,0.0,1,0,
2,Unsupervised Neural Machine Translation with Weight Sharing,title,,machine-translation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0,1,0.00418410041841,1,0.0,1,0,title
3,abstract,,,machine-translation,4,['O'],['O'],0,0.0,2,0.00836820083682,0,0.0,1,0,
4,Unsupervised neural machine translation ( NMT ) is a recently proposed approach for machine translation which aims to train the model without using any labeled data .,abstract,abstract,machine-translation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.2,3,0.0125523012552301,1,0.2,1,0,abstract
5,"The models proposed for unsupervised NMT often use only one shared encoder to map the pairs of sentences from different languages to a shared - latent space , which is weak in keeping the unique and internal characteristics of each language , such as the style , terminology , and sentence structure .",abstract,abstract,machine-translation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.4,4,0.0167364016736401,2,0.4,1,0,abstract
11,The NMT typically consists of two sub neural networks .,Introduction,Introduction,machine-translation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.0571428571428571,10,0.0418410041841004,2,0.0571428571428571,1,0,Introduction
13,NMT can be studied in supervised and unsupervised learning settings .,Introduction,Introduction,machine-translation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.1142857142857142,12,0.0502092050209205,4,0.1142857142857142,1,0,Introduction
14,"In the supervised setting , bilingual corpora is available for training the NMT model .",Introduction,Introduction,machine-translation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.1428571428571428,13,0.0543933054393305,5,0.1428571428571428,1,0,Introduction
15,"In the unsupervised setting , we only have two independent monolingual corpora with one for each language and there is no bilingual training example to provide alignment information for the two languages .",Introduction,Introduction,machine-translation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.1714285714285714,14,0.0585774058577405,6,0.1714285714285714,1,0,Introduction
16,"Due to lack of alignment information , the unsupervised NMT is considered more challenging .",Introduction,Introduction,machine-translation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.2,15,0.0627615062761506,7,0.2,1,0,Introduction
17,"However , this task is very promising , since the monolingual corpora is usually easy to be collected .",Introduction,Introduction,machine-translation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.2285714285714285,16,0.0669456066945606,8,0.2285714285714285,1,0,Introduction
18,"Motivated by recent success in unsupervised cross - lingual embeddings , the models proposed for unsupervised NMT often assume that a pair of sentences from two different languages can be mapped to a same latent representation in a shared - latent space .",Introduction,Introduction,machine-translation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.2571428571428571,17,0.0711297071129707,9,0.2571428571428571,1,0,Introduction
19,"Following this assumption , use a single encoder and a single decoder for both the source and target languages .",Introduction,Introduction,machine-translation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.2857142857142857,18,0.0753138075313807,10,0.2857142857142857,1,0,Introduction
22,"With some good performance , they share a glaring defect , i.e. , only one encoder is shared by the source and target languages .",Introduction,Introduction,machine-translation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.3714285714285714,21,0.0878661087866108,13,0.3714285714285714,1,0,Introduction
23,"Although the shared encoder is vital for mapping sentences from different languages into the shared - latent space , it is weak in keeping the uniqueness and internal characteristics of each language , such as the style , terminology and sentence structure .",Introduction,Introduction,machine-translation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.4,22,0.0920502092050209,14,0.4,1,0,Introduction
24,"Since each language has its own characteristics , the source and target languages should be encoded and learned independently .",Introduction,Introduction,machine-translation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.4285714285714285,23,0.0962343096234309,15,0.4285714285714285,1,0,Introduction
25,"Therefore , we conjecture that the shared encoder maybe a factor limit - ing the potential translation performance .",Introduction,Introduction,machine-translation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.4571428571428571,24,0.100418410041841,16,0.4571428571428571,1,0,Introduction
45,Related Work,,,machine-translation,4,"['O', 'O']","['O', 'O']",0,0.0,44,0.1841004184100418,0,0.0,1,0,
58,Model Architecture,,,machine-translation,4,"['O', 'O']","['O', 'O']",0,0.0,57,0.2384937238493724,0,0.0,1,0,
63,2 .,Model Architecture,Model Architecture,machine-translation,4,"['O', 'O']","['O', 'O']",5,0.1282051282051282,62,0.2594142259414226,5,0.3846153846153846,1,0,Model Architecture
67,"For more details about the multi-head self - attention layer , we refer the reader to .",Model Architecture,Model Architecture,machine-translation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.2307692307692307,66,0.2761506276150627,9,0.6923076923076923,1,0,Model Architecture
69,Several ways exist to interpret the roles of the sub networks are summarised in table,Model Architecture,Model Architecture,machine-translation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.282051282051282,68,0.2845188284518828,11,0.8461538461538461,1,0,Model Architecture
70,1 .,Model Architecture,Model Architecture,machine-translation,4,"['O', 'O']","['O', 'O']",12,0.3076923076923077,69,0.2887029288702928,12,0.9230769230769232,1,0,Model Architecture
72,Networks,Model Architecture,,machine-translation,4,['O'],['O'],14,0.358974358974359,71,0.2970711297071129,0,0.0,1,0,Model Architecture
75,"Compared to recurrent neural network , a disadvantage of the simple self - attention mechanism is that the temporal order information is lost .",Model Architecture,Networks,machine-translation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.4358974358974359,74,0.309623430962343,3,0.12,1,0,Model Architecture: Networks
76,"Although the Transformer applies the positional encoding to the sequence before processed by the self - attention , how to model temporal order information within an attention is still an open question .",Model Architecture,Networks,machine-translation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.4615384615384615,75,0.3138075313807531,4,0.16,1,0,Model Architecture: Networks
78,"More concretely , two positional masks , namely the forward mask M f and backward mask Mb , are calculated as :",Model Architecture,Networks,machine-translation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.5128205128205128,77,0.3221757322175732,6,0.24,1,0,Model Architecture: Networks
79,"With the forward mask M f , the later token only makes attention connections to the early tokens in the sequence , and vice versa with the backward mask .",Model Architecture,Networks,machine-translation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.5384615384615384,78,0.3263598326359833,7,0.28,1,0,Model Architecture: Networks
82,Weight sharing,Model Architecture,,machine-translation,4,"['O', 'O']","['O', 'O']",24,0.6153846153846154,81,0.3389121338912134,10,0.4,1,0,Model Architecture
85,"Similarly , we also share the first few layers of the Dec sand Dec t , which are expected to decode high - level representations that are vital for reconstructing the input sentences .",Model Architecture,Weight sharing,machine-translation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.6923076923076923,84,0.3514644351464435,13,0.52,1,0,Model Architecture: Weight sharing
89,Embedding reinforced encoder,Model Architecture,Weight sharing,machine-translation,4,"['O', 'O', 'O']","['O', 'O', 'O']",31,0.7948717948717948,88,0.3682008368200837,17,0.68,1,0,Model Architecture: Weight sharing
92,"Formally , given the input sequence embedding vectors E = {e 1 , . . . , e t } and the initial output sequence of the encoder stack H = {h 1 , . . . , ht } , we compute H r as :",Model Architecture,Weight sharing,machine-translation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",34,0.8717948717948718,91,0.3807531380753138,20,0.8,1,0,Model Architecture: Weight sharing
93,"where H r is the final output sequence of the encoder which will be attended by the decoder ( In Transformer , H is the final output of the encoder ) , g is agate unit and computed as :",Model Architecture,Weight sharing,machine-translation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",35,0.8974358974358975,92,0.3849372384937238,21,0.84,1,0,Model Architecture: Weight sharing
94,"where W 1 , W 2 and bare trainable parameters and they are shared by the two encoders .",Model Architecture,Weight sharing,machine-translation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",36,0.9230769230769232,93,0.3891213389121339,22,0.88,1,0,Model Architecture: Weight sharing
102,"In this form , each encoder should learn to compose the embeddings of its corresponding language and each decoder is expected to learn to decompose this representation into its corresponding language .",Unsupervised Training,Unsupervised Training,machine-translation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.0930232558139534,101,0.4225941422594142,4,0.0930232558139534,1,0,Unsupervised Training
106,"Specifically , we apply a random permutation ?",Unsupervised Training,Unsupervised Training,machine-translation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.1860465116279069,105,0.4393305439330544,8,0.1860465116279069,1,0,Unsupervised Training
108,"where n is the length of the input sentence , steps is the global steps the model has been updated , k and s are the tunable parameters which can beset by users beforehand .",Unsupervised Training,Unsupervised Training,machine-translation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.2325581395348837,107,0.4476987447698745,10,0.2325581395348837,1,0,Unsupervised Training
109,"This way , the system needs to learn some useful structure of the involved languages to be able to recover the correct word order .",Unsupervised Training,Unsupervised Training,machine-translation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.2558139534883721,108,0.4518828451882845,11,0.2558139534883721,1,0,Unsupervised Training
110,"In practice , we set k = 2 and s = 100000 .",Unsupervised Training,Unsupervised Training,machine-translation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.2790697674418604,109,0.4560669456066946,12,0.2790697674418604,1,0,Unsupervised Training
111,Back - translation,Unsupervised Training,Unsupervised Training,machine-translation,4,"['O', 'O', 'O']","['O', 'O', 'O']",13,0.3023255813953488,110,0.4602510460251046,13,0.3023255813953488,1,0,Unsupervised Training
112,"In spite of denoising autoencoding , the training procedure still involves a single language at each time , without considering our final goal of mapping an input sentence from the source / target language to the target / source language .",Unsupervised Training,Unsupervised Training,machine-translation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.3255813953488372,111,0.4644351464435146,14,0.3255813953488372,1,0,Unsupervised Training
114,Back - translation has shown its great effectiveness on improving NMT model with monolingual data and has been widely investigated by .,Unsupervised Training,Unsupervised Training,machine-translation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.3720930232558139,113,0.4728033472803347,16,0.3720930232558139,1,0,Unsupervised Training
117,Local GAN,Unsupervised Training,,machine-translation,4,"['O', 'O']","['O', 'O']",19,0.4418604651162791,116,0.4853556485355648,19,0.4418604651162791,1,0,Unsupervised Training
118,"Although the weight sharing constraint is vital for the shared - latent space assumption , it alone does not guarantee that the corresponding sentences in two languages will have the same or similar latent code .",Unsupervised Training,Local GAN,machine-translation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.4651162790697674,117,0.4895397489539749,20,0.4651162790697674,1,0,Unsupervised Training: Local GAN
120,"The local discriminator , implemented as a multilayer perceptron with two hidden layers of size 256 , takes the output of the encoder , i.e. , H r calculated as equation 3 , as input , and produces a binary prediction about the language of the input sentence .",Unsupervised Training,Local GAN,machine-translation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.5116279069767442,119,0.497907949790795,22,0.5116279069767442,1,0,Unsupervised Training: Local GAN
121,The local discriminator is trained to predict the language by minimizing the following crossentropy loss :,Unsupervised Training,Local GAN,machine-translation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",23,0.5348837209302325,120,0.502092050209205,23,0.5348837209302325,1,0,Unsupervised Training: Local GAN
122,where ?,Unsupervised Training,Local GAN,machine-translation,4,"['O', 'O']","['O', 'O']",24,0.5581395348837209,121,0.5062761506276151,24,0.5581395348837209,1,0,Unsupervised Training: Local GAN
123,"D l represents the parameters of the local discriminator and f ? {s , t}.",Unsupervised Training,Local GAN,machine-translation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.5813953488372093,122,0.5104602510460251,25,0.5813953488372093,1,0,Unsupervised Training: Local GAN
125,where ?,Unsupervised Training,Local GAN,machine-translation,4,"['O', 'O']","['O', 'O']",27,0.627906976744186,124,0.5188284518828452,27,0.627906976744186,1,0,Unsupervised Training: Local GAN
127,Enct are the parameters of the two encoders .,Unsupervised Training,Local GAN,machine-translation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",29,0.6744186046511628,126,0.5271966527196653,29,0.6744186046511628,1,0,Unsupervised Training: Local GAN
128,Global GAN,Unsupervised Training,,machine-translation,4,"['O', 'O']","['O', 'O']",30,0.6976744186046512,127,0.5313807531380753,30,0.6976744186046512,1,0,Unsupervised Training
132,"In GAN g 1 , the Enc t and Dec s act as the generator , which generates the sentencex t 4 from x t .",Unsupervised Training,Global GAN,machine-translation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",34,0.7906976744186046,131,0.5481171548117155,34,0.7906976744186046,1,0,Unsupervised Training: Global GAN
133,"The D g 1 , implemented based on CNN , assesses whether the generated sentencex t is the true target - language sentence or the generated sentence .",Unsupervised Training,Global GAN,machine-translation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",35,0.813953488372093,132,0.5523012552301255,35,0.813953488372093,1,0,Unsupervised Training: Global GAN
135,"During training , the D g 1 feeds back its assessment to finetune the encoder Enc t and decoder Dec s .",Unsupervised Training,Global GAN,machine-translation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",37,0.8604651162790697,134,0.5606694560669456,37,0.8604651162790697,1,0,Unsupervised Training: Global GAN
137,We apply a similar processing to GAN g2 ( The details about the architecture of the global discriminator and the training procedure of the global GANs can be seen in appendix B and C ) .,Unsupervised Training,Global GAN,machine-translation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",39,0.9069767441860463,136,0.5690376569037657,39,0.9069767441860463,1,0,Unsupervised Training: Global GAN
142,Experiments and Results,,,machine-translation,4,"['O', 'O', 'O']","['O', 'O', 'O']",0,0.0,141,0.5899581589958159,0,0.0,1,0,
146,Data Sets and Preprocessing,Experiments and Results,,machine-translation,4,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",4,0.1142857142857142,145,0.606694560669456,0,0.0,1,0,Experiments and Results
149,"WMT14 English - French Similar to , we use the full training set of 36M sentence pairs and we lower - case them and remove sentences longer than 50 words , resulting in a parallel corpus of about 30M pairs of sentences .",Experiments and Results,Data Sets and Preprocessing,machine-translation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.2,148,0.6192468619246861,3,0.0967741935483871,1,0,Experiments and Results: Data Sets and Preprocessing
150,"To guarantee no exact correspondence between the source and target monolingual sets , we build monolingual corpora by selecting English sentences from 15M random pairs , and selecting the French sentences from the complementary set .",Experiments and Results,Data Sets and Preprocessing,machine-translation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.2285714285714285,149,0.6234309623430963,4,0.1290322580645161,1,0,Experiments and Results: Data Sets and Preprocessing
153,WMT16 English - German,Experiments and Results,Data Sets and Preprocessing,machine-translation,4,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",11,0.3142857142857143,152,0.6359832635983264,7,0.2258064516129032,1,0,Experiments and Results: Data Sets and Preprocessing
154,"We follow the same procedure mentioned above to create monolingual training corpora for English - German translation , and we get two monolingual training data of 1.8 M sentences each .",Experiments and Results,Data Sets and Preprocessing,machine-translation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.3428571428571428,153,0.6401673640167364,8,0.2580645161290322,1,0,Experiments and Results: Data Sets and Preprocessing
158,6 .,Experiments and Results,Data Sets and Preprocessing,machine-translation,4,"['O', 'O']","['O', 'O']",16,0.4571428571428571,157,0.6569037656903766,12,0.3870967741935484,1,0,Experiments and Results: Data Sets and Preprocessing
159,"Since the data set is not big enough , we just build the monolingual data set by randomly shuffling the Chinese and English sentences respectively .",Experiments and Results,Data Sets and Preprocessing,machine-translation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.4857142857142857,158,0.6610878661087866,13,0.4193548387096774,1,0,Experiments and Results: Data Sets and Preprocessing
160,"In spite of the fact that some correspondence between examples in these two monolingual sets may exist , we never utilize this alignment information in our training procedure ( see Section 3.2 ) .",Experiments and Results,Data Sets and Preprocessing,machine-translation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.5142857142857142,159,0.6652719665271967,14,0.4516129032258064,1,0,Experiments and Results: Data Sets and Preprocessing
162,"We get an English vocabulary of about 34000 tokens , and Chinese vocabulary of about 38000 tokens .",Experiments and Results,Data Sets and Preprocessing,machine-translation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.5714285714285714,161,0.6736401673640168,16,0.5161290322580645,1,0,Experiments and Results: Data Sets and Preprocessing
163,The results are reported on N IST 02 .,Experiments and Results,Data Sets and Preprocessing,machine-translation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.6,162,0.6778242677824268,17,0.5483870967741935,1,0,Experiments and Results: Data Sets and Preprocessing
178,Baseline Systems,,,machine-translation,4,"['O', 'O']","['O', 'O']",0,0.0,177,0.7405857740585774,0,0.0,1,0,
181,Lample et al .,Baseline Systems,Baseline Systems,machine-translation,4,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",3,0.3333333333333333,180,0.7531380753138075,3,0.25,1,0,Baseline Systems
182,The second baseline is a previous work that uses the same training and testing sets with this paper .,Baseline Systems,Baseline Systems,machine-translation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.4444444444444444,181,0.7573221757322176,4,0.3333333333333333,1,0,Baseline Systems
183,"Their model belongs to the standard attention - based encoder - decoder framework , which implements the encoder using a bidirectional long short term memory network ( LSTM ) and implements the decoder using a sim - 8 The configuration we used to run these open - source toolkits can be found in appendix D 9 https://github.com/mosessmt/mosesdecoder/blob/617e8c8/scripts/generic/multibleu.perl;mteval-v11b.pl",Baseline Systems,Baseline Systems,machine-translation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.5555555555555556,182,0.7615062761506276,5,0.4166666666666667,1,0,Baseline Systems
184,en - de de - en en - fr fr- en zh - en are copied directly from their paper .,Baseline Systems,Baseline Systems,machine-translation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.6666666666666666,183,0.7656903765690377,6,0.5,1,0,Baseline Systems
186,ple forward LSTM .,Baseline Systems,Baseline Systems,machine-translation,4,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",8,0.8888888888888888,185,0.7740585774058577,8,0.6666666666666666,1,0,Baseline Systems
188,Supervised training,,,machine-translation,4,"['O', 'O']","['O', 'O']",0,0.0,187,0.7824267782426778,10,0.8333333333333334,1,0,
190,This model can be viewed as an upper bound for the proposed unsupervised model .,Supervised training,Supervised training,machine-translation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,1.0,189,0.7907949790794979,12,1.0,1,0,Supervised training
191,Results and Analysis,,,machine-translation,4,"['O', 'O', 'O']","['O', 'O', 'O']",0,0.0,190,0.7949790794979079,0,0.0,1,0,
192,Number of weight - sharing layers,Results and Analysis,Results and Analysis,machine-translation,4,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O']",1,0.0454545454545454,191,0.799163179916318,1,0.0454545454545454,1,0,Results and Analysis
195,"Sharing one layer in AEs means sharing one layer for the encoders and in the meanwhile , sharing one layer for the decoders .",Results and Analysis,Results and Analysis,machine-translation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.1818181818181818,194,0.8117154811715481,4,0.1818181818181818,1,0,Results and Analysis
205,"We explain this as that the more distant the language pair is , the more different characteristics they have .",Results and Analysis,Results and Analysis,machine-translation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.6363636363636364,204,0.8535564853556485,14,0.6363636363636364,1,0,Results and Analysis
206,And the shared encoder is weak in keeping the unique characteristic of each language .,Results and Analysis,Results and Analysis,machine-translation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.6818181818181818,205,0.8577405857740585,15,0.6818181818181818,1,0,Results and Analysis
214,Translation results,,,machine-translation,4,"['O', 'O']","['O', 'O']",0,0.0,213,0.891213389121339,0,0.0,1,0,
215,Ablation study,,,machine-translation,4,"['O', 'O']","['O', 'O']",0,0.0,214,0.895397489539749,0,0.0,1,0,
217,Results are reported in table 3 .,Ablation study,Ablation study,machine-translation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.1818181818181818,216,0.9037656903765692,2,0.1818181818181818,1,0,Ablation study
218,"We do not test the the importance of the auto - encoding , back - translation and the pre-trained embeddings because they have been widely tested in .",Ablation study,Ablation study,machine-translation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.2727272727272727,217,0.9079497907949792,3,0.2727272727272727,1,0,Ablation study
219,shows that the best performance is obtained with the simultaneous use of all the tested elements .,Ablation study,Ablation study,machine-translation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.3636363636363636,218,0.9121338912133892,4,0.3636363636363636,1,0,Ablation study
223,This indicates that it deserves more efforts to investigate the temporal order information in self - attention mechanism .,Ablation study,Ablation study,machine-translation,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.7272727272727273,222,0.9288702928870292,8,0.7272727272727273,1,0,Ablation study
227,Conclusion and Future work,,,machine-translation,4,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",0,0.0,226,0.9456066945606696,0,0.0,1,0,
3,abstract,,,machine-translation,5,['O'],['O'],0,0.0,2,0.0139860139860139,0,0.0,1,0,
10,"Since 2016 when NMT systems first showed to achieve significantly better results than statistical machine translation ( SMT ) systems , the dominant neural network ( NN ) architectures for NMT have changed on a yearly ( and even more frequent ) basis .",Introduction,Introduction,machine-translation,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.1333333333333333,9,0.0629370629370629,2,0.1333333333333333,1,0,Introduction
11,The state - of - the - art in 2016 were shallow attention - based recurrent neural networks ( RNN ) with gated recurrent units ( GRU ) in recurrent layers .,Introduction,Introduction,machine-translation,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.2,10,0.0699300699300699,3,0.2,1,0,Introduction
12,"In 2017 , multiplicative long short - term memory ( MLSTM ) units and deep GRU models were introduced in NMT .",Introduction,Introduction,machine-translation,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.2666666666666666,11,0.0769230769230769,4,0.2666666666666666,1,0,Introduction
13,"The same year , selfattentional ( Transformer ) models were introduced .",Introduction,Introduction,machine-translation,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.3333333333333333,12,0.0839160839160839,5,0.3333333333333333,1,0,Introduction
14,"Consequently , in 2018 , most of the top scoring systems in the shared task on news translation of the Third Conference on Machine Translation ( WMT ) were trained using Transformer models",Introduction,Introduction,machine-translation,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.4,13,0.0909090909090909,6,0.4,1,0,Introduction
15,1 .,Introduction,Introduction,machine-translation,5,"['O', 'O']","['O', 'O']",7,0.4666666666666667,14,0.0979020979020979,7,0.4666666666666667,1,0,Introduction
16,"However , it is already evident that the state - of - the - art architectures will 1 All 14 of the best automatically scored systems according to the information provided by participants in the official submission portal http://matrix.statmt.org were indicated as being based on Transformer models .",Introduction,Introduction,machine-translation,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.5333333333333333,15,0.1048951048951049,8,0.5333333333333333,1,0,Introduction
18,"For instance , have recently proposed RNMT + models that combine deep LSTM - based models with multi-head attention and showed that the models outperform Transformer models .",Introduction,Introduction,machine-translation,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.6666666666666666,17,0.1188811188811188,10,0.6666666666666666,1,0,Introduction
19,"In WMT 2017 , Tilde participated with MLSTM - based NMT systems .",Introduction,Introduction,machine-translation,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.7333333333333333,18,0.1258741258741259,11,0.7333333333333333,1,0,Introduction
21,"Therefore , for WMT 2018 , Tilde submitted NMT systems that were trained using Transformer models .",Introduction,Introduction,machine-translation,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.8666666666666667,20,0.1398601398601398,13,0.8666666666666667,1,0,Introduction
22,The paper is further structured as follows :,Introduction,Introduction,machine-translation,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.9333333333333332,21,0.1468531468531468,14,0.9333333333333332,1,0,Introduction
27,Constrained English - Estonian and Estonian - English NMT systems ( tilde - c-nmt ) that were deployed as ensembles of averaged factored data ( see Section 3 ) Transformer models .,System Overview,System Overview,machine-translation,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.0410958904109589,26,0.1818181818181818,3,0.3333333333333333,1,0,System Overview
29,Unconstrained English - Estonian and Estonian - English NMT systems ( tilde - nc - nmt ) that were deployed as averaged Transformer models .,System Overview,System Overview,machine-translation,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.0684931506849315,28,0.1958041958041958,5,0.5555555555555556,1,0,System Overview
30,"These models were also trained using back - translated data similarly to the constrained systems , however , the data , taking into account their relatively large size , were not factored .",System Overview,System Overview,machine-translation,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.0821917808219178,29,0.2027972027972027,6,0.6666666666666666,1,0,System Overview
34,Data,System Overview,,machine-translation,5,['O'],['O'],10,0.136986301369863,33,0.2307692307692307,0,0.0,1,0,System Overview
49,Data Filtering,System Overview,,machine-translation,5,"['O', 'O']","['O', 'O']",25,0.3424657534246575,48,0.3356643356643357,0,0.0,1,0,System Overview
50,"As NMT systems are sensitive to noise in parallel data , all parallel data were filtered using the parallel data filtering methods described by .",System Overview,Data Filtering,machine-translation,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.3561643835616438,49,0.3426573426573426,1,0.0714285714285714,1,0,System Overview: Data Filtering
54,The dictionaries were filtered using the transliteration - based probabilistic dictionary filtering method by .,System Overview,Data Filtering,machine-translation,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",30,0.410958904109589,53,0.3706293706293706,5,0.3571428571428571,1,0,System Overview: Data Filtering
57,English ParaCrawl corpus,System Overview,Data Filtering,machine-translation,5,"['O', 'O', 'O']","['O', 'O', 'O']",33,0.4520547945205479,56,0.3916083916083916,8,0.5714285714285714,1,0,System Overview: Data Filtering
58,3 .,System Overview,Data Filtering,machine-translation,5,"['O', 'O']","['O', 'O']",34,0.4657534246575342,57,0.3986013986013986,9,0.6428571428571429,1,0,System Overview: Data Filtering
61,"As a result , only 0.17 million sentence pairs from the ParaCrawl corpus were used for training of the constrained systems .",System Overview,Data Filtering,machine-translation,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",37,0.5068493150684932,60,0.4195804195804196,12,0.8571428571428571,1,0,System Overview: Data Filtering
62,"Due to the quality concerns , the corpus was not used for training of the unconstrained systems .",System Overview,Data Filtering,machine-translation,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",38,0.5205479452054794,61,0.4265734265734265,13,0.9285714285714286,1,0,System Overview: Data Filtering
63,The corpora statistics before and after filtering are provided in .,System Overview,Data Filtering,machine-translation,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",39,0.5342465753424658,62,0.4335664335664335,14,1.0,1,0,System Overview: Data Filtering
64,Data Pre-processing,System Overview,,machine-translation,5,"['O', 'O']","['O', 'O']",40,0.547945205479452,63,0.4405594405594406,0,0.0,1,0,System Overview
75,"Finally , data for the constrained systems are factored using an averaged perceptron - based morpho-syntactic tagger for Estonian and the lexicalized probabilistic parser , we introduce also a factor indicating a word part 's position in a word ( beginning , middle , end , or the word part represents the whole word - B , I , E , or O ) .",System Overview,Data Pre-processing,machine-translation,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",51,0.6986301369863014,74,0.5174825174825175,11,0.8461538461538461,1,0,System Overview: Data Pre-processing
76,"As a result , the Estonian data consist of the the following factors : word part , position , lemma , and morpho-syntactic tag .",System Overview,Data Pre-processing,machine-translation,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",52,0.7123287671232876,75,0.5244755244755245,12,0.9230769230769232,1,0,System Overview: Data Pre-processing
77,"The English data consist of the following factors : word part , position , lemma , part - of - speech tag , and syntactic function .",System Overview,Data Pre-processing,machine-translation,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",53,0.726027397260274,76,0.5314685314685315,13,1.0,1,0,System Overview: Data Pre-processing
78,Synthetic Data,System Overview,,machine-translation,5,"['O', 'O']","['O', 'O']",54,0.7397260273972602,77,0.5384615384615384,0,0.0,1,0,System Overview
80,"1 ) back - translated data , and 2 ) data infused with unknown token identifiers .",System Overview,Synthetic Data,machine-translation,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",56,0.7671232876712328,79,0.5524475524475524,2,0.2,1,0,System Overview: Synthetic Data
81,"The back - translated data allow performing domain adaptation and the second type of synthetic data allow training NMT models that are robust to unknown phenomena ( e.g. , code - mixed content , target language words in the source text , rare or unseen words , etc . ) .",System Overview,Synthetic Data,machine-translation,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",57,0.7808219178082192,80,0.5594405594405595,3,0.3,1,0,System Overview: Synthetic Data
82,"To create the synthetic corpora with unknown phenomena , we extracted fast align the parallel corpora and randomly replaced one to three unambiguously ( one - to - one ) aligned content words with unknown word identifiers .",System Overview,Synthetic Data,machine-translation,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",58,0.7945205479452054,81,0.5664335664335665,4,0.4,1,0,System Overview: Synthetic Data
83,"These synthetic corpora were added to the parallel corpora , thereby almost doubling the sizes of the available training data .",System Overview,Synthetic Data,machine-translation,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",59,0.8082191780821918,82,0.5734265734265734,5,0.5,1,0,System Overview: Synthetic Data
85,"1 ) the constrained system data were acquired from initial Transformer - based NMT systems that were trained on the filtered and preprocessed parallel data , which were supplemented with the unknown phenomena infused data , and 2 ) the unconstrained system data were acquired from pre-existing unconstrained MLSTM - based NMT systems - the NMT systems that were developed by Tilde for the Estonian EU Council Presidency in 2017 .",System Overview,Synthetic Data,machine-translation,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",61,0.8356164383561644,84,0.5874125874125874,7,0.7,1,0,System Overview: Synthetic Data
87,"Furthermore , in order to train the final systems , we also generated unknown phenomena infused data for the back - translated filtered data , thereby also almost doubling the sizes of the back - translated data .",System Overview,Synthetic Data,machine-translation,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",63,0.863013698630137,86,0.6013986013986014,9,0.9,1,0,System Overview: Synthetic Data
89,Light Workflow,System Overview,,machine-translation,5,"['O', 'O']","['O', 'O']",65,0.8904109589041096,88,0.6153846153846154,0,0.0,1,0,System Overview
95,"For the final translations , we used a postprocessing script to replace consecutive repeating n-grams and repeating ngrams that have a preposition between them ( i.e. , victim of the victim ) with a single n-gram .",System Overview,Light Workflow,machine-translation,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",71,0.9726027397260274,94,0.6573426573426573,6,0.24,1,0,System Overview: Light Workflow
96,"This problem was more apparent in RNN - based NMT systems , but it was also noticable in our Transformer model outputs .",System Overview,Light Workflow,machine-translation,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",72,0.9863013698630136,95,0.6643356643356644,7,0.28,1,0,System Overview: Light Workflow
97,NMT,System Overview,,machine-translation,5,['O'],['O'],73,1.0,96,0.6713286713286714,8,0.32,1,0,System Overview
98,Systems,,,machine-translation,5,['O'],['O'],0,0.0,97,0.6783216783216783,9,0.36,1,0,
103,"Note that batch size may differ between different architectures and BLEU scores are calculated on raw ( token level ) pre-processed validation sets , therefore , the scores are slightly higher than evaluation results for the final translations !",Systems,Systems,machine-translation,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.3125,102,0.7132867132867133,14,0.56,1,0,Systems
105,"NMT models so far have struggled with translating rare or unseen words ( not different surface forms , but rather different words ) correctly .",Systems,Automatic Post - editing of Named Entities,machine-translation,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.4375,104,0.7272727272727273,16,0.64,1,0,Systems: Automatic Post - editing of Named Entities
106,"Named entities and non-translatable entities ( various product names , identifiers , etc. ) are often rare or unknown .",Systems,Automatic Post - editing of Named Entities,machine-translation,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.5,105,0.7342657342657343,17,0.68,1,0,Systems: Automatic Post - editing of Named Entities
111,"Then named entity recognition ( based on dictionary look - up ) was performed on the source text and , if a named entity was found , the target translation was validated against the entries in the dic-tionary .",Systems,Automatic Post - editing of Named Entities,machine-translation,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.8125,110,0.7692307692307693,22,0.88,1,0,Systems: Automatic Post - editing of Named Entities
113,"If a translation was contradicting the entries in the dictionary , it was replaced with the closest matching ( by looking for the longest matching suffix ) translation from the dictionary .",Systems,Automatic Post - editing of Named Entities,machine-translation,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.9375,112,0.7832167832167832,24,0.96,1,0,Systems: Automatic Post - editing of Named Entities
115,System Combination,,,machine-translation,5,"['O', 'O']","['O', 'O']",0,0.0,114,0.7972027972027972,0,0.0,1,0,
118,We did not perform the combination for English - Estonian due to lack of support for alignment extraction for Estonian in Meteor .,System Combination,System Combination,machine-translation,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.2727272727272727,117,0.8181818181818182,3,0.2727272727272727,1,0,System Combination
122,The table is then used to count the number of occurrences of different translations .,System Combination,System Combination,machine-translation,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.6363636363636364,121,0.8461538461538461,7,0.6363636363636364,1,0,System Combination
127,Results,,,machine-translation,5,['O'],['O'],0,0.0,126,0.8811188811188811,0,0.0,1,0,
132,"Although the unconstrained models were not trained on factored data , the datasets were 17 times larger than the constrained datasets .",Results,Results,machine-translation,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.625,131,0.916083916083916,5,0.625,1,0,Results
133,"However , the difference is rather minimal and shows that the current NMT architectures may notable to learn effectively from large datasets .",Results,Results,machine-translation,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.75,132,0.9230769230769232,6,0.75,1,0,Results
136,Conclusion,,,machine-translation,5,['O'],['O'],0,0.0,135,0.944055944055944,0,0.0,1,0,
3,abstract,,,machine-translation,6,['O'],['O'],0,0.0,2,0.0068728522336769,0,0.0,1,0,
4,Continuous word representation ( aka word embedding ) is a basic building block in many neural network - based models used in natural language processing tasks .,abstract,abstract,machine-translation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.1666666666666666,3,0.0103092783505154,1,0.1666666666666666,1,0,abstract
6,"This makes learned word embeddings ineffective , especially for rare words , and consequently limits the performance of these neural network models .",abstract,abstract,machine-translation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.5,5,0.0171821305841924,3,0.5,1,0,abstract
11,"Word embeddings , which are distributed and continuous vector representations for word tokens , have been one of the basic building blocks for many neural network - based models used in natural language processing ( NLP ) tasks , such as language modeling , text classification and machine translation .",Introduction,Introduction,machine-translation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0294117647058823,10,0.0343642611683848,1,0.0294117647058823,1,0,Introduction
13,"In deep learning approaches for NLP tasks , word embeddings act as the inputs of the neural network and are usually trained together with neural network parameters .",Introduction,Introduction,machine-translation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.088235294117647,12,0.0412371134020618,3,0.088235294117647,1,0,Introduction
15,"Unfortunately , we find the word embeddings learned by many deep learning approaches are far from perfect .",Introduction,Introduction,machine-translation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.1470588235294117,14,0.0481099656357388,5,0.1470588235294117,1,0,Introduction
16,"As shown in ( a ) and 1 ( b ) , in the embedding space learned by word2 vec model , the nearest neighbors of word "" Peking "" includes "" quickest "" , "" multicellular "" , and "" epigenetic "" , which are not semantically similar , while semantically related words such as "" Beijing "" and "" China "" are far from it .",Introduction,Introduction,machine-translation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.1764705882352941,15,0.0515463917525773,6,0.1764705882352941,1,0,Introduction
18,"With a careful study , we find a more general problem which is rooted in low - frequency words in the text corpus .",Introduction,Introduction,machine-translation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.2352941176470588,17,0.0584192439862542,8,0.2352941176470588,1,0,Introduction
19,"Without any confusion , we also call high - frequency words as popular words and call low - frequency words as rare words .",Introduction,Introduction,machine-translation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.2647058823529412,18,0.0618556701030927,9,0.2647058823529412,1,0,Introduction
20,"As is well known , the frequency distribution of words roughly follows a simple mathematical form known as Zipf 's law .",Introduction,Introduction,machine-translation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.2941176470588235,19,0.0652920962199312,10,0.2941176470588235,1,0,Introduction
21,"When the size of a text corpus grows , the frequency of rare words is much smaller than popular words while the number of unique rare words is much larger than popular words .",Introduction,Introduction,machine-translation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.3235294117647059,20,0.0687285223367697,11,0.3235294117647059,1,0,Introduction
23,"In the embedding space , a popular word usually has semantically related neighbors , while a rare word usually does not .",Introduction,Introduction,machine-translation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.3823529411764705,22,0.0756013745704467,13,0.3823529411764705,1,0,Introduction
24,"Moreover , the nearest neighbors of more than 85 % rare words are rare words .",Introduction,Introduction,machine-translation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.4117647058823529,23,0.0790378006872852,14,0.4117647058823529,1,0,Introduction
27,Such a phenomenon is also observed in .,Introduction,Introduction,machine-translation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.5,26,0.0893470790378006,17,0.5,1,0,Introduction
32,"Second , the neighbors of a large number of rare words are semantically unrelated rare words .",Introduction,Introduction,machine-translation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.6470588235294118,31,0.1065292096219931,22,0.6470588235294118,1,0,Introduction
33,"To some extent , those word embeddings encode more frequency information than semantic information which is not good from the view of semantic understanding .",Introduction,Introduction,machine-translation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",23,0.6764705882352942,32,0.1099656357388316,23,0.6764705882352942,1,0,Introduction
34,It will consequently limit the performance of down - stream tasks using the embeddings .,Introduction,Introduction,machine-translation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.7058823529411765,33,0.1134020618556701,24,0.7058823529411765,1,0,Introduction
35,"For example , in text classification , it can not be well guaranteed that the label of a sentence does not change when you replace one popular / rare word in the sentence by its rare / popular alternatives .",Introduction,Introduction,machine-translation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.7352941176470589,34,0.1168384879725085,25,0.7352941176470589,1,0,Introduction
39,"When the whole training process converges and the system achieves an equilibrium , the discriminator can not well differentiate popular words from rare words .",Introduction,Introduction,machine-translation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",29,0.8529411764705882,38,0.1305841924398625,29,0.8529411764705882,1,0,Introduction
40,"Consequently , rare words lie in the same region as and are mixed with popular words in the embedding space .",Introduction,Introduction,machine-translation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",30,0.8823529411764706,39,0.134020618556701,30,0.8823529411764706,1,0,Introduction
45,Background,,,machine-translation,6,['O'],['O'],0,0.0,44,0.1512027491408934,0,0.0,1,0,
52,Adversarial Training,,,machine-translation,6,"['O', 'O']","['O', 'O']",0,0.0,51,0.1752577319587628,0,0.0,1,0,
54,"A representative example of adversarial training is Generative Adversarial Networks ( GANs ) for image generation , in which a discriminator and a generator compete with each other : the generator aims to generate images similar to the natural ones , and the discriminator aims to detect the generated ones from the natural ones .",Adversarial Training,Adversarial Training,machine-translation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.1666666666666666,53,0.1821305841924398,2,0.2222222222222222,1,0,Adversarial Training
55,"Recently , adversarial training has been successfully applied to NLP tasks .",Adversarial Training,Adversarial Training,machine-translation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.25,54,0.1855670103092783,3,0.3333333333333333,1,0,Adversarial Training
58,Our proposed method is under the adversarial training framework but not exactly the conventional generator - discriminator approach since there is no generator in our scenario .,Adversarial Training,Adversarial Training,machine-translation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.5,57,0.1958762886597938,6,0.6666666666666666,1,0,Adversarial Training
65,Experimental Design,,,machine-translation,6,"['O', 'O']","['O', 'O']",0,0.0,64,0.2199312714776632,0,0.0,1,0,
66,"In both tasks , we simply set the top 20 % frequent words in vocabulary as popular words and denote the rest as rare words ( roughly speaking , we set a word as a rare word if it s relative frequency is lower than 10 ? 6 in WMT14 dataset and 10 ? 7 in Google News dataset ) .",Experimental Design,Experimental Design,machine-translation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0526315789473684,65,0.2233676975945017,1,0.0625,1,0,Experimental Design
67,We have tried other thresholds such as 10 % or 25 % and found the observations are similar .,Experimental Design,Experimental Design,machine-translation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.1052631578947368,66,0.2268041237113402,2,0.125,1,0,Experimental Design
71,We also manually chose words which are semantically similar to it .,Experimental Design,Experimental Design,machine-translation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.3157894736842105,70,0.2405498281786941,6,0.375,1,0,Experimental Design
72,"For simplicity , for each word , we call the nearest words predicted from the embeddings as model - predicted neighbors , and call our chosen words as semantic neighbors .",Experimental Design,Experimental Design,machine-translation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.3684210526315789,71,0.2439862542955326,7,0.4375,1,0,Experimental Design
73,Observation,Experimental Design,,machine-translation,6,['O'],['O'],8,0.4210526315789473,72,0.2474226804123711,8,0.5,1,0,Experimental Design
75,More cases and other studies without dimensionality reduction can be found in the supplementary material ( part C ) .,Experimental Design,Observation,machine-translation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.5263157894736842,74,0.2542955326460481,10,0.625,1,0,Experimental Design: Observation
78,"For each rare word , the model - predicted neighbor is usually not semantically related to this word , and semantic neighbors we chose are faraway from it in the embedding space .",Experimental Design,Observation,machine-translation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.6842105263157895,77,0.2646048109965636,13,0.8125,1,0,Experimental Design: Observation
82,Input Tokens Word Embeddings,Experimental Design,,machine-translation,6,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",17,0.8947368421052632,81,0.2783505154639175,0,0.0,1,0,Experimental Design
85,Model,,,machine-translation,6,['O'],['O'],0,0.0,84,0.2886597938144329,3,1.0,1,0,
86,Loss,Model,,machine-translation,6,['O'],['O'],1,0.0434782608695652,85,0.2920962199312715,0,0.0,1,0,Model
87,Rare / Popular Labels Discriminator,Model,,machine-translation,6,"['O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O']",2,0.0869565217391304,86,0.2955326460481099,1,0.0769230769230769,1,0,Model
91,"a certain degree : the rare words and popular words lie in different regions after this linear projection , and thus they occupy different regions in the original embedding space .",Model,Rare / Popular Labels Discriminator,machine-translation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.2608695652173913,90,0.3092783505154639,5,0.3846153846153846,1,0,Model: Rare / Popular Labels Discriminator
92,This strange phenomenon is also observed in other learned embeddings ( e.g. CBOW and GLOVE ) and mentioned in .,Model,Rare / Popular Labels Discriminator,machine-translation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.3043478260869565,91,0.3127147766323024,6,0.4615384615384615,1,0,Model: Rare / Popular Labels Discriminator
93,Explanation,Model,,machine-translation,6,['O'],['O'],8,0.3478260869565217,92,0.3161512027491409,7,0.5384615384615384,1,0,Model
94,"From the empirical study above , we can see that the occupied spaces of popular words and rare words are different and here we intuitively explain a possible reason .",Model,Explanation,machine-translation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.391304347826087,93,0.3195876288659793,8,0.6153846153846154,1,0,Model: Explanation
95,We simply take word2vec as an example which is trained by stochastic gradient descent .,Model,Explanation,machine-translation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.4347826086956521,94,0.3230240549828179,9,0.6923076923076923,1,0,Model: Explanation
97,"For a rare word , the sample rate is low and its embedding rarely updates .",Model,Explanation,machine-translation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.5217391304347826,96,0.3298969072164948,11,0.8461538461538461,1,0,Model: Explanation
98,"According to our study , on average , the moving distance of the embedding fora popular word is twice longer than that of a rare word during training .",Model,Explanation,machine-translation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.5652173913043478,97,0.3333333333333333,12,0.9230769230769232,1,0,Model: Explanation
100,Discussion,Model,,machine-translation,6,['O'],['O'],15,0.6521739130434783,99,0.3402061855670103,0,0.0,1,0,Model
101,We have strong evidence that the current phenomena are problematic .,Model,Discussion,machine-translation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.6956521739130435,100,0.3436426116838488,1,0.125,1,0,Model: Discussion
102,"First , according to our study , in both tasks , more than half of the rare words are nouns , e.g. , company names , city names .",Model,Discussion,machine-translation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.7391304347826086,101,0.3470790378006873,2,0.25,1,0,Model: Discussion
103,"They may share some similar topics to popular entities , e.g. , big companies and cities ; around 10 % percent of rare words include a hyphen ( which is usually used to join popular words ) , and over 30 % rare words are different PoSs of popular words .",Model,Discussion,machine-translation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.782608695652174,102,0.3505154639175257,3,0.375,1,0,Model: Discussion
105,"These facts show that rare words and popular words should lie in the same region of the embedding space , which is different from what we observed .",Model,Discussion,machine-translation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.8695652173913043,104,0.3573883161512027,5,0.625,1,0,Model: Discussion
106,"Second , as we can see from the cases , for rare words , model - predicted neighbors are usually not semantically related words but frequency - related words ( rare words ) .",Model,Discussion,machine-translation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.9130434782608696,105,0.3608247422680412,6,0.75,1,0,Model: Discussion
108,"It is not good to use such word embeddings into semantic understanding tasks , e.g. , text classification , language modeling , language understanding and translation .",Model,Discussion,machine-translation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",23,1.0,107,0.3676975945017182,8,1.0,1,0,Model: Discussion
109,Our Method,,,machine-translation,6,"['O', 'O']","['O', 'O']",0,0.0,108,0.3711340206185567,0,0.0,1,0,
117,Denote ? emb ?,Our Method,Our Method,machine-translation,6,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",8,0.1702127659574468,116,0.3986254295532646,8,0.1702127659574468,1,0,Our Method
118,"R d|V | as the word embedding matrix to be learned , where dis the dimension of the embedding vectors and | V | is the vocabulary size .",Our Method,Our Method,machine-translation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.1914893617021276,117,0.4020618556701031,9,0.1914893617021276,1,0,Our Method
119,Let V pop denote the set of popular words and V rare = V \ V pop denote the set of rare words .,Our Method,Our Method,machine-translation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.2127659574468085,118,0.4054982817869416,10,0.2127659574468085,1,0,Our Method
120,Then the embedding matrix ?,Our Method,Our Method,machine-translation,6,"['O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O']",11,0.2340425531914893,119,0.40893470790378,11,0.2340425531914893,1,0,Our Method
121,emb can be divided into two parts : ?,Our Method,Our Method,machine-translation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.2553191489361702,120,0.4123711340206185,12,0.2553191489361702,1,0,Our Method
122,emb pop for popular words and ?,Our Method,Our Method,machine-translation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.2765957446808511,121,0.415807560137457,13,0.2765957446808511,1,0,Our Method
123,emb rare for rare words .,Our Method,Our Method,machine-translation,6,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O']",14,0.2978723404255319,122,0.4192439862542955,14,0.2978723404255319,1,0,Our Method
124,Let ?,Our Method,Our Method,machine-translation,6,"['O', 'O']","['O', 'O']",15,0.3191489361702128,123,0.422680412371134,15,0.3191489361702128,1,0,Our Method
125,emb w denote the embedding of word w .,Our Method,Our Method,machine-translation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.3404255319148936,124,0.4261168384879725,16,0.3404255319148936,1,0,Our Method
126,Let ?,Our Method,Our Method,machine-translation,6,"['O', 'O']","['O', 'O']",17,0.3617021276595745,125,0.4295532646048109,17,0.3617021276595745,1,0,Our Method
128,"For instance , for language modeling , ?",Our Method,Our Method,machine-translation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.4042553191489361,127,0.436426116838488,19,0.4042553191489361,1,0,Our Method
129,"model is the parameters of the RNN or LSTM ; for neural machine translation , ?",Our Method,Our Method,machine-translation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.425531914893617,128,0.4398625429553264,20,0.425531914893617,1,0,Our Method
130,"model is the parameters of the encoder , attention module and decoder .",Our Method,Our Method,machine-translation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.4468085106382978,129,0.4432989690721649,21,0.4468085106382978,1,0,Our Method
131,"Let L T ( S ; ? model , ? emb ) denote the task - specific loss over a dataset S. Taking language modeling as an example , the loss L T ( S ; ? model , ? emb ) is defined as the negative log likelihood of the data :",Our Method,Our Method,machine-translation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.4680851063829787,130,0.4467353951890034,22,0.4680851063829787,1,0,Our Method
132,where y is a sentence .,Our Method,Our Method,machine-translation,6,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O']",23,0.4893617021276595,131,0.4501718213058419,23,0.4893617021276595,1,0,Our Method
135,"D , which takes a word embedding as input and outputs a confidence score between 0 and 1 indicating how likely the word is a rare word .",Our Method,Our Method,machine-translation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.5531914893617021,134,0.4604810996563573,26,0.5531914893617021,1,0,Our Method
136,"Let L D ( V ; ? D , ? emb ) denote the loss of the discriminator :",Our Method,Our Method,machine-translation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.574468085106383,135,0.4639175257731959,27,0.574468085106383,1,0,Our Method
137,"Following the principle of adversarial training , we develop a minimax objective to train the taskspecific model ( ?",Our Method,Our Method,machine-translation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",28,0.5957446808510638,136,0.4673539518900343,28,0.5957446808510638,1,0,Our Method
138,model and ? emb ) and the discriminator ( ?,Our Method,Our Method,machine-translation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",29,0.6170212765957447,137,0.4707903780068728,29,0.6170212765957447,1,0,Our Method
139,D ) as below :,Our Method,Our Method,machine-translation,6,"['O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O']",30,0.6382978723404256,138,0.4742268041237113,30,0.6382978723404256,1,0,Our Method
140,where ?,Our Method,Our Method,machine-translation,6,"['O', 'O']","['O', 'O']",31,0.6595744680851063,139,0.4776632302405498,31,0.6595744680851063,1,0,Our Method
141,is a coefficient to trade off the two loss terms .,Our Method,Our Method,machine-translation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",32,0.6808510638297872,140,0.4810996563573883,32,0.6808510638297872,1,0,Our Method
142,We can see that when the model parameter ?,Our Method,Our Method,machine-translation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",33,0.7021276595744681,141,0.4845360824742268,33,0.7021276595744681,1,0,Our Method
144,"emb are fixed , the optimization of the discriminator ?",Our Method,Our Method,machine-translation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",35,0.7446808510638298,143,0.4914089347079037,35,0.7446808510638298,1,0,Our Method
145,D becomes,Our Method,,machine-translation,6,"['O', 'O']","['O', 'O']",36,0.7659574468085106,144,0.4948453608247423,36,0.7659574468085106,1,0,Our Method
146,which is to minimize the classification error of popular and rare words .,Our Method,D becomes,machine-translation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",37,0.7872340425531915,145,0.4982817869415807,37,0.7872340425531915,1,0,Our Method: D becomes
147,When the discriminator ?,Our Method,D becomes,machine-translation,6,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",38,0.8085106382978723,146,0.5017182130584192,38,0.8085106382978723,1,0,Our Method: D becomes
148,"Dis fixed , the optimization of ?",Our Method,D becomes,machine-translation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O']",39,0.8297872340425532,147,0.5051546391752577,39,0.8297872340425532,1,0,Our Method: D becomes
149,model and ?,Our Method,D becomes,machine-translation,6,"['O', 'O', 'O']","['O', 'O', 'O']",40,0.851063829787234,148,0.5085910652920962,40,0.851063829787234,1,0,Our Method: D becomes
150,emb becomes,Our Method,D becomes,machine-translation,6,"['O', 'O']","['O', 'O']",41,0.8723404255319149,149,0.5120274914089347,41,0.8723404255319149,1,0,Our Method: D becomes
151,"i.e. , to optimize the task performance as well as fooling the discriminator .",Our Method,D becomes,machine-translation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",42,0.8936170212765957,150,0.5154639175257731,42,0.8936170212765957,1,0,Our Method: D becomes
153,"model , ?",Our Method,D becomes,machine-translation,6,"['O', 'O', 'O']","['O', 'O', 'O']",44,0.9361702127659576,152,0.5223367697594502,44,0.9361702127659576,1,0,Our Method: D becomes
154,emb and ?,Our Method,D becomes,machine-translation,6,"['O', 'O', 'O']","['O', 'O', 'O']",45,0.9574468085106383,153,0.5257731958762887,45,0.9574468085106383,1,0,Our Method: D becomes
157,Experiment,,,machine-translation,6,['O'],['O'],0,0.0,156,0.5360824742268041,0,0.0,1,0,
161,from S.,Experiment,Experiment,machine-translation,6,"['O', 'O']","['O', 'O']",4,0.0784313725490196,160,0.5498281786941581,4,0.2352941176470588,1,0,Experiment
162,4 :,Experiment,Experiment,machine-translation,6,"['O', 'O']","['O', 'O']",5,0.0980392156862745,161,0.5532646048109966,5,0.2941176470588235,1,0,Experiment
163,Sample a minibatchV = V pop ?,Experiment,Experiment,machine-translation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.1176470588235294,162,0.5567010309278351,6,0.3529411764705882,1,0,Experiment
164,V rare from V .,Experiment,Experiment,machine-translation,6,"['O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O']",7,0.1372549019607843,163,0.5601374570446735,7,0.4117647058823529,1,0,Experiment
165,5 :,Experiment,Experiment,machine-translation,6,"['O', 'O']","['O', 'O']",8,0.1568627450980392,164,0.563573883161512,8,0.4705882352941176,1,0,Experiment
166,"Update ? model , ?",Experiment,Experiment,machine-translation,6,"['O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O']",9,0.1764705882352941,165,0.5670103092783505,9,0.5294117647058824,1,0,Experiment
167,emb by gradient descent according to Eqn. ( 5 ) with data ?.,Experiment,Experiment,machine-translation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.196078431372549,166,0.570446735395189,10,0.5882352941176471,1,0,Experiment
168,Update ?,Experiment,Experiment,machine-translation,6,"['O', 'O']","['O', 'O']",11,0.2156862745098039,167,0.5738831615120275,11,0.6470588235294118,1,0,Experiment
169,D by gradient ascent according to Eqn. ( 4 ) with vocabulary V .,Experiment,Experiment,machine-translation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.2352941176470588,168,0.5773195876288659,12,0.7058823529411765,1,0,Experiment
170,"7 : until Converge 8 : Output : ? model , ? emb , ?",Experiment,Experiment,machine-translation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.2549019607843137,169,0.5807560137457045,13,0.7647058823529411,1,0,Experiment
171,D .,Experiment,,machine-translation,6,"['O', 'O']","['O', 'O']",14,0.2745098039215686,170,0.584192439862543,14,0.8235294117647058,1,0,Experiment
173,The only difference is that we use the original task - specific loss function with an additional adversarial loss as in Eqn..,Experiment,D .,machine-translation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.3137254901960784,172,0.5910652920962199,16,0.9411764705882352,1,0,Experiment: D .
175,Settings,Experiment,,machine-translation,6,['O'],['O'],18,0.3529411764705882,174,0.5979381443298969,0,0.0,1,0,Experiment
186,Machine Translation is a popular task in both deep learning and natural language processing .,Experiment,Settings,machine-translation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",29,0.5686274509803921,185,0.6357388316151202,11,0.3333333333333333,1,0,Experiment: Settings
192,"In all tasks , we simply set the top 20 % frequent words in vocabulary as popular words and denote the rest as rare words , which is the same as our empirical study .",Experiment,Settings,machine-translation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",35,0.6862745098039216,191,0.6563573883161512,17,0.5151515151515151,1,0,Experiment: Settings
197,6 http://mattmahoney.net/dc/textdata.html,Experiment,Settings,machine-translation,6,"['O', 'O']","['O', 'O']",40,0.7843137254901961,196,0.6735395189003437,22,0.6666666666666666,1,0,Experiment: Settings
201,"To improve the training for imbalanced labeled data , a common method is to adjust loss function by reweighting the training samples ; To regularize the parameter space , a common method is to use l 2 regularization .",Experiment,Settings,machine-translation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",44,0.8627450980392157,200,0.6872852233676976,26,0.7878787878787878,1,0,Experiment: Settings
203,Detailed analysis is provided in the supplementary material ( part B ) .,Experiment,Settings,machine-translation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",46,0.9019607843137256,202,0.6941580756013745,28,0.8484848484848485,1,0,Experiment: Settings
204,https://github.com/brightmart/text_classification one hidden layer is more efficient and we set the number of nodes in the hidden layer as 1.5 times embedding size .,Experiment,Settings,machine-translation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",47,0.9215686274509804,203,0.697594501718213,29,0.8787878787878788,1,0,Experiment: Settings
209,Results,,,machine-translation,6,['O'],['O'],0,0.0,208,0.7147766323024055,0,0.0,1,0,
210,RG65,Results,,machine-translation,6,['O'],['O'],1,0.25,209,0.718213058419244,0,0.0,1,0,Results
211,Word Similarity,Results,,machine-translation,6,"['O', 'O']","['O', 'O']",2,0.5,210,0.7216494845360825,0,0.0,1,0,Results
213,""" Paras "" denotes the number of model parameters .",Results,Word Similarity,machine-translation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,1.0,212,0.7285223367697594,2,1.0,1,0,Results: Word Similarity
221,Machine Translation,Language Modeling,,machine-translation,6,"['O', 'O']","['O', 'O']",7,0.4117647058823529,220,0.7560137457044673,0,0.0,1,0,Language Modeling
224,"task , respectively .",Language Modeling,Machine Translation,machine-translation,6,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",10,0.5882352941176471,223,0.7663230240549829,3,0.6,1,0,Language Modeling: Machine Translation
227,Text Classification,Language Modeling,,machine-translation,6,"['O', 'O']","['O', 'O']",13,0.7647058823529411,226,0.7766323024054983,0,0.0,1,0,Language Modeling
228,The results are listed in .,Language Modeling,Text Classification,machine-translation,6,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O']",14,0.8235294117647058,227,0.7800687285223368,1,0.25,1,0,Language Modeling: Text Classification
232,Conclusion,,,machine-translation,6,['O'],['O'],0,0.0,231,0.7938144329896907,0,0.0,1,0,
275,"For machine translation , Transformer is a recently developed architecture in which the selfattention network is used during encoding and decoding step .",A.3 Models Description,A.3 Models Description,machine-translation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.2727272727272727,274,0.9415807560137456,6,0.5454545454545454,1,0,A.3 Models Description
277,"WMT14 English - German , WMT14 English - French datasets .",A.3 Models Description,A.3 Models Description,machine-translation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.3636363636363636,276,0.9484536082474226,8,0.7272727272727273,1,0,A.3 Models Description
278,Word2vec is one of the pioneer works on using deep learning to NLP tasks .,A.3 Models Description,A.3 Models Description,machine-translation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.4090909090909091,277,0.9518900343642612,9,0.8181818181818182,1,0,A.3 Models Description
280,"RCNN contains both recurrent and convolutional layers to catch the key components in texts , and is widely used in text classification tasks .",A.3 Models Description,A.3 Models Description,machine-translation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.5,279,0.9587628865979382,11,1.0,1,0,A.3 Models Description
283,Results are listed in .,A.3 Models Description,B Additional Comparisons,machine-translation,6,"['O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O']",14,0.6363636363636364,282,0.9690721649484536,2,0.6666666666666666,1,0,A.3 Models Description: B Additional Comparisons
285,WMT,A.3 Models Description,,machine-translation,6,['O'],['O'],16,0.7272727272727273,284,0.9759450171821306,0,0.0,1,0,A.3 Models Description
286,"Our method is denoted as "" FRAGE "" , "" Reweighting "" denotes reweighting the loss of each word by reciprocal of its frequency , and "" Weight Decay "" denotes putting weight decay rate ( 0.2 ) on embeddings .",A.3 Models Description,WMT,machine-translation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.7727272727272727,285,0.979381443298969,1,0.0,1,0,A.3 Models Description: WMT
288,We provide more word similarity cases in to justify our statement in Section 3 .,A.3 Models Description,WMT,machine-translation,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.8636363636363636,287,0.986254295532646,1,0.25,1,0,A.3 Models Description: WMT
2,OUTRAGEOUSLY LARGE NEURAL NETWORKS : THE SPARSELY - GATED MIXTURE - OF - EXPERTS LAYER,title,title,machine-translation,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0,1,0.002680965147453,1,0.0,1,0,title
3,abstract,,,machine-translation,7,['O'],['O'],0,0.0,2,0.0053619302949061,0,0.0,1,0,
4,The capacity of a neural network to absorb information is limited by its number of parameters .,abstract,abstract,machine-translation,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0108695652173913,3,0.0080428954423592,1,0.0204081632653061,1,0,abstract
5,"Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation .",abstract,abstract,machine-translation,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.0217391304347826,4,0.0107238605898123,2,0.0408163265306122,1,0,abstract
6,"In practice , however , there are significant algorithmic and performance challenges .",abstract,abstract,machine-translation,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.0326086956521739,5,0.0134048257372654,3,0.0612244897959183,1,0,abstract
13,* Equally major contributors Work done as a member of the Google Brain Residency program ( g.co/ brainresidency ),abstract,abstract,machine-translation,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.108695652173913,12,0.032171581769437,10,0.2040816326530612,1,0,abstract
18,"This has been shown in domains such as text , images , and audio .",abstract,abstract,machine-translation,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.1630434782608695,17,0.0455764075067024,15,0.3061224489795918,1,0,abstract
19,"For typical deep learning models , where the entire model is activated for every example , this leads to a roughly quadratic blow - up in training costs , as both the model size and the number of training examples increase .",abstract,abstract,machine-translation,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.1739130434782608,18,0.0482573726541554,16,0.3265306122448979,1,0,abstract
21,Various forms of conditional computation have been proposed as away to increase model capacity without a proportional increase in computational costs .,abstract,abstract,machine-translation,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.1956521739130435,20,0.0536193029490616,18,0.3673469387755102,1,0,abstract
22,"In these schemes , large parts of a network are active or inactive on a per-example basis .",abstract,abstract,machine-translation,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.2065217391304347,21,0.0563002680965147,19,0.3877551020408163,1,0,abstract
23,"The gating decisions maybe binary or sparse and continuous , stochastic or deterministic .",abstract,abstract,machine-translation,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.217391304347826,22,0.0589812332439678,20,0.4081632653061224,1,0,abstract
25,"While these ideas are promising in theory , no work to date has yet demonstrated massive improvements in model capacity , training time , or model quality .",abstract,abstract,machine-translation,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.2391304347826087,24,0.064343163538874,22,0.4489795918367347,1,0,abstract
27,"Modern computing devices , especially GPUs , are much faster at arithmetic than at branching .",abstract,abstract,machine-translation,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.2608695652173913,26,0.0697050938337801,24,0.4897959183673469,1,0,abstract
28,Most of the works above recognize this and propose turning on / off large chunks of the network with each gating decision .,abstract,abstract,machine-translation,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.2717391304347826,27,0.0723860589812332,25,0.5102040816326531,1,0,abstract
32,A cluster of GPUs may have computational power thousands of times greater than the aggregate inter - device network bandwidth .,abstract,abstract,machine-translation,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",29,0.3152173913043478,31,0.0831099195710455,29,0.5918367346938775,1,0,abstract
33,"To be computationally efficient , the relative computational versus network demands of an algorithm must exceed this ratio .",abstract,abstract,machine-translation,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",30,0.3260869565217391,32,0.0857908847184986,30,0.6122448979591837,1,0,abstract
34,"Embedding layers , which can be seen as a form of conditional computation , are handicapped by this very problem .",abstract,abstract,machine-translation,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",31,0.3369565217391304,33,0.0884718498659517,31,0.6326530612244898,1,0,abstract
35,"Since the embeddings generally need to be sent across the network , the number of ( example , parameter ) interactions is limited by network bandwidth instead of computational capacity .",abstract,abstract,machine-translation,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",32,0.3478260869565217,34,0.0911528150134048,32,0.6530612244897959,1,0,abstract
39,Model capacity is most critical for very large data sets .,abstract,abstract,machine-translation,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",36,0.391304347826087,38,0.1018766756032171,36,0.7346938775510204,1,0,abstract
40,"The existing literature on conditional computation deals with relatively small image recognition data sets consisting of up to 600,000 images .",abstract,abstract,machine-translation,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",37,0.4021739130434782,39,0.1045576407506702,37,0.7551020408163265,1,0,abstract
41,"It is hard to imagine that the labels of these images provide a sufficient signal to adequately train a model with millions , let alone billions of parameters .",abstract,abstract,machine-translation,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",38,0.4130434782608695,40,0.1072386058981233,38,0.7755102040816326,1,0,abstract
46,"The MoE consists of a number of experts , each a simple feed - forward neural network , and a trainable gating network which selects a sparse combination of the experts to process each input ( see ) .",abstract,abstract,machine-translation,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",43,0.4673913043478261,45,0.1206434316353887,43,0.8775510204081632,1,0,abstract
51,The different experts tend to become highly specialized based on syntax and semantics ( see Appendix E ) .,abstract,abstract,machine-translation,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",48,0.5217391304347826,50,0.1340482573726541,48,0.979591836734694,1,0,abstract
53,RELATED WORK ON MIXTURES OF EXPERTS,abstract,abstract,machine-translation,7,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O']",50,0.5434782608695652,52,0.1394101876675603,0,0.0,1,0,abstract
54,"Since its introduction more than two decades ago , the mixture - of - experts approach has been the subject of much research .",abstract,abstract,machine-translation,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",51,0.5543478260869565,53,0.1420911528150134,1,0.0333333333333333,1,0,abstract
55,"Different types of expert architectures hae been proposed such as SVMs , Gaussian Processes , Dirichlet Processes , and deep networks .",abstract,abstract,machine-translation,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",52,0.5652173913043478,54,0.1447721179624665,2,0.0666666666666666,1,0,abstract
56,"Other work has focused on different expert configurations such as a hierarchical structure , infinite numbers of experts , and adding experts sequentially .",abstract,abstract,machine-translation,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",53,0.5760869565217391,55,0.1474530831099195,3,0.1,1,0,abstract
59,The works above concern top - level mixtures of experts .,abstract,abstract,machine-translation,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",56,0.6086956521739131,58,0.1554959785522788,6,0.2,1,0,abstract
62,MoEs with their own gating networks as parts of a deep model .,abstract,abstract,machine-translation,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",59,0.6413043478260869,61,0.163538873994638,9,0.3,1,0,abstract
63,"It is intuitive that the latter approach is more powerful , since complex problems may contain many sub-problems each requiring different experts .",abstract,abstract,machine-translation,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",60,0.6521739130434783,62,0.1662198391420911,10,0.3333333333333333,1,0,abstract
64,"They also allude in their conclusion to the potential to introduce sparsity , turning MoEs into a vehicle for computational computation .",abstract,abstract,machine-translation,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",61,0.6630434782608695,63,0.1689008042895442,11,0.3666666666666666,1,0,abstract
66,"While uses two stacked MoEs allowing for two sets of gating decisions , our convolutional application of the MoE allows for different gating decisions at each position in the text .",abstract,abstract,machine-translation,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",63,0.6847826086956522,65,0.1742627345844504,13,0.4333333333333333,1,0,abstract
68,THE STRUCTURE OF THE MIXTURE - OF - EXPERTS LAYER,abstract,abstract,machine-translation,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",65,0.7065217391304348,67,0.1796246648793565,15,0.5,1,0,abstract
69,"The Mixture - of - Experts ( MoE ) layer consists of a set of n "" expert networks "" E 1 , , E n , and a "" gating network "" G whose output is a sparse n-dimensional vector .",abstract,abstract,machine-translation,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",66,0.717391304347826,68,0.1823056300268096,16,0.5333333333333333,1,0,abstract
71,"The experts are themselves neural networks , each with their own parameters .",abstract,abstract,machine-translation,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",68,0.7391304347826086,70,0.1876675603217158,18,0.6,1,0,abstract
72,"Although in principle we only require that the experts accept the same sized inputs and produce the same - sized outputs , in our initial investigations in this paper , we restrict ourselves to the case where the models are feed - forward networks with identical architectures , but with separate parameters .",abstract,abstract,machine-translation,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",69,0.75,71,0.1903485254691689,19,0.6333333333333333,1,0,abstract
73,Let us denote by G ( x ) and E i ( x ) the output of the gating network and the output of the i - th expert network fora given input x .,abstract,abstract,machine-translation,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",70,0.7608695652173914,72,0.1930294906166219,20,0.6666666666666666,1,0,abstract
74,The output y of the MoE module can be written as follows :,abstract,abstract,machine-translation,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",71,0.7717391304347826,73,0.195710455764075,21,0.7,1,0,abstract
75,We save computation based on the sparsity of the output of G ( x ) .,abstract,abstract,machine-translation,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",72,0.782608695652174,74,0.1983914209115281,22,0.7333333333333333,1,0,abstract
76,"Wherever G (x ) i = 0 , we need not compute E i ( x ) .",abstract,abstract,machine-translation,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",73,0.7934782608695652,75,0.2010723860589812,23,0.7666666666666667,1,0,abstract
82,A MoE whose experts are simple weight matrices is similar to the parameterized weight matrix proposed in .,abstract,abstract,machine-translation,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",79,0.8586956521739131,81,0.2171581769436997,29,0.9666666666666668,1,0,abstract
83,"A MoE whose experts have one hidden layer is similar to the block - wise dropout described in , where the dropped - out layer is sandwiched between fully - activated layers .",abstract,abstract,machine-translation,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",80,0.8695652173913043,82,0.2198391420911528,30,1.0,1,0,abstract
84,GATING NETWORK,abstract,abstract,machine-translation,7,"['O', 'O']","['O', 'O']",81,0.8804347826086957,83,0.2225201072386059,0,0.0,1,0,abstract
85,Softmax Gating :,abstract,abstract,machine-translation,7,"['O', 'O', 'O']","['O', 'O', 'O']",82,0.8913043478260869,84,0.225201072386059,1,0.0526315789473684,1,0,abstract
86,A simple choice of non-sparse gating function is to multiply the input by a trainable weight matrix W g and then apply the Sof tmax function .,abstract,abstract,machine-translation,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",83,0.9021739130434784,85,0.227882037533512,2,0.1052631578947368,1,0,abstract
88,Gating :,abstract,Noisy Top - K,machine-translation,7,"['O', 'O']","['O', 'O']",85,0.9239130434782608,87,0.2332439678284182,4,0.2105263157894736,1,0,abstract: Noisy Top - K
90,"Before taking the softmax function , we add tunable Gaussian noise , then keep only the top k values , setting the rest to ??",abstract,Noisy Top - K,machine-translation,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",87,0.9456521739130436,89,0.2386058981233244,6,0.3157894736842105,1,0,abstract: Noisy Top - K
91,( which causes the corresponding gate values to equal 0 ) .,abstract,Noisy Top - K,machine-translation,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",88,0.9565217391304348,90,0.2412868632707774,7,0.3684210526315789,1,0,abstract: Noisy Top - K
92,"The sparsity serves to save computation , as described above .",abstract,Noisy Top - K,machine-translation,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",89,0.967391304347826,91,0.2439678284182305,8,0.4210526315789473,1,0,abstract: Noisy Top - K
93,"While this form of sparsity creates some theoretically scary discontinuities in the output of gating function , we have not yet observed this to be a problem in practice .",abstract,Noisy Top - K,machine-translation,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",90,0.9782608695652174,92,0.2466487935656836,9,0.4736842105263157,1,0,abstract: Noisy Top - K
94,"The noise term helps with load balancing , as will be discussed in Appendix A .",abstract,Noisy Top - K,machine-translation,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",91,0.9891304347826086,93,0.2493297587131367,10,0.5263157894736842,1,0,abstract: Noisy Top - K
95,The amount of noise per component is controlled by a second trainable weight matrix W noise .,abstract,Noisy Top - K,machine-translation,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",92,1.0,94,0.2520107238605898,11,0.5789473684210527,1,0,abstract: Noisy Top - K
96,Training the Gating Network,,,machine-translation,7,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",0,0.0,95,0.2546916890080429,12,0.631578947368421,1,0,
98,"If we choose k > 1 , the gate values for the top k experts have nonzero derivatives with respect to the weights of the gating network .",Training the Gating Network,Training the Gating Network,machine-translation,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.0165289256198347,97,0.260053619302949,14,0.7368421052631579,1,0,Training the Gating Network
103,CHALLENGES,Training the Gating Network,,machine-translation,7,['O'],['O'],7,0.0578512396694214,102,0.2734584450402145,19,1.0,1,0,Training the Gating Network
104,THE SHRINKING BATCH PROBLEM,Training the Gating Network,CHALLENGES,machine-translation,7,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",8,0.0661157024793388,103,0.2761394101876676,0,0.0,1,0,Training the Gating Network: CHALLENGES
105,"On modern CPUs and GPUs , large batch sizes are necessary for computational efficiency , so as to amortize the overhead of parameter loads and updates .",Training the Gating Network,CHALLENGES,machine-translation,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.0743801652892562,104,0.2788203753351206,1,0.032258064516129,1,0,Training the Gating Network: CHALLENGES
106,"If the gating network chooses k out of n experts for each example , then fora batch of b examples , each expert receives a much smaller batch of approximately kb n b examples .",Training the Gating Network,CHALLENGES,machine-translation,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.0826446280991735,105,0.2815013404825737,2,0.064516129032258,1,0,Training the Gating Network: CHALLENGES
107,This causes a naive MoE implementation to become very inefficient as the number of experts increases .,Training the Gating Network,CHALLENGES,machine-translation,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.0909090909090909,106,0.2841823056300268,3,0.0967741935483871,1,0,Training the Gating Network: CHALLENGES
108,The solution to this shrinking batch problem is to make the original batch size as large as possible .,Training the Gating Network,CHALLENGES,machine-translation,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.0991735537190082,107,0.2868632707774799,4,0.1290322580645161,1,0,Training the Gating Network: CHALLENGES
109,"However , batch size tends to be limited by the memory necessary to store activations between the forwards and backwards passes .",Training the Gating Network,CHALLENGES,machine-translation,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.1074380165289256,108,0.289544235924933,5,0.1612903225806451,1,0,Training the Gating Network: CHALLENGES
117,"If the model is distributed over d devices , and each device processes a batch of size b , each expert receives a batch of approximately kbd n examples .",Training the Gating Network,CHALLENGES,machine-translation,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.1735537190082644,116,0.3109919571045576,13,0.4193548387096774,1,0,Training the Gating Network: CHALLENGES
119,"In the case of a hierarchical MoE ( Section B ) , the primary gating network employs data parallelism , and the secondary MoEs employ model parallelism .",Training the Gating Network,CHALLENGES,machine-translation,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",23,0.1900826446280991,118,0.3163538873994638,15,0.4838709677419355,1,0,Training the Gating Network: CHALLENGES
123,"The memory and bandwidth requirements per device also remain constant , as do the step times , as does the amount of time necessary to process a number of training examples equal to the number of parameters in the model .",Training the Gating Network,CHALLENGES,machine-translation,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.2231404958677686,122,0.3270777479892761,19,0.6129032258064516,1,0,Training the Gating Network: CHALLENGES
125,"We have not scaled our systems this far as of the writing of this paper , but it should be possible by adding more hardware .",Training the Gating Network,CHALLENGES,machine-translation,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",29,0.2396694214876033,124,0.3324396782841823,21,0.6774193548387096,1,0,Training the Gating Network: CHALLENGES
130,Increasing Batch Size fora,Training the Gating Network,,machine-translation,7,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",34,0.2809917355371901,129,0.3458445040214477,26,0.8387096774193549,1,0,Training the Gating Network
131,Recurrent MoE :,Training the Gating Network,Increasing Batch Size fora,machine-translation,7,"['O', 'O', 'O']","['O', 'O', 'O']",35,0.2892561983471074,130,0.3485254691689008,27,0.8709677419354839,1,0,Training the Gating Network: Increasing Batch Size fora
132,We suspect that even more powerful models may involve applying a MoE recurrently .,Training the Gating Network,Increasing Batch Size fora,machine-translation,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",36,0.2975206611570248,131,0.3512064343163539,28,0.9032258064516128,1,0,Training the Gating Network: Increasing Batch Size fora
133,"For example , the weight matrices of a LSTM or other RNN could be replaced by a MoE. Sadly , such models break the convolutional trick from the last paragraph , since the input to the MoE atone timestep depends on the output of the MoE at the previous timestep .",Training the Gating Network,Increasing Batch Size fora,machine-translation,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",37,0.3057851239669421,132,0.353887399463807,29,0.935483870967742,1,0,Training the Gating Network: Increasing Batch Size fora
135,This would allow fora large increase in batch size .,Training the Gating Network,Increasing Batch Size fora,machine-translation,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",39,0.3223140495867768,134,0.3592493297587131,31,1.0,1,0,Training the Gating Network: Increasing Batch Size fora
136,NETWORK BANDWIDTH,Training the Gating Network,Increasing Batch Size fora,machine-translation,7,"['O', 'O']","['O', 'O']",40,0.3305785123966942,135,0.3619302949061662,0,0.0,1,0,Training the Gating Network: Increasing Batch Size fora
137,Another major performance concern in distributed computing is network bandwidth .,Training the Gating Network,Increasing Batch Size fora,machine-translation,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",41,0.3388429752066115,136,0.3646112600536193,1,0.1428571428571428,1,0,Training the Gating Network: Increasing Batch Size fora
138,"Since the experts are stationary ( see above ) and the number of gating parameters is small , most of the communication involves sending the inputs and outputs of the experts across the network .",Training the Gating Network,Increasing Batch Size fora,machine-translation,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",42,0.3471074380165289,137,0.3672922252010724,2,0.2857142857142857,1,0,Training the Gating Network: Increasing Batch Size fora
139,"To maintain computational efficiency , the ratio of an expert 's computation to the size of its input and output must exceed the ratio of computational to network capacity of the computing device .",Training the Gating Network,Increasing Batch Size fora,machine-translation,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",43,0.3553719008264462,138,0.3699731903485255,3,0.4285714285714285,1,0,Training the Gating Network: Increasing Batch Size fora
140,"For GPUs , this maybe thousands to one .",Training the Gating Network,Increasing Batch Size fora,machine-translation,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",44,0.3636363636363636,139,0.3726541554959786,4,0.5714285714285714,1,0,Training the Gating Network: Increasing Batch Size fora
142,"Since the weight matrices in the expert have sizes input_sizehidden_size and hidden_size output_size , the ratio of computation to input and output is equal to the size of the hidden layer .",Training the Gating Network,Increasing Batch Size fora,machine-translation,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",46,0.3801652892561983,141,0.3780160857908847,6,0.8571428571428571,1,0,Training the Gating Network: Increasing Batch Size fora
143,"Conveniently , we can increase computational efficiency simply by using a larger hidden layer , or more hidden layers .",Training the Gating Network,Increasing Batch Size fora,machine-translation,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",47,0.3884297520661157,142,0.3806970509383378,7,1.0,1,0,Training the Gating Network: Increasing Batch Size fora
147,"describe the same phenomenon , and use a hard constraint at the beginning of training to avoid this local minimum .",Training the Gating Network,Increasing Batch Size fora,machine-translation,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",51,0.4214876033057851,146,0.3914209115281501,3,0.0789473684210526,1,0,Training the Gating Network: Increasing Batch Size fora
150,We define the importance of an expert relative to a batch of training examples to be the batchwise sum of the gate values for that expert .,Training the Gating Network,Increasing Batch Size fora,machine-translation,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",54,0.4462809917355372,149,0.3994638069705093,6,0.1578947368421052,1,0,Training the Gating Network: Increasing Batch Size fora
152,"This loss is equal to the square of the coefficient of variation of the set of importance values , multiplied by a hand - tuned scaling factor w importance .",Training the Gating Network,Increasing Batch Size fora,machine-translation,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",56,0.4628099173553719,151,0.4048257372654155,8,0.2105263157894736,1,0,Training the Gating Network: Increasing Batch Size fora
154,L importance ( X ) = w importance CV ( Importance ( X ) ),Training the Gating Network,Increasing Batch Size fora,machine-translation,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",58,0.4793388429752066,153,0.4101876675603217,10,0.2631578947368421,1,0,Training the Gating Network: Increasing Batch Size fora
161,"For full details on model architecture , training regimen , additional baselines and results , see Appendix C .",Training the Gating Network,Increasing Batch Size fora,machine-translation,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",65,0.5371900826446281,160,0.4289544235924933,17,0.4473684210526316,1,0,Training the Gating Network: Increasing Batch Size fora
166,"These models had larger LSTMs , and fewer but larger and experts .",Training the Gating Network,Increasing Batch Size fora,machine-translation,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",70,0.5785123966942148,165,0.4423592493297587,22,0.5789473684210527,1,0,Training the Gating Network: Increasing Batch Size fora
171,Computational,Training the Gating Network,,machine-translation,7,['O'],['O'],75,0.6198347107438017,170,0.4557640750670241,27,0.7105263157894737,1,0,Training the Gating Network
174,"The operation counts used here are higher than the ones we report in our ops / timestep numbers in that we include the backwards pass , we include the importance - sampling - based training of the softmax layer , and we count a multiply - and - add as two separate operations .",Training the Gating Network,Computational,machine-translation,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",78,0.6446280991735537,173,0.4638069705093833,30,0.7894736842105263,1,0,Training the Gating Network: Computational
179,These numbers represent a significant fraction of the theoretical maximum of 4.29 TFLOPS / GPU claimed by NVIDIA .,Training the Gating Network,Computational,machine-translation,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",83,0.6859504132231405,178,0.4772117962466488,35,0.9210526315789472,1,0,Training the Gating Network: Computational
187,This corresponds to up to 137 billion parameters in the MoE layer .,Training the Gating Network,Computational,machine-translation,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",91,0.7520661157024794,186,0.4986595174262734,4,0.1481481481481481,1,0,Training the Gating Network: Computational
195,Our model was a modified version of the GNMT model described in .,Training the Gating Network,Computational,machine-translation,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",99,0.8181818181818182,194,0.5201072386058981,12,0.4444444444444444,1,0,Training the Gating Network: Computational
200,Datasets :,Training the Gating Network,Computational,machine-translation,7,"['O', 'O']","['O', 'O']",104,0.859504132231405,199,0.5335120643431636,17,0.6296296296296297,1,0,Training the Gating Network: Computational
202,"De corpora , whose training sets have 36M sentence pairs and 5 M sentence pairs , respectively .",Training the Gating Network,Computational,machine-translation,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",106,0.8760330578512396,201,0.5388739946380697,19,0.7037037037037037,1,0,Training the Gating Network: Computational
203,"The experimental protocols were also similar to those in : newstest2014 was used as the test set to compare against previous work , while the combination of newstest2012 and newstest2013 was used as the development set .",Training the Gating Network,Computational,machine-translation,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",107,0.8842975206611571,202,0.5415549597855228,20,0.7407407407407407,1,0,Training the Gating Network: Computational
209,2,Training the Gating Network,Computational,machine-translation,7,['O'],['O'],113,0.9338842975206612,208,0.5576407506702413,26,0.9629629629629628,1,0,Training the Gating Network: Computational
211,MULTILINGUAL MACHINE TRANSLATION,Training the Gating Network,Computational,machine-translation,7,"['O', 'O', 'O']","['O', 'O', 'O']",115,0.950413223140496,210,0.5630026809651475,0,0.0,1,0,Training the Gating Network: Computational
212,Results :,Training the Gating Network,Computational,machine-translation,7,"['O', 'O']","['O', 'O']",116,0.9586776859504132,211,0.5656836461126006,1,0.1666666666666666,1,0,Training the Gating Network: Computational
216,The poor performance on English ?,Training the Gating Network,Computational,machine-translation,7,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O']",120,0.9917355371900828,215,0.5764075067024129,5,0.8333333333333334,1,0,Training the Gating Network: Computational
217,"Korean seems to be a result of severe overtraining , as for the rarer language pairs a small number of real examples were highly oversampled in the training corpus .",Training the Gating Network,Computational,machine-translation,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",121,1.0,216,0.579088471849866,6,1.0,1,0,Training the Gating Network: Computational
218,CONCLUSION,,,machine-translation,7,['O'],['O'],0,0.0,217,0.5817694369973191,0,0.0,1,0,
3,abstract,,,machine-translation,8,['O'],['O'],3,0.3333333333333333,2,0.0060422960725075,0,0.0,1,0,
4,Neural machine translation is a recently proposed approach to machine translation .,,,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.4444444444444444,3,0.0090634441087613,1,0.1666666666666666,1,0,
6,The models proposed recently for neural machine translation often belong to a family of encoder - decoders and encode a source sentence into a fixed - length vector from which a decoder generates a translation .,,,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.6666666666666666,5,0.0151057401812688,3,0.5,1,0,
11,"Neural machine translation is a newly emerging approach to machine translation , recently proposed by , and .",INTRODUCTION,INTRODUCTION,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0108695652173913,10,0.0302114803625377,1,0.0232558139534883,1,0,INTRODUCTION
12,"Unlike the traditional phrase - based translation system ( see , e.g. , which consists of many small sub-components that are tuned separately , neural machine translation attempts to build and train a single , large neural network that reads a sentence and outputs a correct translation .",INTRODUCTION,INTRODUCTION,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.0217391304347826,11,0.0332326283987915,2,0.0465116279069767,1,0,INTRODUCTION
13,"Most of the proposed neural machine translation models belong to a family of encoderdecoders , with an encoder and a decoder for each language , or involve a language - specific encoder applied to each sentence whose outputs are then compared ) .",INTRODUCTION,INTRODUCTION,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.0326086956521739,12,0.0362537764350453,3,0.0697674418604651,1,0,INTRODUCTION
15,A decoder then outputs a translation from the encoded vector .,INTRODUCTION,INTRODUCTION,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.0543478260869565,14,0.0422960725075528,5,0.1162790697674418,1,0,INTRODUCTION
17,A potential issue with this encoder - decoder approach is that a neural network needs to be able to compress all the necessary information of a source sentence into a fixed - length vector .,INTRODUCTION,INTRODUCTION,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.0760869565217391,16,0.0483383685800604,7,0.1627906976744186,1,0,INTRODUCTION
18,"This may make it difficult for the neural network to cope with long sentences , especially those that are longer than the sentences in the training corpus .",INTRODUCTION,INTRODUCTION,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.0869565217391304,17,0.0513595166163142,8,0.1860465116279069,1,0,INTRODUCTION
19,showed that indeed the performance of a basic encoder - decoder deteriorates rapidly as the length of an input sentence increases .,INTRODUCTION,INTRODUCTION,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.0978260869565217,18,0.0543806646525679,9,0.2093023255813953,1,0,INTRODUCTION
31,BACKGROUND : NEURAL MACHINE TRANSLATION,INTRODUCTION,,machine-translation,8,"['O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O']",21,0.2282608695652173,30,0.0906344410876132,21,0.4883720930232558,1,0,INTRODUCTION
32,"From a probabilistic perspective , translation is equivalent to finding a target sentence y that maximizes the conditional probability of y given a source sentence x , i.e. , arg max y p ( y | x ) .",INTRODUCTION,BACKGROUND : NEURAL MACHINE TRANSLATION,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.2391304347826087,31,0.093655589123867,22,0.5116279069767442,1,0,INTRODUCTION: BACKGROUND : NEURAL MACHINE TRANSLATION
35,"Recently , a number of papers have proposed the use of neural networks to directly learn this conditional distribution ( see , e.g. , .",INTRODUCTION,BACKGROUND : NEURAL MACHINE TRANSLATION,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.2717391304347826,34,0.1027190332326284,25,0.5813953488372093,1,0,INTRODUCTION: BACKGROUND : NEURAL MACHINE TRANSLATION
36,"This neural machine translation approach typically consists of two components , the first of which encodes a source sentence x and the second decodes to a target sentence y .",INTRODUCTION,BACKGROUND : NEURAL MACHINE TRANSLATION,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.2826086956521739,35,0.1057401812688821,26,0.6046511627906976,1,0,INTRODUCTION: BACKGROUND : NEURAL MACHINE TRANSLATION
37,"For instance , two recurrent neural networks ( RNN ) were used by and to encode a variable - length source sentence into a fixed - length vector and to decode the vector into a variable - length target sentence .",INTRODUCTION,BACKGROUND : NEURAL MACHINE TRANSLATION,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.2934782608695652,36,0.1087613293051359,27,0.627906976744186,1,0,INTRODUCTION: BACKGROUND : NEURAL MACHINE TRANSLATION
41,RNN ENCODER - DECODER,INTRODUCTION,,machine-translation,8,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",31,0.3369565217391304,40,0.120845921450151,31,0.7209302325581395,1,0,INTRODUCTION
43,"In the Encoder - Decoder framework , an encoder reads the input sentence , a sequence of vectors x = ( x 1 , , x Tx ) , into a vector c. 2 The most common approach is to use an RNN such that",INTRODUCTION,RNN ENCODER - DECODER,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",33,0.358695652173913,42,0.1268882175226586,33,0.7674418604651163,1,0,INTRODUCTION: RNN ENCODER - DECODER
44,"( 1 ) and c = q ( {h 1 , , h Tx } ) , where ht ?",INTRODUCTION,RNN ENCODER - DECODER,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",34,0.3695652173913043,43,0.1299093655589124,34,0.7906976744186046,1,0,INTRODUCTION: RNN ENCODER - DECODER
45,"Rn is a hidden state at time t , and c is a vector generated from the sequence of the hidden states .",INTRODUCTION,RNN ENCODER - DECODER,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",35,0.3804347826086957,44,0.1329305135951661,35,0.813953488372093,1,0,INTRODUCTION: RNN ENCODER - DECODER
46,f and q are some nonlinear functions .,INTRODUCTION,RNN ENCODER - DECODER,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",36,0.391304347826087,45,0.1359516616314199,36,0.8372093023255814,1,0,INTRODUCTION: RNN ENCODER - DECODER
47,"used an LSTM as f and q ( {h 1 , , h T }) = h T , for instance .",INTRODUCTION,RNN ENCODER - DECODER,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",37,0.4021739130434782,46,0.1389728096676737,37,0.8604651162790697,1,0,INTRODUCTION: RNN ENCODER - DECODER
48,"The decoder is often trained to predict the next wordy t given the context vector c and all the previously predicted words {y 1 , , y t ?1 }.",INTRODUCTION,RNN ENCODER - DECODER,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",38,0.4130434782608695,47,0.1419939577039275,38,0.8837209302325582,1,0,INTRODUCTION: RNN ENCODER - DECODER
50,"where y = y 1 , , y Ty .",INTRODUCTION,RNN ENCODER - DECODER,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",40,0.4347826086956521,49,0.148036253776435,40,0.9302325581395348,1,0,INTRODUCTION: RNN ENCODER - DECODER
51,"With an RNN , each conditional probability is modeled as",INTRODUCTION,RNN ENCODER - DECODER,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",41,0.4456521739130434,50,0.1510574018126888,41,0.9534883720930232,1,0,INTRODUCTION: RNN ENCODER - DECODER
52,"where g is a nonlinear , potentially multi-layered , function that outputs the probability of y t , and st is the hidden state of the RNN .",INTRODUCTION,RNN ENCODER - DECODER,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",42,0.4565217391304347,51,0.1540785498489426,42,0.9767441860465116,1,0,INTRODUCTION: RNN ENCODER - DECODER
53,It should be noted that other architectures such as a hybrid of an RNN and a de-convolutional neural network can be used .,INTRODUCTION,RNN ENCODER - DECODER,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",43,0.4673913043478261,52,0.1570996978851963,43,1.0,1,0,INTRODUCTION: RNN ENCODER - DECODER
58,x 1 x 2 x 3 x T :,INTRODUCTION,DECODER : GENERAL DESCRIPTION,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",48,0.5217391304347826,57,0.1722054380664652,4,0.1025641025641025,1,0,INTRODUCTION: DECODER : GENERAL DESCRIPTION
60,"Ina new model architecture , we define each conditional probability in Eq .",INTRODUCTION,DECODER : GENERAL DESCRIPTION,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",50,0.5434782608695652,59,0.1782477341389728,6,0.1538461538461538,1,0,INTRODUCTION: DECODER : GENERAL DESCRIPTION
61,( 2 ) as :,INTRODUCTION,DECODER : GENERAL DESCRIPTION,machine-translation,8,"['O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O']",51,0.5543478260869565,60,0.1812688821752265,7,0.1794871794871795,1,0,INTRODUCTION: DECODER : GENERAL DESCRIPTION
62,"where s i is an RNN hidden state for time i , computed by",INTRODUCTION,DECODER : GENERAL DESCRIPTION,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",52,0.5652173913043478,61,0.1842900302114803,8,0.2051282051282051,1,0,INTRODUCTION: DECODER : GENERAL DESCRIPTION
63,It should be noted that unlike the existing encoder - decoder approach ( see Eq.,INTRODUCTION,DECODER : GENERAL DESCRIPTION,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",53,0.5760869565217391,62,0.1873111782477341,9,0.2307692307692307,1,0,INTRODUCTION: DECODER : GENERAL DESCRIPTION
64,"( 2 ) ) , here the probability is conditioned on a distinct context vector c i for each target wordy i .",INTRODUCTION,DECODER : GENERAL DESCRIPTION,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",54,0.5869565217391305,63,0.1903323262839879,10,0.2564102564102564,1,0,INTRODUCTION: DECODER : GENERAL DESCRIPTION
65,"The context vector c i depends on a sequence of annotations ( h 1 , , h Tx ) to which an encoder maps the input sentence .",INTRODUCTION,DECODER : GENERAL DESCRIPTION,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",55,0.5978260869565217,64,0.1933534743202417,11,0.282051282051282,1,0,INTRODUCTION: DECODER : GENERAL DESCRIPTION
66,Each annotation hi contains information about the whole input sequence with a strong focus on the parts surrounding the i - th word of the input sequence .,INTRODUCTION,DECODER : GENERAL DESCRIPTION,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",56,0.6086956521739131,65,0.1963746223564954,12,0.3076923076923077,1,0,INTRODUCTION: DECODER : GENERAL DESCRIPTION
68,"The context vector c i is , then , computed as a weighted sum of these annotations hi :",INTRODUCTION,DECODER : GENERAL DESCRIPTION,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",58,0.6304347826086957,67,0.202416918429003,14,0.358974358974359,1,0,INTRODUCTION: DECODER : GENERAL DESCRIPTION
69,The weight ?,INTRODUCTION,DECODER : GENERAL DESCRIPTION,machine-translation,8,"['O', 'O', 'O']","['O', 'O', 'O']",59,0.6413043478260869,68,0.2054380664652568,15,0.3846153846153846,1,0,INTRODUCTION: DECODER : GENERAL DESCRIPTION
70,ij of each annotation h j is computed by,INTRODUCTION,DECODER : GENERAL DESCRIPTION,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",60,0.6521739130434783,69,0.2084592145015106,16,0.4102564102564102,1,0,INTRODUCTION: DECODER : GENERAL DESCRIPTION
71,"where e ij = a (s i?1 , h j ) is an alignment model which scores how well the inputs around position j and the output at position i match .",INTRODUCTION,DECODER : GENERAL DESCRIPTION,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",61,0.6630434782608695,70,0.2114803625377643,17,0.4358974358974359,1,0,INTRODUCTION: DECODER : GENERAL DESCRIPTION
72,"The score is based on the RNN hidden state s i ?1 ( just before emitting y i , Eq. ( 4 ) ) and the j - th annotation h j of the input sentence .",INTRODUCTION,DECODER : GENERAL DESCRIPTION,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",62,0.6739130434782609,71,0.2145015105740181,18,0.4615384615384615,1,0,INTRODUCTION: DECODER : GENERAL DESCRIPTION
74,"Note that unlike in traditional machine translation , the alignment is not considered to be a latent variable .",INTRODUCTION,DECODER : GENERAL DESCRIPTION,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",64,0.6956521739130435,73,0.2205438066465256,20,0.5128205128205128,1,0,INTRODUCTION: DECODER : GENERAL DESCRIPTION
77,"We can understand the approach of taking a weighted sum of all the annotations as computing an expected annotation , where the expectation is over possible alignments .",INTRODUCTION,DECODER : GENERAL DESCRIPTION,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",67,0.7282608695652174,76,0.229607250755287,23,0.5897435897435898,1,0,INTRODUCTION: DECODER : GENERAL DESCRIPTION
78,Let ?,INTRODUCTION,DECODER : GENERAL DESCRIPTION,machine-translation,8,"['O', 'O']","['O', 'O']",68,0.7391304347826086,77,0.2326283987915408,24,0.6153846153846154,1,0,INTRODUCTION: DECODER : GENERAL DESCRIPTION
79,"ij be a probability that the target wordy i is aligned to , or translated from , a source word x j .",INTRODUCTION,DECODER : GENERAL DESCRIPTION,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",69,0.75,78,0.2356495468277945,25,0.6410256410256411,1,0,INTRODUCTION: DECODER : GENERAL DESCRIPTION
80,"Then , the i - th context vector c i is the expected annotation overall the annotations with probabilities ? ij .",INTRODUCTION,DECODER : GENERAL DESCRIPTION,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",70,0.7608695652173914,79,0.2386706948640483,26,0.6666666666666666,1,0,INTRODUCTION: DECODER : GENERAL DESCRIPTION
82,"ij , or it s associated energy e ij , reflects the importance of the annotation h j with respect to the previous hidden state s i ?1 in deciding the next state s i and generating y i .",INTRODUCTION,DECODER : GENERAL DESCRIPTION,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",72,0.782608695652174,81,0.2447129909365558,28,0.717948717948718,1,0,INTRODUCTION: DECODER : GENERAL DESCRIPTION
83,"Intuitively , this implements a mechanism of attention in the decoder .",INTRODUCTION,DECODER : GENERAL DESCRIPTION,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",73,0.7934782608695652,82,0.2477341389728096,29,0.7435897435897436,1,0,INTRODUCTION: DECODER : GENERAL DESCRIPTION
88,"The usual RNN , described in Eq. ( 1 ) , reads an input sequence x in order starting from the first symbol x 1 to the last one x Tx .",INTRODUCTION,ENCODER : BIDIRECTIONAL RNN FOR ANNOTATING SEQUENCES,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",78,0.8478260869565217,87,0.2628398791540785,34,0.8717948717948718,1,0,INTRODUCTION: ENCODER : BIDIRECTIONAL RNN FOR ANNOTATING SEQUENCES
89,"However , in the proposed scheme , we would like the annotation of each word to summarize not only the preceding words , but also the following words .",INTRODUCTION,ENCODER : BIDIRECTIONAL RNN FOR ANNOTATING SEQUENCES,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",79,0.8586956521739131,88,0.2658610271903323,35,0.8974358974358975,1,0,INTRODUCTION: ENCODER : BIDIRECTIONAL RNN FOR ANNOTATING SEQUENCES
90,"Hence , we propose to use a bidirectional RNN ( BiRNN , , which has been successfully used recently in speech recognition ( see , e.g. , .",INTRODUCTION,ENCODER : BIDIRECTIONAL RNN FOR ANNOTATING SEQUENCES,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",80,0.8695652173913043,89,0.2688821752265861,36,0.9230769230769232,1,0,INTRODUCTION: ENCODER : BIDIRECTIONAL RNN FOR ANNOTATING SEQUENCES
91,A BiRNN consists of forward and backward RNN 's .,INTRODUCTION,ENCODER : BIDIRECTIONAL RNN FOR ANNOTATING SEQUENCES,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",81,0.8804347826086957,90,0.2719033232628399,37,0.9487179487179488,1,0,INTRODUCTION: ENCODER : BIDIRECTIONAL RNN FOR ANNOTATING SEQUENCES
92,The forward RNN ? ?,INTRODUCTION,ENCODER : BIDIRECTIONAL RNN FOR ANNOTATING SEQUENCES,machine-translation,8,"['O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O']",82,0.8913043478260869,91,0.2749244712990936,38,0.9743589743589745,1,0,INTRODUCTION: ENCODER : BIDIRECTIONAL RNN FOR ANNOTATING SEQUENCES
93,f reads the input sequence as it is ordered ( from x 1 to x Tx ) and calculates a sequence of forward hidden states (,INTRODUCTION,ENCODER : BIDIRECTIONAL RNN FOR ANNOTATING SEQUENCES,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",83,0.9021739130434784,92,0.2779456193353474,39,1.0,1,0,INTRODUCTION: ENCODER : BIDIRECTIONAL RNN FOR ANNOTATING SEQUENCES
94,The backward RNN,INTRODUCTION,ENCODER : BIDIRECTIONAL RNN FOR ANNOTATING SEQUENCES,machine-translation,8,"['O', 'O', 'O']","['O', 'O', 'O']",84,0.9130434782608696,93,0.2809667673716012,0,0.0,1,0,INTRODUCTION: ENCODER : BIDIRECTIONAL RNN FOR ANNOTATING SEQUENCES
95,? ?,INTRODUCTION,ENCODER : BIDIRECTIONAL RNN FOR ANNOTATING SEQUENCES,machine-translation,8,"['O', 'O']","['O', 'O']",85,0.9239130434782608,94,0.283987915407855,1,0.125,1,0,INTRODUCTION: ENCODER : BIDIRECTIONAL RNN FOR ANNOTATING SEQUENCES
96,"f reads the sequence in the reverse order ( from x Tx to x 1 ) , resulting in a sequence of backward hidden states (",INTRODUCTION,ENCODER : BIDIRECTIONAL RNN FOR ANNOTATING SEQUENCES,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",86,0.9347826086956522,95,0.2870090634441087,2,0.25,1,0,INTRODUCTION: ENCODER : BIDIRECTIONAL RNN FOR ANNOTATING SEQUENCES
97,We obtain an annotation for each word x j by concatenating the forward hidden state ? ?,INTRODUCTION,ENCODER : BIDIRECTIONAL RNN FOR ANNOTATING SEQUENCES,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",87,0.9456521739130436,96,0.2900302114803625,3,0.375,1,0,INTRODUCTION: ENCODER : BIDIRECTIONAL RNN FOR ANNOTATING SEQUENCES
98,h j and the backward one,INTRODUCTION,ENCODER : BIDIRECTIONAL RNN FOR ANNOTATING SEQUENCES,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O']",88,0.9565217391304348,97,0.2930513595166163,4,0.5,1,0,INTRODUCTION: ENCODER : BIDIRECTIONAL RNN FOR ANNOTATING SEQUENCES
99,"In this way , the annotation h j contains the summaries of both the preceding words and the following words .",INTRODUCTION,ENCODER : BIDIRECTIONAL RNN FOR ANNOTATING SEQUENCES,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",89,0.967391304347826,98,0.29607250755287,5,0.625,1,0,INTRODUCTION: ENCODER : BIDIRECTIONAL RNN FOR ANNOTATING SEQUENCES
100,"Due to the tendency of RNNs to better represent recent inputs , the annotation h j will be focused on the words around x j .",INTRODUCTION,ENCODER : BIDIRECTIONAL RNN FOR ANNOTATING SEQUENCES,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",90,0.9782608695652174,99,0.2990936555891239,6,0.75,1,0,INTRODUCTION: ENCODER : BIDIRECTIONAL RNN FOR ANNOTATING SEQUENCES
101,This sequence of annotations is used by the decoder and the alignment model later to compute the context vector ( Eqs. ( 5 ) - ) .,INTRODUCTION,ENCODER : BIDIRECTIONAL RNN FOR ANNOTATING SEQUENCES,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",91,0.9891304347826086,100,0.3021148036253776,7,0.875,1,0,INTRODUCTION: ENCODER : BIDIRECTIONAL RNN FOR ANNOTATING SEQUENCES
103,EXPERIMENT SETTINGS,,,machine-translation,8,"['O', 'O']","['O', 'O']",0,0.0,102,0.3081570996978852,0,0.0,1,0,
106,3,EXPERIMENT SETTINGS,EXPERIMENT SETTINGS,machine-translation,8,['O'],['O'],3,0.2307692307692307,105,0.3172205438066465,3,0.5,1,0,EXPERIMENT SETTINGS
109,4,EXPERIMENT SETTINGS,EXPERIMENT SETTINGS,machine-translation,8,['O'],['O'],6,0.4615384615384615,108,0.3262839879154078,6,1.0,1,0,EXPERIMENT SETTINGS
110,DATASET,EXPERIMENT SETTINGS,,machine-translation,8,['O'],['O'],7,0.5384615384615384,109,0.3293051359516616,0,0.0,1,0,EXPERIMENT SETTINGS
111,"WMT ' 14 contains the following English - French parallel corpora : Europarl ( 61 M words ) , news commentary ( 5.5 M ) , UN ( 421M ) and two crawled corpora of 90 M and 272.5 M words respectively , totaling 850M words .",EXPERIMENT SETTINGS,DATASET,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.6153846153846154,110,0.3323262839879154,1,0.1666666666666666,1,0,EXPERIMENT SETTINGS: DATASET
112,"Following the procedure described in , we reduce the size of the combined corpus to have 348M words using the data selection method by .",EXPERIMENT SETTINGS,DATASET,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.6923076923076923,111,0.3353474320241691,2,0.3333333333333333,1,0,EXPERIMENT SETTINGS: DATASET
113,"We do not use any monolingual data other than the mentioned parallel corpora , although it maybe possible to use a much larger monolingual corpus to pretrain an encoder .",EXPERIMENT SETTINGS,DATASET,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.7692307692307693,112,0.338368580060423,3,0.5,1,0,EXPERIMENT SETTINGS: DATASET
115,Any word not included in the shortlist is mapped to a special token ( [ UNK ] ) .,EXPERIMENT SETTINGS,DATASET,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.9230769230769232,114,0.3444108761329305,5,0.8333333333333334,1,0,EXPERIMENT SETTINGS: DATASET
116,"We do not apply any other special preprocessing , such as lowercasing or stemming , to the data .",EXPERIMENT SETTINGS,DATASET,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,1.0,115,0.3474320241691843,6,1.0,1,0,EXPERIMENT SETTINGS: DATASET
117,MODELS,,,machine-translation,8,['O'],['O'],0,0.0,116,0.3504531722054381,0,0.0,1,0,
119,"The first one is an RNN Encoder - Decoder ( RNNencdec , , and the other is the proposed model , to which we refer as RNNsearch .",MODELS,MODELS,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.1538461538461538,118,0.3564954682779456,2,0.1538461538461538,1,0,MODELS
128,"Once a model is trained , we use abeam search to find a translation that approximately maximizes the conditional probability ( see , e.g. , .",MODELS,MODELS,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.8461538461538461,127,0.3836858006042296,11,0.8461538461538461,1,0,MODELS
129,used this approach to generate translations from their neural machine translation model .,MODELS,MODELS,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.9230769230769232,128,0.3867069486404834,12,0.9230769230769232,1,0,MODELS
131,RESULTS,,,machine-translation,8,['O'],['O'],0,0.0,130,0.3927492447129909,0,0.0,1,0,
132,QUANTITATIVE RESULTS,,,machine-translation,8,"['O', 'O']","['O', 'O']",0,0.0,131,0.3957703927492447,0,0.0,1,0,
135,Each pixel shows the weight ?,QUANTITATIVE RESULTS,QUANTITATIVE RESULTS,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O']",3,0.0461538461538461,134,0.404833836858006,3,0.2307692307692307,1,0,QUANTITATIVE RESULTS
136,"ij of the annotation of the j - th source word for the i - th target word ( see Eq. ) , in grayscale ( 0 : black , 1 : white ) .",QUANTITATIVE RESULTS,QUANTITATIVE RESULTS,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.0615384615384615,135,0.4078549848942598,4,0.3076923076923077,1,0,QUANTITATIVE RESULTS
140,We conjectured that this limitation may make the basic encoder - decoder approach to underperform with long sentences .,QUANTITATIVE RESULTS,QUANTITATIVE RESULTS,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.123076923076923,139,0.4199395770392749,8,0.6153846153846154,1,0,QUANTITATIVE RESULTS
145,tokens when only the sentences having no unknown words were evaluated ( last column ) .,QUANTITATIVE RESULTS,QUANTITATIVE RESULTS,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.2,144,0.4350453172205438,13,1.0,1,0,QUANTITATIVE RESULTS
147,ALIGNMENT,QUANTITATIVE RESULTS,,machine-translation,8,['O'],['O'],15,0.2307692307692307,146,0.4410876132930513,0,0.0,1,0,QUANTITATIVE RESULTS
150,"ij from Eq. , as in .",QUANTITATIVE RESULTS,ALIGNMENT,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.2769230769230769,149,0.4501510574018127,3,0.2727272727272727,1,0,QUANTITATIVE RESULTS: ALIGNMENT
152,From this we see which positions in the source sentence were considered more important when generating the target word .,QUANTITATIVE RESULTS,ALIGNMENT,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.3076923076923077,151,0.4561933534743202,5,0.4545454545454545,1,0,QUANTITATIVE RESULTS: ALIGNMENT
153,We can see from the alignments in that the alignment of words between English and French is largely monotonic .,QUANTITATIVE RESULTS,ALIGNMENT,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.3230769230769231,152,0.459214501510574,6,0.5454545454545454,1,0,QUANTITATIVE RESULTS: ALIGNMENT
156,"Adjectives and nouns are typically ordered differently between French and English , and we see an example in The strength of the soft - alignment , opposed to a hard - alignment , is evident , for instance , from ].",QUANTITATIVE RESULTS,ALIGNMENT,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.3692307692307692,155,0.4682779456193353,9,0.8181818181818182,1,0,QUANTITATIVE RESULTS: ALIGNMENT
158,"An additional benefit of the soft alignment is that it naturally deals with source and target phrases of different lengths , without requiring a counter - intuitive way of mapping some words to or from nowhere ( [ NULL ] ) ( see , e.g. , Chapters 4 and 5 of .",QUANTITATIVE RESULTS,ALIGNMENT,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.4,157,0.4743202416918429,11,1.0,1,0,QUANTITATIVE RESULTS: ALIGNMENT
161,"This is likely due to the fact that the RNNsearch does not require encoding along sentence into a fixed - length vector perfectly , but only accurately encoding the parts of the input sentence that surround a particular word .",QUANTITATIVE RESULTS,LONG SENTENCES,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",29,0.4461538461538462,160,0.4833836858006042,2,0.1052631578947368,1,0,QUANTITATIVE RESULTS: LONG SENTENCES
162,"As an example , consider this source sentence from the test set :",QUANTITATIVE RESULTS,LONG SENTENCES,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",30,0.4615384615384615,161,0.486404833836858,3,0.1578947368421052,1,0,QUANTITATIVE RESULTS: LONG SENTENCES
163,"An admitting privilege is the right of a doctor to admit a patient to a hospital or a medical centre to carryout a diagnosis or a procedure , based on his status as a healthcare worker at a hospital .",QUANTITATIVE RESULTS,LONG SENTENCES,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",31,0.4769230769230769,162,0.4894259818731117,4,0.2105263157894736,1,0,QUANTITATIVE RESULTS: LONG SENTENCES
164,The RNNencdec - 50 translated this sentence into :,QUANTITATIVE RESULTS,LONG SENTENCES,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",32,0.4923076923076923,163,0.4924471299093655,5,0.2631578947368421,1,0,QUANTITATIVE RESULTS: LONG SENTENCES
165,Un privilge d'admission est le droit d'un mdecin de reconnatre un patient l'hpital ou un centre mdical d'un diagnostic ou de prendre un diagnostic en fonction de sontat de sant .,QUANTITATIVE RESULTS,LONG SENTENCES,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",33,0.5076923076923077,164,0.4954682779456193,6,0.3157894736842105,1,0,QUANTITATIVE RESULTS: LONG SENTENCES
168,Let us consider another sentence from the test set :,QUANTITATIVE RESULTS,LONG SENTENCES,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",36,0.5538461538461539,167,0.5045317220543807,9,0.4736842105263157,1,0,QUANTITATIVE RESULTS: LONG SENTENCES
170,The translation by the RNNencdec - 50 is,QUANTITATIVE RESULTS,LONG SENTENCES,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",38,0.5846153846153846,169,0.5105740181268882,11,0.5789473684210527,1,0,QUANTITATIVE RESULTS: LONG SENTENCES
171,"Ce type d'exprience fait partie des initiatives du Disney pour "" prolonger la dure de vie de ses nouvelles et de dvelopper des liens avec les lecteurs numriques qui deviennent plus complexes .",QUANTITATIVE RESULTS,LONG SENTENCES,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",39,0.6,170,0.513595166163142,12,0.631578947368421,1,0,QUANTITATIVE RESULTS: LONG SENTENCES
172,"As with the previous example , the RNNencdec began deviating from the actual meaning of the source sentence after generating approximately 30 words ( see the underlined phrase ) .",QUANTITATIVE RESULTS,LONG SENTENCES,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",40,0.6153846153846154,171,0.5166163141993958,13,0.6842105263157895,1,0,QUANTITATIVE RESULTS: LONG SENTENCES
173,"After that point , the quality of the translation deteriorates , with basic mistakes such as the lack of a closing quotation mark .",QUANTITATIVE RESULTS,LONG SENTENCES,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",41,0.6307692307692307,172,0.5196374622356495,14,0.7368421052631579,1,0,QUANTITATIVE RESULTS: LONG SENTENCES
180,A similar approach of aligning an output symbol with an input symbol was proposed recently by in the context of handwriting synthesis .,QUANTITATIVE RESULTS,LEARNING TO ALIGN,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",48,0.7384615384615385,179,0.540785498489426,1,0.1111111111111111,1,0,QUANTITATIVE RESULTS: LEARNING TO ALIGN
182,"In his work , he used a mixture of Gaussian kernels to compute the weights of the annotations , where the location , width and mixture coefficient of each kernel was predicted from an alignment model .",QUANTITATIVE RESULTS,LEARNING TO ALIGN,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",50,0.7692307692307693,181,0.5468277945619335,3,0.3333333333333333,1,0,QUANTITATIVE RESULTS: LEARNING TO ALIGN
183,"More specifically , his alignment was restricted to predict the location such that the location increases monotonically .",QUANTITATIVE RESULTS,LEARNING TO ALIGN,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",51,0.7846153846153846,182,0.5498489425981873,4,0.4444444444444444,1,0,QUANTITATIVE RESULTS: LEARNING TO ALIGN
184,"The main difference from our approach is that , in , the modes of the weights of the annotations only move in one direction .",QUANTITATIVE RESULTS,LEARNING TO ALIGN,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",52,0.8,183,0.552870090634441,5,0.5555555555555556,1,0,QUANTITATIVE RESULTS: LEARNING TO ALIGN
185,"In the context of machine translation , this is a severe limitation , as ( long - distance ) reordering is often needed to generate a grammatically correct translation ( for instance , English - to - German ) .",QUANTITATIVE RESULTS,LEARNING TO ALIGN,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",53,0.8153846153846154,184,0.5558912386706949,6,0.6666666666666666,1,0,QUANTITATIVE RESULTS: LEARNING TO ALIGN
187,This drawback is not severe with the task of translation in which most of input and output sentences are only 15 - 40 words .,QUANTITATIVE RESULTS,LEARNING TO ALIGN,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",55,0.8461538461538461,186,0.5619335347432024,8,0.8888888888888888,1,0,QUANTITATIVE RESULTS: LEARNING TO ALIGN
188,"However , this may limit the applicability of the proposed scheme to other tasks .",QUANTITATIVE RESULTS,LEARNING TO ALIGN,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",56,0.8615384615384616,187,0.5649546827794562,9,1.0,1,0,QUANTITATIVE RESULTS: LEARNING TO ALIGN
190,"Since introduced a neural probabilistic language model which uses a neural network to model the conditional probability of a word given a fixed number of the preceding words , neural networks have widely been used in machine translation .",QUANTITATIVE RESULTS,NEURAL NETWORKS FOR MACHINE TRANSLATION,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",58,0.8923076923076924,189,0.5709969788519638,1,0.125,1,0,QUANTITATIVE RESULTS: NEURAL NETWORKS FOR MACHINE TRANSLATION
191,"However , the role of neural networks has been largely limited to simply providing a single feature to an existing statistical machine translation system or to re-rank a list of candidate translations provided by an existing system .",QUANTITATIVE RESULTS,NEURAL NETWORKS FOR MACHINE TRANSLATION,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",59,0.9076923076923076,190,0.5740181268882175,2,0.25,1,0,QUANTITATIVE RESULTS: NEURAL NETWORKS FOR MACHINE TRANSLATION
192,"For instance , proposed using a feedforward neural network to compute the score of a pair of source and target phrases and to use the score as an additional feature in the phrase - based statistical machine translation system .",QUANTITATIVE RESULTS,NEURAL NETWORKS FOR MACHINE TRANSLATION,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",60,0.9230769230769232,191,0.5770392749244713,3,0.375,1,0,QUANTITATIVE RESULTS: NEURAL NETWORKS FOR MACHINE TRANSLATION
193,"More recently , and reported the successful use of the neural networks as a sub-component of the existing translation system .",QUANTITATIVE RESULTS,NEURAL NETWORKS FOR MACHINE TRANSLATION,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",61,0.9384615384615383,192,0.5800604229607251,4,0.5,1,0,QUANTITATIVE RESULTS: NEURAL NETWORKS FOR MACHINE TRANSLATION
194,"Traditionally , a neural network trained as a target - side language model has been used to rescore or rerank a list of candidate translations ( see , e.g. , .",QUANTITATIVE RESULTS,NEURAL NETWORKS FOR MACHINE TRANSLATION,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",62,0.953846153846154,193,0.5830815709969789,5,0.625,1,0,QUANTITATIVE RESULTS: NEURAL NETWORKS FOR MACHINE TRANSLATION
195,"Although the above approaches were shown to improve the translation performance over the stateof - the - art machine translation systems , we are more interested in a more ambitious objective of designing a completely new translation system based on neural networks .",QUANTITATIVE RESULTS,NEURAL NETWORKS FOR MACHINE TRANSLATION,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",63,0.9692307692307692,194,0.5861027190332326,6,0.75,1,0,QUANTITATIVE RESULTS: NEURAL NETWORKS FOR MACHINE TRANSLATION
196,The neural machine translation approach we consider in this paper is therefore a radical departure from these earlier works .,QUANTITATIVE RESULTS,NEURAL NETWORKS FOR MACHINE TRANSLATION,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",64,0.9846153846153848,195,0.5891238670694864,7,0.875,1,0,QUANTITATIVE RESULTS: NEURAL NETWORKS FOR MACHINE TRANSLATION
198,CONCLUSION,,,machine-translation,8,['O'],['O'],0,0.0,197,0.595166163141994,0,0.0,1,0,
216,"The proposed scheme in Section 3 is a general framework where one can freely define , for instance , the activation functions f of recurrent neural networks ( RNN ) and the alignment model a .",A MODEL ARCHITECTURE,A.1 ARCHITECTURAL CHOICES,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.0833333333333333,215,0.649546827794562,2,0.6666666666666666,1,0,A MODEL ARCHITECTURE: A.1 ARCHITECTURAL CHOICES
219,"For the activation function f of an RNN , we use the gated hidden unit recently proposed by .",A MODEL ARCHITECTURE,A.1.1 RECURRENT NEURAL NETWORK,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.2083333333333333,218,0.6586102719033232,1,0.05,1,0,A MODEL ARCHITECTURE: A.1.1 RECURRENT NEURAL NETWORK
220,The gated hidden unit is an alternative to the conventional simple units such as an element - wise tanh .,A MODEL ARCHITECTURE,A.1.1 RECURRENT NEURAL NETWORK,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.25,219,0.6616314199395771,2,0.1,1,0,A MODEL ARCHITECTURE: A.1.1 RECURRENT NEURAL NETWORK
221,"This gated unit is similar to along short - term memory ( LSTM ) unit proposed earlier by , sharing with it the ability to better model and learn long - term dependencies .",A MODEL ARCHITECTURE,A.1.1 RECURRENT NEURAL NETWORK,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.2916666666666667,220,0.6646525679758308,3,0.15,1,0,A MODEL ARCHITECTURE: A.1.1 RECURRENT NEURAL NETWORK
222,This is made possible by having computation paths in the unfolded RNN for which the product of derivatives is close to 1 .,A MODEL ARCHITECTURE,A.1.1 RECURRENT NEURAL NETWORK,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.3333333333333333,221,0.6676737160120846,4,0.2,1,0,A MODEL ARCHITECTURE: A.1.1 RECURRENT NEURAL NETWORK
224,"It is therefore possible to use LSTM units instead of the gated hidden unit described here , as was done in a similar context by .",A MODEL ARCHITECTURE,A.1.1 RECURRENT NEURAL NETWORK,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.4166666666666667,223,0.6737160120845922,6,0.3,1,0,A MODEL ARCHITECTURE: A.1.1 RECURRENT NEURAL NETWORK
225,The new state s i of the RNN employing n gated hidden units 8 is computed by,A MODEL ARCHITECTURE,A.1.1 RECURRENT NEURAL NETWORK,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.4583333333333333,224,0.676737160120846,7,0.35,1,0,A MODEL ARCHITECTURE: A.1.1 RECURRENT NEURAL NETWORK
226,"where is an element - wise multiplication , and z i is the output of the update gates ( see below ) .",A MODEL ARCHITECTURE,A.1.1 RECURRENT NEURAL NETWORK,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.5,225,0.6797583081570997,8,0.4,1,0,A MODEL ARCHITECTURE: A.1.1 RECURRENT NEURAL NETWORK
227,The proposed updated states i is computed b ?,A MODEL ARCHITECTURE,A.1.1 RECURRENT NEURAL NETWORK,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.5416666666666666,226,0.6827794561933535,9,0.45,1,0,A MODEL ARCHITECTURE: A.1.1 RECURRENT NEURAL NETWORK
228,where e ( y,A MODEL ARCHITECTURE,A.1.1 RECURRENT NEURAL NETWORK,machine-translation,8,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",14,0.5833333333333334,227,0.6858006042296072,10,0.5,1,0,A MODEL ARCHITECTURE: A.1.1 RECURRENT NEURAL NETWORK
230,"R m is an m-dimensional embedding of a wordy i ?1 , and r i is the output of the reset gates ( see below ) .",A MODEL ARCHITECTURE,A.1.1 RECURRENT NEURAL NETWORK,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.6666666666666666,229,0.6918429003021148,12,0.6,1,0,A MODEL ARCHITECTURE: A.1.1 RECURRENT NEURAL NETWORK
231,"When y i is represented as a 1 - of - K vector , e ( y i ) is simply a column of an embedding matrix E ?",A MODEL ARCHITECTURE,A.1.1 RECURRENT NEURAL NETWORK,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.7083333333333334,230,0.6948640483383686,13,0.65,1,0,A MODEL ARCHITECTURE: A.1.1 RECURRENT NEURAL NETWORK
232,R mK .,A MODEL ARCHITECTURE,A.1.1 RECURRENT NEURAL NETWORK,machine-translation,8,"['O', 'O', 'O']","['O', 'O', 'O']",18,0.75,231,0.6978851963746223,14,0.7,1,0,A MODEL ARCHITECTURE: A.1.1 RECURRENT NEURAL NETWORK
234,"The update gates z i allow each hidden unit to maintain its previous activation , and the reset gates r i control how much and what information from the previous state should be reset .",A MODEL ARCHITECTURE,A.1.1 RECURRENT NEURAL NETWORK,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.8333333333333334,233,0.7039274924471299,16,0.8,1,0,A MODEL ARCHITECTURE: A.1.1 RECURRENT NEURAL NETWORK
235,We compute them by,A MODEL ARCHITECTURE,A.1.1 RECURRENT NEURAL NETWORK,machine-translation,8,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",21,0.875,234,0.7069486404833837,17,0.85,1,0,A MODEL ARCHITECTURE: A.1.1 RECURRENT NEURAL NETWORK
236,where ? ( ) is a logistic sigmoid function .,A MODEL ARCHITECTURE,A.1.1 RECURRENT NEURAL NETWORK,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.9166666666666666,235,0.7099697885196374,18,0.9,1,0,A MODEL ARCHITECTURE: A.1.1 RECURRENT NEURAL NETWORK
239,A.1.2 ALIGNMENT MODEL,,,machine-translation,8,"['O', 'O', 'O']","['O', 'O', 'O']",0,0.0,238,0.7190332326283988,0,0.0,1,0,
240,The alignment model should be designed considering that the model needs to be evaluated T x Ty times for each sentence pair of lengths T x and Ty .,A.1.2 ALIGNMENT MODEL,A.1.2 ALIGNMENT MODEL,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0204081632653061,239,0.7220543806646526,1,0.125,1,0,A.1.2 ALIGNMENT MODEL
241,"In order to reduce computation , we use a singlelayer multilayer perceptron such that",A.1.2 ALIGNMENT MODEL,A.1.2 ALIGNMENT MODEL,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.0408163265306122,240,0.7250755287009063,2,0.25,1,0,A.1.2 ALIGNMENT MODEL
242,where W a ?,A.1.2 ALIGNMENT MODEL,A.1.2 ALIGNMENT MODEL,machine-translation,8,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",3,0.0612244897959183,241,0.7280966767371602,3,0.375,1,0,A.1.2 ALIGNMENT MODEL
243,"R nn , U a ?",A.1.2 ALIGNMENT MODEL,A.1.2 ALIGNMENT MODEL,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O']",4,0.0816326530612244,242,0.7311178247734139,4,0.5,1,0,A.1.2 ALIGNMENT MODEL
245,Rn are the weight matrices .,A.1.2 ALIGNMENT MODEL,A.1.2 ALIGNMENT MODEL,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O']",6,0.1224489795918367,244,0.7371601208459214,6,0.75,1,0,A.1.2 ALIGNMENT MODEL
246,Since,A.1.2 ALIGNMENT MODEL,A.1.2 ALIGNMENT MODEL,machine-translation,8,['O'],['O'],7,0.1428571428571428,245,0.7401812688821753,7,0.875,1,0,A.1.2 ALIGNMENT MODEL
247,"U ah j does not depend on i , we can pre-compute it in advance to minimize the computational cost .",A.1.2 ALIGNMENT MODEL,A.1.2 ALIGNMENT MODEL,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.1632653061224489,246,0.743202416918429,8,1.0,1,0,A.1.2 ALIGNMENT MODEL
251,"From hereon , we omit all bias terms in order to increase readability .",A.1.2 ALIGNMENT MODEL,A.2.1 ENCODER,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.2448979591836734,250,0.7552870090634441,3,0.2142857142857142,1,0,A.1.2 ALIGNMENT MODEL: A.2.1 ENCODER
253,"and outputs a translated sentence of 1 - of - K coded word vectors y = ( y 1 , . . . , y Ty ) , y i ? R Ky ,",A.1.2 ALIGNMENT MODEL,A.2.1 ENCODER,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.2857142857142857,252,0.7613293051359517,5,0.3571428571428571,1,0,A.1.2 ALIGNMENT MODEL: A.2.1 ENCODER
254,"where K x and Ky are the vocabulary sizes of source and target languages , respectively .",A.1.2 ALIGNMENT MODEL,A.2.1 ENCODER,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.3061224489795918,253,0.7643504531722054,6,0.4285714285714285,1,0,A.1.2 ALIGNMENT MODEL: A.2.1 ENCODER
255,T x and Ty respectively denote the lengths of source and target sentences .,A.1.2 ALIGNMENT MODEL,A.2.1 ENCODER,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.3265306122448979,254,0.7673716012084593,7,0.5,1,0,A.1.2 ALIGNMENT MODEL: A.2.1 ENCODER
256,"First , the forward states of the bidirectional recurrent neural network ( BiRNN ) are computed :",A.1.2 ALIGNMENT MODEL,A.2.1 ENCODER,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.3469387755102041,255,0.770392749244713,8,0.5714285714285714,1,0,A.1.2 ALIGNMENT MODEL: A.2.1 ENCODER
257,are weight matrices .,A.1.2 ALIGNMENT MODEL,A.2.1 ENCODER,machine-translation,8,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",18,0.3673469387755102,256,0.7734138972809668,9,0.6428571428571429,1,0,A.1.2 ALIGNMENT MODEL: A.2.1 ENCODER
259,"The backward states ( ? ? h 1 , , ? ? h Tx ) are computed similarly .",A.1.2 ALIGNMENT MODEL,A.2.1 ENCODER,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.4081632653061224,258,0.7794561933534743,11,0.7857142857142857,1,0,A.1.2 ALIGNMENT MODEL: A.2.1 ENCODER
260,"We share the word embedding matrix E between the forward and backward RNNs , unlike the weight matrices .",A.1.2 ALIGNMENT MODEL,A.2.1 ENCODER,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.4285714285714285,259,0.7824773413897281,12,0.8571428571428571,1,0,A.1.2 ALIGNMENT MODEL: A.2.1 ENCODER
261,"We concatenate the forward and backward states to to obtain the annotations ( h 1 , h 2 , , h Tx ) , where",A.1.2 ALIGNMENT MODEL,A.2.1 ENCODER,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.4489795918367347,260,0.7854984894259819,13,0.9285714285714286,1,0,A.1.2 ALIGNMENT MODEL: A.2.1 ENCODER
262,A.,A.1.2 ALIGNMENT MODEL,,machine-translation,8,['O'],['O'],23,0.4693877551020408,261,0.7885196374622356,14,1.0,1,0,A.1.2 ALIGNMENT MODEL
263,DECODER,A.1.2 ALIGNMENT MODEL,,machine-translation,8,['O'],['O'],24,0.4897959183673469,262,0.7915407854984894,0,0.0,1,0,A.1.2 ALIGNMENT MODEL
264,The hidden state s i of the decoder given the annotations from the encoder is computed by,A.1.2 ALIGNMENT MODEL,DECODER,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.5102040816326531,263,0.7945619335347432,1,0.04,1,0,A.1.2 ALIGNMENT MODEL: DECODER
265,E is the word embedding matrix for the target language .,A.1.2 ALIGNMENT MODEL,DECODER,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.5306122448979592,264,0.797583081570997,2,0.08,1,0,A.1.2 ALIGNMENT MODEL: DECODER
266,"W , W z , W r ?",A.1.2 ALIGNMENT MODEL,DECODER,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.5510204081632653,265,0.8006042296072508,3,0.12,1,0,A.1.2 ALIGNMENT MODEL: DECODER
267,"R nm , U , U z , Ur ? R nn , and C , C z , Cr ?",A.1.2 ALIGNMENT MODEL,DECODER,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",28,0.5714285714285714,266,0.8036253776435045,4,0.16,1,0,A.1.2 ALIGNMENT MODEL: DECODER
268,R n 2n are weights .,A.1.2 ALIGNMENT MODEL,DECODER,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O']",29,0.5918367346938775,267,0.8066465256797583,5,0.2,1,0,A.1.2 ALIGNMENT MODEL: DECODER
269,"Again , m and n are the word embedding dimensionality and the number of hidden units , respectively .",A.1.2 ALIGNMENT MODEL,DECODER,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",30,0.6122448979591837,268,0.8096676737160121,6,0.24,1,0,A.1.2 ALIGNMENT MODEL: DECODER
270,The initial hidden state s 0 is computed by,A.1.2 ALIGNMENT MODEL,DECODER,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",31,0.6326530612244898,269,0.8126888217522659,7,0.28,1,0,A.1.2 ALIGNMENT MODEL: DECODER
271,The context vector c i are recomputed at each step by the alignment model : :,A.1.2 ALIGNMENT MODEL,DECODER,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",32,0.6530612244897959,270,0.8157099697885196,8,0.32,1,0,A.1.2 ALIGNMENT MODEL: DECODER
274,One epoch is one pass through the training set .,A.1.2 ALIGNMENT MODEL,DECODER,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",35,0.7142857142857143,273,0.824773413897281,11,0.44,1,0,A.1.2 ALIGNMENT MODEL: DECODER
275,NLL is the average conditional log-probabilities of the sentences in either the training set or the development set .,A.1.2 ALIGNMENT MODEL,DECODER,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",36,0.7346938775510204,274,0.8277945619335347,12,0.48,1,0,A.1.2 ALIGNMENT MODEL: DECODER
277,where,A.1.2 ALIGNMENT MODEL,DECODER,machine-translation,8,['O'],['O'],38,0.7755102040816326,276,0.8338368580060423,14,0.56,1,0,A.1.2 ALIGNMENT MODEL: DECODER
278,and h j is the j - th annotation in the source sentence ( see Eq. ) .,A.1.2 ALIGNMENT MODEL,DECODER,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",39,0.7959183673469388,277,0.8368580060422961,15,0.6,1,0,A.1.2 ALIGNMENT MODEL: DECODER
279,v a ?,A.1.2 ALIGNMENT MODEL,DECODER,machine-translation,8,"['O', 'O', 'O']","['O', 'O', 'O']",40,0.8163265306122449,278,0.8398791540785498,16,0.64,1,0,A.1.2 ALIGNMENT MODEL: DECODER
282,Rn 2n are weight matrices .,A.1.2 ALIGNMENT MODEL,DECODER,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O']",43,0.8775510204081632,281,0.8489425981873112,19,0.76,1,0,A.1.2 ALIGNMENT MODEL: DECODER
283,Note that the model becomes RNN Encoder - Decoder,A.1.2 ALIGNMENT MODEL,DECODER,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",44,0.8979591836734694,282,0.851963746223565,20,0.8,1,0,A.1.2 ALIGNMENT MODEL: DECODER
284,"With the decoder state s i ?1 , the context c i and the last generated wordy i ? 1 , we define the probability of a target wordy i as",A.1.2 ALIGNMENT MODEL,DECODER,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",45,0.9183673469387756,283,0.8549848942598187,21,0.84,1,0,A.1.2 ALIGNMENT MODEL: DECODER
285,"where ti = max t i ,2 j?1 ,t i , 2 j j=1 , ... , l andt i , k is the k - th element of a vectort i which is computed b ?",A.1.2 ALIGNMENT MODEL,DECODER,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",46,0.9387755102040816,284,0.8580060422960725,22,0.88,1,0,A.1.2 ALIGNMENT MODEL: DECODER
287,R 2 l 2n are weight matrices .,A.1.2 ALIGNMENT MODEL,DECODER,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",48,0.979591836734694,286,0.8640483383685801,24,0.96,1,0,A.1.2 ALIGNMENT MODEL: DECODER
288,This can be understood as having a deep output with a single maxout hidden layer .,A.1.2 ALIGNMENT MODEL,DECODER,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",49,1.0,287,0.8670694864048338,25,1.0,1,0,A.1.2 ALIGNMENT MODEL: DECODER
292,and ? ?,A.2.3 MODEL SIZE,A.2.3 MODEL SIZE,machine-translation,8,"['O', 'O', 'O']","['O', 'O', 'O']",3,0.4285714285714285,291,0.879154078549849,3,0.4285714285714285,1,0,A.2.3 MODEL SIZE
293,Ur as random orthogonal matrices .,A.2.3 MODEL SIZE,A.2.3 MODEL SIZE,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O']",4,0.5714285714285714,292,0.8821752265861027,4,0.5714285714285714,1,0,A.2.3 MODEL SIZE
294,"For W a and U a , we initialized them by sampling each element from the Gaussian distribution of mean 0 and variance 0.001 2 .",A.2.3 MODEL SIZE,A.2.3 MODEL SIZE,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.7142857142857143,293,0.8851963746223565,5,0.7142857142857143,1,0,A.2.3 MODEL SIZE
295,All the elements of Va and all the bias vectors were initialized to zero .,A.2.3 MODEL SIZE,A.2.3 MODEL SIZE,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.8571428571428571,294,0.8882175226586103,6,0.8571428571428571,1,0,A.2.3 MODEL SIZE
296,Any other weight matrix was initialized by sampling from the Gaussian distribution of mean 0 and variance 0.01 2 .,A.2.3 MODEL SIZE,A.2.3 MODEL SIZE,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,1.0,295,0.8912386706948641,7,1.0,1,0,A.2.3 MODEL SIZE
300,"We explicitly normalized the L 2 - norm of the gradient of the cost function each time to beat most a predefined threshold of 1 , when the norm was larger than the threshold .",B.2 TRAINING,B.2 TRAINING,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.088235294117647,299,0.9033232628398792,3,0.3333333333333333,1,0,B.2 TRAINING
303,"Hence , to minimize the waste of computation , before every 20 - th update , we retrieved 1600 sentence pairs , sorted them according to the lengths and split them into 20 minibatches .",B.2 TRAINING,B.2 TRAINING,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.1764705882352941,302,0.9123867069486404,6,0.6666666666666666,1,0,B.2 TRAINING
308,"An admitting privilege is the right of a doctor to admit a patient to a hospital or a medical centre to carryout a diagnosis or a procedure , based on his status as a healthcare worker at a hospital .",B.2 TRAINING,C TRANSLATIONS OF LONG SENTENCES,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.3235294117647059,307,0.9274924471299094,1,0.0,1,0,B.2 TRAINING: C TRANSLATIONS OF LONG SENTENCES
311,RNNenc - 50,B.2 TRAINING,C TRANSLATIONS OF LONG SENTENCES,machine-translation,8,"['O', 'O', 'O']","['O', 'O', 'O']",14,0.4117647058823529,310,0.9365558912386708,2,0.2857142857142857,1,0,B.2 TRAINING: C TRANSLATIONS OF LONG SENTENCES
312,"Ce type d'exprience fait partie des initiatives du Disney pour "" prolonger la dure de vie de ses nouvelles et de dvelopper des liens avec les lecteurs numriques qui deviennent plus complexes .",B.2 TRAINING,C TRANSLATIONS OF LONG SENTENCES,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.4411764705882353,311,0.9395770392749244,3,0.4285714285714285,1,0,B.2 TRAINING: C TRANSLATIONS OF LONG SENTENCES
315,Google Translate,B.2 TRAINING,C TRANSLATIONS OF LONG SENTENCES,machine-translation,8,"['O', 'O']","['O', 'O']",18,0.5294117647058824,314,0.9486404833836858,6,0.8571428571428571,1,0,B.2 TRAINING: C TRANSLATIONS OF LONG SENTENCES
318,"Ina press conference on Thursday , Mr Blair stated that there was nothing in this video that might constitute a "" reasonable motive "" that could lead to criminal charges being brought against the mayor .",B.2 TRAINING,C TRANSLATIONS OF LONG SENTENCES,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.6176470588235294,317,0.9577039274924471,1,0.0,1,0,B.2 TRAINING: C TRANSLATIONS OF LONG SENTENCES
320,"En confrence de presse , jeudi , M. Blair a affirm qu'il n'y avait rien dans cette vido qui puisse constituer des "" motifs raisonnables "" pouvant mener au dpt d'une accusation criminelle contre le maire .",B.2 TRAINING,C TRANSLATIONS OF LONG SENTENCES,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",23,0.6764705882352942,319,0.9637462235649548,1,0.1,1,0,B.2 TRAINING: C TRANSLATIONS OF LONG SENTENCES
321,RNNenc - 50,B.2 TRAINING,C TRANSLATIONS OF LONG SENTENCES,machine-translation,8,"['O', 'O', 'O']","['O', 'O', 'O']",24,0.7058823529411765,320,0.9667673716012084,2,0.2,1,0,B.2 TRAINING: C TRANSLATIONS OF LONG SENTENCES
322,"Lors de la confrence de presse de jeudi , M. Blair a dit qu'il n'y avait rien dans cette vido qui pourrait constituer une "" motivation raisonnable "" pouvant entraner des accusations criminelles portes contre le maire .",B.2 TRAINING,C TRANSLATIONS OF LONG SENTENCES,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.7352941176470589,321,0.9697885196374624,3,0.3,1,0,B.2 TRAINING: C TRANSLATIONS OF LONG SENTENCES
324,"Lors d'une confrence de presse jeudi , M. Blair a dclar qu'il n'y avait rien dans cette vido qui pourrait constituer un "" motif raisonnable "" qui pourrait conduire des accusations criminelles contre le maire .",B.2 TRAINING,C TRANSLATIONS OF LONG SENTENCES,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.7941176470588235,323,0.9758308157099698,5,0.5,1,0,B.2 TRAINING: C TRANSLATIONS OF LONG SENTENCES
325,Google Translate,B.2 TRAINING,C TRANSLATIONS OF LONG SENTENCES,machine-translation,8,"['O', 'O']","['O', 'O']",28,0.8235294117647058,324,0.9788519637462236,6,0.6,1,0,B.2 TRAINING: C TRANSLATIONS OF LONG SENTENCES
326,"Lors d'une confrence de presse jeudi , M. Blair a dclar qu'il n'y avait rien dans cette vido qui pourrait constituer un "" motif raisonnable "" qui pourrait mener des accusations criminelles portes contre le maire . :",B.2 TRAINING,C TRANSLATIONS OF LONG SENTENCES,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",29,0.8529411764705882,325,0.9818731117824774,7,0.7,1,0,B.2 TRAINING: C TRANSLATIONS OF LONG SENTENCES
327,The translations generated by RNNenc - 50 and RNNsearch - 50 from long source sentences ( 30 words or more ) selected from the test set .,B.2 TRAINING,C TRANSLATIONS OF LONG SENTENCES,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",30,0.8823529411764706,326,0.9848942598187312,8,0.8,1,0,B.2 TRAINING: C TRANSLATIONS OF LONG SENTENCES
329,The translations by Google Translate were made on 27 August 2014 .,B.2 TRAINING,C TRANSLATIONS OF LONG SENTENCES,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",32,0.9411764705882352,328,0.9909365558912386,10,1.0,1,0,B.2 TRAINING: C TRANSLATIONS OF LONG SENTENCES
331,"Ce type d'exprience entre dans le cadre des efforts de Disney pour "" tendre la dure de vie de ses sries et construire de nouvelles relations avec son public grce des plateformes numriques qui so nt de plus en plus importantes "" , a-t - il ajout .",B.2 TRAINING,C TRANSLATIONS OF LONG SENTENCES,machine-translation,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",34,1.0,330,0.9969788519637462,1,0.0,1,0,B.2 TRAINING: C TRANSLATIONS OF LONG SENTENCES
3,abstract,,,machine-translation,9,['O'],['O'],0,0.0,2,0.0069686411149825,0,0.0,1,0,
4,"Natural language processing ( NLP ) models often require a massive number of parameters for word embeddings , resulting in a large storage or memory footprint .",abstract,abstract,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.1,3,0.0104529616724738,1,0.1,1,0,abstract
9,"Each code is composed of multiple discrete numbers , such as ( 3 , 2 , 1 , 8 ) , where the value of each component is limited to a fixed range .",abstract,abstract,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.6,8,0.0278745644599303,6,0.6,1,0,abstract
17,"However , as each word is assigned an independent embedding vector , the number of parameters in the embedding matrix can be huge .",INTRODUCTION,INTRODUCTION,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.0211267605633802,16,0.0557491289198606,3,0.0576923076923076,1,0,INTRODUCTION
18,"For example , when each embedding has 500 dimensions , the network has to hold 100M embedding parameters to represent 200K words .",INTRODUCTION,INTRODUCTION,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.028169014084507,17,0.0592334494773519,4,0.0769230769230769,1,0,INTRODUCTION
20,"As only a small portion of the word embeddings is selected in the forward pass , the giant embedding matrix usually does not cause a speed issue .",INTRODUCTION,INTRODUCTION,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.0422535211267605,19,0.0662020905923344,6,0.1153846153846153,1,0,INTRODUCTION
21,"However , the massive number of parameters in the neural network results in a large storage or memory footprint .",INTRODUCTION,INTRODUCTION,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.0492957746478873,20,0.0696864111498257,7,0.1346153846153846,1,0,INTRODUCTION
22,"When other components of the neural network are also large , the model may fail to fit into GPU memory during training .",INTRODUCTION,INTRODUCTION,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.056338028169014,21,0.073170731707317,8,0.1538461538461538,1,0,INTRODUCTION
23,"Moreover , as the demand for low - latency neural computation for mobile platforms increases , some neural - based models are expected to run on mobile devices .",INTRODUCTION,INTRODUCTION,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.0633802816901408,22,0.0766550522648083,9,0.173076923076923,1,0,INTRODUCTION
24,"Thus , it is becoming more important to compress the size of NLP models for deployment to devices with limited memory or storage capacity .",INTRODUCTION,INTRODUCTION,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.0704225352112676,23,0.0801393728222996,10,0.1923076923076923,1,0,INTRODUCTION
26,Neural networks are known for the significant redundancy in the connections .,INTRODUCTION,INTRODUCTION,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.0845070422535211,25,0.0871080139372822,12,0.2307692307692307,1,0,INTRODUCTION
28,Some words are very similar regarding the semantics .,INTRODUCTION,INTRODUCTION,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.0985915492957746,27,0.0940766550522648,14,0.2692307692307692,1,0,INTRODUCTION
29,"For example , "" dog "" and "" dogs "" have almost the same meaning , except one is plural .",INTRODUCTION,INTRODUCTION,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.1056338028169014,28,0.0975609756097561,15,0.2884615384615384,1,0,INTRODUCTION
31,"However , a small portion in both vectors still has to be trained independently to capture the syntactic difference .",INTRODUCTION,INTRODUCTION,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.1197183098591549,30,0.1045296167247386,17,0.3269230769230769,1,0,INTRODUCTION
32,"Following the intuition of creating partially shared embeddings , instead of assigning each word a unique ID , we represent each word w with a code C w = ( C 1 w , C 2 w , ... , C M w ) .",INTRODUCTION,INTRODUCTION,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.1267605633802817,31,0.1080139372822299,18,0.3461538461538461,1,0,INTRODUCTION
33,Each component,INTRODUCTION,,machine-translation,9,"['O', 'O']","['O', 'O']",19,0.1338028169014084,32,0.1114982578397212,19,0.3653846153846153,1,0,INTRODUCTION
34,Ci w is an integer number in .,INTRODUCTION,Each component,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.1408450704225352,33,0.1149825783972125,20,0.3846153846153846,1,0,INTRODUCTION: Each component
35,"Ideally , similar words should have similar codes .",INTRODUCTION,Each component,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.1478873239436619,34,0.1184668989547038,21,0.4038461538461538,1,0,INTRODUCTION: Each component
36,"For example , we may desire C dog = ( 3 , 2 , 4 , 1 ) and C dogs = ( 3 , 2 , 4 , 2 ) .",INTRODUCTION,Each component,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.1549295774647887,35,0.1219512195121951,22,0.4230769230769231,1,0,INTRODUCTION: Each component
38,"More specifically , we create M codebooks E 1 , E 2 , ... , EM , each containing K codeword vectors .",INTRODUCTION,Each component,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.1690140845070422,37,0.1289198606271777,24,0.4615384615384615,1,0,INTRODUCTION: Each component
39,The embedding of a word is computed by summing up the codewords corresponding to all the components in the code as,INTRODUCTION,Each component,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.176056338028169,38,0.1324041811846689,25,0.4807692307692308,1,0,INTRODUCTION: Each component
40,( where E i ( C i w ) is the Ci w - th codeword in the codebook E i .,INTRODUCTION,Each component,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.1830985915492957,39,0.1358885017421602,26,0.5,1,0,INTRODUCTION: Each component
41,"In this way , the number of vectors in the embedding matrix will be M K , which is usually much smaller than the vocabulary size .",INTRODUCTION,Each component,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.1901408450704225,40,0.1393728222996515,27,0.5192307692307693,1,0,INTRODUCTION: Each component
43,"The codes of all the words can be stored in an integer matrix , denoted by C. Thus , the storage footprint of the embedding layer now depends on the total size of the combined codebook E and the code matrix C.",INTRODUCTION,Each component,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",29,0.204225352112676,42,0.1463414634146341,29,0.5576923076923077,1,0,INTRODUCTION: Each component
44,"Although the number of embedding vectors can be greatly reduced by using such coding approach , we want to prevent any serious degradation in performance compared to the models using normal embeddings .",INTRODUCTION,Each component,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",30,0.2112676056338028,43,0.1498257839721254,30,0.5769230769230769,1,0,INTRODUCTION: Each component
45,"In other words , given a set of baseline word embeddings ? ( w ) , we wish to find a set of codes ?",INTRODUCTION,Each component,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",31,0.2183098591549295,44,0.1533101045296167,31,0.5961538461538461,1,0,INTRODUCTION: Each component
46,and combined codebook that can produce the embeddings with the same effectiveness as ? ( w ) .,INTRODUCTION,Each component,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",32,0.2253521126760563,45,0.156794425087108,32,0.6153846153846154,1,0,INTRODUCTION: Each component
48,where | V | is the vocabulary size .,INTRODUCTION,Each component,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",34,0.2394366197183098,47,0.1637630662020905,34,0.6538461538461539,1,0,INTRODUCTION: Each component
49,The baseline embeddings can be a set of pre-trained vectors such as word2vec or GloVe embeddings .,INTRODUCTION,Each component,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",35,0.2464788732394366,48,0.1672473867595819,35,0.6730769230769231,1,0,INTRODUCTION: Each component
50,"In Eq. 3 , the baseline embedding matrix ?",INTRODUCTION,Each component,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",36,0.2535211267605634,49,0.1707317073170731,36,0.6923076923076923,1,0,INTRODUCTION: Each component
51,is approximated by M codewords selected from M codebooks .,INTRODUCTION,Each component,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",37,0.2605633802816901,50,0.1742160278745644,37,0.7115384615384616,1,0,INTRODUCTION: Each component
52,The selection of codewords is controlled by the code C w .,INTRODUCTION,Each component,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",38,0.2676056338028169,51,0.1777003484320557,38,0.7307692307692307,1,0,INTRODUCTION: Each component
53,"Such problem of learning compact codes with multiple codebooks is formalized and discussed in the research field of compressionbased source coding , known as product quantization and additive quantization .",INTRODUCTION,Each component,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",39,0.2746478873239437,52,0.181184668989547,39,0.75,1,0,INTRODUCTION: Each component
56,"Due to the discreteness in the hash codes , it is usually difficult to directly optimize the objective function in Eq.",INTRODUCTION,Each component,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",42,0.2957746478873239,55,0.1916376306620209,42,0.8076923076923077,1,0,INTRODUCTION: Each component
57,3 .,INTRODUCTION,Each component,machine-translation,9,"['O', 'O']","['O', 'O']",43,0.3028169014084507,56,0.1951219512195122,43,0.8269230769230769,1,0,INTRODUCTION: Each component
61,The contribution of this work can be summarized as follows :,INTRODUCTION,Each component,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",47,0.3309859154929577,60,0.2090592334494773,47,0.903846153846154,1,0,INTRODUCTION: Each component
67,RELATED WORK,INTRODUCTION,Each component,machine-translation,9,"['O', 'O']","['O', 'O']",53,0.3732394366197183,66,0.229965156794425,0,0.0,1,0,INTRODUCTION: Each component
68,"Existing works for compressing neural networks include low - precision computation , quantization and knowledge distillation .",INTRODUCTION,Each component,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",54,0.380281690140845,67,0.2334494773519163,1,0.0344827586206896,1,0,INTRODUCTION: Each component
69,"Network quantization such as HashedNet forces the weight matrix to have few real weights , with a hash function to determine the weight assignment .",INTRODUCTION,Each component,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",55,0.3873239436619718,68,0.2369337979094076,2,0.0689655172413793,1,0,INTRODUCTION: Each component
72,"However , as the embedding matrix is tremendously big , the number of hash codes a model need to maintain is still large even with Huffman coding .",INTRODUCTION,Each component,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",58,0.4084507042253521,71,0.2473867595818815,5,0.1724137931034483,1,0,INTRODUCTION: Each component
73,Network pruning works in a different way that makes a network sparse .,INTRODUCTION,Each component,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",59,0.4154929577464789,72,0.2508710801393728,6,0.2068965517241379,1,0,INTRODUCTION: Each component
74,Iterative pruning prunes a weight value if its absolute value is smaller than a threshold .,INTRODUCTION,Each component,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",60,0.4225352112676056,73,0.2543554006968641,7,0.2413793103448276,1,0,INTRODUCTION: Each component
76,Some recent works also apply iterative pruning to prune 80 % of the connections for neural machine translation models .,INTRODUCTION,Each component,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",62,0.4366197183098591,75,0.2613240418118467,9,0.3103448275862069,1,0,INTRODUCTION: Each component
79,"Initiated byproduct quantization , subsequent works such as additive quantization explore the use of multiple codebooks for source coding , resulting in compositional codes .",INTRODUCTION,Each component,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",65,0.4577464788732394,78,0.2717770034843205,12,0.4137931034482758,1,0,INTRODUCTION: Each component
81,Previous works mainly focus on performing efficient similarity search of image descriptors .,INTRODUCTION,Each component,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",67,0.4718309859154929,80,0.2787456445993031,14,0.4827586206896552,1,0,INTRODUCTION: Each component
84,"However , to match the baseline performance , much longer hash codes are required byproduct quantization .",INTRODUCTION,Each component,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",70,0.4929577464788732,83,0.289198606271777,17,0.5862068965517241,1,0,INTRODUCTION: Each component
85,This will be detailed in Section 5.2 .,INTRODUCTION,Each component,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",71,0.5,84,0.2926829268292683,18,0.6206896551724138,1,0,INTRODUCTION: Each component
89,"Some recent works in learning to hash also utilize neural networks to produce binary codes by applying binary constrains ( e.g. , sigmoid function ) .",INTRODUCTION,Each component,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",75,0.528169014084507,88,0.3066202090592334,22,0.7586206896551724,1,0,INTRODUCTION: Each component
91,"As an alternative to our approach , one can also reduce the number of unique word types by forcing a character - level segmentation .",INTRODUCTION,Each component,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",77,0.5422535211267606,90,0.313588850174216,24,0.8275862068965517,1,0,INTRODUCTION: Each component
93,"propose to use char-gram as input features , which are further hashed to save space .",INTRODUCTION,Each component,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",79,0.5563380281690141,92,0.3205574912891986,26,0.896551724137931,1,0,INTRODUCTION: Each component
94,"Generally , using characterlevel inputs requires modifications to the model architecture .",INTRODUCTION,Each component,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",80,0.5633802816901409,93,0.3240418118466899,27,0.9310344827586208,1,0,INTRODUCTION: Each component
95,"Moreover , some Asian languages such as Japanese and Chinese retain a large vocabulary at the character level , which makes the character - based approach difficult to be applied .",INTRODUCTION,Each component,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",81,0.5704225352112676,94,0.3275261324041811,28,0.9655172413793104,1,0,INTRODUCTION: Each component
97,ADVANTAGE OF COMPOSITIONAL CODES,INTRODUCTION,Each component,machine-translation,9,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",83,0.5845070422535211,96,0.3344947735191638,0,0.0,1,0,INTRODUCTION: Each component
100,"We represent each word w with a compact code C w that is composed of M components such that , which also indicates that M log 2 K bits are required to store each code .",INTRODUCTION,Each component,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",86,0.6056338028169014,99,0.3449477351916376,3,0.0508474576271186,1,0,INTRODUCTION: Each component
101,"For convenience , K is selected to be a number of a multiple of 2 , so that the codes can be efficiently stored .",INTRODUCTION,Each component,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",87,0.6126760563380281,100,0.3484320557491289,4,0.0677966101694915,1,0,INTRODUCTION: Each component
102,If we restrict each component,INTRODUCTION,Each component,machine-translation,9,"['O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O']",88,0.6197183098591549,101,0.3519163763066202,5,0.0847457627118644,1,0,INTRODUCTION: Each component
103,"Ci w to values of 0 or 1 , the code for each word C w will be a binary code .",INTRODUCTION,Each component,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",89,0.6267605633802817,102,0.3554006968641115,6,0.1016949152542373,1,0,INTRODUCTION: Each component
104,"In this case , the code learning problem is equivalent to a matrix factorization problem with binary components .",INTRODUCTION,Each component,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",90,0.6338028169014085,103,0.3588850174216028,7,0.1186440677966101,1,0,INTRODUCTION: Each component
105,"Forcing the compact codes to be binary numbers can be beneficial , as the learning problem is usually easier to solve in the binary case , and some existing optimization algorithms in learning to hash can be reused .",INTRODUCTION,Each component,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",91,0.6408450704225352,104,0.3623693379790941,8,0.135593220338983,1,0,INTRODUCTION: Each component
106,"However , the compositional coding approach produces shorter codes and is thus more storage efficient .",INTRODUCTION,Each component,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",92,0.647887323943662,105,0.3658536585365853,9,0.1525423728813559,1,0,INTRODUCTION: Each component
107,"As the number of basis vectors is M K regardless of the vocabulary size , the only uncertain factor contributing to the model size is the size of the hash codes , which is proportional to the vocabulary size .",INTRODUCTION,Each component,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",93,0.6549295774647887,106,0.3693379790940767,10,0.1694915254237288,1,0,INTRODUCTION: Each component
108,"Therefore , maintaining short codes is cruicial in our work .",INTRODUCTION,Each component,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",94,0.6619718309859155,107,0.3728222996515679,11,0.1864406779661017,1,0,INTRODUCTION: Each component
109,Suppose we wish the model to have a set of N basis vectors .,INTRODUCTION,Each component,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",95,0.6690140845070423,108,0.3763066202090592,12,0.2033898305084746,1,0,INTRODUCTION: Each component
110,"Then in the binary case , each code will have N / 2 bits .",INTRODUCTION,Each component,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",96,0.676056338028169,109,0.3797909407665505,13,0.2203389830508474,1,0,INTRODUCTION: Each component
111,"For the compositional coding approach , if we can find a M K decomposition such that M K = N , then each code will have M log 2 K bits .",INTRODUCTION,Each component,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",97,0.6830985915492958,110,0.3832752613240418,14,0.2372881355932203,1,0,INTRODUCTION: Each component
112,"For example , a binary code will have a length of 256 bits to support 512 basis vectors .",INTRODUCTION,Each component,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",98,0.6901408450704225,111,0.3867595818815331,15,0.2542372881355932,1,0,INTRODUCTION: Each component
113,"In contrast , a 32 16 compositional coding scheme will produce codes of only 128 bits . :",INTRODUCTION,Each component,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",99,0.6971830985915493,112,0.3902439024390244,16,0.2711864406779661,1,0,INTRODUCTION: Each component
115,"To support N basis vectors , a binary code will have N / 2 bits and the embedding computation is a summation over N / 2 vectors .",INTRODUCTION,Each component,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",101,0.7112676056338029,114,0.397212543554007,18,0.3050847457627119,1,0,INTRODUCTION: Each component
116,"For the compositional approach with M codebooks and K codewords in each codebook , each code has M log 2 K bits , and the computation is a summation over M vectors .",INTRODUCTION,Each component,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",102,0.7183098591549296,115,0.4006968641114982,19,0.3220338983050847,1,0,INTRODUCTION: Each component
119,"For the conventional approach , the number of vectors is identical to the vocabulary size and the computation is basically a single indexing operation .",INTRODUCTION,Each component,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",105,0.7394366197183099,118,0.4111498257839721,22,0.3728813559322034,1,0,INTRODUCTION: Each component
120,"In the case of binary codes , the computation for constructing an embedding involves a summation over N / 2 basis vectors .",INTRODUCTION,Each component,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",106,0.7464788732394366,119,0.4146341463414634,23,0.3898305084745763,1,0,INTRODUCTION: Each component
121,"For the compositional approach , the number of vectors required to construct an embedding vector is M .",INTRODUCTION,Each component,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",107,0.7535211267605634,120,0.4181184668989547,24,0.4067796610169492,1,0,INTRODUCTION: Each component
122,Both the binary and compositional approaches have significantly fewer vectors in the embedding matrix .,INTRODUCTION,Each component,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",108,0.7605633802816901,121,0.4216027874564459,25,0.423728813559322,1,0,INTRODUCTION: Each component
125,Let ? ?,INTRODUCTION,Each component,machine-translation,9,"['O', 'O', 'O']","['O', 'O', 'O']",111,0.7816901408450704,124,0.4320557491289198,28,0.4745762711864407,1,0,INTRODUCTION: Each component
126,"R |V | H be the original embedding matrix , where each embedding vector has H dimensions .",INTRODUCTION,Each component,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",112,0.7887323943661971,125,0.4355400696864111,29,0.4915254237288136,1,0,INTRODUCTION: Each component
127,"By using the reconstruction loss as the objective function in Eq. 3 , we are actually finding an approximate matrix factorization ?",INTRODUCTION,Each component,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",113,0.795774647887324,126,0.4390243902439024,30,0.5084745762711864,1,0,INTRODUCTION: Each component
128,is a basis matrix for the i - th component .,INTRODUCTION,Each component,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",114,0.8028169014084507,127,0.4425087108013937,31,0.5254237288135594,1,0,INTRODUCTION: Each component
129,Di is a | V | K code matrix in which each row is an K-dimensional one - hot vector .,INTRODUCTION,Each component,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",115,0.8098591549295775,128,0.445993031358885,32,0.5423728813559322,1,0,INTRODUCTION: Each component
130,If we let d i w be the one - hot vector corresponding to the code component,INTRODUCTION,Each component,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",116,0.8169014084507042,129,0.4494773519163763,33,0.559322033898305,1,0,INTRODUCTION: Each component
131,"Ci w for word w , the computation of the word embeddings can be reformulated as",INTRODUCTION,Each component,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",117,0.823943661971831,130,0.4529616724738676,34,0.576271186440678,1,0,INTRODUCTION: Each component
132,"Therefore , the problem of learning discrete codes C w can be converted to a problem of finding a set of optimal one - hot vectors d 1 w , ... , d M wand source dictionaries A 1 , ... , AM , that minimize the reconstruction loss .",INTRODUCTION,Each component,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",118,0.8309859154929577,131,0.4564459930313589,35,0.5932203389830508,1,0,INTRODUCTION: Each component
133,The Gumbel - softmax reparameterization trick is useful for parameterizing a discrete distribution such as the K-dimensional one - hot vectors d i win Eq.,INTRODUCTION,Each component,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",119,0.8380281690140845,132,0.4599303135888501,36,0.6101694915254238,1,0,INTRODUCTION: Each component
134,"5 . By applying the Gumbel - softmax trick , the k - th elemement ind i w is computed as",INTRODUCTION,Each component,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",120,0.8450704225352113,133,0.4634146341463415,37,0.6271186440677966,1,0,INTRODUCTION: Each component
135,where,INTRODUCTION,Each component,machine-translation,9,['O'],['O'],121,0.852112676056338,134,0.4668989547038327,38,0.6440677966101694,1,0,INTRODUCTION: Each component
136,Gk is a noise term that is sampled from the Gumbel distribution ? log ( ?,INTRODUCTION,Each component,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",122,0.8591549295774648,135,0.470383275261324,39,0.6610169491525424,1,0,INTRODUCTION: Each component
137,"log ( Uniform [ 0 , 1 ] ) ) , whereas ?",INTRODUCTION,Each component,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",123,0.8661971830985915,136,0.4738675958188153,40,0.6779661016949152,1,0,INTRODUCTION: Each component
138,is the temperature of the softmax .,INTRODUCTION,Each component,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O']",124,0.8732394366197183,137,0.4773519163763066,41,0.6949152542372882,1,0,INTRODUCTION: Each component
139,"In our model , the vector ?",INTRODUCTION,Each component,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O']",125,0.8802816901408451,138,0.4808362369337979,42,0.711864406779661,1,0,INTRODUCTION: Each component
140,i w is computed by a simple neural network with a single hidden layer as,INTRODUCTION,Each component,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",126,0.8873239436619719,139,0.4843205574912892,43,0.7288135593220338,1,0,INTRODUCTION: Each component
141,"In our experiments , the hidden layer h w always has a size of M K /2 .",INTRODUCTION,Each component,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",127,0.8943661971830986,140,0.4878048780487805,44,0.7457627118644068,1,0,INTRODUCTION: Each component
143,The Gumbel - softmax trick is applied to ?,INTRODUCTION,Each component,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",129,0.908450704225352,142,0.494773519163763,46,0.7796610169491526,1,0,INTRODUCTION: Each component
144,i w to obtain d i w .,INTRODUCTION,Each component,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",130,0.9154929577464788,143,0.4982578397212543,47,0.7966101694915254,1,0,INTRODUCTION: Each component
145,"Then , the model reconstructs the embedding E(C w ) with Eq. 5 and computes the reconstruction loss with Eq.",INTRODUCTION,Each component,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",131,0.9225352112676056,144,0.5017421602787456,48,0.8135593220338984,1,0,INTRODUCTION: Each component
146,3 .,INTRODUCTION,Each component,machine-translation,9,"['O', 'O']","['O', 'O']",132,0.9295774647887324,145,0.5052264808362369,49,0.8305084745762712,1,0,INTRODUCTION: Each component
148,"The whole neural network for coding learning has five parameters ( ? , b , ? , b , A ) .",INTRODUCTION,Each component,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",134,0.943661971830986,147,0.5121951219512195,51,0.864406779661017,1,0,INTRODUCTION: Each component
149,"Once the coding learning model is trained , the code C w for each word can be easily obtained by applying argmax to the one - hot vectors d 1 w , ... , d M w .",INTRODUCTION,Each component,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",135,0.9507042253521126,148,0.5156794425087108,52,0.8813559322033898,1,0,INTRODUCTION: Each component
150,The basis vectors ( codewords ) for composing the embeddings can be found as the row vectors in the weight matrix A.,INTRODUCTION,Each component,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",136,0.9577464788732394,149,0.519163763066202,53,0.8983050847457628,1,0,INTRODUCTION: Each component
151,"For general NLP tasks , one can learn the compositional codes from publicly available word vectors such as GloVe vectors .",INTRODUCTION,Each component,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",137,0.9647887323943662,150,0.5226480836236934,54,0.9152542372881356,1,0,INTRODUCTION: Each component
152,"However , for some tasks such as machine translation , the word embeddings are usually jointly learned with other parts of the neural network .",INTRODUCTION,Each component,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",138,0.971830985915493,151,0.5261324041811847,55,0.9322033898305084,1,0,INTRODUCTION: Each component
155,"As the reconstructed embeddings E( C w ) are not identical to the original embeddings ? ( w ) , the model parameters other than the embedding matrix have to be retrained again .",INTRODUCTION,Each component,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",141,0.9929577464788732,154,0.5365853658536586,58,0.9830508474576272,1,0,INTRODUCTION: Each component
156,The code learning model can not be jointly trained with the machine translation model as it takes far more iterations for the coding layer to converge to one - hot vectors .,INTRODUCTION,Each component,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",142,1.0,155,0.5400696864111498,59,1.0,1,0,INTRODUCTION: Each component
157,EXPERIMENTS,,,machine-translation,9,['O'],['O'],0,0.0,156,0.5435540069686411,0,0.0,1,0,
160,Please note that the sizes of other parts in the neural networks are not included in our results .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.0277777777777777,159,0.554006968641115,3,0.4285714285714285,1,0,EXPERIMENTS
161,"For dense matrices , we report the size of dumped numpy arrays .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.037037037037037,160,0.5574912891986062,4,0.5714285714285714,1,0,EXPERIMENTS
162,"For the sparse matrices , we report the size of dumped compressed sparse column matrices ( csc matrix ) in scipy .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.0462962962962962,161,0.5609756097560976,5,0.7142857142857143,1,0,EXPERIMENTS
163,All float numbers take 32 bits storage .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.0555555555555555,162,0.5644599303135889,6,0.8571428571428571,1,0,EXPERIMENTS
164,"We enable the "" compressed "" option when dumping the matrices , without this option , the file size is about 1.1 times bigger .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.0648148148148148,163,0.5679442508710801,7,1.0,1,0,EXPERIMENTS
165,CODE LEARNING,EXPERIMENTS,EXPERIMENTS,machine-translation,9,"['O', 'O']","['O', 'O']",8,0.074074074074074,164,0.5714285714285714,0,0.0,1,0,EXPERIMENTS
182,"The vocabulary for the model training contains all words appears both in the IMDB dataset and the GloVe vocabulary , which results in around 75 K words .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.2314814814814814,181,0.6306620209059234,4,0.1290322580645161,1,0,EXPERIMENTS
189,"Suppose we use a 32 16 coding scheme , the basis matrix will then have a shape of 512 300 , which is initialized by the concatenated weight matrices [ A 1 ; A 2 ; ... ; AM ] in the code learning model .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",32,0.2962962962962963,188,0.6550522648083623,11,0.3548387096774194,1,0,EXPERIMENTS
198,"For reference , we also report the total size ( MB ) of the embedding layer in the right table , which includes the sizes of the basis matrix and the hash table .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",41,0.3796296296296296,197,0.686411149825784,20,0.6451612903225806,1,0,EXPERIMENTS
200,"However , setting M to a large number will result in longer hash codes , thus significantly increase the size of the embedding layer .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",43,0.3981481481481481,199,0.6933797909407665,22,0.7096774193548387,1,0,EXPERIMENTS
207,"In this case , the total size of the embedding layer is 1.23 MB , which is equivalent to a compression rate of 98.4 % .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",50,0.4629629629629629,206,0.7177700348432056,29,0.935483870967742,1,0,EXPERIMENTS
210,MACHINE TRANSLATION,EXPERIMENTS,EXPERIMENTS,machine-translation,9,"['O', 'O']","['O', 'O']",53,0.4907407407407407,209,0.7282229965156795,0,0.0,1,0,EXPERIMENTS
219,The English texts are tokenized by moses toolkit whereas the Japanese texts are tokenized by kytea .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",62,0.5740740740740741,218,0.759581881533101,9,0.2142857142857142,1,0,EXPERIMENTS
241,"Note that the embedding layer is fixed in this phase , other parameters are retrained from random initial values .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",84,0.7777777777777778,240,0.8362369337979094,31,0.7380952380952381,1,0,EXPERIMENTS
242,Results :,EXPERIMENTS,EXPERIMENTS,machine-translation,9,"['O', 'O']","['O', 'O']",85,0.7870370370370371,241,0.8397212543554007,32,0.7619047619047619,1,0,EXPERIMENTS
250,"Note that the sizes of NMT models are still quite large due to the big softmax layer and the recurrent layers , which are not reported in the , we show some examples of learned codes based on the 300 - dimensional uncased GloVe embeddings used in the sentiment analysis task .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",93,0.8611111111111112,249,0.867595818815331,40,0.9523809523809524,1,0,EXPERIMENTS
253,ANALYSIS OF CODE EFFICIENCY,EXPERIMENTS,EXPERIMENTS,machine-translation,9,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",96,0.8888888888888888,252,0.8780487804878049,0,0.0,1,0,EXPERIMENTS
255,"In the ideal situation , all codewords shall be fully utilized to convey a fraction of meaning .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",98,0.9074074074074074,254,0.8850174216027874,2,0.1666666666666666,1,0,EXPERIMENTS
256,"However , as the codes are category word 8 8 code 16 16 code dog 0 7 0 1 7 3 7 0 7 7 0 8 3 5 8 5 B 2 E E 0 B 0 A animal cat 7 7 0 1 7 3 7 0 7 7 2 8 B 5 8 CB 2 E E 4 B 0 A penguin 0 7 0 1 7 3 6 0 7 7 E 8 7 6 4 CF DE 3 D 8 0 A go 7 7 0 6 4 3 3 0 2 C C 8 2 C 1 1 B D 0 E 0 B 5 8 verb went 4 0 7 6 4 3 2 0 BC C 6 BC 7 5 B 8 6 E 0 D 0 4 gone 7 7 0 6 4 3 3 0 2 C C 8 0 B 1 5 B D 6 E 0 2 5 A : Examples of learned compositional codes based on Glo Ve embedding vectors automatically learned , it is possible that some codewords are abandoned during the training .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",99,0.9166666666666666,255,0.8885017421602788,3,0.25,1,0,EXPERIMENTS
257,"In extreme cases , some "" dead "" codewords can be used by none of the words .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",100,0.925925925925926,256,0.89198606271777,4,0.3333333333333333,1,0,EXPERIMENTS
258,"To analyze the code efficiency , we count the number of words that contain a specific subcode in each component .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",101,0.9351851851851852,257,0.8954703832752613,5,0.4166666666666667,1,0,EXPERIMENTS
260,Each column shows the counts of the subcodes of a specific component .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",103,0.9537037037037036,259,0.902439024390244,7,0.5833333333333334,1,0,EXPERIMENTS
261,"In our experiments , when using a 8 8 coding scheme , we found 31 % of the words have a subcode "" 0 "" for the first component , while the subcode "" 1 "" is only used by 5 % of the words .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",104,0.9629629629629628,260,0.9059233449477352,8,0.6666666666666666,1,0,EXPERIMENTS
263,"In any coding scheme , even the most unpopular codeword is used by about 1000 words .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",106,0.9814814814814816,262,0.9128919860627178,10,0.8333333333333334,1,0,EXPERIMENTS
266,CONCLUSION,,,machine-translation,9,['O'],['O'],0,0.0,265,0.9233449477351916,0,0.0,1,0,
3,abstract,,,named-entity-recognition,0,['O'],['O'],0,0.0,2,0.007380073800738,0,0.0,1,0,
4,Subset selection from massive data with noised information is increasingly popular for various applications .,abstract,abstract,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.1428571428571428,3,0.011070110701107,1,0.1428571428571428,1,0,abstract
5,This problem is still highly challenging as current methods are generally slow in speed and sensitive to outliers .,abstract,abstract,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.2857142857142857,4,0.014760147601476,2,0.2857142857142857,1,0,abstract
12,"Due to the explosive growth of data , subset selection methods are increasingly popular fora wide range of machine learning and computer vision applications .",Introduction,Introduction,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.010752688172043,11,0.040590405904059,1,0.0909090909090909,1,0,Introduction
14,"By analyzing a few , we can roughly know all .",Introduction,Introduction,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.032258064516129,13,0.047970479704797,3,0.2727272727272727,1,0,Introduction
16,"Besides , by only using the selected exemplars for succeeding tasks , the cost of memories and computational time will be greatly reduced .",Introduction,Introduction,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.053763440860215,15,0.055350553505535,5,0.4545454545454545,1,0,Introduction
17,"Additionally , as outliers are generally less representative , the side effect of outliers will be reduced , thus boosting the performance of subsequent applications .",Introduction,Introduction,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.064516129032258,16,0.059040590405904,6,0.5454545454545454,1,0,Introduction
18,There have been several subset selection methods .,Introduction,Introduction,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.075268817204301,17,0.062730627306273,7,0.6363636363636364,1,0,Introduction
19,The most intuitional method is to randomly select a fixed number of samples .,Introduction,Introduction,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.086021505376344,18,0.066420664206642,8,0.7272727272727273,1,0,Introduction
20,"Although highly efficient , there is no guarantee for an effective selection .",Introduction,Introduction,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.0967741935483871,19,0.070110701107011,9,0.8181818181818182,1,0,Introduction
21,"For the other methods , depending on the mechanism of representative exemplars , there are mainly three categories of selection methods .",Introduction,Introduction,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.1075268817204301,20,0.07380073800738,10,0.9090909090909092,1,0,Introduction
22,One category Data size ( N ) Selection Time,Introduction,Introduction,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.1182795698924731,21,0.077490774907749,11,1.0,1,0,Introduction
23,Classifiers,Introduction,,named-entity-recognition,0,['O'],['O'],12,0.1290322580645161,22,0.081180811808118,0,0.0,1,0,Introduction
24,Classification Accuracy Performance TED RRSSNie RRSSour ARSSour : Comparisons of four algorithms on Optdigit .,Introduction,Classifiers,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.1397849462365591,23,0.084870848708487,1,0.0056497175141242,1,0,Introduction: Classifiers
28,relies on the assumption that the data points lie in one or multiple low - dimensional subspaces .,Introduction,Classifiers,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.1827956989247312,27,0.0996309963099631,5,0.0282485875706214,1,0,Introduction: Classifiers
31,Another category assumes that the samples are distributed around centers .,Introduction,Classifiers,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.2150537634408602,30,0.1107011070110701,8,0.0451977401129943,1,0,Introduction: Classifiers
32,The center or its nearest neighbour are selected as exemplars .,Introduction,Classifiers,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.2258064516129032,31,0.1143911439114391,9,0.0508474576271186,1,0,Introduction: Classifiers
33,"Perhaps , Kmeans and Kmedoids are the most typical methods ( Kmedoids is a variant of Kmeans ) .",Introduction,Classifiers,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.2365591397849462,32,0.1180811808118081,10,0.0564971751412429,1,0,Introduction: Classifiers
35,"Thus , the results depend tightly on the initialization , and they are highly unstable for large K ( i.e. the number of centers or selected samples ) .",Introduction,Classifiers,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.2580645161290322,34,0.1254612546125461,12,0.0677966101694915,1,0,Introduction: Classifiers
36,"Recently , there area few methods that assume exemplars are the samples that can best represent the whole dataset .",Introduction,Classifiers,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.2688172043010752,35,0.1291512915129151,13,0.0734463276836158,1,0,Introduction: Classifiers
37,"However , for , the optimization is a combinatorial problem ( NP - hard ) , which is computationally intractable to solve .",Introduction,Classifiers,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.2795698924731182,36,0.1328413284132841,14,0.0790960451977401,1,0,Introduction: Classifiers
41,The solver of ) is theoretically perfect due to its ability of convergence to global optima .,Introduction,Classifiers,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",30,0.3225806451612903,40,0.1476014760147601,18,0.1016949152542373,1,0,Introduction: Classifiers
42,"Unfortunately , in terms of computational costs , the solver is highly complex .",Introduction,Classifiers,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",31,0.3333333333333333,41,0.1512915129151291,19,0.1073446327683615,1,0,Introduction: Classifiers
43,It takes ON 4 for one iteration as shown in .,Introduction,Classifiers,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",32,0.3440860215053763,42,0.1549815498154981,20,0.1129943502824858,1,0,Introduction: Classifiers
44,This is infeasible for the case of large N ( e.g. it takes 2000 + hours fora case of N = 13000 ) .,Introduction,Classifiers,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",33,0.3548387096774194,43,0.1586715867158671,21,0.1186440677966101,1,0,Introduction: Classifiers
46,"Such case is worth improvement , as there may exist outlier elements in real data .",Introduction,Classifiers,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",35,0.3763440860215054,45,0.1660516605166051,23,0.1299435028248587,1,0,Introduction: Classifiers
47,Contributions .,Introduction,,named-entity-recognition,0,"['O', 'O']","['O', 'O']",36,0.3870967741935484,46,0.1697416974169741,24,0.135593220338983,1,0,Introduction
57,Notations .,Introduction,,named-entity-recognition,0,"['O', 'O']","['O', 'O']",46,0.4946236559139785,56,0.2066420664206642,34,0.192090395480226,1,0,Introduction
59,For a matrix Y = [ Y ln ] ?,Introduction,Notations .,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",48,0.5161290322580645,58,0.2140221402214022,36,0.2033898305084746,1,0,Introduction: Notations .
60,"R LN , we denote it s l throw and n th column as y land y n respectively .",Introduction,Notations .,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",49,0.5268817204301075,59,0.2177121771217712,37,0.2090395480225988,1,0,Introduction: Notations .
61,"The 2 ,1 - norm of a matrix is defined as",Introduction,Notations .,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",50,0.5376344086021505,60,0.2214022140221402,38,0.2146892655367231,1,0,Introduction: Notations .
63,"In the problem of subset selection , we are often given a set of N unlabelled points X = x 1 , x 2 , , x N | x n ?",Introduction,Subset Selection via Self - Representation,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",52,0.5591397849462365,62,0.2287822878228782,40,0.2259887005649717,1,0,Introduction: Subset Selection via Self - Representation
64,"R L , where L is the feature length .",Introduction,Subset Selection via Self - Representation,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",53,0.5698924731182796,63,0.2324723247232472,41,0.231638418079096,1,0,Introduction: Subset Selection via Self - Representation
67,Such a motivation could be formulated as the Transductive Experimental Design ( TED ) model :,Introduction,Subset Selection via Self - Representation,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",56,0.6021505376344086,66,0.2435424354243542,44,0.2485875706214689,1,0,Introduction: Subset Selection via Self - Representation
68,where Q ?,Introduction,Subset Selection via Self - Representation,named-entity-recognition,0,"['O', 'O', 'O']","['O', 'O', 'O']",57,0.6129032258064516,67,0.2472324723247232,45,0.2542372881355932,1,0,Introduction: Subset Selection via Self - Representation
69,"R LK is the selected subset matrix , whose column vectors all come from X , i.e. q k ?",Introduction,Subset Selection via Self - Representation,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",58,0.6236559139784946,68,0.2509225092250922,46,0.2598870056497175,1,0,Introduction: Subset Selection via Self - Representation
70,"X , ?k ?",Introduction,Subset Selection via Self - Representation,named-entity-recognition,0,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",59,0.6344086021505376,69,0.2546125461254612,47,0.2655367231638418,1,0,Introduction: Subset Selection via Self - Representation
71,"{ 1 , , K} ; A = [ a 1 , , a N ] ?",Introduction,Subset Selection via Self - Representation,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",60,0.6451612903225806,70,0.2583025830258302,48,0.2711864406779661,1,0,Introduction: Subset Selection via Self - Representation
72,R KN is the corresponding linear combination coefficients .,Introduction,Subset Selection via Self - Representation,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",61,0.6559139784946236,71,0.2619926199261992,49,0.2768361581920904,1,0,Introduction: Subset Selection via Self - Representation
73,"By minimizing ( 1 ) , TED could select the highly informative and representative samples , as they have to well represent all the samples in X .",Introduction,Subset Selection via Self - Representation,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",62,0.6666666666666666,72,0.2656826568265683,50,0.2824858757062147,1,0,Introduction: Subset Selection via Self - Representation
74,"Although TED ( 1 ) is well modeled - very accurate and intuitive , there are two bottlenecks .",Introduction,Subset Selection via Self - Representation,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",63,0.6774193548387096,73,0.2693726937269373,51,0.288135593220339,1,0,Introduction: Subset Selection via Self - Representation
78,"Second , similar to the existing least square loss based models in machine learning and statistics , ( 1 ) is sensitive to the presence of outliers",Introduction,Subset Selection via Self - Representation,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",67,0.7204301075268817,77,0.2841328413284132,55,0.3107344632768362,1,0,Introduction: Subset Selection via Self - Representation
79,where ?,Introduction,Subset Selection via Self - Representation,named-entity-recognition,0,"['O', 'O']","['O', 'O']",68,0.7311827956989247,78,0.2878228782287823,56,0.3163841807909605,1,0,Introduction: Subset Selection via Self - Representation
80,"is a nonnegative parameter ; A is constrained to be row - sparse , and thus to select the most representative and informative samples .",Introduction,Subset Selection via Self - Representation,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",69,0.7419354838709677,79,0.2915129151291513,57,0.3220338983050847,1,0,Introduction: Subset Selection via Self - Representation
82,"Equivalently , ( 2 ) is rewritten in the matrix format :",Introduction,Subset Selection via Self - Representation,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",71,0.7634408602150538,81,0.2988929889298893,59,0.3333333333333333,1,0,Introduction: Subset Selection via Self - Representation
83,"Since the objective is convex in A , the global minimum maybe found by differentiating ( 3 ) and setting the derivative to zero , resulting in a linear system 1",Introduction,Subset Selection via Self - Representation,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",72,0.7741935483870968,82,0.3025830258302583,60,0.3389830508474576,1,0,Introduction: Subset Selection via Self - Representation
84,where V ?,Introduction,Subset Selection via Self - Representation,named-entity-recognition,0,"['O', 'O', 'O']","['O', 'O', 'O']",73,0.7849462365591398,83,0.3062730627306273,61,0.3446327683615819,1,0,Introduction: Subset Selection via Self - Representation
85,RN,Introduction,,named-entity-recognition,0,['O'],['O'],74,0.7956989247311828,84,0.3099630996309963,62,0.3502824858757062,1,0,Introduction
86,N is a diagonal matrix with then th diagonal entry as V nn = 1 2 an 2 and U nn = 1 2 xn ?,Introduction,RN,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",75,0.8064516129032258,85,0.3136531365313653,63,0.3559322033898305,1,0,Introduction: RN
87,Xan 2 .,Introduction,RN,named-entity-recognition,0,"['O', 'O', 'O']","['O', 'O', 'O']",76,0.8172043010752689,86,0.3173431734317343,64,0.3615819209039548,1,0,Introduction: RN
88,"It seems perfect to use ( 4 ) to solve the objective ( 3 ) , because ( 4 ) looks simple and the global optimum is theoretically guaranteed .",Introduction,RN,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",77,0.8279569892473119,87,0.3210332103321033,65,0.3672316384180791,1,0,Introduction: RN
89,"Unfortunately , in terms of speed , ( 4 ) is usually infeasible due to the incredible computational demand in the case of large N ( the number of samples ) .",Introduction,RN,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",78,0.8387096774193549,88,0.3247232472324723,66,0.3728813559322034,1,0,Introduction: RN
93,1 . Since U nn X T X +? V ?,Introduction,Remark,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",82,0.8817204301075269,92,0.3394833948339483,70,0.3954802259887006,1,0,Introduction: Remark
94,"RN N , the major computational cost of ( 4 ) focuses on a N N linear system .",Introduction,Remark,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",83,0.8924731182795699,93,0.3431734317343173,71,0.4011299435028249,1,0,Introduction: Remark
95,"If solved by the Cholesky factorization method , it costs 1 3 N 3 for factorization as well as 2N 2 for forward and backward substitution .",Introduction,Remark,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",84,0.9032258064516128,94,0.3468634686346863,72,0.4067796610169492,1,0,Introduction: Remark
96,This amounts to ON 3 in total .,Introduction,Remark,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",85,0.913978494623656,95,0.3505535055350554,73,0.4124293785310734,1,0,Introduction: Remark
97,"By now , we only solve an .",Introduction,Remark,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",86,0.9247311827956988,96,0.3542435424354243,74,0.4180790960451977,1,0,Introduction: Remark
98,"Once solving all the set of {a n } N n= 1 , the total complexity amounts to ON 4 for one iteration step .",Introduction,Remark,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",87,0.935483870967742,97,0.3579335793357933,75,0.423728813559322,1,0,Introduction: Remark
99,Accelerated Robust Subset Selection ( ARSS ),Introduction,Remark,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O']",88,0.946236559139785,98,0.3616236162361623,76,0.4293785310734463,1,0,Introduction: Remark
100,"Due to the huge computational costs , Nie 's method is infeasible for the case of large N - the computational time is up to 2088 hours fora case of 13000 samples .",Introduction,Remark,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",89,0.956989247311828,99,0.3653136531365313,77,0.4350282485875706,1,0,Introduction: Remark
101,"Besides , Nie 's model imposes the 2 - norm among features , which is prone to outliers in features .",Introduction,Remark,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",90,0.967741935483871,100,0.3690036900369003,78,0.4406779661016949,1,0,Introduction: Remark
105,Modeling .,,,named-entity-recognition,0,"['O', 'O']","['O', 'O']",0,0.0,104,0.3837638376383764,82,0.4632768361581921,1,0,
107,"There are theoretical and empirical evidences to verify that compared with 2 or 1 norms , the p - norm is more able to prevent outlier elements from dominating the objective , enhancing the robustness ) .",Modeling .,Modeling .,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.0210526315789473,106,0.3911439114391143,84,0.4745762711864407,1,0,Modeling .
108,"Thus , we have the following objective min",Modeling .,Modeling .,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.031578947368421,107,0.3948339483394834,85,0.480225988700565,1,0,Modeling .
109,where ?,Modeling .,Modeling .,named-entity-recognition,0,"['O', 'O']","['O', 'O']",4,0.0421052631578947,108,0.3985239852398524,86,0.4858757062146893,1,0,Modeling .
110,"is a balancing parameter ; A is a row sparse matrix , used to select the most informative and representative samples .",Modeling .,Modeling .,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.0526315789473684,109,0.4022140221402214,87,0.4915254237288136,1,0,Modeling .
111,"By minimizing the energy of ( 5 ) , we could capture the most essential properties of the dataset X.",Modeling .,Modeling .,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.0631578947368421,110,0.4059040590405904,88,0.4971751412429379,1,0,Modeling .
113,The samples specified by the top K indexes are selected as exemplars .,Modeling .,Modeling .,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.0842105263157894,112,0.4132841328413284,90,0.5084745762711864,1,0,Modeling .
114,Note that the model ( 5 ) could be applied to the unsupervised feature selection problem by only transposing the data matrix X .,Modeling .,Modeling .,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.0947368421052631,113,0.4169741697416974,91,0.5141242937853108,1,0,Modeling .
115,"In this case , A is a L L row sparse matrix , used to select the most representative features .",Modeling .,Modeling .,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.1052631578947368,114,0.4206642066420664,92,0.519774011299435,1,0,Modeling .
117,The acceleration owes to the ALM and an equivalent derivation .,Modeling .,Modeling .,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.1263157894736842,116,0.4280442804428044,94,0.5310734463276836,1,0,Modeling .
118,ALM,Modeling .,,named-entity-recognition,0,['O'],['O'],13,0.1368421052631579,117,0.4317343173431734,95,0.536723163841808,1,0,Modeling .
119,"The most intractable challenge of ( 5 ) is that , the p ( 0 < p ? 1 ) - norm is non-convex , non-smooth and notdifferentiable at the zero point .",Modeling .,ALM,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.1473684210526315,118,0.4354243542435424,96,0.5423728813559322,1,0,Modeling .: ALM
120,"Therefore , it is beneficial to use the Augmented Lagrangian Method ( ALM ) to solve ( 5 ) , resulting in several easily tackled unconstrained subproblems .",Modeling .,ALM,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.1578947368421052,119,0.4391143911439114,97,0.5480225988700564,1,0,Modeling .: ALM
121,"By solving them iteratively , the solutions of subproblems could eventually converge to a minimum .",Modeling .,ALM,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.1684210526315789,120,0.4428044280442804,98,0.5536723163841808,1,0,Modeling .: ALM
122,"Specifically , we introduce an auxiliary variable E = X ? XA ?",Modeling .,ALM,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.1789473684210526,121,0.4464944649446494,99,0.559322033898305,1,0,Modeling .: ALM
124,"Thus , the objective ( 5 ) becomes : min",Modeling .,ALM,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.2,123,0.4538745387453874,101,0.5706214689265536,1,0,Modeling .: ALM
125,"To deal with the equality constraint in , the most convenient method is to add a penalty , resulting in",Modeling .,ALM,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.2105263157894736,124,0.4575645756457565,102,0.576271186440678,1,0,Modeling .: ALM
126,where is a penalty parameter .,Modeling .,ALM,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O']",21,0.2210526315789473,125,0.4612546125461255,103,0.5819209039548022,1,0,Modeling .: ALM
127,"To guarantee the equality constraint , it requires approaching infinity , which may cause bad numerical conditions .",Modeling .,ALM,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.231578947368421,126,0.4649446494464944,104,0.5875706214689266,1,0,Modeling .: ALM
128,"Instead , once introducing a Lagrangian multiplier , it is no longer requiring ? ?.",Modeling .,ALM,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",23,0.2421052631578947,127,0.4686346863468634,105,0.5932203389830508,1,0,Modeling .: ALM
129,"Thus , we rewrite into the standard ALM formulation as :",Modeling .,ALM,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.2526315789473684,128,0.4723247232472324,106,0.5988700564971752,1,0,Modeling .: ALM
130,where ?,Modeling .,ALM,named-entity-recognition,0,"['O', 'O']","['O', 'O']",25,0.2631578947368421,129,0.4760147601476014,107,0.6045197740112994,1,0,Modeling .: ALM
132,"In the following , a highly efficient solver will be given .",Modeling .,ALM,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.2842105263157894,131,0.4833948339483395,109,0.615819209039548,1,0,Modeling .: ALM
133,The updating rule for ?,Modeling .,ALM,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O']",28,0.2947368421052631,132,0.4870848708487085,110,0.6214689265536724,1,0,Modeling .: ALM
134,"Similar to the iterative thresholding ( IT ) in , the degree of violations of the L N equality constraints are used to update the Lagrangian multiplier :",Modeling .,ALM,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",29,0.3052631578947368,133,0.4907749077490775,111,0.6271186440677966,1,0,Modeling .: ALM
135,where is a monotonically increasing parameter over iteration steps .,Modeling .,ALM,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",30,0.3157894736842105,134,0.4944649446494465,112,0.632768361581921,1,0,Modeling .: ALM
136,"For example , ? ? , where 1 < ? < 2 is a predefined parameter .",Modeling .,ALM,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",31,0.3263157894736842,135,0.4981549815498155,113,0.6384180790960452,1,0,Modeling .: ALM
137,"Efficient solver for E Removing irrelevant terms with E from ( 8 ) , we have",Modeling .,ALM,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",32,0.3368421052631579,136,0.5018450184501845,114,0.6440677966101694,1,0,Modeling .: ALM
138,where H = X ? XA ? ? ?,Modeling .,ALM,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",33,0.3473684210526316,137,0.5055350553505535,115,0.6497175141242938,1,0,Modeling .: ALM
140,"According to the definition of the p - norm and the Frobenius - norm , ( 10 ) could be decoupled into L N independent and unconstrained subproblems .",Modeling .,ALM,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",35,0.3684210526315789,139,0.5129151291512916,117,0.6610169491525424,1,0,Modeling .: ALM
141,The standard form of these subproblems is,Modeling .,ALM,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O']",36,0.3789473684210526,140,0.5166051660516605,118,0.6666666666666666,1,0,Modeling .: ALM
142,"where ? = 1 is a given positive parameter , y is the scalar variable need to deal with , c is a known scalar constant .",Modeling .,ALM,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",37,0.3894736842105263,141,0.5202952029520295,119,0.672316384180791,1,0,Modeling .: ALM
143,Zuo et al. ) has recently proposed a generalized iterative shrinkage algorithm to solve ( 11 ) .,Modeling .,ALM,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",38,0.4,142,0.5239852398523985,120,0.6779661016949152,1,0,Modeling .: ALM
145,"Thus , we use it for our problem as :",Modeling .,ALM,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",40,0.4210526315789473,144,0.5313653136531366,122,0.6892655367231638,1,0,Modeling .: ALM
146,is obtained by solving the following equation :,Modeling .,ALM,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",41,0.431578947368421,145,0.5350553505535055,123,0.6949152542372882,1,0,Modeling .: ALM
147,which could be solved efficiently via an iterative algorithm .,Modeling .,ALM,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",42,0.4421052631578947,146,0.5387453874538746,124,0.7005649717514124,1,0,Modeling .: ALM
148,"In this manner , ( 10 ) could be sovled extremely fast .",Modeling .,ALM,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",43,0.4526315789473684,147,0.5424354243542435,125,0.7062146892655368,1,0,Modeling .: ALM
149,"Accelerated solver for A The main acceleration focuses on the solver of A. Removing irrelevant terms with A from ( 8 ) , we have",Modeling .,ALM,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",44,0.4631578947368421,148,0.5461254612546126,126,0.711864406779661,1,0,Modeling .: ALM
150,where ? = ?,Modeling .,ALM,named-entity-recognition,0,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",45,0.4736842105263157,149,0.5498154981549815,127,0.7175141242937854,1,0,Modeling .: ALM
151,"is a nonnegative parameter , P = X ? E ? ? ?",Modeling .,ALM,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",46,0.4842105263157895,150,0.5535055350553506,128,0.7231638418079096,1,0,Modeling .: ALM
152,"R LN . Since ( 13 ) is convex in A , the optimum could be found by differentiating ( 13 ) and setting the derivative to zero .",Modeling .,ALM,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",47,0.4947368421052631,151,0.5571955719557196,129,0.7288135593220338,1,0,Modeling .: ALM
153,This amounts to tackling the following linear system 2 :,Modeling .,ALM,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",48,0.5052631578947369,152,0.5608856088560885,130,0.7344632768361582,1,0,Modeling .: ALM
154,As V + ? X TX ?,Modeling .,ALM,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O']",49,0.5157894736842106,153,0.5645756457564576,131,0.7401129943502824,1,0,Modeling .: ALM
155,"RN N , is mainly a N N linear system .",Modeling .,ALM,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",50,0.5263157894736842,154,0.5682656826568265,132,0.7457627118644068,1,0,Modeling .: ALM
157,This is by no means a good choice for real applications with large N .,Modeling .,ALM,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",52,0.5473684210526316,156,0.5756457564575646,134,0.7570621468926554,1,0,Modeling .: ALM
159,Theorem,Modeling .,,named-entity-recognition,0,['O'],['O'],54,0.5684210526315789,158,0.5830258302583026,136,0.768361581920904,1,0,Modeling .
160,2 . The N N linear system is equivalent to the following L L linear system :,Modeling .,Theorem,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",55,0.5789473684210527,159,0.5867158671586716,137,0.7740112994350282,1,0,Modeling .: Theorem
161,where IL is a L L identity matrix .,Modeling .,Theorem,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",56,0.5894736842105263,160,0.5904059040590406,138,0.7796610169491526,1,0,Modeling .: Theorem
163,"Note that V is a N N diagonal and positive - definite matrix , the exponent of V is efficient to achieve , i.e. V ? = { V ? nn } N n=1 , ?? ?",Modeling .,Proof .,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",58,0.6105263157894737,162,0.5977859778597786,140,0.7909604519774012,1,0,Modeling .: Proof .
165,We have the following equations,Modeling .,R.,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O']",60,0.631578947368421,164,0.6051660516605166,142,0.8022598870056498,1,0,Modeling .: R.
166,"where Z = XV ? 1 2 , IN is a N N identity matrix .",Modeling .,R.,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",61,0.6421052631578947,165,0.6088560885608856,143,0.807909604519774,1,0,Modeling .: R.
167,The following equation holds for any conditions,Modeling .,R.,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O']",62,0.6526315789473685,166,0.6125461254612546,144,0.8135593220338984,1,0,Modeling .: R.
168,"Multiplying with IN + ?Z T Z ?1 on the left and IL + ? ZZ T ? 1 on the right of both sides of the equal - sign , we have the equation as :",Modeling .,R.,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",63,0.6631578947368421,167,0.6162361623616236,145,0.8192090395480226,1,0,Modeling .: R.
169,"Therefore , substituting ( 18 ) and Z = XV ? 1 2 into ( 16 ) , we have the simplified updating rule as :",Modeling .,R.,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",64,0.6736842105263158,168,0.6199261992619927,146,0.8248587570621468,1,0,Modeling .: R.
170,"When N L , the most complex operation is the matrix multiplications , not the L L linear system .",Modeling .,R.,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",65,0.6842105263157895,169,0.6236162361623616,147,0.8305084745762712,1,0,Modeling .: R.
171,Corollary,Modeling .,,named-entity-recognition,0,['O'],['O'],66,0.6947368421052632,170,0.6273062730627307,148,0.8361581920903954,1,0,Modeling .
173,If using ( 14 ) when N ?,Modeling .,Corollary,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",68,0.7157894736842105,172,0.6346863468634686,150,0.847457627118644,1,0,Modeling .: Corollary
174,"L , and otherwise using ( 15 ) as shown in Algorithm 1 , the computational complexity of solvers for ( 13 ) is ON 2 L .",Modeling .,Corollary,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",69,0.7263157894736842,173,0.6383763837638377,151,0.8531073446327684,1,0,Modeling .: Corollary
175,"Due to N L , we have highly reduced the complexity from ON 4 to ON 2 L compared with Nie 's method .",Modeling .,Corollary,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",70,0.7368421052631579,174,0.6420664206642066,152,0.8587570621468926,1,0,Modeling .: Corollary
176,"Algorithm 1 for ( 13 ) : A * = ARSS A ( X , V , P , IL , ?)",Modeling .,Corollary,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",71,0.7473684210526316,175,0.6457564575645757,153,0.864406779661017,1,0,Modeling .: Corollary
177,"Input : X , V , P , IL , ? 1 : if N ?",Modeling .,Corollary,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",72,0.7578947368421053,176,0.6494464944649446,154,0.8700564971751412,1,0,Modeling .: Corollary
178,L then 2 :,Modeling .,Corollary,named-entity-recognition,0,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",73,0.7684210526315789,177,0.6531365313653137,155,0.8757062146892656,1,0,Modeling .: Corollary
179,"update A via the updating rule , that is 3 : update ? by the updating rule ( 9 ) , ? ?.",Modeling .,Corollary,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",74,0.7789473684210526,178,0.6568265682656826,156,0.8813559322033898,1,0,Modeling .: Corollary
181,Output : A The solver to update A is given in Algorithm,Modeling .,Corollary,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",76,0.8,180,0.6642066420664207,158,0.8926553672316384,1,0,Modeling .: Corollary
183,2 .,Modeling .,Corollary,named-entity-recognition,0,"['O', 'O']","['O', 'O']",78,0.8210526315789474,182,0.6715867158671587,160,0.903954802259887,1,0,Modeling .: Corollary
184,"According to Theorem 2 and Corollary 3 , the solver for our model ( 13 ) is highly simplified , as feature length is generally much smaller than data size , i.e L N .",Modeling .,Corollary,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",79,0.8315789473684211,183,0.6752767527675276,161,0.9096045197740112,1,0,Modeling .: Corollary
185,"Similarly ,",Modeling .,,named-entity-recognition,0,"['O', 'O']","['O', 'O']",80,0.8421052631578947,184,0.6789667896678967,162,0.9152542372881356,1,0,Modeling .
186,"Nie 's method could be highly accelerated by Theorem 4 , obtaining 500 + times speedup , as shown in and .",Modeling .,"Similarly ,",named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",81,0.8526315789473684,185,0.6826568265682657,163,0.9209039548022598,1,0,"Modeling .: Similarly ,"
187,Theorem,Modeling .,,named-entity-recognition,0,['O'],['O'],82,0.8631578947368421,186,0.6863468634686347,164,0.9265536723163842,1,0,Modeling .
188,4 . Nie 's N N solver ( 20 ) ) is equivalent to the following L L linear system ( 21 ) an = U nn U nn X TX + ? V,Modeling .,Theorem,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",83,0.8736842105263158,187,0.6900369003690037,165,0.9322033898305084,1,0,Modeling .: Theorem
189,?1 X T x n ( 20 ),Modeling .,Theorem,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",84,0.8842105263157894,188,0.6937269372693727,166,0.9378531073446328,1,0,Modeling .: Theorem
190,"?n ? { 1 , 2 , , N } , where IL is a L L identity matrix .",Modeling .,Theorem,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",85,0.8947368421052632,189,0.6974169741697417,167,0.943502824858757,1,0,Modeling .: Theorem
195,RN,Modeling .,,named-entity-recognition,0,['O'],['O'],90,0.9473684210526316,194,0.7158671586715867,172,0.9717514124293786,1,0,Modeling .
196,"N is a positive and diagonal matrix with then th diagonal entry as Vnn = 1 ? an 2 2 + > 0 , where is a small value to avoid singular failures Corollary 5 .",Modeling .,RN,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",91,0.9578947368421052,195,0.7195571955719557,173,0.9774011299435028,1,0,Modeling .: RN
197,"Since feature length is generally much smaller than data size , i.e. L N , our accelerated solver ( 20 ) for Nie 's model ( 3 ) is highly faster than the authorial solver ( 21 ) .",Modeling .,RN,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",92,0.968421052631579,196,0.7232472324723247,174,0.9830508474576272,1,0,Modeling .: RN
200,Extensive empirical results will verify the huge acceleration,Modeling .,RN,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",95,1.0,199,0.7343173431734318,177,1.0,1,0,Modeling .: RN
201,Experiments Experimental Settings,,,named-entity-recognition,0,"['O', 'O', 'O']","['O', 'O', 'O']",0,0.0,200,0.7380073800738007,0,0.0,1,0,
204,"Brief descriptions often benchmark datasets are summarized in , where ' Total ( N * ) ' denotes the total set of samples in each data .",Experiments Experimental Settings,Experiments Experimental Settings,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.0517241379310344,203,0.7490774907749077,3,0.375,1,0,Experiments Experimental Settings
205,"Due to the high computational complexity , other methods can only handle small datasets ( while our method can handle the total set ) .",Experiments Experimental Settings,Experiments Experimental Settings,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.0689655172413793,204,0.7527675276752768,4,0.5,1,0,Experiments Experimental Settings
206,"Thus , we randomly choose the candidate set from the total set to reduce the sample size , i.e. N < N * ( cf. ' Total ( N * ) ' and ' candid . ( N ) ' in ) .",Experiments Experimental Settings,Experiments Experimental Settings,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.0862068965517241,205,0.7564575645756457,5,0.625,1,0,Experiments Experimental Settings
207,The remainder ( except candidate set ) are used for test .,Experiments Experimental Settings,Experiments Experimental Settings,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.1034482758620689,206,0.7601476014760148,6,0.75,1,0,Experiments Experimental Settings
208,"Specifically , to simulate the varying quality of samples , ten percentage of candidate samples from each class are randomly selected and arbitrarily added one of the following three kinds of noise : "" Gaussian "" , "" Laplace "" and "" Salt & pepper "" respectively .",Experiments Experimental Settings,Experiments Experimental Settings,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.1206896551724138,207,0.7638376383763837,7,0.875,1,0,Experiments Experimental Settings
213,Then the comparison of specific speed is summarized in .,Experiments Experimental Settings,Speed Comparisons,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.2068965517241379,212,0.7822878228782287,3,0.1304347826086956,1,0,Experiments Experimental Settings: Speed Comparisons
214,Note that TED and RRSS Nie denote the authorial solver ( via authorial codes ) ; RRSS our is our accelerated solver for Nie 's model via Theorem 4 ; ARSS is the proposed method .,Experiments Experimental Settings,Speed Comparisons,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.2241379310344827,213,0.7859778597785978,4,0.1739130434782608,1,0,Experiments Experimental Settings: Speed Comparisons
217,"As we shall see , both selection time of TED and RRSS Nie increases dramatically as N increases .",Experiments Experimental Settings,Speed Comparisons,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.2758620689655172,216,0.7970479704797048,7,0.3043478260869565,1,0,Experiments Experimental Settings: Speed Comparisons
218,"No surprisingly , RRSS Nie is incredibly time - consuming as N grows the order of curves looks higher than quadratic .",Experiments Experimental Settings,Speed Comparisons,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.293103448275862,217,0.8007380073800738,8,0.3478260869565217,1,0,Experiments Experimental Settings: Speed Comparisons
219,"Actually , the theoretical complexity of RRSS Nie is highly up to ON 4 as analyzed in Remark",Experiments Experimental Settings,Speed Comparisons,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.3103448275862069,218,0.8044280442804428,9,0.391304347826087,1,0,Experiments Experimental Settings: Speed Comparisons
220,1 .,Experiments Experimental Settings,Speed Comparisons,named-entity-recognition,0,"['O', 'O']","['O', 'O']",19,0.3275862068965517,219,0.8081180811808119,10,0.4347826086956521,1,0,Experiments Experimental Settings: Speed Comparisons
222,This is owing to the speedup techniques of ALM and equivalent derivations .,Experiments Experimental Settings,Speed Comparisons,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.3620689655172414,221,0.8154981549815498,12,0.5217391304347826,1,0,Experiments Experimental Settings: Speed Comparisons
224,3 .,Experiments Experimental Settings,Speed Comparisons,named-entity-recognition,0,"['O', 'O']","['O', 'O']",23,0.396551724137931,223,0.8228782287822878,14,0.6086956521739131,1,0,Experiments Experimental Settings: Speed Comparisons
226,"Speed with fixed N The speed of four algorithms is summarized in , where each row shows the results on one dataset and the last row displays the average results .",Experiments Experimental Settings,Speed Comparisons,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.4310344827586206,225,0.8302583025830258,16,0.6956521739130435,1,0,Experiments Experimental Settings: Speed Comparisons
230,"Third , ARSS is dramatically faster than :",Experiments Experimental Settings,Speed Comparisons,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",29,0.5,229,0.8450184501845018,20,0.8695652173913043,1,0,Experiments Experimental Settings: Speed Comparisons
234,Datasets,Experiments Experimental Settings,,named-entity-recognition,0,['O'],['O'],33,0.5689655172413793,233,0.8597785977859779,0,0.0,1,0,Experiments Experimental Settings
235,"Speed 1 ' ARSS ( N * ) ' means the task of selecting samples from the whole dataset ( with N * samples as shown in the 2 nd column in ) , while ' TED ' to ' ARSS ' indicate the problem of dealing with the candidate sample sets ( with N samples as shown in the 3 rd column in ) .",Experiments Experimental Settings,Datasets,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",34,0.5862068965517241,234,0.8634686346863468,1,0.1666666666666666,1,0,Experiments Experimental Settings: Datasets
237,"This means that for example if it takes RRSS Nie 100 years to do a subset selection task , it only takes our method 1.6 days to address the same problem .",Experiments Experimental Settings,Datasets,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",36,0.6206896551724138,236,0.8708487084870848,3,0.5,1,0,Experiments Experimental Settings: Datasets
239,RRSS Nie and TED ; the results in,Experiments Experimental Settings,Datasets,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",38,0.6551724137931034,238,0.8782287822878229,5,0.8333333333333334,1,0,Experiments Experimental Settings: Datasets
241,Prediction Accuracy,Experiments Experimental Settings,,named-entity-recognition,0,"['O', 'O']","['O', 'O']",40,0.6896551724137931,240,0.8856088560885609,0,0.0,1,0,Experiments Experimental Settings
242,Accuracy comparison,Experiments Experimental Settings,,named-entity-recognition,0,"['O', 'O']","['O', 'O']",41,0.7068965517241379,241,0.8892988929889298,1,0.0555555555555555,1,0,Experiments Experimental Settings
250,The above analyses are better illustrated in the last row of .,Experiments Experimental Settings,Accuracy comparison,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",49,0.8448275862068966,249,0.918819188191882,9,0.5,1,0,Experiments Experimental Settings: Accuracy comparison
252,"Prediction accuracies vs. increasing K To give a more detailed comparison , shows the prediction accuracies versus growing K ( the number of selected samples ) .",Experiments Experimental Settings,Accuracy comparison,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",51,0.8793103448275862,251,0.92619926199262,11,0.6111111111111112,1,0,Experiments Experimental Settings: Accuracy comparison
257,Such case is consistent with the common view that more training data will boost the prediction accuracy .,Experiments Experimental Settings,Accuracy comparison,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",56,0.9655172413793104,256,0.9446494464944648,16,0.8888888888888888,1,0,Experiments Experimental Settings: Accuracy comparison
259,This case implies that our robust objective ( 5 ) via the p - norm is feasible to select subsets from the data of varying qualities .,Experiments Experimental Settings,Accuracy comparison,named-entity-recognition,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",58,1.0,258,0.9520295202952028,18,1.0,1,0,Experiments Experimental Settings: Accuracy comparison
260,Conclusion,,,named-entity-recognition,0,['O'],['O'],0,0.0,259,0.955719557195572,0,0.0,1,0,
2,Neural Architectures for Named Entity Recognition,title,,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O']",1,0.0,1,0.0048309178743961,1,0.0,1,0,title
3,abstract,,,named-entity-recognition,1,['O'],['O'],0,0.0,2,0.0096618357487922,0,0.0,1,0,
4,"State - of - the - art named entity recognition systems rely heavily on hand - crafted features and domain - specific knowledge in order to learn effectively from the small , supervised training corpora that are available .",abstract,abstract,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.2,3,0.0144927536231884,1,0.2,1,0,abstract
8,1,abstract,abstract,named-entity-recognition,1,['O'],['O'],5,1.0,7,0.0338164251207729,5,1.0,1,0,abstract
11,"One the one hand , inmost languages and domains , there is only a very small amount of supervised training data available .",Introduction,Introduction,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.1176470588235294,10,0.0483091787439613,2,0.1,1,0,Introduction
12,"On the other , there are few constraints on the kinds of words that can be names , so generalizing from this small sample of data is difficult .",Introduction,Introduction,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.1764705882352941,11,0.0531400966183574,3,0.15,1,0,Introduction
13,"As a result , carefully constructed orthographic features and language - specific knowledge resources , such as gazetteers , are widely used for solving this task .",Introduction,Introduction,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.2352941176470588,12,0.0579710144927536,4,0.2,1,0,Introduction
14,"Unfortunately , languagespecific resources and features are costly to develop in new languages and new domains , making NER a challenge to adapt .",Introduction,Introduction,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.2941176470588235,13,0.0628019323671497,5,0.25,1,0,Introduction
16,"However , even systems that have relied extensively on unsupervised features have used these to augment , rather than replace , hand - engineered features ( e.g. , knowledge about capitalization patterns and character classes in a particular language ) and specialized knowledge resources ( e.g. , gazetteers ) .",Introduction,Introduction,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.4117647058823529,15,0.072463768115942,7,0.35,1,0,Introduction
19,"First , since names often consist of multiple tokens , reasoning jointly over tagging decisions for each token is important .",Introduction,Introduction,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.5882352941176471,18,0.0869565217391304,10,0.5,1,0,Introduction
26,LSTM - CRF,Introduction,Introduction,named-entity-recognition,1,"['O', 'O', 'O']","['O', 'O', 'O']",17,1.0,25,0.1207729468599033,17,0.85,1,0,Introduction
27,Model,,,named-entity-recognition,1,['O'],['O'],0,0.0,26,0.1256038647342995,18,0.9,1,0,
29,This architecture is similar to the ones presented by .,Model,Model,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.0909090909090909,28,0.1352657004830917,20,1.0,1,0,Model
30,LSTM,Model,,named-entity-recognition,1,['O'],['O'],3,0.1363636363636363,29,0.1400966183574879,0,0.0,1,0,Model
31,Recurrent neural networks ( RNNs ) area family of neural networks that operate on sequential data .,Model,LSTM,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.1818181818181818,30,0.144927536231884,1,0.0263157894736842,1,0,Model: LSTM
32,"They take as input a sequence of vectors ( x 1 , x 2 , . . . , x n ) and return another sequence ( h 1 , h 2 , . . . , h n ) that represents some information about the sequence at every step in the input .",Model,LSTM,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.2272727272727272,31,0.1497584541062802,2,0.0526315789473684,1,0,Model: LSTM
33,"Although RNNs can , in theory , learn long dependencies , in practice they fail to do so and tend to be biased towards their most recent inputs in the sequence .",Model,LSTM,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.2727272727272727,32,0.1545893719806763,3,0.0789473684210526,1,0,Model: LSTM
34,Long Short - term Memory Networks ( LSTMs ) have been designed to combat this issue by incorporating a memory - cell and have been shown to capture long - range dependencies .,Model,LSTM,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.3181818181818182,33,0.1594202898550724,4,0.1052631578947368,1,0,Model: LSTM
35,"They do so using several gates that control the proportion of the input to give to the memory cell , and the proportion from the previous state to forget .",Model,LSTM,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.3636363636363636,34,0.1642512077294686,5,0.131578947368421,1,0,Model: LSTM
37,where ?,Model,LSTM,named-entity-recognition,1,"['O', 'O']","['O', 'O']",10,0.4545454545454545,36,0.1739130434782608,7,0.1842105263157894,1,0,Model: LSTM
38,"is the element - wise sigmoid function , and is the element - wise product .",Model,LSTM,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.5,37,0.178743961352657,8,0.2105263157894736,1,0,Model: LSTM
39,"For a given sentence ( x 1 , x 2 , . . . , x n ) containing n words , each represented as a d-dimensional vector , an LSTM computes a representation ? ?",Model,LSTM,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.5454545454545454,38,0.1835748792270531,9,0.2368421052631578,1,0,Model: LSTM
40,ht of the left context of the sentence at every word t.,Model,LSTM,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.5909090909090909,39,0.1884057971014492,10,0.2631578947368421,1,0,Model: LSTM
42,ht as well should add useful information .,Model,LSTM,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.6818181818181818,41,0.1980676328502415,12,0.3157894736842105,1,0,Model: LSTM
43,This can be achieved using a second LSTM that reads the same sequence in reverse .,Model,LSTM,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.7272727272727273,42,0.2028985507246377,13,0.3421052631578947,1,0,Model: LSTM
44,We will refer to the former as the forward LSTM and the latter as the backward LSTM .,Model,LSTM,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.7727272727272727,43,0.2077294685990338,14,0.3684210526315789,1,0,Model: LSTM
46,This forward and backward LSTM pair is referred to as a bidirectional LSTM .,Model,LSTM,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.8636363636363636,45,0.217391304347826,16,0.4210526315789473,1,0,Model: LSTM
47,"The representation of a word using this model is obtained by concatenating its left and right context representations ,",Model,LSTM,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.9090909090909092,46,0.2222222222222222,17,0.4473684210526316,1,0,Model: LSTM
49,CRF,Model,,named-entity-recognition,1,['O'],['O'],22,1.0,48,0.2318840579710145,19,0.5,1,0,Model
51,Avery simple - but surprisingly effective - tagging model is to use the ht 's as features to make independent tagging decisions for each output y t.,Tagging Models,Tagging Models,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0555555555555555,50,0.2415458937198067,21,0.5526315789473685,1,0,Tagging Models
52,"Despite this model 's success in simple problems like POS tagging , its independent classification decisions are limiting when there are strong dependencies across output labels .",Tagging Models,Tagging Models,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.1111111111111111,51,0.2463768115942029,22,0.5789473684210527,1,0,Tagging Models
53,"NER is one such task , since the "" grammar "" that characterizes interpretable sequences of tags imposes several hard constraints ( e.g. , I - PER can not follow B - LOC ; see 2.4 for details ) that would be impossible to model with independence assumptions .",Tagging Models,Tagging Models,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.1666666666666666,52,0.251207729468599,23,0.6052631578947368,1,0,Tagging Models
55,For an input sentence,Tagging Models,Tagging Models,named-entity-recognition,1,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",5,0.2777777777777778,54,0.2608695652173913,25,0.6578947368421053,1,0,Tagging Models
56,we consider P to be the matrix of scores output by the bidirectional LSTM network .,Tagging Models,Tagging Models,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.3333333333333333,55,0.2657004830917874,26,0.6842105263157895,1,0,Tagging Models
57,"P is of size n k , where k is the number of distinct tags , and P i , j corresponds to the score of the j th tag of the i th word in a sentence .",Tagging Models,Tagging Models,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.3888888888888889,56,0.2705314009661835,27,0.7105263157894737,1,0,Tagging Models
58,"For a sequence of predictions y = ( y 1 , y 2 , . . . , y n ) , we define it s score to be",Tagging Models,Tagging Models,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.4444444444444444,57,0.2753623188405797,28,0.7368421052631579,1,0,Tagging Models
59,"where A is a matrix of transition scores such that A i , j represents the score of a transition from the tag i to tag j. y 0 and y n are the start and end tags of a sentence , that we add to the set of possible tags .",Tagging Models,Tagging Models,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.5,58,0.2801932367149758,29,0.7631578947368421,1,0,Tagging Models
60,A is therefore a square matrix of size k + 2 .,Tagging Models,Tagging Models,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.5555555555555556,59,0.2850241545893719,30,0.7894736842105263,1,0,Tagging Models
61,A softmax overall possible tag sequences yields a probability for the sequence y:,Tagging Models,Tagging Models,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.6111111111111112,60,0.2898550724637681,31,0.8157894736842105,1,0,Tagging Models
62,"y ?Y Xe s ( X , y) .",Tagging Models,Tagging Models,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.6666666666666666,61,0.2946859903381642,32,0.8421052631578947,1,0,Tagging Models
63,"During training , we maximize the log-probability of the correct tag sequence :",Tagging Models,Tagging Models,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.7222222222222222,62,0.2995169082125604,33,0.868421052631579,1,0,Tagging Models
64,where Y X represents all possible tag sequences ( even those that do not verify the IOB format ) fora sentence X .,Tagging Models,Tagging Models,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.7777777777777778,63,0.3043478260869565,34,0.8947368421052632,1,0,Tagging Models
65,"From the formulation above , it is evident that we encourage our network to produce a valid sequence of output labels .",Tagging Models,Tagging Models,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.8333333333333334,64,0.3091787439613526,35,0.9210526315789472,1,0,Tagging Models
66,"While decoding , we predict the output sequence that obtains the maximum score given by :",Tagging Models,Tagging Models,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.8888888888888888,65,0.3140096618357488,36,0.9473684210526316,1,0,Tagging Models
67,"Since we are only modeling bigram interactions between outputs , both the summation in Eq. 1 and the maximum a posteriori sequence y * in Eq.",Tagging Models,Tagging Models,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.9444444444444444,66,0.3188405797101449,37,0.9736842105263158,1,0,Tagging Models
68,2 can be computed using dynamic programming .,Tagging Models,Tagging Models,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,1.0,67,0.323671497584541,38,1.0,1,0,Tagging Models
69,Parameterization and Training,,,named-entity-recognition,1,"['O', 'O', 'O']","['O', 'O', 'O']",0,0.0,68,0.3285024154589372,0,0.0,1,0,
70,"The scores associated with each tagging decision for each token ( i.e. , the P i , y 's ) are defined to be the dot product between the embedding of a wordin - context computed with a bidirectional LSTMexactly the same as the POS tagging model of and these are combined with bigram compatibility scores ( i.e. , the A y , y 's ) .",Parameterization and Training,Parameterization and Training,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0125,69,0.3333333333333333,1,0.0833333333333333,1,0,Parameterization and Training
71,This architecture is shown in figure,Parameterization and Training,Parameterization and Training,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O']",2,0.025,70,0.3381642512077294,2,0.1666666666666666,1,0,Parameterization and Training
72,"1 . Circles represent observed variables , diamonds are deterministic functions of their parents , and double circles are random variables .",Parameterization and Training,Parameterization and Training,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.0375,71,0.3429951690821256,3,0.25,1,0,Parameterization and Training
73,"The parameters of this model are thus the matrix of bigram compatibility scores A , and the parameters that give rise to the matrix P , namely the parameters of the bidirectional LSTM , the linear feature weights , and the word embeddings .",Parameterization and Training,Parameterization and Training,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.05,72,0.3478260869565217,4,0.3333333333333333,1,0,Parameterization and Training
74,"As in part 2.2 , let x i denote the sequence of word embeddings for every word in a sentence , and y i be their associated tags .",Parameterization and Training,Parameterization and Training,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.0625,73,0.3526570048309179,5,0.4166666666666667,1,0,Parameterization and Training
82,Tagging Schemes,Parameterization and Training,,named-entity-recognition,1,"['O', 'O']","['O', 'O']",13,0.1625,81,0.391304347826087,0,0.0,1,0,Parameterization and Training
84,A single named entity could span several tokens within a sentence .,Parameterization and Training,Tagging Schemes,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.1875,83,0.4009661835748792,2,0.125,1,0,Parameterization and Training: Tagging Schemes
85,"Sentences are usually represented in the IOB format ( Inside , Outside , Beginning ) where every token is labeled as B- label if the token is the beginning of a named entity , I-label if it is inside a named entity but not the first token within the named entity , or O otherwise .",Parameterization and Training,Tagging Schemes,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.2,84,0.4057971014492754,3,0.1875,1,0,Parameterization and Training: Tagging Schemes
86,"However , we decided to use the IOBES tagging scheme , a variant of IOB commonly used for named entity recognition , which encodes information about singleton entities ( S ) and explicitly marks the end of named entities ( E ) .",Parameterization and Training,Tagging Schemes,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.2125,85,0.4106280193236715,4,0.25,1,0,Parameterization and Training: Tagging Schemes
87,"Using this scheme , tagging a word as I-label with high - confidence narrows down the choices for the subsequent word to I-label or E-label , however , the IOB scheme is only capable of determining that the subsequent word can not be the interior of another label .",Parameterization and Training,Tagging Schemes,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.225,86,0.4154589371980676,5,0.3125,1,0,Parameterization and Training: Tagging Schemes
88,Ratinov and Roth and showed that using a more expressive tagging scheme like IOBES improves model performance marginally .,Parameterization and Training,Tagging Schemes,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.2375,87,0.4202898550724637,6,0.375,1,0,Parameterization and Training: Tagging Schemes
90,Transition - Based Chunking Model,Parameterization and Training,,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O']",21,0.2625,89,0.429951690821256,8,0.5,1,0,Parameterization and Training
92,"This model directly constructs representations of the multi-token names ( e.g. , the name Mark Watney is composed into a single representation ) .",Parameterization and Training,Transition - Based Chunking Model,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",23,0.2875,91,0.4396135265700483,10,0.625,1,0,Parameterization and Training: Transition - Based Chunking Model
94,"To obtain representations of this stack used for predicting subsequent actions , we use the Stack - LSTM presented by , in which the LSTM is augmented with a "" stack pointer . """,Parameterization and Training,Transition - Based Chunking Model,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.3125,93,0.4492753623188406,12,0.75,1,0,Parameterization and Training: Transition - Based Chunking Model
95,"While sequential LSTMs model sequences from left to right , stack LSTMs permit embedding of a stack of objects that are both added to ( using a push operation ) and removed from ( using a pop operation ) .",Parameterization and Training,Transition - Based Chunking Model,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.325,94,0.4541062801932367,13,0.8125,1,0,Parameterization and Training: Transition - Based Chunking Model
97,We refer to this model as Stack - LSTM or S - LSTM model for simplicity .,Parameterization and Training,Transition - Based Chunking Model,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",28,0.35,96,0.463768115942029,15,0.9375,1,0,Parameterization and Training: Transition - Based Chunking Model
98,"Finally , we refer interested readers to the original paper for details about the Stack - LSTM model since in this paper we merely use the same architecture through anew transition - based algorithm presented in the following Section .",Parameterization and Training,Transition - Based Chunking Model,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",29,0.3625,97,0.4685990338164251,16,1.0,1,0,Parameterization and Training: Transition - Based Chunking Model
99,Chunking Algorithm,Parameterization and Training,,named-entity-recognition,1,"['O', 'O']","['O', 'O']",30,0.375,98,0.4734299516908212,0,0.0,1,0,Parameterization and Training
101,"In this algorithm , we make use of two stacks ( designated output and stack representing , respectively , completed chunks and scratch space ) and a buffer that contains the words that have yet to be processed .",Parameterization and Training,Chunking Algorithm,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",32,0.4,100,0.4830917874396135,2,0.1428571428571428,1,0,Parameterization and Training: Chunking Algorithm
103,"The SHIFT transition moves a word from the buffer to the stack , the OUT transition moves a word from the buffer directly into the output stack while the REDUCE ( y ) transition pops all items from the top of the stack creating a "" chunk , "" labels this with label y , and pushes a representation of this chunk onto the output stack .",Parameterization and Training,Chunking Algorithm,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",34,0.425,102,0.4927536231884058,4,0.2857142857142857,1,0,Parameterization and Training: Chunking Algorithm
105,"The algorithm is depicted in , which shows the sequence of operations required to process the sentence Mark Watney visited Mars .",Parameterization and Training,Chunking Algorithm,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",36,0.45,104,0.5024154589371981,6,0.4285714285714285,1,0,Parameterization and Training: Chunking Algorithm
112,"Since each token is either moved directly to the output ( 1 action ) or first to the stack and then the output ( 2 actions ) , the total number of actions fora sequence of length n is maximally 2n .",Parameterization and Training,Chunking Algorithm,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",43,0.5375,111,0.5362318840579711,13,0.9285714285714286,1,0,Parameterization and Training: Chunking Algorithm
113,It is worth noting that the nature of this algorithm,Parameterization and Training,Chunking Algorithm,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",44,0.55,112,0.5410628019323671,14,1.0,1,0,Parameterization and Training: Chunking Algorithm
115,"When the REDUCE ( y ) operation is executed , the algorithm shifts a sequence of tokens ( together with their vector embeddings ) from the stack to the output buffer as a single completed chunk .",Parameterization and Training,Representing Labeled Chunks,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",46,0.575,114,0.5507246376811594,1,0.25,1,0,Parameterization and Training: Representing Labeled Chunks
117,"This function is given as g ( u , . . . , v , r y ) , where r y is a learned embedding of a label type .",Parameterization and Training,Representing Labeled Chunks,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",48,0.6,116,0.5603864734299517,3,0.75,1,0,Parameterization and Training: Representing Labeled Chunks
118,"Thus , the output buffer contains a single vector representation for each labeled chunk that is generated , regardless of its length .",Parameterization and Training,Representing Labeled Chunks,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",49,0.6125,117,0.5652173913043478,4,1.0,1,0,Parameterization and Training: Representing Labeled Chunks
121,Learning independent representations for word types from the limited NER training data is a difficult problem : there are simply too many parameters to reliably estimate .,Parameterization and Training,Input Word Embeddings,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",52,0.65,120,0.5797101449275363,2,0.0869565217391304,1,0,Parameterization and Training: Input Word Embeddings
122,"Since many languages have orthographic or morphological evidence that something is a name ( or not a name ) , we want representations that are sensitive to the spelling of words .",Parameterization and Training,Input Word Embeddings,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",53,0.6625,121,0.5845410628019324,3,0.1304347826086956,1,0,Parameterization and Training: Input Word Embeddings
124,"Our second intuition is that names , which may individually be quite varied , appear in regular contexts in large corpora .",Parameterization and Training,Input Word Embeddings,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",55,0.6875,123,0.5942028985507246,5,0.217391304347826,1,0,Parameterization and Training: Input Word Embeddings
135,"During testing , words that do not have an embedding in the lookup table are mapped to a UNK embedding .",Parameterization and Training,Input Word Embeddings,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",66,0.825,134,0.6473429951690821,16,0.6956521739130435,1,0,Parameterization and Training: Input Word Embeddings
138,"Recurrent models like RNNs and LSTMs are capable of encoding very long sequences , however , they have a representation biased towards their most recent inputs .",Parameterization and Training,Input Word Embeddings,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",69,0.8625,137,0.6618357487922706,19,0.8260869565217391,1,0,Parameterization and Training: Input Word Embeddings
140,Alternative approachesmost notably like convolutional networks - have been proposed to learn representations of words from their characters .,Parameterization and Training,Input Word Embeddings,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",71,0.8875,139,0.6714975845410628,21,0.9130434782608696,1,0,Parameterization and Training: Input Word Embeddings
141,"However , convnets are designed to discover position - invariant features of their inputs .",Parameterization and Training,Input Word Embeddings,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",72,0.9,140,0.6763285024154589,22,0.9565217391304348,1,0,Parameterization and Training: Input Word Embeddings
142,"While this is appropriate for many problems , e.g. , image recognition ( a cat can appear anywhere in a picture ) , we argue that important information is position dependent ( e.g. , prefixes and suffixes encode different information than stems ) , making LSTMs an a priori better function class for modeling the relationship between words and their characters .",Parameterization and Training,Input Word Embeddings,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",73,0.9125,141,0.6811594202898551,23,1.0,1,0,Parameterization and Training: Input Word Embeddings
150,Dropout training,,,named-entity-recognition,1,"['O', 'O']","['O', 'O']",0,0.0,149,0.7198067632850241,0,0.0,1,0,
154,Experiments,,,named-entity-recognition,1,['O'],['O'],0,0.0,153,0.7391304347826086,0,0.0,1,0,
156,Training,,,named-entity-recognition,1,['O'],['O'],0,0.0,155,0.748792270531401,0,0.0,1,0,
158,"Several methods have been proposed to enhance the performance of SGD , such as Adadelta or Adam ( Kingma and Ba , 2014 ) .",Training,Training,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.0666666666666666,157,0.7584541062801933,2,0.1666666666666666,1,0,Training
167,3,Training,Training,named-entity-recognition,1,['O'],['O'],11,0.3666666666666666,166,0.8019323671497585,11,0.9166666666666666,1,0,Training
168,"It is a greedy model that apply locally optimal actions until the entire sentence is processed , further improvements might be obtained with beam search or training with exploration .",Training,Training,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.4,167,0.8067632850241546,12,1.0,1,0,Training
169,Data Sets,Training,,named-entity-recognition,1,"['O', 'O']","['O', 'O']",13,0.4333333333333333,168,0.8115942028985508,0,0.0,1,0,Training
172,"All datasets contain four different types of named entities : locations , persons , organizations , and miscellaneous entities that do not belong in any of the three previous categories .",Training,Data Sets,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.5333333333333333,171,0.8260869565217391,3,0.1764705882352941,1,0,Training: Data Sets
173,"Although POS tags were made available for all datasets , we did not include them in our models .",Training,Data Sets,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.5666666666666667,172,0.8309178743961353,4,0.2352941176470588,1,0,Training: Data Sets
174,"We did not perform any dataset preprocessing , apart from replacing every digit with a zero in the English NER dataset .",Training,Data Sets,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.6,173,0.8357487922705314,5,0.2941176470588235,1,0,Training: Data Sets
177,Our models do not use gazetteers or any external labeled resources .,Training,Data Sets,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.7,176,0.8502415458937198,8,0.4705882352941176,1,0,Training: Data Sets
178,The best score reported on this task is by .,Training,Data Sets,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.7333333333333333,177,0.855072463768116,9,0.5294117647058824,1,0,Training: Data Sets
185,"The only exception is Dutch , where the model of can perform better by leveraging the information from other NER datasets .",Training,Data Sets,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",29,0.9666666666666668,184,0.8888888888888888,16,0.9411764705882352,1,0,Training: Data Sets
187,Results,,,named-entity-recognition,1,['O'],['O'],0,0.0,186,0.8985507246376812,0,0.0,1,0,
188,"As we can see in the tables , the Stack - LSTM model is more dependent on character - based representations to achieve competitive performance ; we hypothesize that the LSTM - CRF model requires less orthographic information since it gets more contextual information out of the bidirectional LSTMs ; however , the Stack - LSTM model consumes the words one by one and it just relies on the word representations when it chunks words .",Results,Results,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0,187,0.9033816425120772,1,0.0,1,0,Results
192,also used a similar model and adapted it to the semantic role labeling task .,Network architectures,Network architectures,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.2307692307692307,191,0.9227053140096618,3,0.2307692307692307,1,0,Network architectures
194,also used a linear chain CRF with spelling features and gazetteers .,Network architectures,Network architectures,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.3846153846153846,193,0.932367149758454,5,0.3846153846153846,1,0,Network architectures
196,NER models like ours have also been proposed in the past .,Network architectures,Language independent,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.5384615384615384,195,0.9420289855072465,7,0.5384615384615384,1,0,Network architectures: Language independent
200,"Finally , there is currently a lot of interest in models for NER that use letter - based representations .",Network architectures,Language independent,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.8461538461538461,199,0.961352657004831,11,0.8461538461538461,1,0,Network architectures: Language independent
202,"employ an architecture similar to ours , but instead use CNNs to learn character - level features , in away similar to the work by .",Network architectures,Language independent,named-entity-recognition,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,1.0,201,0.9710144927536232,13,1.0,1,0,Network architectures: Language independent
203,Conclusion,,,named-entity-recognition,1,['O'],['O'],0,0.0,202,0.9758454106280192,0,0.0,1,0,
2,Fast and Accurate Entity Recognition with Iterated Dilated Convolutions,title,,named-entity-recognition,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0,1,0.0046948356807511,1,0.0,1,0,title
3,abstract,,,named-entity-recognition,2,['O'],['O'],0,0.0,2,0.0093896713615023,0,0.0,1,0,
5,Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) .,abstract,abstract,named-entity-recognition,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.2857142857142857,4,0.0187793427230046,2,0.2857142857142857,1,0,abstract
13,Speed is not sufficient of course : they must also be expressive enough to tolerate the tremendous lexical variation in input data .,Introduction,Introduction,named-entity-recognition,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.0588235294117647,12,0.056338028169014,2,0.0350877192982456,1,0,Introduction
15,"While these models are expressive and accurate , they fail to fully exploit the parallelism opportunities of a GPU , and thus their speed is limited .",Introduction,Introduction,named-entity-recognition,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.1176470588235294,14,0.0657276995305164,4,0.0701754385964912,1,0,Introduction
18,Convolutional neural networks ( CNNs ) provide exactly this property .,Introduction,Introduction,named-entity-recognition,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.2058823529411764,17,0.0798122065727699,7,0.1228070175438596,1,0,Introduction
21,"This provides , for example , audio generation models that can be trained in parallel ( van den .",Introduction,Introduction,named-entity-recognition,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.2941176470588235,20,0.0938967136150234,10,0.175438596491228,1,0,Introduction
22,"Despite the clear computational advantages of CNNs , RNNs have become the standard method for composing deep representations of text .",Introduction,Introduction,named-entity-recognition,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.3235294117647059,21,0.0985915492957746,11,0.1929824561403508,1,0,Introduction
23,"This is because a token encoded by a bidirectional RNN will incorporate evidence from the entire input sequence , but the CNN 's representation is limited by the effective input width 1 of the network : the size of the input context which is observed , directly or indirectly , by the representation of a token at a given layer in the network .",Introduction,Introduction,named-entity-recognition,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.3529411764705882,22,0.1032863849765258,12,0.2105263157894736,1,0,Introduction
24,"Specifically , in a network composed of a series of stacked convolutional layers of convolution width w , the number r of context tokens incorporated into a token 's representation at a given layer l , is given by r = l ( w ?",Introduction,Introduction,named-entity-recognition,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.3823529411764705,23,0.107981220657277,13,0.2280701754385964,1,0,Introduction
25,1 ) + 1 . The number of layers required to incorporate the entire input context grows linearly with the length of the sequence .,Introduction,Introduction,named-entity-recognition,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.4117647058823529,24,0.1126760563380281,14,0.2456140350877192,1,0,Introduction
26,"To avoid this scaling , one could pool representations across the sequence , but this is not appropriate for sequence labeling , since it reduces the output resolution of the representation .",Introduction,Introduction,named-entity-recognition,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.4411764705882353,25,0.1173708920187793,15,0.2631578947368421,1,0,Introduction
28,"For dilated convolutions , the effective input width can grow exponentially with the depth , with no loss in resolution at each layer and with a modest number of parameters to estimate .",Introduction,Introduction,named-entity-recognition,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.5,27,0.1267605633802817,17,0.2982456140350877,1,0,Introduction
29,"Like typical CNN layers , dilated convolutions operate on a sliding window of context over the sequence , but unlike conventional convolutions , the context need not be consecutive ; the dilated window skips over every dilation width d inputs .",Introduction,Introduction,named-entity-recognition,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.5294117647058824,28,0.1314553990610328,18,0.3157894736842105,1,0,Introduction
31,The size of the effective input width fora token at layer l is now given by 2 l +1 ?1 .,Introduction,Introduction,named-entity-recognition,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.5882352941176471,30,0.1408450704225352,20,0.3508771929824561,1,0,Introduction
36,"In experiments on CoNLL 2003 and OntoNotes 1 What we call effective input width here is known as the receptive field in the vision literature , drawing an analogy to the visual receptive field of a neuron in the retina . :",Introduction,Introduction,named-entity-recognition,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.7352941176470589,35,0.1643192488262911,25,0.4385964912280701,1,0,Introduction
45,Conditional Probability,Introduction,,named-entity-recognition,2,"['O', 'O']","['O', 'O']",34,1.0,44,0.2065727699530516,34,0.5964912280701754,1,0,Introduction
47,"Let x = [ x 1 , . . . , x T ] be our input text and y = [ y 1 , . . . , y T ] be per-token output tags .",Models for Tagging,Models for Tagging,named-entity-recognition,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0222222222222222,46,0.215962441314554,36,0.631578947368421,1,0,Models for Tagging
48,Let D be the domain size of each y i .,Models for Tagging,Models for Tagging,named-entity-recognition,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.0444444444444444,47,0.2206572769953051,37,0.6491228070175439,1,0,Models for Tagging
52,where the tags are conditionally independent given some features for x .,Models for Tagging,Models for Tagging,named-entity-recognition,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.1333333333333333,51,0.2394366197183098,41,0.7192982456140351,1,0,Models for Tagging
54,"However , feature extraction may not necessarily be parallelizable .",Models for Tagging,Models for Tagging,named-entity-recognition,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.1777777777777777,53,0.2488262910798122,43,0.7543859649122807,1,0,Models for Tagging
55,"For example , RNN - based features require iterative passes along the length of x .",Models for Tagging,Models for Tagging,named-entity-recognition,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.2,54,0.2535211267605634,44,0.7719298245614035,1,0,Models for Tagging
57,where ?,Models for Tagging,Models for Tagging,named-entity-recognition,2,"['O', 'O']","['O', 'O']",11,0.2444444444444444,56,0.2629107981220657,46,0.8070175438596491,1,0,Models for Tagging
58,"t is a local factor , ?",Models for Tagging,Models for Tagging,named-entity-recognition,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.2666666666666666,57,0.2676056338028169,47,0.8245614035087719,1,0,Models for Tagging
59,"p is a pairwise factor that scores consecutive tags , and Z x is the partition function .",Models for Tagging,Models for Tagging,named-entity-recognition,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.2888888888888888,58,0.272300469483568,48,0.8421052631578947,1,0,Models for Tagging
60,"To avoid overfitting , ?",Models for Tagging,Models for Tagging,named-entity-recognition,2,"['O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O']",14,0.3111111111111111,59,0.2769953051643192,49,0.8596491228070176,1,0,Models for Tagging
61,p does not depend on the timestep tor the input x in our experiments .,Models for Tagging,Models for Tagging,named-entity-recognition,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.3333333333333333,60,0.2816901408450704,50,0.8771929824561403,1,0,Models for Tagging
62,Prediction in this model requires global search using the O ( D 2 T ),Models for Tagging,Models for Tagging,named-entity-recognition,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.3555555555555555,61,0.2863849765258216,51,0.8947368421052632,1,0,Models for Tagging
63,Viterbi algorithm .,Models for Tagging,Models for Tagging,named-entity-recognition,2,"['O', 'O', 'O']","['O', 'O', 'O']",17,0.3777777777777777,62,0.2910798122065727,52,0.912280701754386,1,0,Models for Tagging
65,The suitability of such compilation depends on the properties and quantity of the data .,Models for Tagging,Models for Tagging,named-entity-recognition,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.4222222222222222,64,0.3004694835680751,54,0.9473684210526316,1,0,Models for Tagging
66,"While CRF prediction requires non-trivial search in output space , it can guarantee that certain output constraints , such as for IOB tagging , will always be satisfied .",Models for Tagging,Models for Tagging,named-entity-recognition,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.4444444444444444,65,0.3051643192488263,55,0.9649122807017544,1,0,Models for Tagging
67,"It may also have better sample complexity , as it imposes more prior knowledge about the structure of the interactions among the tags .",Models for Tagging,Models for Tagging,named-entity-recognition,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.4666666666666667,66,0.3098591549295774,56,0.9824561403508772,1,0,Models for Tagging
68,"However , it has worse computational complexity than independent prediction .",Models for Tagging,Models for Tagging,named-entity-recognition,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.4888888888888889,67,0.3145539906103286,57,1.0,1,0,Models for Tagging
69,Dilated Convolutions,Models for Tagging,,named-entity-recognition,2,"['O', 'O']","['O', 'O']",23,0.5111111111111111,68,0.3192488262910798,0,0.0,1,0,Models for Tagging
70,"CNNs in NLP are typically one - dimensional , applied to a sequence of vectors representing tokens rather than to a two -dimensional grid of vectors representing pixels .",Models for Tagging,Dilated Convolutions,named-entity-recognition,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.5333333333333333,69,0.323943661971831,1,0.0454545454545454,1,0,Models for Tagging: Dilated Convolutions
71,"In this setting , a convolutional neural network layer is equivalent to applying an affine transformation , W c to a sliding window of width r tokens on either side of each token in the sequence .",Models for Tagging,Dilated Convolutions,named-entity-recognition,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.5555555555555556,70,0.3286384976525822,2,0.0909090909090909,1,0,Models for Tagging: Dilated Convolutions
72,"Here , and throughout the paper , we do not explicitly write the bias terms in affine transformations .",Models for Tagging,Dilated Convolutions,named-entity-recognition,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.5777777777777777,71,0.3333333333333333,3,0.1363636363636363,1,0,Models for Tagging: Dilated Convolutions
73,The convolutional operator applied to each token x t with output ct is defined as :,Models for Tagging,Dilated Convolutions,named-entity-recognition,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.6,72,0.3380281690140845,4,0.1818181818181818,1,0,Models for Tagging: Dilated Convolutions
74,where ?,Models for Tagging,Dilated Convolutions,named-entity-recognition,2,"['O', 'O']","['O', 'O']",28,0.6222222222222222,73,0.3427230046948357,5,0.2272727272727272,1,0,Models for Tagging: Dilated Convolutions
75,is vector concatenation .,Models for Tagging,Dilated Convolutions,named-entity-recognition,2,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",29,0.6444444444444445,74,0.3474178403755869,6,0.2727272727272727,1,0,Models for Tagging: Dilated Convolutions
76,"Dilated convolutions perform the same operation , except rather than transforming adjacent in - puts , the convolution is defined over a wider effective input width by skipping over ?",Models for Tagging,Dilated Convolutions,named-entity-recognition,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",30,0.6666666666666666,75,0.352112676056338,7,0.3181818181818182,1,0,Models for Tagging: Dilated Convolutions
77,"inputs at a time , where ?",Models for Tagging,Dilated Convolutions,named-entity-recognition,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O']",31,0.6888888888888889,76,0.3568075117370892,8,0.3636363636363636,1,0,Models for Tagging: Dilated Convolutions
78,is the dilation width .,Models for Tagging,Dilated Convolutions,named-entity-recognition,2,"['O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O']",32,0.7111111111111111,77,0.3615023474178404,9,0.4090909090909091,1,0,Models for Tagging: Dilated Convolutions
79,We define the dilated convolution operator :,Models for Tagging,Dilated Convolutions,named-entity-recognition,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O']",33,0.7333333333333333,78,0.3661971830985915,10,0.4545454545454545,1,0,Models for Tagging: Dilated Convolutions
80,A dilated convolution of width 1 is equivalent to a simple convolution .,Models for Tagging,Dilated Convolutions,named-entity-recognition,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",34,0.7555555555555555,79,0.3708920187793427,11,0.5,1,0,Models for Tagging: Dilated Convolutions
81,"Using the same number of parameters as a simple convolution with the same radius ( i.e. W c has the same dimensionality ) , the ? > 1 dilated convolution incorporates broader context into the representation of a token than a simple convolution .",Models for Tagging,Dilated Convolutions,named-entity-recognition,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",35,0.7777777777777778,80,0.3755868544600939,12,0.5454545454545454,1,0,Models for Tagging: Dilated Convolutions
86,Performing a dilation - 1 convolution in the first layer ensures that no pixels within the effective input width of any pixel are excluded .,Models for Tagging,Multi - Scale Context Aggregation,named-entity-recognition,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",40,0.8888888888888888,85,0.3990610328638497,17,0.7727272727272727,1,0,Models for Tagging: Multi - Scale Context Aggregation
88,"Unfortunately , simply increasing the depth of stacked dilated CNNs causes considerable overfitting in our experiments .",Models for Tagging,Multi - Scale Context Aggregation,named-entity-recognition,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",42,0.9333333333333332,87,0.4084507042253521,19,0.8636363636363636,1,0,Models for Tagging: Multi - Scale Context Aggregation
92,Model Architecture,,,named-entity-recognition,2,"['O', 'O']","['O', 'O']",0,0.0,91,0.4272300469483568,0,0.0,1,0,
93,"The network takes as input a sequence of T vectors x t , and outputs a sequence of per-class scores ht , which serve either as the local conditional distributions of Eqn. ( 1 ) or the local factors ?",Model Architecture,Model Architecture,named-entity-recognition,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0833333333333333,92,0.431924882629108,1,0.0833333333333333,1,0,Model Architecture
94,t of Eqn..,Model Architecture,Model Architecture,named-entity-recognition,2,"['O', 'O', 'O']","['O', 'O', 'O']",2,0.1666666666666666,93,0.4366197183098591,2,0.1666666666666666,1,0,Model Architecture
95,We denote the jth dilated convolutional layer of dilation width ? as D ( j ) ? .,Model Architecture,Model Architecture,named-entity-recognition,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.25,94,0.4413145539906103,3,0.25,1,0,Model Architecture
96,The first layer in the network is a dilation - 1 convolution D ( 0 ) 1 that transforms the input to a representation it :,Model Architecture,Model Architecture,named-entity-recognition,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.3333333333333333,95,0.4460093896713615,4,0.3333333333333333,1,0,Model Architecture
98,Let r ( ) denote the ReLU activation function .,Model Architecture,Model Architecture,named-entity-recognition,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.5,97,0.4553990610328638,6,0.5,1,0,Model Architecture
99,Beginning with ct ( 0 ) = it we define the stack of layers with the following recurrence :,Model Architecture,Model Architecture,named-entity-recognition,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.5833333333333334,98,0.460093896713615,7,0.5833333333333334,1,0,Model Architecture
101,"We refer to this stack of dilated convolutions as a block B ( ) , which has output resolution equal to its input resolution .",Model Architecture,Model Architecture,named-entity-recognition,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.75,100,0.4694835680751174,9,0.75,1,0,Model Architecture
102,"To incorporate even broader context without over - fitting , we avoid making B deeper , and instead iteratively apply B L b times , introducing no extra parameters .",Model Architecture,Model Architecture,named-entity-recognition,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.8333333333333334,101,0.4741784037558685,10,0.8333333333333334,1,0,Model Architecture
103,Starting,Model Architecture,,named-entity-recognition,2,['O'],['O'],11,0.9166666666666666,102,0.4788732394366197,11,0.9166666666666666,1,0,Model Architecture
105,Training,,,named-entity-recognition,2,['O'],['O'],0,0.0,104,0.488262910798122,0,0.0,1,0,
107,"Here , maximum likelihood training is straightforward because the likelihood decouples into the sum of the likelihoods of independent logistic regression problems for every tag , with natural parameters given by Eqn. :",Training,Training,named-entity-recognition,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.1,106,0.4976525821596244,2,0.1,1,0,Training
108,"We can also use the ID - CNN as logits for the CRF model ( Eqn. ( 2 ) ) , where the partition function and its gradient are computed using the forward - backward algorithm .",Training,Training,named-entity-recognition,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.15,107,0.5023474178403756,3,0.15,1,0,Training
110,Sec. 2.1 identifies that the CRF has preferable sample complexity and accuracy since prediction directly reasons in the space of structured outputs .,Training,Training,named-entity-recognition,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.25,109,0.5117370892018779,5,0.25,1,0,Training
114,"To do so , we first define predictions of the model after each of the L b applications of the block .",Training,Training,named-entity-recognition,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.45,113,0.5305164319248826,9,0.45,1,0,Training
115,"Let ht ( k ) be the result of applying the matrix W o from ( 9 ) to b t ( k ) , the output of block k.",Training,Training,named-entity-recognition,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.5,114,0.5352112676056338,10,0.5,1,0,Training
116,We minimize the average of the losses for each application of the block :,Training,Training,named-entity-recognition,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.55,115,0.539906103286385,11,0.55,1,0,Training
119,"Such an approach has been applied in a variety of contexts for training very deep networks in computer vision , but not to our knowledge in NLP .",Training,Training,named-entity-recognition,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.7,118,0.5539906103286385,14,0.7,1,0,Training
121,The version of dropout typically used in practice has the undesirable property that the randomized predictor used at train time differs from the fixed one used attest time .,Training,Training,named-entity-recognition,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.8,120,0.5633802816901409,16,0.8,1,0,Training
125,We encourage its further application .,Training,Training,named-entity-recognition,2,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O']",20,1.0,124,0.5821596244131455,20,1.0,1,0,Training
126,Related work,,,named-entity-recognition,2,"['O', 'O']","['O', 'O']",0,0.0,125,0.5868544600938967,0,0.0,1,0,
150,Experimental Results,,,named-entity-recognition,2,"['O', 'O']","['O', 'O']",0,0.0,149,0.6995305164319249,0,0.0,1,0,
154,Data and Evaluation,,,named-entity-recognition,2,"['O', 'O', 'O']","['O', 'O', 'O']",0,0.0,153,0.7183098591549296,0,0.0,1,0,
161,Baselines,,,named-entity-recognition,2,['O'],['O'],0,0.0,160,0.7511737089201878,0,0.0,1,0,
165,We do not compare with deeper or more elaborate CNN architectures fora number of reasons :,Baselines,Baselines,named-entity-recognition,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.1142857142857142,164,0.7699530516431925,4,0.125,1,0,Baselines
166,"1 ) Fast train and test performance are highly desirable for NLP practitioners , and deeper models require more computation time 2 ) more complicated models tend to over - fit on this relatively small dataset and 3 ) most accurate deep CNN architectures repeatedly up - sample and down - sample the inputs .",Baselines,Baselines,named-entity-recognition,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.1428571428571428,165,0.7746478873239436,5,0.15625,1,0,Baselines
167,We do not compare to stacked LSTMs for similar reasons - a single LSTM is already slower than a 4 - layer CNN .,Baselines,Baselines,named-entity-recognition,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.1714285714285714,166,0.7793427230046949,6,0.1875,1,0,Baselines
182,"With a better implementation of dilated convolutions than currently included in TensorFlow , we would expect the ID - CNN to be notably faster than the 5 - layer CNN .",Baselines,Baselines,named-entity-recognition,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.6,181,0.8497652582159625,21,0.65625,1,0,Baselines
184,"Dropout is important for training neural network models that generalize well , especially on relatively small NLP datasets such as .",Baselines,Baselines,named-entity-recognition,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",23,0.6571428571428571,183,0.8591549295774648,23,0.71875,1,0,Baselines
196,The greedy Bi - LSTM out - performs the lex -,Baselines,Baselines,named-entity-recognition,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",35,1.0,195,0.9154929577464788,2,1.0,1,0,Baselines
197,Model,,,named-entity-recognition,2,['O'],['O'],0,0.0,196,0.92018779342723,0,0.0,1,0,
202,"We believe this is due to the more diverse set of entities in OntoNotes , which also tend to be much longer - the average length of a multi-token named entity segment in CoNLL is about one token shorter than in OntoNotes .",Model,Model,named-entity-recognition,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.4166666666666667,201,0.943661971830986,5,0.4166666666666667,1,0,Model
203,These long entities benefit more from explicit structured constraints enforced in Viterbi decoding .,Model,Model,named-entity-recognition,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.5,202,0.9483568075117372,6,0.5,1,0,Model
208,"We believe the decrease in the Bi - LSTM - CRF model occurs because of the nature of the OntoNotes dataset compared to contains a particularly high proportion of ambiguous entities , 7 perhaps leading to more benefit from document context that helps with disambiguation .",Model,Model,named-entity-recognition,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.9166666666666666,207,0.971830985915493,11,0.9166666666666666,1,0,Model
209,"In this scenario , adding the wider context may just add noise to the high - scoring Bi - LSTM - CRF model , whereas the less accurate dilated model can still benefit from the refined predictions of the iterated dilated convolutions .",Model,Model,named-entity-recognition,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,1.0,208,0.976525821596244,12,1.0,1,0,Model
210,Conclusion,,,named-entity-recognition,2,['O'],['O'],0,0.0,209,0.9812206572769951,0,0.0,1,0,
3,abstract,,,named-entity-recognition,3,['O'],['O'],0,0.0,2,0.0108108108108108,0,0.0,1,0,
4,Pre-trained word embeddings learned from unlabeled text have become a standard component of neural network architectures for NLP tasks .,abstract,abstract,named-entity-recognition,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.25,3,0.0162162162162162,1,0.25,1,0,abstract
5,"However , inmost cases , the recurrent network that operates on word - level representations to produce context sensitive representations is trained on relatively little labeled data .",abstract,abstract,named-entity-recognition,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.5,4,0.0216216216216216,2,0.5,1,0,abstract
9,"Due to their simplicity and efficacy , pre-trained word embedding have become ubiquitous in NLP systems .",Introduction,Introduction,named-entity-recognition,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0555555555555555,8,0.0432432432432432,1,0.0555555555555555,1,0,Introduction
10,Many prior studies have shown that they capture useful semantic and syntactic information and including them in NLP systems has been shown to be enormously helpful fora variety of downstream tasks .,Introduction,Introduction,named-entity-recognition,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.1111111111111111,9,0.0486486486486486,2,0.1111111111111111,1,0,Introduction
11,"However , in many NLP tasks it is essential to represent not just the meaning of a word , but also the word in context .",Introduction,Introduction,named-entity-recognition,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.1666666666666666,10,0.054054054054054,3,0.1666666666666666,1,0,Introduction
12,"For example , in the two phrases "" A Central Bank spokesman "" and "" The Central African Republic "" , the word ' Central ' is used as part of both an Organization and Location .",Introduction,Introduction,named-entity-recognition,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.2222222222222222,11,0.0594594594594594,4,0.2222222222222222,1,0,Introduction
13,"Accordingly , current state of the art sequence tagging models typically include a bidirectional re-current neural network ( RNN ) that encodes token sequences into a context sensitive representation before making token specific predictions .",Introduction,Introduction,named-entity-recognition,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.2777777777777778,12,0.0648648648648648,5,0.2777777777777778,1,0,Introduction
15,"Previous work has explored methods for jointly learning the bidirectional RNN with supplemental labeled data from other tasks ( e.g. , .",Introduction,Introduction,named-entity-recognition,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.3888888888888889,14,0.0756756756756756,7,0.3888888888888889,1,0,Introduction
27,Baseline sequence tagging model,,,named-entity-recognition,3,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",0,0.0,26,0.1405405405405405,0,0.0,1,0,
28,"Our baseline sequence tagging model is a hierarchical neural tagging model , closely following a number of recent studies ) ( left side of ) .",Baseline sequence tagging model,Baseline sequence tagging model,named-entity-recognition,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0238095238095238,27,0.1459459459459459,1,0.05,1,0,Baseline sequence tagging model
29,"Given a sentence of tokens ( t 1 , t 2 , . . . , t N ) it first forms a representation , x k , for each token by concatenating a character based representation ck with a token embedding wk :",Baseline sequence tagging model,Baseline sequence tagging model,named-entity-recognition,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.0476190476190476,28,0.1513513513513513,2,0.1,1,0,Baseline sequence tagging model
31,"It is parameterized by C ( , ? c ) with parameters ? c .",Baseline sequence tagging model,Baseline sequence tagging model,named-entity-recognition,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.0952380952380952,30,0.1621621621621621,4,0.2,1,0,Baseline sequence tagging model
32,"The token embeddings , wk , are obtained as a lookup E ( , ? w ) , initialized using pre-trained word embeddings , and fine tuned during training .",Baseline sequence tagging model,Baseline sequence tagging model,named-entity-recognition,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.119047619047619,31,0.1675675675675675,5,0.25,1,0,Baseline sequence tagging model
34,"For each token position , k , the hidden state h k , i of RNN layer i is formed by concatenating the hidden states from the forward ( ? ? h k , i ) and backward ( ? ? h k , i ) RNNs .",Baseline sequence tagging model,Baseline sequence tagging model,named-entity-recognition,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.1666666666666666,33,0.1783783783783784,7,0.35,1,0,Baseline sequence tagging model
36,"More formally , for the first RNN layer that operates on x k to output h k,1 :",Baseline sequence tagging model,Baseline sequence tagging model,named-entity-recognition,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.2142857142857142,35,0.1891891891891892,9,0.45,1,0,Baseline sequence tagging model
38,"Two representations of the word "" York """,Baseline sequence tagging model,,named-entity-recognition,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.2619047619047619,37,0.2,11,0.55,1,0,Baseline sequence tagging model
40,New York is located :,Baseline sequence tagging model,"Two representations of the word "" York """,named-entity-recognition,3,"['O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O']",13,0.3095238095238095,39,0.2108108108108108,13,0.65,1,0,"Baseline sequence tagging model: Two representations of the word "" York """
43,"The second RNN layer is similar and uses h k , 1 to output h k ,2 .",Baseline sequence tagging model,"Two representations of the word "" York """,named-entity-recognition,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.3809523809523809,42,0.227027027027027,16,0.8,1,0,"Baseline sequence tagging model: Two representations of the word "" York """
48,Bidirectional LM,Baseline sequence tagging model,,named-entity-recognition,3,"['O', 'O']","['O', 'O']",21,0.5,47,0.254054054054054,0,0.0,1,0,Baseline sequence tagging model
49,"A language model computes the probability of a token sequence ( t 1 , t 2 , . . . , t N )",Baseline sequence tagging model,Bidirectional LM,named-entity-recognition,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.5238095238095238,48,0.2594594594594595,1,0.0909090909090909,1,0,Baseline sequence tagging model: Bidirectional LM
50,"Recent state of the art neural language models ) use a similar architecture to our baseline sequence tagger where they pass a token representation ( either from a CNN over characters or as token embeddings ) through multiple layers of LSTMs to embed the history ( t 1 , t 2 , . . . , t k ) into a fixed dimensional vector ? ? h LM k .",Baseline sequence tagging model,Bidirectional LM,named-entity-recognition,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",23,0.5476190476190477,49,0.2648648648648649,2,0.1818181818181818,1,0,Baseline sequence tagging model: Bidirectional LM
51,This is the forward LM embedding of the token at position k and is the output of the top LSTM layer in the language model .,Baseline sequence tagging model,Bidirectional LM,named-entity-recognition,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.5714285714285714,50,0.2702702702702703,3,0.2727272727272727,1,0,Baseline sequence tagging model: Bidirectional LM
53,The need to capture future context in the LM embeddings suggests it is beneficial to also consider a backward LM in additional to the traditional forward LM .,Baseline sequence tagging model,Bidirectional LM,named-entity-recognition,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.6190476190476191,52,0.2810810810810811,5,0.4545454545454545,1,0,Baseline sequence tagging model: Bidirectional LM
55,"Given a sentence with N tokens , it computes",Baseline sequence tagging model,,named-entity-recognition,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",28,0.6666666666666666,54,0.2918918918918919,7,0.6363636363636364,1,0,Baseline sequence tagging model
56,A backward LM can be implemented in an analogous way to a forward LM and produces the backward LM embedding ? ?,Baseline sequence tagging model,"Given a sentence with N tokens , it computes",named-entity-recognition,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",29,0.6904761904761905,55,0.2972972972972973,8,0.7272727272727273,1,0,"Baseline sequence tagging model: Given a sentence with N tokens , it computes"
57,"h LM k , for the sequence ( t k , t k+1 , . . . , t N ) , the output embeddings of the top layer LSTM .",Baseline sequence tagging model,"Given a sentence with N tokens , it computes",named-entity-recognition,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",30,0.7142857142857143,56,0.3027027027027027,9,0.8181818181818182,1,0,"Baseline sequence tagging model: Given a sentence with N tokens , it computes"
59,"Note that in our formulation , the forward and backward LMs are independent , without any shared parameters .",Baseline sequence tagging model,"Given a sentence with N tokens , it computes",named-entity-recognition,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",32,0.7619047619047619,58,0.3135135135135135,11,1.0,1,0,"Baseline sequence tagging model: Given a sentence with N tokens , it computes"
64,"More formally , we simply replace ( 2 ) with",Baseline sequence tagging model,Combining LM with sequence model,named-entity-recognition,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",37,0.8809523809523809,63,0.3405405405405405,4,0.4444444444444444,1,0,Baseline sequence tagging model: Combining LM with sequence model
65,There are alternate possibilities for adding the LM embeddings to the sequence model .,Baseline sequence tagging model,Combining LM with sequence model,named-entity-recognition,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",38,0.9047619047619048,64,0.3459459459459459,5,0.5555555555555556,1,0,Baseline sequence tagging model: Combining LM with sequence model
66,One pos-sibility adds a non-linear mapping after the concatenation and before the second RNN ( e.g. re -,Baseline sequence tagging model,Combining LM with sequence model,named-entity-recognition,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",39,0.9285714285714286,65,0.3513513513513513,6,0.6666666666666666,1,0,Baseline sequence tagging model: Combining LM with sequence model
67,where f is a non-linear function ) .,Baseline sequence tagging model,Combining LM with sequence model,named-entity-recognition,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",40,0.9523809523809524,66,0.3567567567567568,7,0.7777777777777778,1,0,Baseline sequence tagging model: Combining LM with sequence model
68,Another possibility introduces an attention - like mechanism that weights the all LM embeddings in a sentence before including them in the sequence model .,Baseline sequence tagging model,Combining LM with sequence model,named-entity-recognition,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",41,0.9761904761904762,67,0.3621621621621622,8,0.8888888888888888,1,0,Baseline sequence tagging model: Combining LM with sequence model
69,"Our initial results with the simple concatenation were encouraging so we did not explore these alternatives in this study , preferring to leave them for future work .",Baseline sequence tagging model,Combining LM with sequence model,named-entity-recognition,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",42,1.0,68,0.3675675675675676,9,1.0,1,0,Baseline sequence tagging model: Combining LM with sequence model
70,Experiments,,,named-entity-recognition,3,['O'],['O'],0,0.0,69,0.372972972972973,0,0.0,1,0,
75,CoNLL 2003 NER .,Experiments,,named-entity-recognition,3,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",5,0.2631578947368421,74,0.4,5,0.119047619047619,1,0,Experiments
85,"It defines 11 syntactic chunk types ( e.g. , NP , VP , ADJP ) in addition to other .",Experiments,CoNLL 2000 chunking .,named-entity-recognition,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.7894736842105263,84,0.4540540540540541,15,0.3571428571428571,1,0,Experiments: CoNLL 2000 chunking .
94,"Following , they used linear projection layers at the output of each LSTM layer to reduce the computation time but still maintain a large LSTM state .",Pre-trained language models .,Pre-trained language models .,named-entity-recognition,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.1818181818181818,93,0.5027027027027027,24,0.5714285714285714,1,0,Pre-trained language models .
97,We use CNN - BIG - LSTM to refer to this language model in our results .,Pre-trained language models .,Pre-trained language models .,named-entity-recognition,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.3181818181818182,96,0.518918918918919,27,0.6428571428571429,1,0,Pre-trained language models .
102,2,Pre-trained language models .,Pre-trained language models .,named-entity-recognition,3,['O'],['O'],12,0.5454545454545454,101,0.5459459459459459,32,0.7619047619047619,1,0,Pre-trained language models .
108,"Then , at the epoch with the highest development performance , we start a simple learning rate annealing schedule : decrease ?",Pre-trained language models .,Pre-trained language models .,named-entity-recognition,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.8181818181818182,107,0.5783783783783784,38,0.9047619047619048,1,0,Pre-trained language models .
109,"an order of magnitude ( i.e. , divide by ten ) , train for five epochs , decrease ?",Pre-trained language models .,Pre-trained language models .,named-entity-recognition,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.8636363636363636,108,0.5837837837837838,39,0.9285714285714286,1,0,Pre-trained language models .
110,"an order of magnitude again , train for five more epochs and stop .",Pre-trained language models .,Pre-trained language models .,named-entity-recognition,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.9090909090909092,109,0.5891891891891892,40,0.9523809523809524,1,0,Pre-trained language models .
112,It is important to estimate the variance of model performance since the test data sets are relatively small .,Pre-trained language models .,Pre-trained language models .,named-entity-recognition,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,1.0,111,0.6,42,1.0,1,0,Pre-trained language models .
113,Overall system results,,,named-entity-recognition,3,"['O', 'O', 'O']","['O', 'O', 'O']",0,0.0,112,0.6054054054054054,0,0.0,1,0,
115,compare results of,Overall system results,Overall system results,named-entity-recognition,3,"['O', 'O', 'O']","['O', 'O', 'O']",2,0.0526315789473684,114,0.6162162162162163,0,0.0,1,0,Overall system results
116,Tag LM to other systems that include additional labeled data or gazetteers .,Overall system results,Overall system results,named-entity-recognition,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.0789473684210526,115,0.6216216216216216,1,0.0909090909090909,1,0,Overall system results
125,"For example , noted an improvement of only 0.06 F 1 in the NER task when transfer learning from both CoNLL 2000 chunks and PTB POS tags and reported an increase of 0.71 F 1 when adding gazetteers to their baseline .",Overall system results,Adding external resources .,named-entity-recognition,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.3157894736842105,124,0.6702702702702703,10,0.9090909090909092,1,0,Overall system results: Adding external resources .
127,Analysis,Overall system results,,named-entity-recognition,3,['O'],['O'],14,0.3684210526315789,126,0.6810810810810811,0,0.0,1,0,Overall system results
135,The results are reported in .,Overall system results,,named-entity-recognition,3,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O']",22,0.5789473684210527,134,0.7243243243243244,8,0.3333333333333333,1,0,Overall system results
138,"Accordingly , we hypothesize ( but have not tested ) that replacing the backward LSTM - 2048 - 512 with a backward LM analogous to the CNN - BIG - LSTM would further improve performance .",Overall system results,The results are reported in .,named-entity-recognition,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.6578947368421053,137,0.7405405405405405,11,0.4583333333333333,1,0,Overall system results: The results are reported in .
147,One artifact of our evaluation framework is that both the labeled data in the chunking and NER tasks and the unlabeled text in the 1 Billion Word Benchmark used to train the bidirectional LMs are derived from news articles .,Overall system results,Importance of task specific RNN .,named-entity-recognition,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",34,0.8947368421052632,146,0.7891891891891892,20,0.8333333333333334,1,0,Overall system results: Importance of task specific RNN .
152,Related work,,,named-entity-recognition,3,"['O', 'O']","['O', 'O']",0,0.0,151,0.8162162162162162,0,0.0,1,0,
165,LMs have always been a critical component in statistical machine translation systems .,Neural language models .,Neural language models .,named-entity-recognition,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0625,164,0.8864864864864865,13,0.4642857142857143,1,0,Neural language models .
166,"Recently , neural LMs have also been integrated in neural machine translation systems ( e.g. , to score candidate translations .",Neural language models .,Neural language models .,named-entity-recognition,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.125,165,0.8918918918918919,14,0.5,1,0,Neural language models .
167,"In contrast , Tag LM uses neural LMs to encode words in the input sequence .",Neural language models .,Neural language models .,named-entity-recognition,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.1875,166,0.8972972972972973,15,0.5357142857142857,1,0,Neural language models .
168,"Unlike forward LMs , bidirectional LMs have received little prior attention .",Neural language models .,Neural language models .,named-entity-recognition,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.25,167,0.9027027027027028,16,0.5714285714285714,1,0,Neural language models .
169,"Most similar to our formulation , used a bidirectional neural LM in a statistical machine translation system for instance selection .",Neural language models .,Neural language models .,named-entity-recognition,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.3125,168,0.9081081081081082,17,0.6071428571428571,1,0,Neural language models .
171,Frinken et al. ( 2012 ) also used a bidirectional n-gram LM for handwriting recognition .,Neural language models .,Neural language models .,named-entity-recognition,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.4375,170,0.918918918918919,19,0.6785714285714286,1,0,Neural language models .
173,"Recently , there has been some interest in interpreting the activations of RNNs .",Neural language models .,Interpreting RNN states .,named-entity-recognition,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.5625,172,0.9297297297297298,21,0.75,1,0,Neural language models .: Interpreting RNN states .
174,showed that single LSTM units can learn to predict singular - plural distinctions .,Neural language models .,Interpreting RNN states .,named-entity-recognition,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.625,173,0.9351351351351352,22,0.7857142857142857,1,0,Neural language models .: Interpreting RNN states .
178,Current state of the art results in sequence tagging problems are based on bidirectional RNN models .,Neural language models .,Other sequence tagging models .,named-entity-recognition,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.875,177,0.9567567567567568,26,0.9285714285714286,1,0,Neural language models .: Other sequence tagging models .
179,"However , many other sequence tagging models have been proposed in the literature for this class of problems ( e.g. , .",Neural language models .,Other sequence tagging models .,named-entity-recognition,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.9375,178,0.9621621621621622,27,0.9642857142857144,1,0,Neural language models .: Other sequence tagging models .
180,"LM embeddings could also be used as additional features in other models , although it is not clear whether the model complexity would be sufficient to effectively make use of them .",Neural language models .,Other sequence tagging models .,named-entity-recognition,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,1.0,179,0.9675675675675676,28,1.0,1,0,Neural language models .: Other sequence tagging models .
181,Conclusion,,,named-entity-recognition,3,['O'],['O'],0,0.0,180,0.972972972972973,0,0.0,1,0,
3,abstract,,,named-entity-recognition,4,['O'],['O'],0,0.0,2,0.0073529411764705,0,0.0,1,0,
9,Pre-trained word representations area key component in many neural language understanding models .,Introduction,Introduction,named-entity-recognition,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0256410256410256,8,0.0294117647058823,1,0.0263157894736842,1,0,Introduction
27,1,Introduction,Introduction,named-entity-recognition,4,['O'],['O'],19,0.4871794871794871,26,0.0955882352941176,19,0.5,1,0,Introduction
28,"2 Related work Due to their ability to capture syntactic and semantic information of words from large scale unlabeled text , pretrained word vectors area standard component of most state - of the - art NLP architectures , including for question answering , textual entailment and semantic role labeling .",Introduction,Introduction,named-entity-recognition,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.5128205128205128,27,0.0992647058823529,20,0.5263157894736842,1,0,Introduction
29,"However , these approaches for learning word vectors only allow a single contextindependent representation for each word .",Introduction,Introduction,named-entity-recognition,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.5384615384615384,28,0.1029411764705882,21,0.5526315789473685,1,0,Introduction
30,"Previously proposed methods overcome some of the shortcomings of traditional word vectors by either enriching them with subword information ( e.g. , or learning separate vectors for each word sense ( e.g. , .",Introduction,Introduction,named-entity-recognition,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.5641025641025641,29,0.1066176470588235,22,0.5789473684210527,1,0,Introduction
32,Other recent work has also focused on learning context - dependent representations .,Introduction,Introduction,named-entity-recognition,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.6153846153846154,31,0.1139705882352941,24,0.631578947368421,1,0,Introduction
35,Other approaches for learning contextual embeddings include the pivot word itself in the representation and are computed with the encoder of either a supervised neural machine translation ( MT ) system or an unsupervised language model .,Introduction,Introduction,named-entity-recognition,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.6923076923076923,34,0.125,27,0.7105263157894737,1,0,Introduction
39,1 http://allennlp.org/elmo,Introduction,Introduction,named-entity-recognition,4,"['O', 'O']","['O', 'O']",31,0.7948717948717948,38,0.1397058823529411,31,0.8157894736842105,1,0,Introduction
40,Previous work has also shown that different layers of deep biRNNs encode different types of information .,Introduction,Introduction,named-entity-recognition,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",32,0.8205128205128205,39,0.1433823529411764,32,0.8421052631578947,1,0,Introduction
47,Dai and Le,Introduction,,named-entity-recognition,4,"['O', 'O', 'O']","['O', 'O', 'O']",39,1.0,46,0.1691176470588235,0,0.0,1,0,Introduction
49,"Given a sequence of N tokens , ( t 1 , t 2 , ... , t N ) , a forward language model computes the probability of the sequence by modeling the probability of to - ken t k given the history ( t 1 , ... , t k?1 ) :",Bidirectional language models,Bidirectional language models,named-entity-recognition,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0175438596491228,48,0.1764705882352941,1,0.1,1,0,Bidirectional language models
52,"A backward LM is similar to a forward LM , except it runs over the sequence in reverse , predicting the previous token given the future context :",Bidirectional language models,Bidirectional language models,named-entity-recognition,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.0701754385964912,51,0.1875,4,0.4,1,0,Bidirectional language models
53,"It can be implemented in an analogous way to a forward LM , with each backward LSTM layer j in a L layer deep model producing representations ? ? h LM k , j oft k given ( t k+1 , . . . , t N ) .",Bidirectional language models,Bidirectional language models,named-entity-recognition,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.087719298245614,52,0.1911764705882352,5,0.5,1,0,Bidirectional language models
54,A bi LM combines both a forward and backward LM .,Bidirectional language models,Bidirectional language models,named-entity-recognition,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.1052631578947368,53,0.1948529411764706,6,0.6,1,0,Bidirectional language models
59,ELMo,Bidirectional language models,,named-entity-recognition,4,['O'],['O'],11,0.1929824561403508,58,0.213235294117647,0,0.0,1,0,Bidirectional language models
62,"where h LM k ,0 is the token layer and h LM",Bidirectional language models,ELMo,named-entity-recognition,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.2456140350877192,61,0.2242647058823529,3,0.2727272727272727,1,0,Bidirectional language models: ELMo
63,", for each biLSTM layer .",Bidirectional language models,ELMo,named-entity-recognition,4,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O']",15,0.2631578947368421,62,0.2279411764705882,4,0.3636363636363636,1,0,Bidirectional language models: ELMo
64,"For inclusion in a downstream model , ELMo collapses all layers in R into a single vector , ELMo k = E(R k ; ? e ) .",Bidirectional language models,ELMo,named-entity-recognition,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.2807017543859649,63,0.2316176470588235,5,0.4545454545454545,1,0,Bidirectional language models: ELMo
65,"In the simplest case , ELMo just selects the top layer , E( R k ) = h LM k , L , as in Tag LM and CoVe .",Bidirectional language models,ELMo,named-entity-recognition,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.2982456140350877,64,0.2352941176470588,6,0.5454545454545454,1,0,Bidirectional language models: ELMo
66,"More generally , we compute a task specific weighting of all biLM layers :",Bidirectional language models,ELMo,named-entity-recognition,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.3157894736842105,65,0.2389705882352941,7,0.6363636363636364,1,0,Bidirectional language models: ELMo
67,"( 1 ) In ( 1 ) , s task are softmax - normalized weights and the scalar parameter ?",Bidirectional language models,ELMo,named-entity-recognition,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.3333333333333333,66,0.2426470588235294,8,0.7272727272727273,1,0,Bidirectional language models: ELMo
69,is of practical importance to aid the optimization process ( see supplemental material for details ) .,Bidirectional language models,ELMo,named-entity-recognition,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.3684210526315789,68,0.25,10,0.9090909090909092,1,0,Bidirectional language models: ELMo
70,"Considering that the activations of each biLM layer have a different distribution , in some cases it also helped to apply layer normalization to each biLM layer before weighting .",Bidirectional language models,ELMo,named-entity-recognition,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.3859649122807017,69,0.2536764705882353,11,1.0,1,0,Bidirectional language models: ELMo
77,"Given a sequence of tokens ( t 1 , . . . , t N ) , it is standard to form a context - independent token representation x k for each token position using pre-trained word embeddings and optionally character - based representations .",Bidirectional language models,Using biLMs for supervised NLP tasks,named-entity-recognition,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",29,0.5087719298245614,76,0.2794117647058823,6,0.4285714285714285,1,0,Bidirectional language models: Using biLMs for supervised NLP tasks
78,"Then , the model forms a context - sensitive representation h k , typically using either bidirectional RNNs , CNNs , or feed forward networks .",Bidirectional language models,Using biLMs for supervised NLP tasks,named-entity-recognition,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",30,0.5263157894736842,77,0.2830882352941176,7,0.5,1,0,Bidirectional language models: Using biLMs for supervised NLP tasks
79,"To add ELMo to the supervised model , we first freeze the weights of the biLM and then concatenate the ELMo vector ELMo task k with x k and pass the ELMo enhanced representation [ x k ; ELMo task k ] into the task RNN .",Bidirectional language models,Using biLMs for supervised NLP tasks,named-entity-recognition,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",31,0.543859649122807,78,0.2867647058823529,8,0.5714285714285714,1,0,Bidirectional language models: Using biLMs for supervised NLP tasks
80,"For some tasks ( e.g. , SNLI , SQuAD ) , we observe further improvements by also including ELMo at the output of the task RNN by introducing another set of output specific linear weights and replacing h k with [ h k ; ELMo task k ] .",Bidirectional language models,Using biLMs for supervised NLP tasks,named-entity-recognition,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",32,0.5614035087719298,79,0.2904411764705882,9,0.6428571428571429,1,0,Bidirectional language models: Using biLMs for supervised NLP tasks
81,"As the remainder of the supervised model remains unchanged , these additions can happen within the context of more complex neural models .",Bidirectional language models,Using biLMs for supervised NLP tasks,named-entity-recognition,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",33,0.5789473684210527,80,0.2941176470588235,10,0.7142857142857143,1,0,Bidirectional language models: Using biLMs for supervised NLP tasks
82,"For example , seethe SNLI experiments in Sec. 4 where a bi-attention layer follows the biLSTMs , or the coreference resolution experiments where a clustering model is layered on top of the biLSTMs .",Bidirectional language models,Using biLMs for supervised NLP tasks,named-entity-recognition,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",34,0.5964912280701754,81,0.2977941176470588,11,0.7857142857142857,1,0,Bidirectional language models: Using biLMs for supervised NLP tasks
83,"Finally , we found it beneficial to add a moderate amount of dropout to and in some cases to regularize the ELMo weights by adding ?",Bidirectional language models,Using biLMs for supervised NLP tasks,named-entity-recognition,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",35,0.6140350877192983,82,0.3014705882352941,12,0.8571428571428571,1,0,Bidirectional language models: Using biLMs for supervised NLP tasks
84,w 2 2 to the loss .,Bidirectional language models,Using biLMs for supervised NLP tasks,named-entity-recognition,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O']",36,0.631578947368421,83,0.3051470588235294,13,0.9285714285714286,1,0,Bidirectional language models: Using biLMs for supervised NLP tasks
85,This imposes an inductive bias on the ELMo weights to stay close to an average of all biLM layers .,Bidirectional language models,Using biLMs for supervised NLP tasks,named-entity-recognition,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",37,0.6491228070175439,84,0.3088235294117647,14,1.0,1,0,Bidirectional language models: Using biLMs for supervised NLP tasks
86,Pre-trained bidirectional language model architecture,Bidirectional language models,,named-entity-recognition,4,"['O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O']",38,0.6666666666666666,85,0.3125,0,0.0,1,0,Bidirectional language models
88,"We focus on large scale biLMs in this work , as Peters et al .",Bidirectional language models,Pre-trained bidirectional language model architecture,named-entity-recognition,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",40,0.7017543859649122,87,0.3198529411764705,2,0.1052631578947368,1,0,Bidirectional language models: Pre-trained bidirectional language model architecture
93,"As a result , the biLM provides three layers of representations for each input token , including those outside the training set due to the purely character input .",Bidirectional language models,Pre-trained bidirectional language model architecture,named-entity-recognition,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",45,0.7894736842105263,92,0.3382352941176471,7,0.3684210526315789,1,0,Bidirectional language models: Pre-trained bidirectional language model architecture
94,"In contrast , traditional word embedding methods only provide one layer of representation for tokens in a fixed vocabulary .",Bidirectional language models,Pre-trained bidirectional language model architecture,named-entity-recognition,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",46,0.8070175438596491,93,0.3419117647058823,8,0.4210526315789473,1,0,Bidirectional language models: Pre-trained bidirectional language model architecture
99,This can be seen as a type of domain transfer for the biLM .,Bidirectional language models,Pre-trained bidirectional language model architecture,named-entity-recognition,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",51,0.8947368421052632,98,0.3602941176470588,13,0.6842105263157895,1,0,Bidirectional language models: Pre-trained bidirectional language model architecture
100,"As a result , inmost cases we used a fine - tuned biLM in the downstream task .",Bidirectional language models,Pre-trained bidirectional language model architecture,named-entity-recognition,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",52,0.912280701754386,99,0.3639705882352941,14,0.7368421052631579,1,0,Bidirectional language models: Pre-trained bidirectional language model architecture
106,Evaluation,,,named-entity-recognition,4,['O'],['O'],0,0.0,105,0.3860294117647059,0,0.0,1,0,
107,Question Textual entailment,Evaluation,,named-entity-recognition,4,"['O', 'O', 'O']","['O', 'O', 'O']",1,0.008695652173913,106,0.3897058823529412,1,0.0714285714285714,1,0,Evaluation
108,"Textual entailment is the task of determining whether a "" hypothesis "" is true , given a "" premise "" .",Evaluation,Question Textual entailment,named-entity-recognition,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.017391304347826,107,0.3933823529411764,2,0.1428571428571428,1,0,Evaluation: Question Textual entailment
114,"A semantic role labeling ( SRL ) system models the predicate - argument structure of a sentence , and is often described as answering "" Who did what to whom "" .",Evaluation,Semantic role labeling,named-entity-recognition,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.0695652173913043,113,0.4154411764705882,8,0.5714285714285714,1,0,Evaluation: Semantic role labeling
115,He et al .,Evaluation,,named-entity-recognition,4,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",9,0.0782608695652174,114,0.4191176470588235,9,0.6428571428571429,1,0,Evaluation
116,"( 2017 ) modeled SRL as a BIO tagging problem and used an 8 - layer deep biLSTM with forward and backward directions interleaved , following .",Evaluation,He et al .,named-entity-recognition,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.0869565217391304,115,0.4227941176470588,10,0.7142857142857143,1,0,Evaluation: He et al .
121,Analysis,Evaluation,,named-entity-recognition,4,['O'],['O'],15,0.1304347826086956,120,0.4411764705882353,0,0.0,1,0,Evaluation
127,Alternate layer weighting schemes,Evaluation,,named-entity-recognition,4,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",21,0.1826086956521739,126,0.4632352941176471,0,0.0,1,0,Evaluation
128,There are many alternatives to Equation 1 for combining the biLM layers .,Evaluation,Alternate layer weighting schemes,named-entity-recognition,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.1913043478260869,127,0.4669117647058823,1,0.0212765957446808,1,0,Evaluation: Alternate layer weighting schemes
129,"Previous work on contextual representations used only the last layer , whether it be from a biLM or an MT encoder ( CoVe ; .",Evaluation,Alternate layer weighting schemes,named-entity-recognition,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",23,0.2,128,0.4705882352941176,2,0.0425531914893617,1,0,Evaluation: Alternate layer weighting schemes
131,"is also important , as large values such as ? = 1 effectively reduce the weighting function to a simple average over the layers , while smaller values ( e.g. , ? = 0.001 ) allow the layer weights to vary .",Evaluation,Alternate layer weighting schemes,named-entity-recognition,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.217391304347826,130,0.4779411764705882,4,0.0851063829787234,1,0,Evaluation: Alternate layer weighting schemes
136,A small ?,Evaluation,Alternate layer weighting schemes,named-entity-recognition,4,"['O', 'O', 'O']","['O', 'O', 'O']",30,0.2608695652173913,135,0.4963235294117647,9,0.1914893617021276,1,0,Evaluation: Alternate layer weighting schemes
137,"is preferred inmost cases with ELMo , although for NER , a task with a smaller training set , the results are insensitive to ?",Evaluation,Alternate layer weighting schemes,named-entity-recognition,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",31,0.2695652173913043,136,0.5,10,0.2127659574468085,1,0,Evaluation: Alternate layer weighting schemes
138,( not shown ) .,Evaluation,Alternate layer weighting schemes,named-entity-recognition,4,"['O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O']",32,0.2782608695652174,137,0.5036764705882353,11,0.2340425531914893,1,0,Evaluation: Alternate layer weighting schemes
146,"One possible explanation for this result is that both the SNLI and SQuAD architectures use attention layers after the biRNN , so introducing ELMo at this layer allows the model to attend directly to the biLM 's internal representations .",Evaluation,Alternate layer weighting schemes,named-entity-recognition,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",40,0.3478260869565217,145,0.5330882352941176,19,0.4042553191489361,1,0,Evaluation: Alternate layer weighting schemes
147,"In the SRL case , . . } they were actors who had been handed fat roles in a successful play , and had tale nt enough to fill the roles competently , with nice understatement . the task - specific context representations are likely more important than those from the biLM .",Evaluation,Alternate layer weighting schemes,named-entity-recognition,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",41,0.3565217391304348,146,0.5367647058823529,20,0.425531914893617,1,0,Evaluation: Alternate layer weighting schemes
149,Since adding,Evaluation,,named-entity-recognition,4,"['O', 'O']","['O', 'O']",43,0.3739130434782609,148,0.5441176470588235,22,0.4680851063829787,1,0,Evaluation
152,"Consider "" play "" , a highly polysemous word .",Evaluation,Since adding,named-entity-recognition,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",46,0.4,151,0.5551470588235294,25,0.5319148936170213,1,0,Evaluation: Since adding
154,"They are spread across several parts of speech ( e.g. , "" played "" , "" playing "" as verbs , and "" player "" , "" game "" as nouns ) but concentrated in the sportsrelated senses of "" play "" .",Evaluation,Since adding,named-entity-recognition,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",48,0.417391304347826,153,0.5625,27,0.574468085106383,1,0,Evaluation: Since adding
157,These observations can be quantified using an intrinsic evaluation of the contextual representations similar to .,Evaluation,Since adding,named-entity-recognition,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",51,0.4434782608695652,156,0.5735294117647058,30,0.6382978723404256,1,0,Evaluation: Since adding
159,"Using this approach , it is also possible to compare to CoVe , and across each of the individual layers .",Evaluation,Since adding,named-entity-recognition,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",53,0.4608695652173913,158,0.5808823529411765,32,0.6808510638297872,1,0,Evaluation: Since adding
160,"Word sense disambiguation Given a sentence , we can use the biLM representations to predict the sense of a target word using a simple 1nearest neighbor approach , similar to .",Evaluation,Since adding,named-entity-recognition,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",54,0.4695652173913043,159,0.5845588235294118,33,0.7021276595744681,1,0,Evaluation: Since adding
162,"At test time , we again use the biLM to compute representations fora given target word and take the nearest neighbor sense from the training set , falling back to the first sense from WordNet for lemmas not observed during training .",Evaluation,Since adding,named-entity-recognition,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",56,0.4869565217391304,161,0.5919117647058824,35,0.7446808510638298,1,0,Evaluation: Since adding
169,"As the linear classifier adds only a small amount of model capacity , this is direct test of the biLM 's representations .",Evaluation,POS tagging,named-entity-recognition,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",63,0.5478260869565217,168,0.6176470588235294,42,0.8936170212765957,1,0,Evaluation: POS tagging
175,Sample efficiency,Evaluation,,named-entity-recognition,4,"['O', 'O']","['O', 'O']",69,0.6,174,0.6397058823529411,0,0.0,1,0,Evaluation
195,"To fine tune on a given task , the supervised labels were temporarily ignored , the biLM fine tuned for one epoch on the training split and evaluated on the development split .",Evaluation,Fine tuning biLM,named-entity-recognition,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",89,0.7739130434782608,194,0.7132352941176471,6,0.375,1,0,Evaluation: Fine tuning biLM
203,"The ? parameter in Eqn. ( 1 ) was of practical importance to aid optimization , due to the different distributions between the biLM internal representations and the task specific representations .",Evaluation,Fine tuning biLM,named-entity-recognition,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",97,0.8434782608695652,202,0.7426470588235294,14,0.875,1,0,Evaluation: Fine tuning biLM
204,It is especially important in the last - only casein Sec. 5.1 .,Evaluation,Fine tuning biLM,named-entity-recognition,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",98,0.8521739130434782,203,0.7463235294117647,15,0.9375,1,0,Evaluation: Fine tuning biLM
213,"The best ELMo configuration added ELMo vectors to both the input and output of the lowest layer LSTM , using ( 1 ) with layer normalization and ? = 0.001 .",Evaluation,The batch size was 32 .,named-entity-recognition,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",107,0.9304347826086956,212,0.7794117647058824,7,0.7,1,0,Evaluation: The batch size was 32 .
218,Our QA model is a simplified version of the model from .,Evaluation,The batch size was 32 .,named-entity-recognition,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",112,0.9739130434782608,217,0.7977941176470589,1,0.25,1,0,Evaluation: The batch size was 32 .
221,The augmented con-,Evaluation,,named-entity-recognition,4,"['O', 'O', 'O']","['O', 'O', 'O']",115,1.0,220,0.8088235294117647,4,1.0,1,0,Evaluation
222,Model,,,named-entity-recognition,4,['O'],['O'],0,0.0,221,0.8125,0,0.0,1,0,
236,Our baseline SRL model is an exact reimplementation of .,Model,Acc.,named-entity-recognition,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.28,235,0.8639705882352942,1,0.0526315789473684,1,0,Model: Acc.
246,"Models are trained for 500 epochs or until validation F1 does not improve for 200 epochs , whichever is sooner .",Model,Gradients are clipped if their value exceeds 1.0 .,named-entity-recognition,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.48,245,0.9007352941176472,11,0.5789473684210527,1,0,Model: Gradients are clipped if their value exceeds 1.0 .
249,"The forget gate bias is initialized to 1 for all LSTMs , with all other gates initialized to 0 , as per . perparameters exactly following the original implementation .",Model,Gradients are clipped if their value exceeds 1.0 .,named-entity-recognition,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.54,248,0.9117647058823528,14,0.7368421052631579,1,0,Model: Gradients are clipped if their value exceeds 1.0 .
250,The best configuration added ELMo to the input of the lowest layer biLSTM and weighted the biLM layers using ( 1 ) without any regularization ( ? = 0 ) or layer normalization .,Model,Gradients are clipped if their value exceeds 1.0 .,named-entity-recognition,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",28,0.56,249,0.9154411764705882,15,0.7894736842105263,1,0,Model: Gradients are clipped if their value exceeds 1.0 .
265,"As the CoNLL 2003 NER data set is relatively small , we found the best performance by constraining the trainable layer weights to be effectively constant by setting ? = 0.1 with ( 1 ) .",Model,Gradients are clipped if their value exceeds 1.0 .,named-entity-recognition,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",43,0.86,264,0.9705882352941176,10,0.9090909090909092,1,0,Model: Gradients are clipped if their value exceeds 1.0 .
270,"To match the CoVe training setup , we only train on phrases that contain four or more tokens .",Model,A.8 Sentiment classification,named-entity-recognition,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",48,0.96,269,0.9889705882352942,3,0.6,1,0,Model: A.8 Sentiment classification
3,abstract,,,named-entity-recognition,5,['O'],['O'],0,0.0,2,0.0095693779904306,0,0.0,1,0,
4,Bi-directional,abstract,,named-entity-recognition,5,['O'],['O'],1,0.1666666666666666,3,0.0143540669856459,1,0.1666666666666666,1,0,abstract
5,LSTMs area powerful tool for text representation .,abstract,Bi-directional,named-entity-recognition,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.3333333333333333,4,0.0191387559808612,2,0.3333333333333333,1,0,abstract: Bi-directional
6,"On the other hand , they have been shown to suffer various limitations due to their sequential nature .",abstract,Bi-directional,named-entity-recognition,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.5,5,0.0239234449760765,3,0.5,1,0,abstract: Bi-directional
11,Neural models have become the dominant approach in the NLP literature .,Introduction,Introduction,named-entity-recognition,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0476190476190476,10,0.0478468899521531,1,0.0476190476190476,1,0,Introduction
12,"Compared to handcrafted indicator features , neural sentence representations are less sparse , and more flexible in encoding intricate syntactic and semantic information .",Introduction,Introduction,named-entity-recognition,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.0952380952380952,11,0.0526315789473684,2,0.0952380952380952,1,0,Introduction
14,"Despite their success , BiLSTMs have been shown to suffer several limitations .",Introduction,Introduction,named-entity-recognition,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.1904761904761904,13,0.062200956937799,4,0.1904761904761904,1,0,Introduction
15,"For example , their inherently sequential nature endows computation non-parallel within the same sentence , which can lead to a computational bottleneck , hindering their use in the in - dustry .",Introduction,Introduction,named-entity-recognition,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.238095238095238,14,0.0669856459330143,5,0.238095238095238,1,0,Introduction
16,"In addition , local ngrams , which have been shown a highly useful source of contextual information for NLP , are not explicitly modelled .",Introduction,Introduction,named-entity-recognition,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.2857142857142857,15,0.0717703349282296,6,0.2857142857142857,1,0,Introduction
22,"Consequently , we refer to our model as sentence - state LSTM , or S - LSTM in short .",Introduction,Introduction,named-entity-recognition,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.5714285714285714,21,0.1004784688995215,12,0.5714285714285714,1,0,Introduction
24,"In contrast , the number of recurrent steps necessary for BiLSTM scales with the size of the sentence .",Introduction,Introduction,named-entity-recognition,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.6666666666666666,23,0.1100478468899521,14,0.6666666666666666,1,0,Introduction
32,Related Work,,,named-entity-recognition,5,"['O', 'O']","['O', 'O']",0,0.0,31,0.1483253588516746,0,0.0,1,0,
54,Model,,,named-entity-recognition,5,['O'],['O'],0,0.0,53,0.2535885167464115,0,0.0,1,0,
56,"This can be be explained by the intuition that information exchange between distant nodes can be achieved using more recurrent steps under a smaller window size , as can be achieved using fewer steps under a larger window size .",Model,Model,named-entity-recognition,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.1176470588235294,55,0.2631578947368421,2,0.1176470588235294,1,0,Model
57,"Considering efficiency , we choose a window size of 1 for the remaining experiments , setting the number of recurrent steps to 9 according to .",Model,Model,named-entity-recognition,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.1764705882352941,56,0.2679425837320574,3,0.1764705882352941,1,0,Model
62,"We additionally make comparisons with stacked CNNs and hierarchical attention , shown in ( the CNN and Transformer rows ) , where N indicates the number of attention layers .",Model,Model,named-entity-recognition,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.4705882352941176,61,0.291866028708134,8,0.4705882352941176,1,0,Model
71,Similar observations are found using external CRF layers for sequence labelling .,Model,Model,named-entity-recognition,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,1.0,70,0.3349282296650718,17,1.0,1,0,Model
72,Baseline BiLSTM,,,named-entity-recognition,5,"['O', 'O']","['O', 'O']",0,0.0,71,0.3397129186602871,0,0.0,1,0,
75,"Given an initial value , the state changes its value recurrently , each time consuming an incoming word .",Baseline BiLSTM,Baseline BiLSTM,named-entity-recognition,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.0410958904109589,74,0.354066985645933,3,0.0517241379310344,1,0,Baseline BiLSTM
76,Take the forward LSTM component for example .,Baseline BiLSTM,Baseline BiLSTM,named-entity-recognition,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.0547945205479452,75,0.3588516746411483,4,0.0689655172413793,1,0,Baseline BiLSTM
77,"Denoting the initial state as ? ? h 0 , which is a model parameter , the recurrent state transition step for calculating ? ? h 1 , . . . , ? ? h n+1 is defined as follows :",Baseline BiLSTM,Baseline BiLSTM,named-entity-recognition,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.0684931506849315,76,0.3636363636363636,5,0.0862068965517241,1,0,Baseline BiLSTM
78,"where x t denotes the word representation of wt ; it , o t , ft and u t represent the values of an input gate , an output gate , a forget gate and an actual input at time step t , respectively , which controls the information flow fora recurrent cell ? ?",Baseline BiLSTM,Baseline BiLSTM,named-entity-recognition,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.0821917808219178,77,0.3684210526315789,6,0.1034482758620689,1,0,Baseline BiLSTM
79,ct and the state vector,Baseline BiLSTM,Baseline BiLSTM,named-entity-recognition,5,"['O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O']",7,0.0958904109589041,78,0.3732057416267942,7,0.1206896551724138,1,0,Baseline BiLSTM
80,The backward LSTM component follows the same recurrent state transition process as described in Eq 1 .,Baseline BiLSTM,Baseline BiLSTM,named-entity-recognition,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.1095890410958904,79,0.3779904306220095,8,0.1379310344827586,1,0,Baseline BiLSTM
81,"Starting from an initial state h n + 1 , which is a model parameter , it reads the input x n ,",Baseline BiLSTM,Baseline BiLSTM,named-entity-recognition,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.1232876712328767,80,0.3827751196172249,9,0.1551724137931034,1,0,Baseline BiLSTM
82,The BiLSTM model uses the concatenated value of ? ?,Baseline BiLSTM,Baseline BiLSTM,named-entity-recognition,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.136986301369863,81,0.3875598086124402,10,0.1724137931034483,1,0,Baseline BiLSTM
83,ht and ? ?,Baseline BiLSTM,Baseline BiLSTM,named-entity-recognition,5,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",11,0.1506849315068493,82,0.3923444976076555,11,0.1896551724137931,1,0,Baseline BiLSTM
84,ht as the hidden vector for wt :,Baseline BiLSTM,Baseline BiLSTM,named-entity-recognition,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.1643835616438356,83,0.3971291866028708,12,0.2068965517241379,1,0,Baseline BiLSTM
85,A single hidden vector representation g of the whole input sentence can be obtained using the final state values of the two LSTM components :,Baseline BiLSTM,Baseline BiLSTM,named-entity-recognition,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.1780821917808219,84,0.4019138755980861,13,0.2241379310344827,1,0,Baseline BiLSTM
86,Stacked BiLSTM,Baseline BiLSTM,,named-entity-recognition,5,"['O', 'O']","['O', 'O']",14,0.1917808219178082,85,0.4066985645933014,14,0.2413793103448276,1,0,Baseline BiLSTM
87,"Multiple layers of BiLTMs can be stacked for increased representation power , where the hidden vectors of a lower layer are used as inputs for an upper layer .",Baseline BiLSTM,Stacked BiLSTM,named-entity-recognition,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.2054794520547945,86,0.4114832535885167,15,0.2586206896551724,1,0,Baseline BiLSTM: Stacked BiLSTM
89,Sentence - State LSTM,Baseline BiLSTM,Stacked BiLSTM,named-entity-recognition,5,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",17,0.2328767123287671,88,0.4210526315789473,17,0.293103448275862,1,0,Baseline BiLSTM: Stacked BiLSTM
90,"Formally , an S - LSTM state at time step t can be denoted by :",Baseline BiLSTM,Stacked BiLSTM,named-entity-recognition,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.2465753424657534,89,0.4258373205741627,18,0.3103448275862069,1,0,Baseline BiLSTM: Stacked BiLSTM
91,which consists of a sub state ht i for each word w i and a sentence - level sub state gt .,Baseline BiLSTM,Stacked BiLSTM,named-entity-recognition,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.2602739726027397,90,0.430622009569378,19,0.3275862068965517,1,0,Baseline BiLSTM: Stacked BiLSTM
93,"For the initial state H 0 , we set h 0",Baseline BiLSTM,Stacked BiLSTM,named-entity-recognition,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.2876712328767123,92,0.4401913875598086,21,0.3620689655172414,1,0,Baseline BiLSTM: Stacked BiLSTM
94,to ht i and from g t?1 tog t .,Baseline BiLSTM,Stacked BiLSTM,named-entity-recognition,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.3013698630136986,93,0.4449760765550239,22,0.3793103448275862,1,0,Baseline BiLSTM: Stacked BiLSTM
95,"We take an LSTM structure similar to the baseline BiLSTM for modelling state transition , using a recurrent cell ct i for each w i and a cell ct g for g.",Baseline BiLSTM,Stacked BiLSTM,named-entity-recognition,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",23,0.3150684931506849,94,0.4497607655502392,23,0.396551724137931,1,0,Baseline BiLSTM: Stacked BiLSTM
96,"As shown in , the value of each ht i is computed based on the values of",Baseline BiLSTM,Stacked BiLSTM,named-entity-recognition,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.3287671232876712,95,0.4545454545454545,24,0.4137931034482758,1,0,Baseline BiLSTM: Stacked BiLSTM
97,"i + 1 and g t?1 , together with their corresponding cell values :",Baseline BiLSTM,Stacked BiLSTM,named-entity-recognition,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.3424657534246575,96,0.4593301435406698,25,0.4310344827586206,1,0,Baseline BiLSTM: Stacked BiLSTM
98,where ?,Baseline BiLSTM,Stacked BiLSTM,named-entity-recognition,5,"['O', 'O']","['O', 'O']",26,0.3561643835616438,97,0.4641148325358851,26,0.4482758620689655,1,0,Baseline BiLSTM: Stacked BiLSTM
99,"ti is the concatenation of hidden vectors of a context window , and l ti , rt i , ft i , st i and it i are gates that control information flow from ?",Baseline BiLSTM,Stacked BiLSTM,named-entity-recognition,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.3698630136986301,98,0.4688995215311005,27,0.4655172413793103,1,0,Baseline BiLSTM: Stacked BiLSTM
100,ti and xi to ct i .,Baseline BiLSTM,Stacked BiLSTM,named-entity-recognition,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O']",28,0.3835616438356164,99,0.4736842105263157,28,0.4827586206896552,1,0,Baseline BiLSTM: Stacked BiLSTM
101,"In particular , it i controls information from the input xi ; l ti , rt i , ft i and st i control information from the left context cell c t ?1 i ? 1 , the right context cell c t ?1 i + 1 , c t?1 i and the sentence context cell c t ? 1 g , respectively .",Baseline BiLSTM,Stacked BiLSTM,named-entity-recognition,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",29,0.3972602739726027,100,0.4784688995215311,29,0.5,1,0,Baseline BiLSTM: Stacked BiLSTM
102,"The values of it i , l ti , rt i , ft i and st i are normalised such that they sum to 1 .",Baseline BiLSTM,Stacked BiLSTM,named-entity-recognition,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",30,0.410958904109589,101,0.4832535885167464,30,0.5172413793103449,1,0,Baseline BiLSTM: Stacked BiLSTM
103,o ti is an output gate from the cell state ct i to the hidden state,Baseline BiLSTM,Stacked BiLSTM,named-entity-recognition,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",31,0.4246575342465753,102,0.4880382775119617,31,0.5344827586206896,1,0,Baseline BiLSTM: Stacked BiLSTM
104,The value of gt is computed based on the values,Baseline BiLSTM,Stacked BiLSTM,named-entity-recognition,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",32,0.4383561643835616,103,0.492822966507177,32,0.5517241379310345,1,0,Baseline BiLSTM: Stacked BiLSTM
105,". . , ft n+1 and ft g are gates controlling information from c t ? 1 0 , . . . , c t?1 n+1 and c t ? 1 g , respectively , which are normalised .",Baseline BiLSTM,Stacked BiLSTM,named-entity-recognition,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",33,0.4520547945205479,104,0.4976076555023923,33,0.5689655172413793,1,0,Baseline BiLSTM: Stacked BiLSTM
106,o t is an output gate from the recurrent cell ct g tog t .,Baseline BiLSTM,Stacked BiLSTM,named-entity-recognition,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",34,0.4657534246575342,105,0.5023923444976076,34,0.5862068965517241,1,0,Baseline BiLSTM: Stacked BiLSTM
107,"W x , U x and bx ( x ? {g , f , o} ) are model parameters .",Baseline BiLSTM,Stacked BiLSTM,named-entity-recognition,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",35,0.4794520547945205,106,0.507177033492823,35,0.603448275862069,1,0,Baseline BiLSTM: Stacked BiLSTM
109,The difference between S - LSTM and BiLSTM can be understood with respect to their recurrent states .,Baseline BiLSTM,Contrast with BiLSTM,named-entity-recognition,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",37,0.5068493150684932,108,0.5167464114832536,37,0.6379310344827587,1,0,Baseline BiLSTM: Contrast with BiLSTM
111,"Different from BiLSTMs , for which ht at different time steps are used to represent w 0 , . . . , w n + 1 , respectively , the word - level states ht i and sentence - level state gt of S - LSTMs directly correspond to the goal outputs hi and g , as introduced in the beginning of this section .",Baseline BiLSTM,Contrast with BiLSTM,named-entity-recognition,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",39,0.5342465753424658,110,0.5263157894736842,39,0.6724137931034483,1,0,Baseline BiLSTM: Contrast with BiLSTM
112,"As t increases from 0 , ht i and gt are enriched with increasingly deeper context information .",Baseline BiLSTM,Contrast with BiLSTM,named-entity-recognition,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",40,0.547945205479452,111,0.5311004784688995,40,0.6896551724137931,1,0,Baseline BiLSTM: Contrast with BiLSTM
114,"As a result , the number of time steps scales with the size of the input .",Baseline BiLSTM,Contrast with BiLSTM,named-entity-recognition,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",42,0.5753424657534246,113,0.5406698564593302,42,0.7241379310344828,1,0,Baseline BiLSTM: Contrast with BiLSTM
116,"At each step , each hi captures an increasing larger ngram context , while additionally communicating globally to all other h j via g.",Baseline BiLSTM,Contrast with BiLSTM,named-entity-recognition,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",44,0.6027397260273972,115,0.5502392344497608,44,0.7586206896551724,1,0,Baseline BiLSTM: Contrast with BiLSTM
117,"The optimal number of recurrent steps is decided by the end - task performance , and does not necessarily scale with the sentence size .",Baseline BiLSTM,Contrast with BiLSTM,named-entity-recognition,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",45,0.6164383561643836,116,0.5550239234449761,45,0.7758620689655172,1,0,Baseline BiLSTM: Contrast with BiLSTM
118,"As a result , S - LSTM can potentially be both more efficient and more accurate compared with BiLSTMs .",Baseline BiLSTM,Contrast with BiLSTM,named-entity-recognition,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",46,0.6301369863013698,117,0.5598086124401914,46,0.7931034482758621,1,0,Baseline BiLSTM: Contrast with BiLSTM
120,"By default S - LSTM exchanges information only between neighbouring words , which can be seen as adopting a 1 word window on each side .",Baseline BiLSTM,Contrast with BiLSTM,named-entity-recognition,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",48,0.6575342465753424,119,0.569377990430622,48,0.8275862068965517,1,0,Baseline BiLSTM: Contrast with BiLSTM
122,"To this end , we modify Eq 2 , integrating additional context words to ? ti , with extended gates and cells .",Baseline BiLSTM,Contrast with BiLSTM,named-entity-recognition,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",50,0.684931506849315,121,0.5789473684210527,50,0.8620689655172413,1,0,Baseline BiLSTM: Contrast with BiLSTM
123,"For example , with a window size of 2 ,",Baseline BiLSTM,Contrast with BiLSTM,named-entity-recognition,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",51,0.6986301369863014,122,0.583732057416268,51,0.8793103448275862,1,0,Baseline BiLSTM: Contrast with BiLSTM
125,Additional sentence - level nodes .,Baseline BiLSTM,Contrast with BiLSTM,named-entity-recognition,5,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O']",53,0.726027397260274,124,0.5933014354066986,53,0.913793103448276,1,0,Baseline BiLSTM: Contrast with BiLSTM
127,"One way of enriching the parameter space is to add more sentence - level nodes , each communicating with word - level nodes in the same way as described by Eq 3 .",Baseline BiLSTM,Contrast with BiLSTM,named-entity-recognition,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",55,0.7534246575342466,126,0.6028708133971292,55,0.9482758620689656,1,0,Baseline BiLSTM: Contrast with BiLSTM
129,"When one sentence - level node is used for classification outputs , the other sentencelevel node can serve as hidden memory units , or latent features .",Baseline BiLSTM,Contrast with BiLSTM,named-entity-recognition,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",57,0.7808219178082192,128,0.6124401913875598,57,0.9827586206896552,1,0,Baseline BiLSTM: Contrast with BiLSTM
131,Task settings,Baseline BiLSTM,,named-entity-recognition,5,"['O', 'O']","['O', 'O']",59,0.8082191780821918,130,0.6220095693779905,0,0.0,1,0,Baseline BiLSTM
133,"For classification , g is fed to a softmax classification layer :",Baseline BiLSTM,Task settings,named-entity-recognition,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",61,0.8356164383561644,132,0.631578947368421,2,0.1428571428571428,1,0,Baseline BiLSTM: Task settings
134,where y is the probability distribution of output class labels and W c and b care model parameters .,Baseline BiLSTM,Task settings,named-entity-recognition,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",62,0.8493150684931506,133,0.6363636363636364,3,0.2142857142857142,1,0,Baseline BiLSTM: Task settings
135,"For sequence labelling , each hi can be used as feature representation fora corresponding word w i .",Baseline BiLSTM,Task settings,named-entity-recognition,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",63,0.863013698630137,134,0.6411483253588517,4,0.2857142857142857,1,0,Baseline BiLSTM: Task settings
136,External attention,Baseline BiLSTM,,named-entity-recognition,5,"['O', 'O']","['O', 'O']",64,0.8767123287671232,135,0.645933014354067,5,0.3571428571428571,1,0,Baseline BiLSTM
139,"In particular , additive attention ( Bahdanau",Baseline BiLSTM,External attention,named-entity-recognition,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O']",67,0.9178082191780822,138,0.6602870813397129,8,0.5714285714285714,1,0,Baseline BiLSTM: External attention
140,"Here W ? , u and b ?",Baseline BiLSTM,External attention,named-entity-recognition,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",68,0.9315068493150684,139,0.6650717703349283,9,0.6428571428571429,1,0,Baseline BiLSTM: External attention
141,are model parameters .,Baseline BiLSTM,External attention,named-entity-recognition,5,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",69,0.9452054794520548,140,0.6698564593301436,10,0.7142857142857143,1,0,Baseline BiLSTM: External attention
142,External CRF,Baseline BiLSTM,,named-entity-recognition,5,"['O', 'O']","['O', 'O']",70,0.958904109589041,141,0.6746411483253588,11,0.7857142857142857,1,0,Baseline BiLSTM
144,are parameters specific to two consecutive labels y i ?1 and y i .,Baseline BiLSTM,External CRF,named-entity-recognition,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",72,0.9863013698630136,143,0.6842105263157895,13,0.9285714285714286,1,0,Baseline BiLSTM: External CRF
146,Experiments,,,named-entity-recognition,5,['O'],['O'],0,0.0,145,0.69377990430622,0,0.0,1,0,
149,Development Experiments,,,named-entity-recognition,5,"['O', 'O']","['O', 'O']",0,0.0,148,0.7081339712918661,0,0.0,1,0,
158,"Hyperparameters for BiLSTM models are also set according to the development data , which we omit here .",Development Experiments,Development Experiments,named-entity-recognition,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.6,157,0.7511961722488039,9,0.6,1,0,Development Experiments
159,State transition .,Development Experiments,Development Experiments,named-entity-recognition,5,"['O', 'O', 'O']","['O', 'O', 'O']",10,0.6666666666666666,158,0.7559808612440191,10,0.6666666666666666,1,0,Development Experiments
160,"In , the number of recurrent state transition steps of S - LSTM is decided according to the best development performance .",Development Experiments,Development Experiments,named-entity-recognition,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.7333333333333333,159,0.7607655502392344,11,0.7333333333333333,1,0,Development Experiments
164,"On the other hand , no significant differences are observed on the peak accuracies given by different window sizes , although a larger window size ( e.g.",Development Experiments,Development Experiments,named-entity-recognition,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,1.0,163,0.7799043062200957,15,1.0,1,0,Development Experiments
165,Final Results for Classification,,,named-entity-recognition,5,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",0,0.0,164,0.784688995215311,0,0.0,1,0,
177,This is because the average length of inputs is larger on the 16 datasets ( see Section 4.5 ) .,Final Results for Classification,Final Results for Classification,named-entity-recognition,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.3157894736842105,176,0.8421052631578947,12,1.0,1,0,Final Results for Classification
179,Bi-directional,Final Results for Classification,,named-entity-recognition,5,['O'],['O'],14,0.3684210526315789,178,0.8516746411483254,1,0.0666666666666666,1,0,Final Results for Classification
182,"For the latter , we decide the number of recurrent stepson the respective development sets for sequence labelling .",Final Results for Classification,Bi-directional,named-entity-recognition,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.4473684210526316,181,0.8660287081339713,4,0.2666666666666666,1,0,Final Results for Classification: Bi-directional
193,"On the other hand , these semi-supervised learning techniques are orthogonal to our work , and can potentially be used for S - LSTM also .",Final Results for Classification,Bi-directional,named-entity-recognition,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",28,0.7368421052631579,192,0.9186602870813396,15,1.0,1,0,Final Results for Classification: Bi-directional
194,Analysis,Final Results for Classification,,named-entity-recognition,5,['O'],['O'],29,0.7631578947368421,193,0.9234449760765552,0,0.0,1,0,Final Results for Classification
199,"these comparisons , we mix all training instances , order them by the size , and put them into 10 equal groups , the medium sentence lengths of which are shown .",Final Results for Classification,Analysis,named-entity-recognition,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",34,0.8947368421052632,198,0.9473684210526316,5,0.5555555555555556,1,0,Final Results for Classification: Analysis
201,"Similar to hierarchical attention , there is a relative disadvantage of S - LSTM in comparison with BiLSTM , which is that the memory consumption is relatively larger .",Final Results for Classification,Analysis,named-entity-recognition,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",36,0.9473684210526316,200,0.9569377990430622,7,0.7777777777777778,1,0,Final Results for Classification: Analysis
202,"For example , over the movie review development set , the actual GPU memory consumption by S - LSTM , Bi LSTM , 2 - layer stacked BiLSTM and 4 - layer stacked BiLSTM are 252M , 89M , 146 M and 253M , respectively .",Final Results for Classification,Analysis,named-entity-recognition,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",37,0.9736842105263158,201,0.9617224880382776,8,0.8888888888888888,1,0,Final Results for Classification: Analysis
203,This is due to the fact that computation is performed in parallel by S - LSTM and hierarchical attention .,Final Results for Classification,Analysis,named-entity-recognition,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",38,1.0,202,0.9665071770334928,9,1.0,1,0,Final Results for Classification: Analysis
204,Conclusion,,,named-entity-recognition,5,['O'],['O'],0,0.0,203,0.971291866028708,0,0.0,1,0,
3,abstract,,,named-entity-recognition,6,['O'],['O'],0,0.0,2,0.0094339622641509,0,0.0,1,0,
5,"While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers .",abstract,abstract,named-entity-recognition,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.2222222222222222,4,0.0188679245283018,2,0.2222222222222222,1,0,abstract
11,This work is licensed under a Creative Commons Attribution 4.0 International License .,abstract,abstract,named-entity-recognition,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.8888888888888888,10,0.0471698113207547,8,0.8888888888888888,1,0,abstract
15,"Various approaches have been proposed to tackle the task , from hand - crafted feature - based machine learning models like conditional random fields and perceptron , to deep neural models .",Introduction,Introduction,named-entity-recognition,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.0526315789473684,14,0.0660377358490566,2,0.1111111111111111,1,0,Introduction
16,"Word representations , also known as word embeddings , area key element for multiple NLP tasks including NER .",Introduction,Introduction,named-entity-recognition,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.0789473684210526,15,0.070754716981132,3,0.1666666666666666,1,0,Introduction
27,"We call this vector an LS representation , for Lexical Similarity .",Introduction,Introduction,named-entity-recognition,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.3684210526315789,26,0.1226415094339622,14,0.7777777777777778,1,0,Introduction
32,Motivation,Introduction,,named-entity-recognition,6,['O'],['O'],19,0.5,31,0.1462264150943396,0,0.0,1,0,Introduction
33,Gazetteers are lists of entities that are associated with specific NE categories .,Introduction,Motivation,named-entity-recognition,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.5263157894736842,32,0.1509433962264151,1,0.0526315789473684,1,0,Introduction: Motivation
34,"They are widely used as a feature source in NER , and have been successfully included in feature - based models .",Introduction,Motivation,named-entity-recognition,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.5526315789473685,33,0.1556603773584905,2,0.1052631578947368,1,0,Introduction: Motivation
35,"Typically , lists of entities are compiled from structured data sources such as DBpedia or Freebase .",Introduction,Motivation,named-entity-recognition,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.5789473684210527,34,0.160377358490566,3,0.1578947368421052,1,0,Introduction: Motivation
38,"Despite their importance , gazetteer - based features suffer from a number of limitations .",Introduction,Motivation,named-entity-recognition,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.6578947368421053,37,0.1745283018867924,6,0.3157894736842105,1,0,Introduction: Motivation
39,Binary representation .,Introduction,Motivation,named-entity-recognition,6,"['O', 'O', 'O']","['O', 'O', 'O']",26,0.6842105263157895,38,0.1792452830188679,7,0.3684210526315789,1,0,Introduction: Motivation
41,"For example , the word "" France "" can be used as a person , an organization , or a location , while it likely refers to the country most of the time .",Introduction,Motivation,named-entity-recognition,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",28,0.7368421052631579,40,0.1886792452830188,9,0.4736842105263157,1,0,Introduction: Motivation
42,Binary features can not capture this preference .,Introduction,Motivation,named-entity-recognition,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",29,0.7631578947368421,41,0.1933962264150943,10,0.5263157894736842,1,0,Introduction: Motivation
43,Generation .,Introduction,,named-entity-recognition,6,"['O', 'O']","['O', 'O']",30,0.7894736842105263,42,0.1981132075471698,11,0.5789473684210527,1,0,Introduction
47,"Gazetteer features do not capture signal from non-entity words , while earlier feature - based models strived to encode that some words ( or n-grams ) trigger specific entity types .",Introduction,Generation .,named-entity-recognition,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",34,0.8947368421052632,46,0.2169811320754717,15,0.7894736842105263,1,0,Introduction: Generation .
48,"For instance , words such as "" eat "" , "" directed "" or "" born "" are words that typically appear after a mention of type PER .",Introduction,Generation .,named-entity-recognition,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",35,0.9210526315789472,47,0.2216981132075471,16,0.8421052631578947,1,0,Introduction: Generation .
51,"Note that attest time , we only have to feed our model with this feature vector , which is efficient .",Introduction,Generation .,named-entity-recognition,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",38,1.0,50,0.2358490566037736,19,1.0,1,0,Introduction: Generation .
52,Our Method,,,named-entity-recognition,6,"['O', 'O']","['O', 'O']",0,0.0,51,0.240566037735849,0,0.0,1,0,
54,Turning Wikipedia into a corpus of named - entities annotated with types is a task that received continuous attention over the years .,Our Method,Embedding Words and Entity Types,named-entity-recognition,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.0408163265306122,53,0.25,1,0.0344827586206896,1,0,Our Method: Embedding Words and Entity Types
65,"For instance , the embedding for / product / software will be trained using context words that surround all entities that were ( automatically ) labelled as / product / software in Wikipedia .",Our Method,Embedding Words and Entity Types,named-entity-recognition,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.2653061224489796,64,0.3018867924528302,12,0.4137931034482758,1,0,Our Method: Embedding Words and Entity Types
71,"For visualization proposes , we only plot single - word mentions that were annotated in WiFiNE with one of those 6 types .",Our Method,Embedding Words and Entity Types,named-entity-recognition,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.3877551020408163,70,0.330188679245283,18,0.6206896551724138,1,0,Our Method: Embedding Words and Entity Types
73,"In addition , words have the color associated with the most frequent type they were annotated within WiFiNE .",Our Method,Embedding Words and Entity Types,named-entity-recognition,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.4285714285714285,72,0.3396226415094339,20,0.6896551724137931,1,0,Our Method: Embedding Words and Entity Types
75,"For instance , "" firefox "" is close to the type / product / software , while "" enzyme "" is close to the / biology entity type .",Our Method,Embedding Words and Entity Types,named-entity-recognition,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",23,0.4693877551020408,74,0.3490566037735849,22,0.7586206896551724,1,0,Our Method: Embedding Words and Entity Types
76,We also notice that words that are labelled with different types tend to appear between types they were annotated with .,Our Method,Embedding Words and Entity Types,named-entity-recognition,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.4897959183673469,75,0.3537735849056603,23,0.7931034482758621,1,0,Our Method: Embedding Words and Entity Types
77,"For instance , "" gpx2 "" , which is used both as a software and as a gene , has it s embedding in between / product / software and / biology .",Our Method,Embedding Words and Entity Types,named-entity-recognition,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.5102040816326531,76,0.3584905660377358,24,0.8275862068965517,1,0,Our Method: Embedding Words and Entity Types
78,"We inspected some of the words plotted in , and found that "" jrun "" and "" xp "" are incorrectly labelled as / product / weapon in WiFiNE .",Our Method,Embedding Words and Entity Types,named-entity-recognition,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.5306122448979592,77,0.3632075471698113,25,0.8620689655172413,1,0,Our Method: Embedding Words and Entity Types
79,"But since these words are seen in a software context , their embeddings are closer to the / product / software embedding than the / product / weapon one .",Our Method,Embedding Words and Entity Types,named-entity-recognition,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.5510204081632653,78,0.3679245283018867,26,0.896551724137931,1,0,Our Method: Embedding Words and Entity Types
80,"We feel this tolerance to noise is a desirable feature , one that hopefully allows a more efficient use of distant supervision .",Our Method,Embedding Words and Entity Types,named-entity-recognition,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",28,0.5714285714285714,79,0.3726415094339622,27,0.9310344827586208,1,0,Our Method: Embedding Words and Entity Types
82,"For instance , "" iota "" and "" x.org "" are embedded near their respective types , despite the fact that they appear less than 30 times in the version of Wikipedia used to compile WiFiNE .",Our Method,Embedding Words and Entity Types,named-entity-recognition,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",30,0.6122448979591837,81,0.3820754716981132,29,1.0,1,0,Our Method: Embedding Words and Entity Types
83,LS Representation,Our Method,,named-entity-recognition,6,"['O', 'O']","['O', 'O']",31,0.6326530612244898,82,0.3867924528301887,0,0.0,1,0,Our Method
84,"This joint vector space only serves the purpose of associating to each word a LS representation , that is , a 120 - dimensional vector where the ith coefficient is a value in the [ ? 1 , + 1 ] interval , equal to the cosine similarity 1 between the word embedding and the embedding of the ith entity type ( we have 120 types ) .",Our Method,LS Representation,named-entity-recognition,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",32,0.6530612244897959,83,0.3915094339622642,1,0.0,1,0,Our Method: LS Representation
85,Word,Our Method,,named-entity-recognition,6,['O'],['O'],33,0.673469387755102,84,0.3962264150943396,0,0.0,1,0,Our Method
86,Entity .,Our Method,,named-entity-recognition,6,"['O', 'O']","['O', 'O']",34,0.6938775510204082,85,0.4009433962264151,1,0.1,1,0,Our Method
89,"For instance , the LS representation of the word "" hilton "" encodes that it more often refers to a hotel or a restaurant than to an actress .",Our Method,Entity .,named-entity-recognition,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",37,0.7551020408163265,88,0.4150943396226415,4,0.4,1,0,Our Method: Entity .
91,"For instance , "" dammstadt "" , which appears only 5 times in WiFiNE , and which refers to the Damm city in Germany , is most similar to / location / city and / location / railway .",Our Method,Entity .,named-entity-recognition,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",39,0.7959183673469388,90,0.4245283018867924,6,0.6,1,0,Our Method: Entity .
92,"Interestingly , this mention does not have its page in English Wikipedia .",Our Method,Entity .,named-entity-recognition,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",40,0.8163265306122449,91,0.4292452830188679,7,0.7,1,0,Our Method: Entity .
94,"For instance the verb "" directed "" is very close to / person / director , an entity type that usually precedes it , and to / art / film , that usually follows it .",Our Method,Entity .,named-entity-recognition,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",42,0.8571428571428571,93,0.4386792452830189,9,0.9,1,0,Our Method: Entity .
95,"Likewise , the preposition "" in "" is near / date and / location / city , which frequently follow "" in "" .",Our Method,Entity .,named-entity-recognition,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",43,0.8775510204081632,94,0.4433962264150943,10,1.0,1,0,Our Method: Entity .
102,Our NER System,,,named-entity-recognition,6,"['O', 'O', 'O']","['O', 'O', 'O']",0,0.0,101,0.4764150943396226,0,0.0,1,0,
104,Bi-LSTM- CRF,Our NER System,Our NER System,named-entity-recognition,6,"['O', 'O']","['O', 'O']",2,1.0,103,0.4858490566037736,2,1.0,1,0,Our NER System
105,Model,,,named-entity-recognition,6,['O'],['O'],0,0.0,104,0.490566037735849,0,0.0,1,0,
107,Features,Model,,named-entity-recognition,6,['O'],['O'],2,0.1052631578947368,106,0.5,0,0.0,1,0,Model
110,Word Embeddings,Model,,named-entity-recognition,6,"['O', 'O']","['O', 'O']",5,0.2631578947368421,109,0.5141509433962265,0,0.0,1,0,Model
114,"These embeddings were previously used by , who report good performance on CONLL , and state - of - the - art results on ONTONOTES respectively .",Model,Word Embeddings,named-entity-recognition,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.4736842105263157,113,0.5330188679245284,4,0.8,1,0,Model: Word Embeddings
115,Note that these pre-trained embeddings are adjusted during training .,Model,Word Embeddings,named-entity-recognition,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.5263157894736842,114,0.5377358490566038,5,1.0,1,0,Model: Word Embeddings
117,"Following , we use a forward and a backward LSTM to derive a representation of each word from its characters ( right part of .",Model,Character Embeddings,named-entity-recognition,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.631578947368421,116,0.5471698113207547,1,0.5,1,0,Model: Character Embeddings
122,LS Vectors,Model,,named-entity-recognition,6,"['O', 'O']","['O', 'O']",17,0.8947368421052632,121,0.5707547169811321,0,0.0,1,0,Model
123,"Contrarily to previous features , lexical vectors are computed offline and are not adjusted during training .",Model,LS Vectors,named-entity-recognition,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.9473684210526316,122,0.5754716981132075,1,0.5,1,0,Model: LS Vectors
124,"We found useful in practice to apply a MinMax scaler in the range [ ? 1 , + 1 ] to each LS vector we computed ; thus , [.. , 0.095 , .. , 0.20 , .. , 0.76 , .. ] becomes [.. , ? 1 , .. , ? 0.67 , .. , 1 , ..].",Model,LS Vectors,named-entity-recognition,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,1.0,123,0.5801886792452831,2,1.0,1,0,Model: LS Vectors
125,Experiments,,,named-entity-recognition,6,['O'],['O'],0,0.0,124,0.5849056603773585,0,0.0,1,0,
126,Data and Evaluation,,,named-entity-recognition,6,"['O', 'O', 'O']","['O', 'O', 'O']",0,0.0,125,0.589622641509434,0,0.0,1,0,
129,"As we can see , ONTONOTES is much larger .",Data and Evaluation,Data and Evaluation,named-entity-recognition,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.2727272727272727,128,0.6037735849056604,3,0.2727272727272727,1,0,Data and Evaluation
132,The ) is a well known collection of Reuters newswire articles that contains a large portion of sports news .,Data and Evaluation,Data and Evaluation,named-entity-recognition,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.5454545454545454,131,0.6179245283018868,6,0.5454545454545454,1,0,Data and Evaluation
135,"This dataset is annotated with 18 entity types , and is much larger than CONLL .",Data and Evaluation,Data and Evaluation,named-entity-recognition,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.8181818181818182,134,0.6320754716981132,9,0.8181818181818182,1,0,Data and Evaluation
137,"Also , we exclude ( both during training and testing ) the New Testaments portion as it does not contain gold NE annotations .",Data and Evaluation,Data and Evaluation,named-entity-recognition,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,1.0,136,0.6415094339622641,11,1.0,1,0,Data and Evaluation
151,"The authors use an architecture similar to ours , but use a binary gazetteer feature set , while we use our LS representation .",Training and Implementation,Training and Implementation,named-entity-recognition,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.40625,150,0.7075471698113207,13,0.4333333333333333,1,0,Training and Implementation
159,Our model is much simpler and faster .,Training and Implementation,Training and Implementation,named-entity-recognition,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.65625,158,0.7452830188679245,21,0.7,1,0,Training and Implementation
160,They report a performance of 90.43 when using an architecture similar to ours .,Training and Implementation,Training and Implementation,named-entity-recognition,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.6875,159,0.75,22,0.7333333333333333,1,0,Training and Implementation
162,"They report gains between 0.8 and 1.2 points by using such LM embeddings , which suggests that LS vectors are indeed efficient .",Training and Implementation,Training and Implementation,named-entity-recognition,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.75,161,0.7594339622641509,24,0.8,1,0,Training and Implementation
163,"Unfortunately , due to time and resource constraints , 4 we were notable to measure whether both features complement each other .",Training and Implementation,Training and Implementation,named-entity-recognition,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.78125,162,0.7641509433962265,25,0.8333333333333334,1,0,Training and Implementation
164,This is left for future investigations .,Training and Implementation,Training and Implementation,named-entity-recognition,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.8125,163,0.7688679245283019,26,0.8666666666666667,1,0,Training and Implementation
169,Results on the Development Set,Training and Implementation,,named-entity-recognition,6,"['O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O']",31,0.96875,168,0.7924528301886793,0,0.0,1,0,Training and Implementation
170,CONLL,Training and Implementation,,named-entity-recognition,6,['O'],['O'],32,1.0,169,0.7971698113207547,0,0.0,1,0,Training and Implementation
171,Results on CONLL,,,named-entity-recognition,6,"['O', 'O', 'O']","['O', 'O', 'O']",0,0.0,170,0.8018867924528302,0,0.0,1,0,
172,Results on ONTONOTES,,,named-entity-recognition,6,"['O', 'O', 'O']","['O', 'O', 'O']",0,0.0,171,0.8066037735849056,0,0.0,1,0,
173,Model,,,named-entity-recognition,6,['O'],['O'],0,0.0,172,0.8113207547169812,0,0.0,1,0,
175,"To confirm that the gains came from our feature vector and not from increasing the number of hidden units , we tested several SSKIP models by increasing the LSTM hidden layer dimension so that number of parameters is the same as the model with LS vectors .",Model,Model,named-entity-recognition,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.5,174,0.8207547169811321,2,0.5,1,0,Model
178,Ablation Results,,,named-entity-recognition,6,"['O', 'O']","['O', 'O']",0,0.0,177,0.8349056603773585,0,0.0,1,0,
181,"We want to point out that LS vectors are not adapted during training , contrarily to the SSKIP embeddings .",Ablation Results,Ablation Results,named-entity-recognition,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.375,180,0.8490566037735849,3,0.375,1,0,Ablation Results
186,"Also , it is worth mentioning that our embeddings are trained on 1.3B words compared to 42B for SSKIP .",Ablation Results,Ablation Results,named-entity-recognition,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,1.0,185,0.8726415094339622,8,1.0,1,0,Ablation Results
187,Related Works,,,named-entity-recognition,6,"['O', 'O']","['O', 'O']",0,0.0,186,0.8773584905660378,0,0.0,1,0,
205,Conclusion and Future Work,,,named-entity-recognition,6,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",0,0.0,204,0.9622641509433962,0,0.0,1,0,
3,abstract,,,named-entity-recognition,7,['O'],['O'],0,0.0,2,0.0126582278481012,0,0.0,1,0,
4,It is common that entity mentions can contain other mentions recursively .,abstract,abstract,named-entity-recognition,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.1428571428571428,3,0.0189873417721519,1,0.1428571428571428,1,0,abstract
10,1,abstract,abstract,named-entity-recognition,7,['O'],['O'],7,1.0,9,0.0569620253164556,7,1.0,1,0,abstract
12,"There has been an increasing interest in named entity recognition or more generally recognizing entity mentions 2 ) that the nested hierarchical structure of entity mentions should betaken into account to better facilitate downstream tasks like question answering , relation extraction , event extraction , and coreference resolution .",Introduction,Introduction,named-entity-recognition,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0588235294117647,11,0.0696202531645569,1,0.0588235294117647,1,0,Introduction
13,"Practically , the mentions with nested structures frequently exist in news and biomedical documents .",Introduction,Introduction,named-entity-recognition,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.1176470588235294,12,0.0759493670886076,2,0.1176470588235294,1,0,Introduction
14,"For example in Traditional sequence labeling models such as conditional random fields ( CRF ) do not allow hierarchical structures between segments , making them incapable to handle such problems .",Introduction,Introduction,named-entity-recognition,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.1764705882352941,13,0.0822784810126582,3,0.1764705882352941,1,0,Introduction
16,The issue of using a chart - based parser is its cubic time complexity in the number of words in the sentence .,Introduction,Introduction,named-entity-recognition,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.2941176470588235,15,0.0949367088607595,5,0.2941176470588235,1,0,Introduction
18,"Generally , each sentence with nested mentions is mapped to a forest where each outermost mention forms a tree consisting of its inner mentions .",Introduction,Introduction,named-entity-recognition,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.4117647058823529,17,0.1075949367088607,7,0.4117647058823529,1,0,Introduction
20,shows an example of such a forest .,Introduction,Introduction,named-entity-recognition,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.5294117647058824,19,0.120253164556962,9,0.5294117647058824,1,0,Introduction
21,"In contrast , the tree structure by further uses a root node to connect all tree elements .",Introduction,Introduction,named-entity-recognition,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.5882352941176471,20,0.1265822784810126,10,0.5882352941176471,1,0,Introduction
29,Related Work,,,named-entity-recognition,7,"['O', 'O']","['O', 'O']",0,0.0,28,0.1772151898734177,0,0.0,1,0,
44,Model,,,named-entity-recognition,7,['O'],['O'],0,0.0,43,0.2721518987341772,0,0.0,1,0,
45,"Specifically , given a sequence of words {x 0 , x 1 , . . . , x n } , the goal of our system is to output a set of mentions where nested structures are allowed .",Model,Model,named-entity-recognition,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.25,44,0.2784810126582278,1,0.0344827586206896,1,0,Model
47,The mapping is straightforward : each outermost mention forms a tree where the mention is the root and its contained mentions correspond to constituents of the tree .,Model,Model,named-entity-recognition,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.75,46,0.2911392405063291,3,0.1034482758620689,1,0,Model
48,4,Model,Model,named-entity-recognition,7,['O'],['O'],4,1.0,47,0.2974683544303797,4,0.1379310344827586,1,0,Model
49,Shift - Reduce System,,,named-entity-recognition,7,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",0,0.0,48,0.3037974683544304,5,0.1724137931034483,1,0,
52,"The system 's state is defined as [ S , i , A ] which denotes stack , buffer front index and action history respectively .",Shift - Reduce System,Shift - Reduce System,named-entity-recognition,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.0483870967741935,51,0.3227848101265823,8,0.2758620689655172,1,0,Shift - Reduce System
53,In each step .,Shift - Reduce System,Shift - Reduce System,named-entity-recognition,7,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",4,0.064516129032258,52,0.3291139240506329,9,0.3103448275862069,1,0,Shift - Reduce System
55,"Our system consists of three types of transition actions , which are also summarized in :",Shift - Reduce System,Shift - Reduce System,named-entity-recognition,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.0967741935483871,54,0.3417721518987341,11,0.3793103448275862,1,0,Shift - Reduce System
56,SHIFT pushes the next word from buffer to the stack .,Shift - Reduce System,Shift - Reduce System,named-entity-recognition,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.1129032258064516,55,0.3481012658227848,12,0.4137931034482758,1,0,Shift - Reduce System
57,REDUCE - X pops the top two items t 0 and t 1 from the tack and combines them as anew tree element { X ? t 0 t 1 } which is then pushed onto the stack .,Shift - Reduce System,Shift - Reduce System,named-entity-recognition,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.1290322580645161,56,0.3544303797468354,13,0.4482758620689655,1,0,Shift - Reduce System
58,UNARY - X pops the top item t 0 from the stack and constructs anew tree element { X ? t 0 } which is pushed back to the stack .,Shift - Reduce System,Shift - Reduce System,named-entity-recognition,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.1451612903225806,57,0.360759493670886,14,0.4827586206896552,1,0,Shift - Reduce System
60,"For example , if three consecutive words A , B , C are annotated as Person , we convert it into a binary tree { P erson ?",Shift - Reduce System,Shift - Reduce System,named-entity-recognition,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.1774193548387097,59,0.3734177215189873,16,0.5517241379310345,1,0,Shift - Reduce System
61,{ P erson * ?,Shift - Reduce System,Shift - Reduce System,named-entity-recognition,7,"['O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O']",12,0.1935483870967742,60,0.379746835443038,17,0.5862068965517241,1,0,Shift - Reduce System
62,"A , B} , C} where P erson * is a temporary label for P erson .",Shift - Reduce System,Shift - Reduce System,named-entity-recognition,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.2096774193548387,61,0.3860759493670886,18,0.6206896551724138,1,0,Shift - Reduce System
63,"Hence , the X in reduce - actions will also include such temporary labels .",Shift - Reduce System,Shift - Reduce System,named-entity-recognition,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.2258064516129032,62,0.3924050632911392,19,0.6551724137931034,1,0,Shift - Reduce System
64,"Note that since most words are not contained in any mention , they are only shifted to the stack and not involved in any reduce - or unary - actions .",Shift - Reduce System,Shift - Reduce System,named-entity-recognition,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.2419354838709677,63,0.3987341772151899,20,0.6896551724137931,1,0,Shift - Reduce System
65,An example sequence of transitions can be found in .,Shift - Reduce System,Shift - Reduce System,named-entity-recognition,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.2580645161290322,64,0.4050632911392405,21,0.7241379310344828,1,0,Shift - Reduce System
68,"Instead , the final stack should be a forest consisting of multiple nested elements with tree structures .",Shift - Reduce System,Shift - Reduce System,named-entity-recognition,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.3064516129032258,67,0.4240506329113924,24,0.8275862068965517,1,0,Shift - Reduce System
69,"2 ) To conveniently determine the ending of our transition process , we add an auxiliary symbol $ to each sentence .",Shift - Reduce System,Shift - Reduce System,named-entity-recognition,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.3225806451612903,68,0.430379746835443,25,0.8620689655172413,1,0,Shift - Reduce System
70,"Once it is pushed to the stack , it implies that all deductions of actual words are finished .",Shift - Reduce System,Shift - Reduce System,named-entity-recognition,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.3387096774193548,69,0.4367088607594936,26,0.896551724137931,1,0,Shift - Reduce System
71,Since we do not allow unary rules between labels like X1 ?,Shift - Reduce System,Shift - Reduce System,named-entity-recognition,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.3548387096774194,70,0.4430379746835443,27,0.9310344827586208,1,0,Shift - Reduce System
72,"X2 , the length of maximal action sequence is 3 n .",Shift - Reduce System,Shift - Reduce System,named-entity-recognition,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",23,0.3709677419354839,71,0.4493670886075949,28,0.9655172413793104,1,0,Shift - Reduce System
73,5,Shift - Reduce System,Shift - Reduce System,named-entity-recognition,7,['O'],['O'],24,0.3870967741935484,72,0.4556962025316455,29,1.0,1,0,Shift - Reduce System
74,Action Constraints,Shift - Reduce System,,named-entity-recognition,7,"['O', 'O']","['O', 'O']",25,0.4032258064516129,73,0.4620253164556962,0,0.0,1,0,Shift - Reduce System
75,"To make sure that each action sequence is valid , we need to make some hard constraints on the ac - 5",Shift - Reduce System,Action Constraints,named-entity-recognition,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.4193548387096774,74,0.4683544303797468,1,0.0833333333333333,1,0,Shift - Reduce System: Action Constraints
76,"In this case , each word is shifted ( n ) and involved in a unary action ( n ) .",Shift - Reduce System,Action Constraints,named-entity-recognition,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.4354838709677419,75,0.4746835443037974,2,0.1666666666666666,1,0,Shift - Reduce System: Action Constraints
77,Then all elements are reduced to a single node ( n ? 1 ) .,Shift - Reduce System,Action Constraints,named-entity-recognition,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",28,0.4516129032258064,76,0.4810126582278481,3,0.25,1,0,Shift - Reduce System: Action Constraints
78,The last action is to shift the symbol $. tion to take .,Shift - Reduce System,Action Constraints,named-entity-recognition,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",29,0.4677419354838709,77,0.4873417721518987,4,0.3333333333333333,1,0,Shift - Reduce System: Action Constraints
79,"For example , reduce - action can only be conducted when there are at least two elements in the stack .",Shift - Reduce System,Action Constraints,named-entity-recognition,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",30,0.4838709677419355,78,0.4936708860759494,5,0.4166666666666667,1,0,Shift - Reduce System: Action Constraints
81,"Formally , we use V(S , i , A ) to denote the valid actions given the parser state .",Shift - Reduce System,Action Constraints,named-entity-recognition,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",32,0.5161290322580645,80,0.5063291139240507,7,0.5833333333333334,1,0,Shift - Reduce System: Action Constraints
82,Let us denote the feature vector for the parser state at time step k asp k .,Shift - Reduce System,Action Constraints,named-entity-recognition,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",33,0.532258064516129,81,0.5126582278481012,8,0.6666666666666666,1,0,Shift - Reduce System: Action Constraints
83,The distribution of actions is computed as follows :,Shift - Reduce System,Action Constraints,named-entity-recognition,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",34,0.5483870967741935,82,0.5189873417721519,9,0.75,1,0,Shift - Reduce System: Action Constraints
84,"( 1 ) where w z is a column weight vector for action z , and b z is a bias term .",Shift - Reduce System,Action Constraints,named-entity-recognition,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",35,0.5645161290322581,83,0.5253164556962026,10,0.8333333333333334,1,0,Shift - Reduce System: Action Constraints
85,Neural Transition - based Model,Shift - Reduce System,Action Constraints,named-entity-recognition,7,"['O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O']",36,0.5806451612903226,84,0.5316455696202531,11,0.9166666666666666,1,0,Shift - Reduce System: Action Constraints
87,Representation of Words,Shift - Reduce System,,named-entity-recognition,7,"['O', 'O', 'O']","['O', 'O', 'O']",38,0.6129032258064516,86,0.5443037974683544,0,0.0,1,0,Shift - Reduce System
89,where e w i and e pi denote the embeddings for i - th word and it s POS tag respectively .,Shift - Reduce System,Representation of Words,named-entity-recognition,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",40,0.6451612903225806,88,0.5569620253164557,2,0.5,1,0,Shift - Reduce System: Representation of Words
90,cw i denotes the representation learned by a character - level model using a bidirectional LSTM .,Shift - Reduce System,Representation of Words,named-entity-recognition,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",41,0.6612903225806451,89,0.5632911392405063,3,0.75,1,0,Shift - Reduce System: Representation of Words
91,"Specifically , for character sequence s 0 , s 1 , . . . , s n in the i - th word , we use the last hidden states of forward and backward LSTM as the character - based representation of this word , as shown below :",Shift - Reduce System,Representation of Words,named-entity-recognition,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",42,0.6774193548387096,90,0.569620253164557,4,1.0,1,0,Shift - Reduce System: Representation of Words
95,"Formally , if the unprocessed word sequence in the buffer is x i , x i +1 , . . . , x n and action history sequence is a 0 , a 1 , . . . , a k?1 , then we can compute buffer representation bk and action history representation a k at time step k as follows :",Shift - Reduce System,Representation of Parser States,named-entity-recognition,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",46,0.7419354838709677,94,0.5949367088607594,3,0.125,1,0,Shift - Reduce System: Representation of Parser States
96,where each action is also mapped to a distributed representation ea .,Shift - Reduce System,Representation of Parser States,named-entity-recognition,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",47,0.7580645161290323,95,0.6012658227848101,4,0.1666666666666666,1,0,Shift - Reduce System: Representation of Parser States
98,"However , the top elements of the stack are updated frequently .",Shift - Reduce System,Representation of Parser States,named-entity-recognition,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",49,0.7903225806451613,97,0.6139240506329114,6,0.25,1,0,Shift - Reduce System: Representation of Parser States
100,7,Shift - Reduce System,Representation of Parser States,named-entity-recognition,7,['O'],['O'],51,0.8225806451612904,99,0.6265822784810127,8,0.3333333333333333,1,0,Shift - Reduce System: Representation of Parser States
101,"Formally , the state of the stack bk at time step k is computed as :",Shift - Reduce System,Representation of Parser States,named-entity-recognition,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",52,0.8387096774193549,100,0.6329113924050633,9,0.375,1,0,Shift - Reduce System: Representation of Parser States
102,"where ht i denotes the representation of the i - th tree element from the top , which can be computed recursively similar to Recursive Neural Network as follows :",Shift - Reduce System,Representation of Parser States,named-entity-recognition,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",53,0.8548387096774194,101,0.6392405063291139,10,0.4166666666666667,1,0,Shift - Reduce System: Representation of Parser States
103,"where W u , l and W b , l denote the weight matrices for unary ( u ) and binary ( b ) composition with parent node being label ( l ) .",Shift - Reduce System,Representation of Parser States,named-entity-recognition,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",54,0.8709677419354839,102,0.6455696202531646,11,0.4583333333333333,1,0,Shift - Reduce System: Representation of Parser States
104,Note that the composition function is distinct for each label l .,Shift - Reduce System,Representation of Parser States,named-entity-recognition,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",55,0.8870967741935484,103,0.6518987341772152,12,0.5,1,0,Shift - Reduce System: Representation of Parser States
105,Recall that the leaf nodes of each tree element are raw words .,Shift - Reduce System,Representation of Parser States,named-entity-recognition,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",56,0.9032258064516128,104,0.6582278481012658,13,0.5416666666666666,1,0,Shift - Reduce System: Representation of Parser States
106,"Instead of representing them with their original embeddings introduced in Section 3.3 , we found that 6 Note that LSTM b runs in a right - to - left order such that the output can represent the contextual information of x i.",Shift - Reduce System,Representation of Parser States,named-entity-recognition,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",57,0.9193548387096774,105,0.6645569620253164,14,0.5833333333333334,1,0,Shift - Reduce System: Representation of Parser States
107,7,Shift - Reduce System,Representation of Parser States,named-entity-recognition,7,['O'],['O'],58,0.935483870967742,106,0.6708860759493671,15,0.625,1,0,Shift - Reduce System: Representation of Parser States
108,Please refer to for details .,Shift - Reduce System,Representation of Parser States,named-entity-recognition,7,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O']",59,0.9516129032258064,107,0.6772151898734177,16,0.6666666666666666,1,0,Shift - Reduce System: Representation of Parser States
109,concatenating the buffer state in ( 5 ) are beneficial during our initial experiments .,Shift - Reduce System,Representation of Parser States,named-entity-recognition,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",60,0.967741935483871,108,0.6835443037974683,17,0.7083333333333334,1,0,Shift - Reduce System: Representation of Parser States
110,"Formally , when a word xi is shifted to the stack at time step k , it s representation is computed as :",Shift - Reduce System,Representation of Parser States,named-entity-recognition,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",61,0.9838709677419356,109,0.689873417721519,18,0.75,1,0,Shift - Reduce System: Representation of Parser States
111,"Finally , the state of the system pk is the concatenation of the states of buffer , stack and action history :",Shift - Reduce System,Representation of Parser States,named-entity-recognition,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",62,1.0,110,0.6962025316455697,19,0.7916666666666666,1,0,Shift - Reduce System: Representation of Parser States
112,Training,,,named-entity-recognition,7,['O'],['O'],0,0.0,111,0.7025316455696202,20,0.8333333333333334,1,0,
114,"Specifically , let z ik denote the k - th action for the i - th sentence , the loss function with 2 norm is :",Training,Training,named-entity-recognition,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.5,113,0.7151898734177216,22,0.9166666666666666,1,0,Training
115,where ?,Training,Training,named-entity-recognition,7,"['O', 'O']","['O', 'O']",3,0.75,114,0.7215189873417721,23,0.9583333333333334,1,0,Training
116,is the 2 coefficient .,Training,Training,named-entity-recognition,7,"['O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O']",4,1.0,115,0.7278481012658228,24,1.0,1,0,Training
117,Experiments,,,named-entity-recognition,7,['O'],['O'],0,0.0,116,0.7341772151898734,0,0.0,1,0,
119,"In ACE datasets , more than 40 % of the mentions form nested structures with some other mention .",Experiments,Experiments,named-entity-recognition,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.1538461538461538,118,0.7468354430379747,2,0.5,1,0,Experiments
120,"In GENIA , this number is 18 % .",Experiments,Experiments,named-entity-recognition,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.2307692307692307,119,0.7531645569620253,3,0.75,1,0,Experiments
122,Setup,Experiments,,named-entity-recognition,7,['O'],['O'],5,0.3846153846153846,121,0.7658227848101266,0,0.0,1,0,Experiments
129,The 2 coefficient ?,Experiments,Setup,named-entity-recognition,7,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",12,0.9230769230769232,128,0.810126582278481,7,0.875,1,0,Experiments: Setup
130,is also tuned during development process .,Experiments,Setup,named-entity-recognition,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O']",13,1.0,129,0.8164556962025317,8,1.0,1,0,Experiments: Setup
131,Results,,,named-entity-recognition,7,['O'],['O'],0,0.0,130,0.8227848101265823,0,0.0,1,0,
137,The idea is that we split the test data into two portions : sentences with and without nested mentions .,Results,Handling Nested Mentions,named-entity-recognition,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.4,136,0.8607594936708861,1,0.2,1,0,Results: Handling Nested Mentions
138,The results of GENIA are listed in .,Results,Handling Nested Mentions,named-entity-recognition,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.4666666666666667,137,0.8670886075949367,2,0.4,1,0,Results: Handling Nested Mentions
142,Decoding Speed,Results,,named-entity-recognition,7,"['O', 'O']","['O', 'O']",11,0.7333333333333333,141,0.8924050632911392,0,0.0,1,0,Results
143,"Note that and also feature linear - time complexity , but with a greater constant factor .",Results,Decoding Speed,named-entity-recognition,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.8,142,0.8987341772151899,1,0.25,1,0,Results: Decoding Speed
146,We also additionally tried using embeddings trained on PubMed for GENIA but the performance was comparable .,Results,Decoding Speed,named-entity-recognition,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,1.0,145,0.9177215189873418,4,1.0,1,0,Results: Decoding Speed
147,Ablation Study,,,named-entity-recognition,7,"['O', 'O']","['O', 'O']",0,0.0,146,0.9240506329113924,0,0.0,1,0,
149,The results are listed in .,Ablation Study,Ablation Study,named-entity-recognition,7,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O']",2,0.6666666666666666,148,0.9367088607594936,2,0.6666666666666666,1,0,Ablation Study
151,Conclusion and Future Work,,,named-entity-recognition,7,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",0,0.0,150,0.9493670886075948,0,0.0,1,0,
3,abstract,,,named-entity-recognition,8,['O'],['O'],0,0.0,2,0.0051679586563307,0,0.0,1,0,
7,BERT is conceptually simple and empirically powerful .,abstract,abstract,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.4444444444444444,6,0.0155038759689922,4,0.4444444444444444,1,0,abstract
16,There are two existing strategies for applying pre-trained language representations to downstream tasks : feature - based and fine - tuning .,Introduction,Introduction,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.125,15,0.0387596899224806,3,0.125,1,0,Introduction
21,"The major limitation is that standard language models are unidirectional , and this limits the choice of architectures that can be used during pre-training .",Introduction,Introduction,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.3333333333333333,20,0.0516795865633074,8,0.3333333333333333,1,0,Introduction
22,"For example , in Open AI GPT , the authors use a left - toright architecture , where every token can only attend to previous tokens in the self - attention layers of the Transformer .",Introduction,Introduction,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.375,21,0.0542635658914728,9,0.375,1,0,Introduction
23,"Such restrictions are sub-optimal for sentence - level tasks , and could be very harmful when applying finetuning based approaches to token - level tasks such as question answering , where it is crucial to incorporate context from both directions .",Introduction,Introduction,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.4166666666666667,22,0.0568475452196382,10,0.4166666666666667,1,0,Introduction
29,The contributions of our paper are as follows :,Introduction,Introduction,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.6666666666666666,28,0.0723514211886304,16,0.6666666666666666,1,0,Introduction
32,"This is also in contrast to , which uses a shallow concatenation of independently trained left - to - right and right - to - left LMs .",Introduction,Introduction,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.7916666666666666,31,0.0801033591731266,19,0.7916666666666666,1,0,Introduction
37,google-research/bert .,Introduction,Introduction,named-entity-recognition,8,"['O', 'O']","['O', 'O']",24,1.0,36,0.0930232558139534,24,1.0,1,0,Introduction
38,Related Work,,,named-entity-recognition,8,"['O', 'O']","['O', 'O']",0,0.0,37,0.0956072351421188,0,0.0,1,0,
54,"As with the feature - based approaches , the first works in this direction only pre-trained word embedding parameters from unlabeled text .",Unsupervised Fine- tuning Approaches,Unsupervised Fine- tuning Approaches,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.04,53,0.1369509043927648,16,0.5714285714285714,1,0,Unsupervised Fine- tuning Approaches
55,"More recently , sentence or document encoders which produce contextual token representations have been pre-trained from unlabeled text and fine - tuned fora supervised downstream task .",Unsupervised Fine- tuning Approaches,Unsupervised Fine- tuning Approaches,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.08,54,0.1395348837209302,17,0.6071428571428571,1,0,Unsupervised Fine- tuning Approaches
56,The advantage of these approaches is that few parameters need to be learned from scratch .,Unsupervised Fine- tuning Approaches,Unsupervised Fine- tuning Approaches,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.12,55,0.1421188630490956,18,0.6428571428571429,1,0,Unsupervised Fine- tuning Approaches
58,Left - to - right language model - BERT BERT E E 1 E ...,Unsupervised Fine- tuning Approaches,Unsupervised Fine- tuning Approaches,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.2,57,0.1472868217054263,20,0.7142857142857143,1,0,Unsupervised Fine- tuning Approaches
61,Overall pre-training and fine - tuning procedures for BERT .,Unsupervised Fine- tuning Approaches,Unsupervised Fine- tuning Approaches,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.32,60,0.1550387596899224,23,0.8214285714285714,1,0,Unsupervised Fine- tuning Approaches
65,"[ CLS ] is a special symbol added in front of every input example , and [ SEP ] is a special separator token ( e.g. separating questions / answers ) .",Unsupervised Fine- tuning Approaches,Unsupervised Fine- tuning Approaches,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.48,64,0.165374677002584,27,0.9642857142857144,1,0,Unsupervised Fine- tuning Approaches
68,"There has also been work showing effective transfer from supervised tasks with large datasets , such as natural language inference and machine translation .",Unsupervised Fine- tuning Approaches,Transfer Learning from Supervised Data,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.6,67,0.1731266149870801,1,0.5,1,0,Unsupervised Fine- tuning Approaches: Transfer Learning from Supervised Data
69,"Computer vision research has also demonstrated the importance of transfer learning from large pre-trained models , where an effective recipe is to fine - tune models pre-trained with I ma - geNet .",Unsupervised Fine- tuning Approaches,Transfer Learning from Supervised Data,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.64,68,0.1757105943152454,2,1.0,1,0,Unsupervised Fine- tuning Approaches: Transfer Learning from Supervised Data
70,BERT,Unsupervised Fine- tuning Approaches,,named-entity-recognition,8,['O'],['O'],17,0.68,69,0.1782945736434108,0,0.0,1,0,Unsupervised Fine- tuning Approaches
77,A distinctive feature of BERT is its unified architecture across different tasks .,Unsupervised Fine- tuning Approaches,BERT,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.96,76,0.1963824289405684,7,0.0853658536585365,1,0,Unsupervised Fine- tuning Approaches: BERT
78,There is mini-mal difference between the pre-trained architecture and the final downstream architecture .,Unsupervised Fine- tuning Approaches,BERT,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,1.0,77,0.1989664082687338,8,0.0975609756097561,1,0,Unsupervised Fine- tuning Approaches: BERT
79,Model Architecture,,,named-entity-recognition,8,"['O', 'O']","['O', 'O']",0,0.0,78,0.2015503875968992,9,0.1097560975609756,1,0,
80,BERT 's model architecture is a multi - layer bidirectional Transformer encoder based on the original implementation and released in the tensor2 tensor library .,Model Architecture,Model Architecture,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0136986301369863,79,0.2041343669250646,10,0.1219512195121951,1,0,Model Architecture
81,1,Model Architecture,Model Architecture,named-entity-recognition,8,['O'],['O'],2,0.0273972602739726,80,0.2067183462532299,11,0.1341463414634146,1,0,Model Architecture
83,2,Model Architecture,Model Architecture,named-entity-recognition,8,['O'],['O'],4,0.0547945205479452,82,0.2118863049095607,13,0.1585365853658536,1,0,Model Architecture
84,"In this work , we denote the number of layers ( i.e. , Transformer blocks ) as L , the hidden size as H , and the number of self - attention heads as A .",Model Architecture,Model Architecture,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.0684931506849315,83,0.2144702842377261,14,0.1707317073170731,1,0,Model Architecture
86,BERT BASE was chosen to have the same model size as OpenAI GPT for comparison purposes .,Model Architecture,Model Architecture,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.0958904109589041,85,0.2196382428940568,16,0.1951219512195122,1,0,Model Architecture
87,"Critically , however , the BERT Transformer uses bidirectional self - attention , while the GPT Transformer uses constrained self - attention where every token can only attend to context to its left .",Model Architecture,Model Architecture,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.1095890410958904,86,0.2222222222222222,17,0.2073170731707317,1,0,Model Architecture
88,4,Model Architecture,Model Architecture,named-entity-recognition,8,['O'],['O'],9,0.1232876712328767,87,0.2248062015503876,18,0.2195121951219512,1,0,Model Architecture
89,Input / Output Representations,Model Architecture,,named-entity-recognition,8,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",10,0.136986301369863,88,0.2273901808785529,19,0.2317073170731707,1,0,Model Architecture
91,"Throughout this work , a "" sentence "" can bean arbitrary span of contiguous text , rather than an actual linguistic sentence .",Model Architecture,Input / Output Representations,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.1643835616438356,90,0.2325581395348837,21,0.2560975609756097,1,0,Model Architecture: Input / Output Representations
92,"A "" sequence "" refers to the input token sequence to BERT , which maybe a single sentence or two sentences packed together .",Model Architecture,Input / Output Representations,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.1780821917808219,91,0.2351421188630491,22,0.2682926829268293,1,0,Model Architecture: Input / Output Representations
94,"( Wu et al. , 2016 ) with a 30,000 token vocabulary .",Model Architecture,Input / Output Representations,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.2054794520547945,93,0.2403100775193798,24,0.2926829268292683,1,0,Model Architecture: Input / Output Representations
95,The first token of every sequence is always a special classification token ( [ CLS ] ) .,Model Architecture,Input / Output Representations,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.2191780821917808,94,0.2428940568475452,25,0.3048780487804878,1,0,Model Architecture: Input / Output Representations
101,"As shown in , we denote input embedding as E , the final hidden vector of the special [ CLS ] token as C ? R H , and the final hidden vector for the i th input token as",Model Architecture,Input / Output Representations,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.3013698630136986,100,0.2583979328165374,31,0.3780487804878049,1,0,Model Architecture: Input / Output Representations
103,"A visualization of this construction can be seen in . ( 2018 ) , we do not use traditional left - to - right or right - to - left language models to pre-train BERT .",Model Architecture,Input / Output Representations,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.3287671232876712,102,0.2635658914728682,33,0.4024390243902439,1,0,Model Architecture: Input / Output Representations
105,This step is presented in the left part of .,Model Architecture,Input / Output Representations,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.3561643835616438,104,0.268733850129199,35,0.4268292682926829,1,0,Model Architecture: Input / Output Representations
107,"Intuitively , it is reasonable to believe that a deep bidirectional model is strictly more powerful than either a left - to - right model or the shallow concatenation of a left - toright and a right - to - left model .",Model Architecture,Input / Output Representations,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",28,0.3835616438356164,106,0.2739018087855297,37,0.4512195121951219,1,0,Model Architecture: Input / Output Representations
108,"Unfortunately , standard conditional language models can only be trained left - to - right or right - to - left , since bidirectional conditioning would allow each word to indirectly "" see itself "" , and the model could trivially predict the target word in a multi - layered context .",Model Architecture,Input / Output Representations,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",29,0.3972602739726027,107,0.276485788113695,38,0.4634146341463415,1,0,Model Architecture: Input / Output Representations
109,"former is often referred to as a "" Transformer encoder "" while the left - context - only version is referred to as a "" Transformer decoder "" since it can be used for text generation .",Model Architecture,Input / Output Representations,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",30,0.410958904109589,108,0.2790697674418604,39,0.4756097560975609,1,0,Model Architecture: Input / Output Representations
111,"We refer to this procedure as a "" masked LM "" ( MLM ) , although it is often referred to as a Cloze task in the literature .",Model Architecture,Input / Output Representations,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",32,0.4383561643835616,110,0.2842377260981912,41,0.5,1,0,Model Architecture: Input / Output Representations
112,"In this case , the final hidden vectors corresponding to the mask tokens are fed into an output softmax over the vocabulary , as in a standard LM .",Model Architecture,Input / Output Representations,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",33,0.4520547945205479,111,0.2868217054263566,42,0.5121951219512195,1,0,Model Architecture: Input / Output Representations
115,"Although this allows us to obtain a bidirectional pre-trained model , a downside is that we are creating a mismatch between pre-training and fine - tuning , since the [ MASK ] token does not appear during fine - tuning .",Model Architecture,Input / Output Representations,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",36,0.4931506849315068,114,0.2945736434108527,45,0.5487804878048781,1,0,Model Architecture: Input / Output Representations
116,"To mitigate this , we do not always replace "" masked "" words with the actual [ MASK ] token .",Model Architecture,Input / Output Representations,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",37,0.5068493150684932,115,0.2971576227390181,46,0.5609756097560976,1,0,Model Architecture: Input / Output Representations
118,"If the i - th token is chosen , we replace the i - th token with ( 1 ) the [ MASK ] token 80 % of the time ( 2 ) a random token 10 % of the time ( 3 ) the unchanged i - th token 10 % of the time .",Model Architecture,Input / Output Representations,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",39,0.5342465753424658,117,0.3023255813953488,48,0.5853658536585366,1,0,Model Architecture: Input / Output Representations
119,"Then ,",Model Architecture,,named-entity-recognition,8,"['O', 'O']","['O', 'O']",40,0.547945205479452,118,0.3049095607235142,49,0.5975609756097561,1,0,Model Architecture
120,Ti will be used to predict the original token with cross entropy loss .,Model Architecture,"Then ,",named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",41,0.5616438356164384,119,0.3074935400516795,50,0.6097560975609756,1,0,"Model Architecture: Then ,"
124,"Specifically , when choosing the sentences A and B for each pretraining example , 50 % of the time B is the actual next sentence that follows A ( labeled as IsNext ) , and 50 % of the time it is a random sentence from the corpus ( labeled as NotNext ) .",Model Architecture,"Then ,",named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",45,0.6164383561643836,123,0.3178294573643411,54,0.6585365853658537,1,0,"Model Architecture: Then ,"
125,"As we show in , C is used for next sentence prediction ( NSP ) .",Model Architecture,"Then ,",named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",46,0.6301369863013698,124,0.3204134366925064,55,0.6707317073170732,1,0,"Model Architecture: Then ,"
127,6,Model Architecture,"Then ,",named-entity-recognition,8,['O'],['O'],48,0.6575342465753424,126,0.3255813953488372,57,0.6951219512195121,1,0,"Model Architecture: Then ,"
129,"The vector C is not a meaningful sentence representation without fine - tuning , since it was trained with NSP .",Model Architecture,"Then ,",named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",50,0.684931506849315,128,0.330749354005168,59,0.7195121951219512,1,0,"Model Architecture: Then ,"
130,he likes play ##ing my dog is cute Input,Model Architecture,"Then ,",named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",51,0.6986301369863014,129,0.3333333333333333,60,0.7317073170731707,1,0,"Model Architecture: Then ,"
131,Position,Model Architecture,,named-entity-recognition,8,['O'],['O'],52,0.7123287671232876,130,0.3359173126614987,61,0.7439024390243902,1,0,Model Architecture
134,The NSP task is closely related to representationlearning objectives used in Jernite et al. and Logeswaran and Lee ( 2018 ) .,Model Architecture,Position,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",55,0.7534246575342466,133,0.3436692506459948,64,0.7804878048780488,1,0,Model Architecture: Position
135,"However , in prior work , only sentence embeddings are transferred to down - stream tasks , where BERT transfers all parameters to initialize end - task model parameters .",Model Architecture,Position,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",56,0.7671232876712328,134,0.3462532299741602,65,0.7926829268292683,1,0,Model Architecture: Position
136,Pre-training data,Model Architecture,Position,named-entity-recognition,8,"['O', 'O']","['O', 'O']",57,0.7808219178082192,135,0.3488372093023256,66,0.8048780487804879,1,0,Model Architecture: Position
140,It is critical to use a document - level corpus rather than a shuffled sentence - level corpus such as the Billion Word Benchmark in order to extract long contiguous sequences .,Model Architecture,Position,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",61,0.8356164383561644,139,0.359173126614987,70,0.8536585365853658,1,0,Model Architecture: Position
142,Fine- tuning is straightforward since the selfattention mechanism in the Transformer allows BERT to model many downstream tasks whether they involve single text or text pairs - by swapping out the appropriate inputs and outputs .,Model Architecture,Position,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",63,0.863013698630137,141,0.3643410852713178,72,0.8780487804878049,1,0,Model Architecture: Position
143,"For applications involving text pairs , a common pattern is to independently encode text pairs before applying bidirectional cross attention , such as Parikh et al. ; Seo et al. ( 2017 ) .",Model Architecture,Position,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",64,0.8767123287671232,142,0.3669250645994832,73,0.8902439024390244,1,0,Model Architecture: Position
146,"At the input , sentence A and sentence B from pre-training are analogous to ( 1 ) sentence pairs in paraphrasing , ( 2 ) hypothesis - premise pairs in entailment , ( 3 ) question - passage pairs in question answering , and ( 4 ) a degenerate text -?",Model Architecture,Position,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",67,0.9178082191780822,145,0.3746770025839793,76,0.926829268292683,1,0,Model Architecture: Position
153,Experiments,,,named-entity-recognition,8,['O'],['O'],0,0.0,152,0.3927648578811369,0,0.0,1,0,
155,GLUE,Experiments,,named-entity-recognition,8,['O'],['O'],2,0.0285714285714285,154,0.3979328165374677,0,0.0,1,0,Experiments
159,R H corresponding to the first input token ( [ CLS ] ) as the aggregate representation .,Experiments,GLUE,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.0857142857142857,158,0.4082687338501292,4,0.5714285714285714,1,0,Experiments: GLUE
160,The only new parameters introduced during fine - tuning are classification layer weights W ?,Experiments,GLUE,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.1,159,0.4108527131782946,5,0.7142857142857143,1,0,Experiments: GLUE
161,"R KH , where K is the number of labels .",Experiments,GLUE,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.1142857142857142,160,0.4134366925064599,6,0.8571428571428571,1,0,Experiments: GLUE
162,"We compute a standard classification loss with C and W , i.e. , log ( softmax ( CW T ) ) . :",Experiments,GLUE,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.1285714285714285,161,0.4160206718346253,7,1.0,1,0,Experiments: GLUE
163,GLUE,Experiments,,named-entity-recognition,8,['O'],['O'],10,0.1428571428571428,162,0.4186046511627907,0,0.0,1,0,Experiments
165,The number below each task denotes the number of training examples .,Experiments,GLUE,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.1714285714285714,164,0.4237726098191214,2,0.0384615384615384,1,0,Experiments: GLUE
166,"The "" Average "" column is slightly different than the official GLUE score , since we exclude the problematic WNLI set .",Experiments,GLUE,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.1857142857142857,165,0.4263565891472868,3,0.0576923076923076,1,0,Experiments: GLUE
167,"8 BERT and OpenAI GPT are singlemodel , single task .",Experiments,GLUE,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.2,166,0.4289405684754522,4,0.0769230769230769,1,0,Experiments: GLUE
169,We exclude entries that use BERT as one of their components .,Experiments,GLUE,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.2285714285714285,168,0.4341085271317829,6,0.1153846153846153,1,0,Experiments: GLUE
172,"Additionally , for BERT LARGE we found that finetuning was sometimes unstable on small datasets , so we ran several random restarts and selected the best model on the Dev set .",Experiments,GLUE,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.2714285714285714,171,0.4418604651162791,9,0.173076923076923,1,0,Experiments: GLUE
174,Results are presented in .,Experiments,GLUE,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O']",21,0.3,173,0.4470284237726098,11,0.2115384615384615,1,0,Experiments: GLUE
176,Note that BERT BASE and OpenAI GPT are nearly identical in terms of model architecture apart from the attention masking .,Experiments,GLUE,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",23,0.3285714285714285,175,0.4521963824289405,13,0.25,1,0,Experiments: GLUE
181,SQuAD v 1.1,Experiments,GLUE,named-entity-recognition,8,"['O', 'O', 'O']","['O', 'O', 'O']",28,0.4,180,0.4651162790697674,18,0.3461538461538461,1,0,Experiments: GLUE
183,"Given a question and a passage from The GLUE data set distribution does not include the Test labels , and we only made a single GLUE evaluation server submission for each of BERTBASE and BERTLARGE .",Experiments,GLUE,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",30,0.4285714285714285,182,0.4702842377260982,20,0.3846153846153846,1,0,Experiments: GLUE
187,We only introduce a start vector S ?,Experiments,GLUE,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",34,0.4857142857142857,186,0.4806201550387597,24,0.4615384615384615,1,0,Experiments: GLUE
189,R H during fine - tuning .,Experiments,GLUE,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O']",36,0.5142857142857142,188,0.4857881136950904,26,0.5,1,0,Experiments: GLUE
190,The probability of word i being the start of the answer span is computed as a dot product between Ti and S followed by a softmax overall of the words in the paragraph : P i = e ST i j e ST j .,Experiments,GLUE,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",37,0.5285714285714286,189,0.4883720930232558,27,0.5192307692307693,1,0,Experiments: GLUE
191,The analogous formula is used for the end of the answer span .,Experiments,GLUE,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",38,0.5428571428571428,190,0.4909560723514212,28,0.5384615384615384,1,0,Experiments: GLUE
192,"The score of a candidate span from position i to position j is defined as ST i + ET j , and the maximum scoring span where j ?",Experiments,GLUE,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",39,0.5571428571428572,191,0.4935400516795866,29,0.5576923076923077,1,0,Experiments: GLUE
193,i is used as a prediction .,Experiments,GLUE,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O']",40,0.5714285714285714,192,0.4961240310077519,30,0.5769230769230769,1,0,Experiments: GLUE
197,"The top results from the SQuAD leaderboard do not have up - to - date public system descriptions available , 11 and are allowed to use any public data when training their systems .",Experiments,GLUE,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",44,0.6285714285714286,196,0.5064599483204134,34,0.6538461538461539,1,0,Experiments: GLUE
202,12,Experiments,GLUE,named-entity-recognition,8,['O'],['O'],49,0.7,201,0.5193798449612403,39,0.75,1,0,Experiments: GLUE
203,SQuAD v 2.0,Experiments,GLUE,named-entity-recognition,8,"['O', 'O', 'O']","['O', 'O', 'O']",50,0.7142857142857143,202,0.5219638242894057,40,0.7692307692307693,1,0,Experiments: GLUE
206,We treat questions that do not have an answer as having an answer span with start and end at the [ CLS ] token .,Experiments,GLUE,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",53,0.7571428571428571,205,0.5297157622739018,43,0.8269230769230769,1,0,Experiments: GLUE
208,"For prediction , we compare the score of the no -answer span : s null = SC + EC to the score of the best non - null span The Trivia QA data we used consists of paragraphs from TriviaQA - Wiki formed of the first 400 tokens in documents , that contain at least one of the provided possible answers .",Experiments,GLUE,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",55,0.7857142857142857,207,0.5348837209302325,45,0.8653846153846154,1,0,Experiments: GLUE
209,We predict a non-null answer when ?,Experiments,GLUE,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O']",56,0.8,208,0.537467700258398,46,0.8846153846153846,1,0,Experiments: GLUE
210,"i , j > s null + ? , where the threshold ?",Experiments,GLUE,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",57,0.8142857142857143,209,0.5400516795865633,47,0.903846153846154,1,0,Experiments: GLUE
211,is selected on the dev set to maximize F 1 .,Experiments,GLUE,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",58,0.8285714285714286,210,0.5426356589147286,48,0.9230769230769232,1,0,Experiments: GLUE
212,We did not use Trivia QA data for this model .,Experiments,GLUE,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",59,0.8428571428571429,211,0.5452196382428941,49,0.9423076923076924,1,0,Experiments: GLUE
216,SWAG,Experiments,,named-entity-recognition,8,['O'],['O'],63,0.9,215,0.5555555555555556,0,0.0,1,0,Experiments
220,The only task - specific parameters introduced is a vector whose dot product with the [ CLS ] token representation C denotes a score for each choice which is normalized with a softmax layer .,Experiments,SWAG,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",67,0.9571428571428572,219,0.5658914728682171,4,0.5714285714285714,1,0,Experiments: SWAG
222,Results are presented in .,Experiments,SWAG,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O']",69,0.9857142857142858,221,0.5710594315245479,6,0.8571428571428571,1,0,Experiments: SWAG
224,Ablation Studies,,,named-entity-recognition,8,"['O', 'O']","['O', 'O']",0,0.0,223,0.5762273901808785,0,0.0,1,0,
227,""" No NSP "" is trained without the next sentence prediction task .",Ablation Studies,Ablation Studies,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.5,226,0.5839793281653747,3,0.5,1,0,Ablation Studies
228,""" LTR & No NSP "" is trained as a left - to - right LM without the next sentence prediction , like OpenAI GPT .",Ablation Studies,Ablation Studies,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.6666666666666666,227,0.58656330749354,4,0.6666666666666666,1,0,Ablation Studies
230,ablation studies can be found in Appendix C.,Ablation Studies,Ablation Studies,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,1.0,229,0.5917312661498708,6,1.0,1,0,Ablation Studies
231,Effect of Pre-training Tasks,,,named-entity-recognition,8,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",0,0.0,230,0.5943152454780362,0,0.0,1,0,
237,"The left - only constraint was also applied at fine - tuning , because removing it introduced a pre-train / fine - tune mismatch that degraded downstream performance .",Effect of Pre-training Tasks,Effect of Pre-training Tasks,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.3333333333333333,236,0.6098191214470284,6,0.3333333333333333,1,0,Effect of Pre-training Tasks
238,"Additionally , this model was pre-trained without the NSP task .",Effect of Pre-training Tasks,Effect of Pre-training Tasks,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.3888888888888889,237,0.6124031007751938,7,0.3888888888888889,1,0,Effect of Pre-training Tasks
239,"This is directly comparable to OpenAI GPT , but using our larger training dataset , our input representation , and our fine - tuning scheme .",Effect of Pre-training Tasks,Effect of Pre-training Tasks,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.4444444444444444,238,0.6149870801033591,8,0.4444444444444444,1,0,Effect of Pre-training Tasks
244,"For SQuAD it is intuitively clear that a LTR model will perform poorly at token predictions , since the token - level hidden states have no rightside context .",Effect of Pre-training Tasks,Effect of Pre-training Tasks,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.7222222222222222,243,0.627906976744186,13,0.7222222222222222,1,0,Effect of Pre-training Tasks
245,"In order to make a good faith attempt at strengthening the LTR system , we added a randomly initialized BiLSTM on top .",Effect of Pre-training Tasks,Effect of Pre-training Tasks,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.7777777777777778,244,0.6304909560723514,14,0.7777777777777778,1,0,Effect of Pre-training Tasks
248,"We recognize that it would also be possible to train separate LTR and RTL models and represent each token as the concatenation of the two models , as ELMo does .",Effect of Pre-training Tasks,Effect of Pre-training Tasks,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.9444444444444444,247,0.6382428940568475,17,0.9444444444444444,1,0,Effect of Pre-training Tasks
249,"However : ( a ) this is twice as expensive as a single bidirectional model ; ( b ) this is non-intuitive for tasks like QA , since the RTL model would not be able to condition the answer on the question ; ( c ) this it is strictly less powerful than a deep bidirectional model , since it can use both left and right context at every layer .",Effect of Pre-training Tasks,Effect of Pre-training Tasks,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,1.0,248,0.6408268733850129,18,1.0,1,0,Effect of Pre-training Tasks
250,Effect of Model Size,,,named-entity-recognition,8,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",0,0.0,249,0.6434108527131783,0,0.0,1,0,
257,"For example , the largest Transformer explored in is ( L=6 , H = 1024 , A = 16 ) with 100M parameters for the encoder , and the largest Transformer we have found in the literature is ( L=64 , H = 512 , A=2 ) with 235M parameters .",Effect of Model Size,Effect of Model Size,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.2592592592592592,256,0.661498708010336,7,0.2592592592592592,1,0,Effect of Model Size
258,"By contrast , BERT BASE contains 110M parameters and BERT LARGE contains 340M parameters .",Effect of Model Size,Effect of Model Size,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.2962962962962963,257,0.6640826873385013,8,0.2962962962962963,1,0,Effect of Model Size
259,"It has long been known that increasing the model size will lead to continual improvements on large - scale tasks such as machine translation and language modeling , which is demonstrated by the LM perplexity of held - out training data shown in .",Effect of Model Size,Effect of Model Size,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.3333333333333333,258,0.6666666666666666,9,0.3333333333333333,1,0,Effect of Model Size
261,"presented mixed results on the downstream task impact of increasing the pre-trained bi - LM size from two to four layers and mentioned in passing that increasing hidden dimension size from 200 to 600 helped , but increasing further to 1,000 did not bring further improvements .",Effect of Model Size,Effect of Model Size,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.4074074074074074,260,0.6718346253229974,11,0.4074074074074074,1,0,Effect of Model Size
262,"Both of these prior works used a featurebased approach - we hypothesize that when the model is fine - tuned directly on the downstream tasks and uses only a very small number of randomly initialized additional parameters , the taskspecific models can benefit from the larger , more expressive pre-trained representations even when downstream task data is very small .",Effect of Model Size,Effect of Model Size,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.4444444444444444,261,0.6744186046511628,12,0.4444444444444444,1,0,Effect of Model Size
265,"However , the feature - based approach , where fixed features are extracted from the pretrained model , has certain advantages .",Effect of Model Size,Effect of Model Size,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.5555555555555556,264,0.6821705426356589,15,0.5555555555555556,1,0,Effect of Model Size
266,"First , not all tasks can be easily represented by a Transformer encoder architecture , and therefore require a task - specific model architecture to be added .",Effect of Model Size,Effect of Model Size,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.5925925925925926,265,0.6847545219638242,16,0.5925925925925926,1,0,Effect of Model Size
270,"Following standard practice , we formulate this as a tagging task but do not use a CRF layer in the output .",Effect of Model Size,Effect of Model Size,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.7407407407407407,269,0.6950904392764858,20,0.7407407407407407,1,0,Effect of Model Size
274,Results are presented in .,Effect of Model Size,Effect of Model Size,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O']",24,0.8888888888888888,273,0.7054263565891473,24,0.8888888888888888,1,0,Effect of Model Size
278,Conclusion,,,named-entity-recognition,8,['O'],['O'],0,0.0,277,0.7157622739018088,0,0.0,1,0,
363,C.1 Effect of Number of Training Steps presents MNLI,C Additional Ablation Studies,,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.04,362,0.9354005167958656,1,0.0909090909090909,1,0,C Additional Ablation Studies
364,Dev accuracy after finetuning from a checkpoint that has been pre-trained fork steps .,C Additional Ablation Studies,C.1 Effect of Number of Training Steps presents MNLI,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.08,363,0.937984496124031,2,0.1818181818181818,1,0,C Additional Ablation Studies: C.1 Effect of Number of Training Steps presents MNLI
365,This allows us to answer the following questions :,C Additional Ablation Studies,C.1 Effect of Number of Training Steps presents MNLI,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.12,364,0.9405684754521964,3,0.2727272727272727,1,0,C Additional Ablation Studies: C.1 Effect of Number of Training Steps presents MNLI
366,1 .,C Additional Ablation Studies,C.1 Effect of Number of Training Steps presents MNLI,named-entity-recognition,8,"['O', 'O']","['O', 'O']",4,0.16,365,0.9431524547803618,4,0.3636363636363636,1,0,C Additional Ablation Studies: C.1 Effect of Number of Training Steps presents MNLI
368,"Does BERT really need such a large amount of pre-training ( 128,000 words / batch * 1,000,000 steps ) to achieve high fine - tuning accuracy ?",C Additional Ablation Studies,C.1 Effect of Number of Training Steps presents MNLI,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.24,367,0.9483204134366924,6,0.5454545454545454,1,0,C Additional Ablation Studies: C.1 Effect of Number of Training Steps presents MNLI
372,Answer : The MLM model does converge slightly slower than the LTR model .,C Additional Ablation Studies,C.1 Effect of Number of Training Steps presents MNLI,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.4,371,0.958656330749354,10,0.9090909090909092,1,0,C Additional Ablation Studies: C.1 Effect of Number of Training Steps presents MNLI
375,"In Section 3.1 , we mention that BERT uses a mixed strategy for masking the target tokens when pre-training with the masked language model ( MLM ) objective .",C Additional Ablation Studies,C.2 Ablation for Different Masking Procedures,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.52,374,0.96640826873385,1,0.0769230769230769,1,0,C Additional Ablation Studies: C.2 Ablation for Different Masking Procedures
377,"Note that the purpose of the masking strategies is to reduce the mismatch between pre-training and fine - tuning , as the [ MASK ] symbol never appears during the fine - tuning stage .",C Additional Ablation Studies,C.2 Ablation for Different Masking Procedures,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.6,376,0.9715762273901808,3,0.2307692307692307,1,0,C Additional Ablation Studies: C.2 Ablation for Different Masking Procedures
379,"For NER , we report both fine - tuning and feature - based approaches , as we expect the mismatch will be amplified for the feature - based approach as the model will not have the chance to adjust the representations .",C Additional Ablation Studies,C.2 Ablation for Different Masking Procedures,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.68,378,0.9767441860465116,5,0.3846153846153846,1,0,C Additional Ablation Studies: C.2 Ablation for Different Masking Procedures
380,The results are presented in .,C Additional Ablation Studies,C.2 Ablation for Different Masking Procedures,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O']",18,0.72,379,0.979328165374677,6,0.4615384615384615,1,0,C Additional Ablation Studies: C.2 Ablation for Different Masking Procedures
381,"In the table , MASK means that we replace the target token with the [ MASK ] symbol for MLM ; SAME means that we keep the target token as is ; RND means that we replace the target token with another random token .",C Additional Ablation Studies,C.2 Ablation for Different Masking Procedures,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.76,380,0.9819121447028424,7,0.5384615384615384,1,0,C Additional Ablation Studies: C.2 Ablation for Different Masking Procedures
382,"The numbers in the left part of the table represent the probabilities of the specific strategies used during MLM pre-training ( BERT uses 80 % , 10 % , 10 % ) .",C Additional Ablation Studies,C.2 Ablation for Different Masking Procedures,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.8,381,0.9844961240310076,8,0.6153846153846154,1,0,C Additional Ablation Studies: C.2 Ablation for Different Masking Procedures
386,"However , as expected , using only the MASK strategy was problematic when applying the featurebased approach to NER .",C Additional Ablation Studies,C.2 Ablation for Different Masking Procedures,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.96,385,0.9948320413436692,12,0.9230769230769232,1,0,C Additional Ablation Studies: C.2 Ablation for Different Masking Procedures
3,abstract,,,named-entity-recognition,9,['O'],['O'],0,0.0,2,0.0100502512562814,0,0.0,1,0,
4,Motivation :,abstract,abstract,named-entity-recognition,9,"['O', 'O']","['O', 'O']",1,0.0833333333333333,3,0.0150753768844221,1,0.0833333333333333,1,0,abstract
5,Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows .,abstract,abstract,named-entity-recognition,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.1666666666666666,4,0.0201005025125628,2,0.1666666666666666,1,0,abstract
6,"With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models .",abstract,abstract,named-entity-recognition,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.25,5,0.0251256281407035,3,0.25,1,0,abstract
7,"However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora .",abstract,abstract,named-entity-recognition,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.3333333333333333,6,0.0301507537688442,4,0.3333333333333333,1,0,abstract
9,Results :,abstract,abstract,named-entity-recognition,9,"['O', 'O']","['O', 'O']",6,0.5,8,0.0402010050251256,6,0.5,1,0,abstract
17,The volume of biomedical literature continues to rapidly increase .,Introduction,Introduction,named-entity-recognition,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0625,16,0.0804020100502512,1,0.0625,1,0,Introduction
21,"Consequently , there is increasingly more demand for accurate biomedical text mining tools for extracting information from the literature .",Introduction,Introduction,named-entity-recognition,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.3125,20,0.100502512562814,5,0.3125,1,0,Introduction
22,Recent progress of biomedical text mining models was made possible by the advancements of deep learning techniques used in natural language processing ( NLP ) .,Introduction,Introduction,named-entity-recognition,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.375,21,0.1055276381909547,6,0.375,1,0,Introduction
24,Other deep learning based models have made improvements in biomedical text mining tasks such as relation extraction ( RE ) and question answering ( QA ) .,Introduction,Introduction,named-entity-recognition,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.5,23,0.1155778894472361,8,0.5,1,0,Introduction
25,"However , directly applying state - of - the - art NLP methodologies to biomedical text mining has limitations .",Introduction,Introduction,named-entity-recognition,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.5625,24,0.1206030150753768,9,0.5625,1,0,Introduction
26,"First , as recent word representation models such as Word2 Vec , ELMo and BERT are trained and tested mainly on datasets containing general domain texts ( e.g. Wikipedia ) , it is difficult to estimate their performance on datasets containing biomedical texts .",Introduction,Introduction,named-entity-recognition,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.625,25,0.1256281407035175,10,0.625,1,0,Introduction
27,"Also , the word distributions of general and biomedical corpora are quite different , which can often be a problem for biomedical text mining models .",Introduction,Introduction,named-entity-recognition,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.6875,26,0.1306532663316583,11,0.6875,1,0,Introduction
28,"As a result , recent models in biomedical text mining rely largely on adapted versions of word representations .",Introduction,Introduction,named-entity-recognition,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.75,27,0.135678391959799,12,0.75,1,0,Introduction
30,"Previously , Word2 Vec , which is one of the most widely known context independent word representation models , was trained on biomedical corpora which contain terms and expressions that are usually not included in a general domain corpus .",Introduction,Introduction,named-entity-recognition,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.875,29,0.1457286432160804,14,0.875,1,0,Introduction
31,"While ELMo and BERT have proven the effectiveness of contextualized word representations , they can not obtain high performance on biomedical corpora because they are pre-trained on only general domain corpora .",Introduction,Introduction,named-entity-recognition,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.9375,30,0.1507537688442211,15,0.9375,1,0,Introduction
33,Approach,,,named-entity-recognition,9,['O'],['O'],0,0.0,32,0.1608040201005025,0,0.0,1,0,
41,The contributions of our paper are as follows :,Approach,Approach,named-entity-recognition,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.6153846153846154,40,0.2010050251256281,8,0.6153846153846154,1,0,Approach
47,Materials and methods,,,named-entity-recognition,9,"['O', 'O', 'O']","['O', 'O', 'O']",0,0.0,46,0.2311557788944723,0,0.0,1,0,
48,BioBERT basically has the same structure as BERT .,Materials and methods,Materials and methods,named-entity-recognition,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0204081632653061,47,0.236180904522613,1,0.0769230769230769,1,0,Materials and methods
51,Learning word representations from a large amount of unannotated text is a long - established method .,Materials and methods,Materials and methods,named-entity-recognition,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.0816326530612244,50,0.2512562814070351,4,0.3076923076923077,1,0,Materials and methods
52,"While previous models ( e.g. Word2 Vec , GloVe ) focused on learning context independent word representations , recent works have focused on learning context dependent word representations .",Materials and methods,Materials and methods,named-entity-recognition,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.1020408163265306,51,0.2562814070351759,5,0.3846153846153846,1,0,Materials and methods
53,"For instance , ELMo uses a bidirectional language model , while uses machine translation to embed context information into word representations .",Materials and methods,Materials and methods,named-entity-recognition,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.1224489795918367,52,0.2613065326633166,6,0.4615384615384615,1,0,Materials and methods
55,"Due to the nature of language modeling where future words can not be seen , previous language models were limited to a combination of two unidirectional language models ( i.e. left - to - right and right - toleft ) .",Materials and methods,Materials and methods,named-entity-recognition,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.1632653061224489,54,0.271356783919598,8,0.6153846153846154,1,0,Materials and methods
58,"According to the authors of BERT , incorporating information from bidirectional representations , rather than unidirectional representations , is crucial for representing words in natural language .",Materials and methods,Materials and methods,named-entity-recognition,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.2244897959183673,57,0.2864321608040201,11,0.8461538461538461,1,0,Materials and methods
60,"Due to the space limitations , we refer readers to fora more detailed description of BERT .",Materials and methods,Materials and methods,named-entity-recognition,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.2653061224489796,59,0.2964824120603015,13,1.0,1,0,Materials and methods
61,Pre-training BioBERT,Materials and methods,Materials and methods,named-entity-recognition,9,"['O', 'O']","['O', 'O']",14,0.2857142857142857,60,0.3015075376884422,0,0.0,1,0,Materials and methods
63,"However , biomedical domain texts contain a considerable number of domain - specific .",Materials and methods,Materials and methods,named-entity-recognition,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.3265306122448979,62,0.3115577889447236,2,0.1538461538461538,1,0,Materials and methods
64,"Overview of the pre-training and fine - tuning of BioBERT proper nouns ( e.g. BRCA1 , c.248T > C ) and terms ( e.g. transcriptional , antimicrobial ) , which are understood mostly by biomedical researchers .",Materials and methods,Materials and methods,named-entity-recognition,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.3469387755102041,63,0.3165829145728643,3,0.2307692307692307,1,0,Materials and methods
65,"As a result , NLP models designed for general purpose language understanding often obtains poor performance in biomedical text mining tasks .",Materials and methods,Materials and methods,named-entity-recognition,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.3673469387755102,64,0.321608040201005,4,0.3076923076923077,1,0,Materials and methods
67,"The text corpora used for pre-training of BioBERT are listed in , and the tested combinations of text corpora are listed in .",Materials and methods,Materials and methods,named-entity-recognition,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.4081632653061224,66,0.3316582914572864,6,0.4615384615384615,1,0,Materials and methods
68,"For computational efficiency , whenever the Wiki Books corpora were used for pre-training , we initialized BioBERT with the pre-trained BERT model provided by .",Materials and methods,Materials and methods,named-entity-recognition,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.4285714285714285,67,0.3366834170854271,7,0.5384615384615384,1,0,Materials and methods
71,"With WordPiece tokenization , any new words can be represented by frequent subwords ( e.g. Immunoglobulin >",Materials and methods,Materials and methods,named-entity-recognition,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.4897959183673469,70,0.3517587939698492,10,0.7692307692307693,1,0,Materials and methods
72,I ##mm ##uno ##g ##lo # #bul # #in ) .,Materials and methods,Materials and methods,named-entity-recognition,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.5102040816326531,71,0.3567839195979899,11,0.8461538461538461,1,0,Materials and methods
74,"Although we could have constructed new WordPiece vocabulary based on biomedical corpora , we used the original vocabulary of BERT BASE for the following reasons : ( i ) compatibility of BioBERT with BERT , which allows BERT pre-trained on general domain corpora to be re-used , and makes it easier to interchangeably use existing models based on BERT and BioBERT and ( ii ) any new words may still be represented and fine - tuned for the biomedical domain using the original WordPiece vocabulary of BERT .",Materials and methods,Materials and methods,named-entity-recognition,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.5510204081632653,73,0.3668341708542713,13,1.0,1,0,Materials and methods
75,Fine-tuning BioBERT,Materials and methods,Materials and methods,named-entity-recognition,9,"['O', 'O']","['O', 'O']",28,0.5714285714285714,74,0.371859296482412,0,0.0,1,0,Materials and methods
78,"Named entity recognition is one of the most fundamental biomedical text mining tasks , which involves recognizing numerous domain - specific proper nouns in a biomedical corpus .",Materials and methods,Materials and methods,named-entity-recognition,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",31,0.6326530612244898,77,0.3869346733668342,3,0.1428571428571428,1,0,Materials and methods
79,"While most previous works were built upon different combinations of LSTMs and CRFs , BERT has a simple architecture based on bidirectional transformers .",Materials and methods,Materials and methods,named-entity-recognition,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",32,0.6530612244897959,78,0.3919597989949748,4,0.1904761904761904,1,0,Materials and methods
81,"Note that while previous works in biomedical NER often used word embeddings trained on PubMed or PMC corpora , BioBERT directly learns WordPiece embeddings during pre-training and fine - tuning .",Materials and methods,Materials and methods,named-entity-recognition,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",34,0.6938775510204082,80,0.4020100502512563,6,0.2857142857142857,1,0,Materials and methods
84,"We utilized the sentence classifier of the original version of BERT , which uses a [ CLS ] token for the classification of relations .",Materials and methods,Materials and methods,named-entity-recognition,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",37,0.7551020408163265,83,0.4170854271356783,9,0.4285714285714285,1,0,Materials and methods
86,We anonymized target named entities in a sentence using pre-defined tags such as @ GENE $ or @DISEASE $ .,Materials and methods,Materials and methods,named-entity-recognition,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",39,0.7959183673469388,85,0.4271356783919598,11,0.5238095238095238,1,0,Materials and methods
87,"For instance , a sentence with two target entities ( gene and disease in this case ) is represented as "" Serine at position 986 of @GENE $ maybe an independent genetic predictor of angiographic @DISEASE $ . """,Materials and methods,Materials and methods,named-entity-recognition,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",40,0.8163265306122449,86,0.4321608040201005,12,0.5714285714285714,1,0,Materials and methods
89,Question answering is a task of answering questions posed in natural language given related passages .,Materials and methods,Materials and methods,named-entity-recognition,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",42,0.8571428571428571,88,0.4422110552763819,14,0.6666666666666666,1,0,Materials and methods
91,We used the BioASQ factoid datasets because their format is similar to that of SQuAD .,Materials and methods,Materials and methods,named-entity-recognition,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",44,0.8979591836734694,90,0.4522613065326633,16,0.7619047619047619,1,0,Materials and methods
93,"However , we observed that about 30 % of the BioASQ factoid questions were unanswerable in an extractive QA setting as the exact answers did not appear in the given passages .",Materials and methods,Materials and methods,named-entity-recognition,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",46,0.9387755102040816,92,0.4623115577889447,18,0.8571428571428571,1,0,Materials and methods
94,"Like , we excluded the samples with unanswerable questions from the training sets .",Materials and methods,Materials and methods,named-entity-recognition,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",47,0.9591836734693876,93,0.4673366834170854,19,0.9047619047619048,1,0,Materials and methods
97,Results,,,named-entity-recognition,9,['O'],['O'],0,0.0,96,0.4824120603015075,0,0.0,1,0,
98,Datasets,Results,,named-entity-recognition,9,['O'],['O'],1,0.05,97,0.4874371859296482,0,0.0,1,0,Results
99,The statistics of biomedical NER datasets are listed in .,Results,Datasets,named-entity-recognition,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.1,98,0.4924623115577889,1,0.0526315789473684,1,0,Results: Datasets
101,The pre-processed NCBI Disease dataset has fewer annotations than the original dataset due to the removal of duplicate articles from its training set .,Results,Datasets,named-entity-recognition,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.2,100,0.5025125628140703,3,0.1578947368421052,1,0,Results: Datasets
104,"We did not use alternate annotations for the BC2 GM dataset , and all NER evaluations are based on entity - level exact matches .",Results,Datasets,named-entity-recognition,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.35,103,0.5175879396984925,6,0.3157894736842105,1,0,Results: Datasets
105,"Note that although there are several other recently introduced high quality biomedical NER datasets , we use datasets that are frequently used by many biomedical NLP researchers , which makes it much easier to compare our work with theirs .",Results,Datasets,named-entity-recognition,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.4,104,0.5226130653266332,7,0.3684210526315789,1,0,Results: Datasets
106,The RE datasets contain gene - disease relations and protein - chemical relations ) .,Results,Datasets,named-entity-recognition,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.45,105,0.5276381909547738,8,0.4210526315789473,1,0,Results: Datasets
108,"For the CHEMPROT dataset , we used the same pre-processing procedure described in .",Results,Datasets,named-entity-recognition,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.55,107,0.5376884422110553,10,0.5263157894736842,1,0,Results: Datasets
109,"We used the BioASQ factoid datasets , which can be converted into the same format as the SQuAD dataset ) .",Results,Datasets,named-entity-recognition,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.6,108,0.542713567839196,11,0.5789473684210527,1,0,Results: Datasets
112,"For all the datasets , we used the same dataset splits used in previous works ) fora fair evaluation ; however , the splits of LINAAEUS and Species - 800 could not be found from and maybe different .",Results,Datasets,named-entity-recognition,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.75,111,0.5577889447236181,14,0.7368421052631579,1,0,Results: Datasets
113,"Like previous work , we reported the performance of 10 - fold cross-validation on datasets that do not have separate test sets ( e.g. GAD , EU - ADR ) .",Results,Datasets,named-entity-recognition,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.8,112,0.5628140703517588,15,0.7894736842105263,1,0,Results: Datasets
115,Note that the state - of - the - art models each have a different architecture and training procedure .,Results,Datasets,named-entity-recognition,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.9,114,0.5728643216080402,17,0.8947368421052632,1,0,Results: Datasets
116,"For instance , the state - of - the - art model by trained on the JNLPBA dataset is based on multiple Bi - LSTM CRF models with character level CNNs , while the state - of - the - art model by trained on the LINNAEUS dataset uses a Bi - LSTM CRF model with character level LSTMs and is additionally trained on silver - standard datasets .",Results,Datasets,named-entity-recognition,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.95,115,0.5778894472361809,18,0.9473684210526316,1,0,Results: Datasets
117,"On the other hand , BERT and BioBERT have exactly the same structure , and use only the gold standard datasets and not any additional datasets .",Results,Datasets,named-entity-recognition,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,1.0,116,0.5829145728643216,19,1.0,1,0,Results: Datasets
118,Experimental setups,,,named-entity-recognition,9,"['O', 'O']","['O', 'O']",0,0.0,117,0.5879396984924623,0,0.0,1,0,
120,BioBERT v1.0 ( PubMed PMC ) is the version of BioBERT ( PubMed PMC ) trained for 470 K steps .,Experimental setups,Experimental setups,named-entity-recognition,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.125,119,0.5979899497487438,2,0.125,1,0,Experimental setups
123,"After our initial release of BioBERT v 1.0 , we pre-trained BioBERT on PubMed for 1 M steps , and we refer to this version as BioBERT v 1.1 ( PubMed ) .",Experimental setups,Experimental setups,named-entity-recognition,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.3125,122,0.6130653266331658,5,0.3125,1,0,Experimental setups
124,Other hyper - parameters such as batch size and learning rate scheduling for pre-training BioBERT are the same as those for pre-training BERT unless stated otherwise .,Experimental setups,Experimental setups,named-entity-recognition,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.375,123,0.6180904522613065,6,0.375,1,0,Experimental setups
128,It takes more than 10 days to pre-train BioBERT v 1.0 ( PubMed PMC ) nearly 23 days for BioBERT v 1.1 ( PubMed ) in this setting .,Experimental setups,Experimental setups,named-entity-recognition,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.625,127,0.6381909547738693,10,0.625,1,0,Experimental setups
129,"Despite our best efforts to use BERT LARGE , we used only BERT BASE due to the computational complexity of BERT LARGE .",Experimental setups,Experimental setups,named-entity-recognition,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.6875,128,0.6432160804020101,11,0.6875,1,0,Experimental setups
131,Note that the fine - tuning process is more computationally efficient than pre-training BioBERT .,Experimental setups,Experimental setups,named-entity-recognition,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.8125,130,0.6532663316582915,13,0.8125,1,0,Experimental setups
133,Fine - tuning BioBERT on QA and RE tasks took less than an hour as the size of the training data is much smaller than that of the training data used by .,Experimental setups,Experimental setups,named-entity-recognition,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.9375,132,0.6633165829145728,15,0.9375,1,0,Experimental setups
135,Experimental results,,,named-entity-recognition,9,"['O', 'O']","['O', 'O']",0,0.0,134,0.6733668341708543,0,0.0,1,0,
140,"The relatively low scores on the LINNAEUS dataset can be attributed to the following : ( i ) the lack of a silver - standard dataset for training previous state - of - the - art models and ( ii ) different training / test set splits used in previous work , which were unavailable .",Experimental results,Experimental results,named-entity-recognition,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.1612903225806451,139,0.6984924623115578,5,0.3571428571428571,1,0,Experimental results
150,Discussion,Experimental results,,named-entity-recognition,9,['O'],['O'],15,0.4838709677419355,149,0.7487437185929648,0,0.0,1,0,Experimental results
153,"shows that the performance of BioBERT v 1.0 ( PubMed ) on three NER datasets ( NCBI Disease , BC2GM , BC4CHEMD ) changes in relation to the size of the PubMed corpus .",Experimental results,Discussion,named-entity-recognition,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.5806451612903226,152,0.7638190954773869,3,0.1875,1,0,Experimental results: Discussion
161,"As shown in , we sampled predictions from BERT and BioBERT v 1.1 ( PubMed ) to seethe effect of pre-training on downstream tasks .",Experimental results,Discussion,named-entity-recognition,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.8387096774193549,160,0.8040201005025126,11,0.6875,1,0,Experimental results: Discussion
162,BioBERT can recognize biomedical named entities that BERT can not and can find the exact boundaries of named Disease 19 665 BC5 CDR Disease 12 694 BC5CDR Drug / Chem. 15 411 BC4CHEMD Drug / Chem. 79 842 BC2 GM Gene / Protein 20 703 JNLPBA Gene / Protein 35 460 LINNAEUS Species 4077 Species - 800 Species 3708,Experimental results,Discussion,named-entity-recognition,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.8709677419354839,161,0.8090452261306532,12,0.75,1,0,Experimental results: Discussion
163,"Note Note : For the CHEMPROT dataset , the number of relations in the training , validation and test sets was summed . 486 150 BioASQ 6 b-factoid 618 161",Experimental results,Discussion,named-entity-recognition,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",28,0.9032258064516128,162,0.8140703517587939,13,0.8125,1,0,Experimental results: Discussion
164,entities .,Experimental results,Discussion,named-entity-recognition,9,"['O', 'O']","['O', 'O']",29,0.935483870967742,163,0.8190954773869347,14,0.875,1,0,Experimental results: Discussion
165,"While BERT often gives incorrect answers to simple biomedical questions , BioBERT provides correct answers to such questions .",Experimental results,Discussion,named-entity-recognition,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",30,0.967741935483871,164,0.8241206030150754,15,0.9375,1,0,Experimental results: Discussion
166,"Also , BioBERT can provide longer named entities as answers .",Experimental results,Discussion,named-entity-recognition,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",31,1.0,165,0.8291457286432161,16,1.0,1,0,Experimental results: Discussion
167,Conclusion,,,named-entity-recognition,9,['O'],['O'],0,0.0,166,0.8341708542713567,0,0.0,1,0,
3,abstract,,,question-answering,0,['O'],['O'],0,0.0,2,0.0077519379844961,0,0.0,1,0,
6,Such approaches can be effective but at the cost of either large amounts of human - labeled data or by defining lexicons and grammars tailored by practitioners .,abstract,abstract,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.4285714285714285,5,0.0193798449612403,3,0.4285714285714285,1,0,abstract
15,"They are organized as databases of triples connecting pairs of entities by various relationships and of the form ( left entity , relationship , right entity ) .",Introduction,Introduction,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.1333333333333333,14,0.0542635658914728,4,0.1333333333333333,1,0,Introduction
16,Question answering is then defined as the task of retrieving the correct entity or set of entities from a KB given a query expressed as a question in natural language .,Introduction,Introduction,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.1666666666666666,15,0.0581395348837209,5,0.1666666666666666,1,0,Introduction
18,"However , open question answering remains challenging because of the scale of these KBs ( billions of triples , millions of entities and relationships ) and of the difficulty for machines to interpret natural language .",Introduction,Introduction,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.2333333333333333,17,0.065891472868217,7,0.2333333333333333,1,0,Introduction
19,Recent progress has been made by tackling this problem with semantic parsers .,Introduction,Introduction,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.2666666666666666,18,0.0697674418604651,8,0.2666666666666666,1,0,Introduction
21,"Even if such systems have shown the ability to handle large - scale KBs , they require practitioners to hand - craft lexicons , grammars , and KB schema for the parsing to be effective .",Introduction,Introduction,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.3333333333333333,20,0.0775193798449612,10,0.3333333333333333,1,0,Introduction
22,"This nonnegligible human intervention might not be generic enough to conveniently scale up to new databases with other schema , broader vocabularies or other languages than English .",Introduction,Introduction,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.3666666666666666,21,0.0813953488372093,11,0.3666666666666666,1,0,Introduction
24,"Following , we focus on answering simple factual questions on abroad range of topics , more specifically , those for which single KB triples stand for both the question and an answer ( of which there maybe many ) .",Introduction,Introduction,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.4333333333333333,23,0.0891472868217054,13,0.4333333333333333,1,0,Introduction
25,"For example , ( parrotfish.e , live - in.r , southern - water .e ) stands for",Introduction,Introduction,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.4666666666666667,24,0.0930232558139534,14,0.4666666666666667,1,0,Introduction
27,"and southern - water.e and ( cantonese.e , be-major - language - in.r , hong - kong.e ) for",Introduction,Introduction,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.5333333333333333,26,0.1007751937984496,16,0.5333333333333333,1,0,Introduction
29,and cantonese.e.,Introduction,Introduction,question-answering,0,"['O', 'O']","['O', 'O']",18,0.6,28,0.1085271317829457,18,0.6,1,0,Introduction
30,"In this task , the main difficulties come from lexical variability rather than from complex syntax , having multiple answers per question , and the absence of a supervised training signal .",Introduction,Introduction,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.6333333333333333,29,0.1124031007751938,19,0.6333333333333333,1,0,Introduction
32,"Unfortunately , we do not have access to any human labeled ( query , answer ) supervision for this task .",Introduction,Introduction,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.7,31,0.1201550387596899,21,0.7,1,0,Introduction
37,"Even if the embeddings obtained after training are of good quality , the scale of the optimization problem makes it hard to control and to lead to convergence .",Introduction,Introduction,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.8666666666666667,36,0.1395348837209302,26,0.8666666666666667,1,0,Introduction
39,The rest of the paper is organized as follows .,Introduction,Introduction,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",28,0.9333333333333332,38,0.1472868217054263,28,0.9333333333333332,1,0,Introduction
42,Related Work,,,question-answering,0,"['O', 'O']","['O', 'O']",0,0.0,41,0.1589147286821705,0,0.0,1,0,
75,Training Data,,,question-answering,0,"['O', 'O']","['O', 'O']",0,0.0,74,0.2868217054263566,0,0.0,1,0,
77,Knowledge Base,Training Data,,question-answering,0,"['O', 'O']","['O', 'O']",2,0.048780487804878,76,0.2945736434108527,2,0.2,1,0,Training Data
78,The set of potential answers K is given by the KB ReVerb .,Training Data,Knowledge Base,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.073170731707317,77,0.2984496124031007,3,0.3,1,0,Training Data: Knowledge Base
79,"ReVerb is an open - source database composed of more than 14M triples , made of more than 2 M entities and 600 k relationships , which have been automatically extracted from the ClueWeb09 corpus .",Training Data,Knowledge Base,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.0975609756097561,78,0.3023255813953488,4,0.4,1,0,Training Data: Knowledge Base
82,"But , as a result , ReVerb is ambiguous and noisy with many useless triples and entities as well as numerous duplicates .",Training Data,Knowledge Base,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.1707317073170731,81,0.313953488372093,7,0.7,1,0,Training Data: Knowledge Base
83,"For instance , winston - churchill.e , churchill.e and even roosevelt - and - churchill.e are all distinct entities .. 2 presents some examples of triples : some make sense , some others are completely unclear or useless .",Training Data,Knowledge Base,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.1951219512195122,82,0.3178294573643411,8,0.8,1,0,Training Data: Knowledge Base
84,"In contrast to highly curated databases such Freebase , ReVerb has more noise but also many more relation types ( Freebase has around 20 k ) .",Training Data,Knowledge Base,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.2195121951219512,83,0.3217054263565891,9,0.9,1,0,Training Data: Knowledge Base
85,"So for some types of triple it has much better coverage , despite the larger size of Freebase ; for example Freebase does not cover verbs like afraid - of or suffer - from .",Training Data,Knowledge Base,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.2439024390243902,84,0.3255813953488372,10,1.0,1,0,Training Data: Knowledge Base
86,Questions Generation,Training Data,,question-answering,0,"['O', 'O']","['O', 'O']",11,0.2682926829268293,85,0.3294573643410852,0,0.0,1,0,Training Data
87,"We have no available data of questions q labeled with their answers , i.e. with the corresponding triples t ?",Training Data,Questions Generation,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.2926829268292683,86,0.3333333333333333,1,0.0149253731343283,1,0,Training Data: Questions Generation
90,These pairs are generated using the 16 seed questions displayed in .,Training Data,K .,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.3658536585365853,89,0.3449612403100775,4,0.0597014925373134,1,0,Training Data: K .
92,"Note only triples with a *-in.r relation ( denoted r- in in ) can generate from the pattern where did er ? , for example , and similar for other constraints .",Training Data,K .,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.4146341463414634,91,0.3527131782945736,6,0.0895522388059701,1,0,Training Data: K .
93,"Otherwise , the pattern is chosen randomly .",Training Data,,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.4390243902439024,92,0.3565891472868217,7,0.1044776119402985,1,0,Training Data
94,"Except for these exceptions , we used all 16 seed questions for all triples hence generating approximately 16 14M questions stored in a training set we denote D.",Training Data,"Otherwise , the pattern is chosen randomly .",question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.4634146341463415,93,0.3604651162790697,8,0.1194029850746268,1,0,"Training Data: Otherwise , the pattern is chosen randomly ."
95,The generated questions are imperfect and noisy and create a weak training signal .,Training Data,"Otherwise , the pattern is chosen randomly .",question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.4878048780487805,94,0.3643410852713178,9,0.1343283582089552,1,0,"Training Data: Otherwise , the pattern is chosen randomly ."
96,"Firstly , their syntactic structure is rather simplistic , and real questions as posed by humans ( such as in our actual test ) can look quite different to them .",Training Data,"Otherwise , the pattern is chosen randomly .",question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.5121951219512195,95,0.3682170542635659,10,0.1492537313432835,1,0,"Training Data: Otherwise , the pattern is chosen randomly ."
97,"Secondly , many generated questions do not correspond to semantically valid English sentences .",Training Data,"Otherwise , the pattern is chosen randomly .",question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.5365853658536586,96,0.3720930232558139,11,0.1641791044776119,1,0,"Training Data: Otherwise , the pattern is chosen randomly ."
98,"For instance , since the type of entities in ReVerb is unknown , a pattern like who does er ?",Training Data,"Otherwise , the pattern is chosen randomly .",question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",23,0.5609756097560976,97,0.375968992248062,12,0.1791044776119403,1,0,"Training Data: Otherwise , the pattern is chosen randomly ."
99,"can be chosen fora triple where the type of ? in ( ? , r , e ) is not a person , and similar for other types ( e.g. when ) .",Training Data,"Otherwise , the pattern is chosen randomly .",question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.5853658536585366,98,0.3798449612403101,13,0.1940298507462686,1,0,"Training Data: Otherwise , the pattern is chosen randomly ."
100,"Besides , for the strings representing entities and relationships in the questions , we simply used their names in ReVerb , replacingby spaces and stripping off .",Training Data,"Otherwise , the pattern is chosen randomly .",question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.6097560975609756,99,0.3837209302325581,14,0.208955223880597,1,0,"Training Data: Otherwise , the pattern is chosen randomly ."
102,"their suffixes , i.e. the string representing winston - churchill.e is simply winston churchill .",Training Data,Patterns for generating questions from ReVerb triples following .,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.6585365853658537,101,0.3914728682170542,16,0.2388059701492537,1,0,Training Data: Patterns for generating questions from ReVerb triples following .
103,"While this is often fine , this is also very limited and caused many incoherences in the data .",Training Data,Patterns for generating questions from ReVerb triples following .,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",28,0.6829268292682927,102,0.3953488372093023,17,0.2537313432835821,1,0,Training Data: Patterns for generating questions from ReVerb triples following .
104,"Generating questions with a richer KB than ReVerb , such as Freebase or DBpedia , would lead to better quality because typing and better lexicons could be used .",Training Data,Patterns for generating questions from ReVerb triples following .,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",29,0.7073170731707317,103,0.3992248062015504,18,0.2686567164179104,1,0,Training Data: Patterns for generating questions from ReVerb triples following .
105,"However , this would contradict one of our motivations which is to train a system with as little human intervention as possible ( and hence choosing ReVerb over hand - curated KBs ) .",Training Data,Patterns for generating questions from ReVerb triples following .,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",30,0.7317073170731707,104,0.4031007751937984,19,0.2835820895522388,1,0,Training Data: Patterns for generating questions from ReVerb triples following .
106,Paraphrases,Training Data,,question-answering,0,['O'],['O'],31,0.7560975609756098,105,0.4069767441860465,20,0.2985074626865671,1,0,Training Data
108,"However , they do not allow fora satisfactory modeling of English language because of their poor wording .",Training Data,Paraphrases,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",33,0.8048780487804879,107,0.4147286821705426,22,0.3283582089552239,1,0,Training Data: Paraphrases
109,"To overcome this issue , we again follow and supplement our training data with an indirect supervision signal made of pairs of question paraphrases collected from the WikiAnswers website .",Training Data,Paraphrases,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",34,0.8292682926829268,108,0.4186046511627907,23,0.3432835820895522,1,0,Training Data: Paraphrases
112,These pairs have been labeled collaboratively .,Training Data,,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O']",37,0.902439024390244,111,0.4302325581395348,26,0.3880597014925373,1,0,Training Data
113,This is cheap but also causes the data to be noisy .,Training Data,These pairs have been labeled collaboratively .,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",38,0.926829268292683,112,0.4341085271317829,27,0.4029850746268656,1,0,Training Data: These pairs have been labeled collaboratively .
114,"Hence , estimated that only 55 % of the pairs were actual paraphrases .",Training Data,These pairs have been labeled collaboratively .,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",39,0.951219512195122,113,0.437984496124031,28,0.417910447761194,1,0,Training Data: These pairs have been labeled collaboratively .
115,The set of paraphrases is denoted P in the following .,Training Data,These pairs have been labeled collaboratively .,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",40,0.975609756097561,114,0.4418604651162791,29,0.4328358208955223,1,0,Training Data: These pairs have been labeled collaboratively .
116,"By considering all words and tokens appearing in P and D , we end up with a size for the vocabulary V of more than 800k .",Training Data,These pairs have been labeled collaboratively .,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",41,1.0,115,0.4457364341085271,30,0.4477611940298507,1,0,Training Data: These pairs have been labeled collaboratively .
119,Question - KB Triple Scoring,Embedding - based Model,Embedding - based Model,question-answering,0,"['O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O']",2,1.0,118,0.4573643410852713,33,0.4925373134328358,1,0,Embedding - based Model
120,Architecture,,,question-answering,0,['O'],['O'],0,0.0,119,0.4612403100775193,34,0.5074626865671642,1,0,
123,"Intuitively , it consists of projecting questions , treated as a bag of words ( and possibly n-grams as well ) , on the one hand , and triples on the other hand , into a shared embedding space and then computing a similarity measure ( the dot product in this paper ) between both projections .",Architecture,Architecture,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.0491803278688524,122,0.4728682170542636,37,0.5522388059701493,1,0,Architecture
124,The scoring function is then :,Architecture,Architecture,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O']",4,0.0655737704918032,123,0.4767441860465116,38,0.5671641791044776,1,0,Architecture
125,"with f ( ) a function mapping words from questions into Contrary to previous work modeling KBs with embeddings ( e.g. ) , in our model , an entity does not have the same embedding when appearing in the lefthand or in the right - hand side of a triple .",Architecture,Architecture,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.081967213114754,124,0.4806201550387597,39,0.582089552238806,1,0,Architecture
126,"Since , g ( ) sums embeddings of all constituents of a triple , we need to use 2 embeddings per entity to encode for the fact that relationships in the KB are not symmetric and so that appearing as a left - hand or right - hand entity is different .",Architecture,Architecture,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.0983606557377049,125,0.4844961240310077,40,0.5970149253731343,1,0,Architecture
127,"This approach can be easily applied attest time to score any ( question , triple ) pairs .",Architecture,Architecture,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.1147540983606557,126,0.4883720930232558,41,0.6119402985074627,1,0,Architecture
128,"Given a question q , one can predict the corresponding answer ( a triple ) t ( q ) wit h:t ( q ) = arg max",Architecture,Architecture,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.1311475409836065,127,0.4922480620155038,42,0.6268656716417911,1,0,Architecture
129,Training by Ranking Previous work has shown that this kind of model can be conveniently trained using a ranking loss .,Architecture,Architecture,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.1475409836065573,128,0.4961240310077519,43,0.6417910447761194,1,0,Architecture
130,"Hence , given our data set D = { ( q i , ti ) , i = 1 , . . . , | D |} consisting of ( question , answer triple ) training pairs , one could learn the embeddings using constraints of the form :",Architecture,Architecture,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.1639344262295081,129,0.5,44,0.6567164179104478,1,0,Architecture
131,where 0.1 is the margin .,Architecture,Architecture,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O']",11,0.180327868852459,130,0.5038759689922481,45,0.6716417910447762,1,0,Architecture
133,"We also enforce a constraint on the norms of the columns of V and W , i.e. ?",Architecture,Architecture,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.2131147540983606,132,0.5116279069767442,47,0.7014925373134329,1,0,Architecture
135,1 .,Architecture,Architecture,question-answering,0,"['O', 'O']","['O', 'O']",15,0.2459016393442623,134,0.5193798449612403,49,0.7313432835820896,1,0,Architecture
136,"To train our model , we need positive and negative examples of ( q , t ) pairs .",Architecture,Architecture,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.2622950819672131,135,0.5232558139534884,50,0.746268656716418,1,0,Architecture
137,"However , D only contains positive samples , for which the triple actually corresponds to the question .",Architecture,Architecture,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.2786885245901639,136,0.5271317829457365,51,0.7611940298507462,1,0,Architecture
138,"Hence , during training , we use a procedure to corrupt triples .",Architecture,Architecture,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.2950819672131147,137,0.5310077519379846,52,0.7761194029850746,1,0,Architecture
139,"Given ( q , t ) ?",Architecture,Architecture,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.3114754098360656,138,0.5348837209302325,53,0.7910447761194029,1,0,Architecture
140,"D , we create a corrupted triplet with the following method : pick another random triplet tmp from K , and then , replace with 66 % chance each member oft ( left entity , relationship and right entity ) by the corresponding element int tmp .",Architecture,Architecture,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.3278688524590163,139,0.5387596899224806,54,0.8059701492537313,1,0,Architecture
141,"This heuristic creates negative triples t somewhat similar to their positive counterpart t , and is similar to schemes of previous work ( e.g. in ) .",Architecture,Architecture,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.3442622950819672,140,0.5426356589147286,55,0.8208955223880597,1,0,Architecture
143,At the start of training the parameters off ( ) and g ( ) ( the n v k word embeddings in V and then e k entities and rel .,Architecture,Architecture,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",23,0.3770491803278688,142,0.5503875968992248,57,0.8507462686567164,1,0,Architecture
144,"embeddings in W ) are initialized to random weights ( mean 0 , standard deviation 1 k ) .",Architecture,Architecture,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.3934426229508196,143,0.5542635658914729,58,0.8656716417910447,1,0,Architecture
147,2 .,Architecture,Architecture,question-answering,0,"['O', 'O']","['O', 'O']",27,0.4426229508196721,146,0.5658914728682171,61,0.9104477611940298,1,0,Architecture
149,3 .,Architecture,Architecture,question-answering,0,"['O', 'O']","['O', 'O']",29,0.4754098360655737,148,0.5736434108527132,63,0.9402985074626866,1,0,Architecture
150,Make a stochastic gradient step to minimize 0.1 ? f ( q i ) g (t i ) +f ( q i ) g (t i ) + .,Architecture,Architecture,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",30,0.4918032786885246,149,0.5775193798449613,64,0.9552238805970148,1,0,Architecture
153,x + is the positive part of x .,Architecture,Architecture,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",33,0.5409836065573771,152,0.5891472868217055,67,1.0,1,0,Architecture
156,We use the same architecture simply replacing g ( ) by a copy off ( ) .,Architecture,Architecture,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",36,0.5901639344262295,155,0.6007751937984496,2,0.074074074074074,1,0,Architecture
157,This leads to the following function that scores the similarity between two questions :,Architecture,Architecture,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",37,0.6065573770491803,156,0.6046511627906976,3,0.1111111111111111,1,0,Architecture
158,"The matrix W containing embeddings of words is shared between Sand S prp , allowing it to encode information from examples from both D and P. Training of S prp is also conducted with SGD ( and adagrad ) as for S , but , in this case , negative examples are created by replacing one of the questions from the pair by another question chosen at random in P.",Architecture,Architecture,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",38,0.6229508196721312,157,0.6085271317829457,4,0.1481481481481481,1,0,Architecture
165,The training algorithm must also stay simple to scale on a training set of around 250 M of examples ( D and P combined ) ; SGD appears as the only viable option .,Architecture,Fine - tuning the Similarity between Embeddings,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",45,0.7377049180327869,164,0.6356589147286822,11,0.4074074074074074,1,0,Architecture: Fine - tuning the Similarity between Embeddings
167,"However , the scale of the optimization problem makes it very hard to control and conduct properly until convergence .",Architecture,Fine - tuning the Similarity between Embeddings,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",47,0.7704918032786885,166,0.6434108527131783,13,0.4814814814814814,1,0,Architecture: Fine - tuning the Similarity between Embeddings
170,"Updating the embeddings involves working on too many parameters , but ultimately , these embeddings are meant to be used in a dot -product that computes the similarity between q and t.",Architecture,Fine - tuning the Similarity between Embeddings,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",50,0.819672131147541,169,0.6550387596899225,16,0.5925925925925926,1,0,Architecture: Fine - tuning the Similarity between Embeddings
171,We propose to learn a matrix M ?,Architecture,Fine - tuning the Similarity between Embeddings,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",51,0.8360655737704918,170,0.6589147286821705,17,0.6296296296296297,1,0,Architecture: Fine - tuning the Similarity between Embeddings
172,R kk parameterizing the similarity between words and triples embeddings .,Architecture,Fine - tuning the Similarity between Embeddings,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",52,0.8524590163934426,171,0.6627906976744186,18,0.6666666666666666,1,0,Architecture: Fine - tuning the Similarity between Embeddings
173,The scoring function becomes :,Architecture,Fine - tuning the Similarity between Embeddings,question-answering,0,"['O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O']",53,0.8688524590163934,172,0.6666666666666666,19,0.7037037037037037,1,0,Architecture: Fine - tuning the Similarity between Embeddings
174,M has only k 2 parameters and can be efficiently determined by solving the following convex problem ( fixing the embedding matrices W and V ) :,Architecture,Fine - tuning the Similarity between Embeddings,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",54,0.8852459016393442,173,0.6705426356589147,20,0.7407407407407407,1,0,Architecture: Fine - tuning the Similarity between Embeddings
175,where X F is the Frobenius norm of X .,Architecture,Fine - tuning the Similarity between Embeddings,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",55,0.9016393442622952,174,0.6744186046511628,21,0.7777777777777778,1,0,Architecture: Fine - tuning the Similarity between Embeddings
178,"We then retrain the model on the whole 10M examples using the selected value , which happened to be ? = 1.7 10 ?5 .",Architecture,Fine - tuning the Similarity between Embeddings,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",58,0.9508196721311476,177,0.686046511627907,24,0.8888888888888888,1,0,Architecture: Fine - tuning the Similarity between Embeddings
179,"This fine - tuning is related to learning anew metric in the embedding space , but since the resulting M is not symmetric , it does not define a dot-product .",Architecture,Fine - tuning the Similarity between Embeddings,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",59,0.9672131147540984,178,0.689922480620155,25,0.925925925925926,1,0,Architecture: Fine - tuning the Similarity between Embeddings
180,"Still , M is close to a constant factor times identity ( as in the original score S ( ) ) .",Architecture,Fine - tuning the Similarity between Embeddings,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",60,0.9836065573770492,179,0.6937984496124031,26,0.9629629629629628,1,0,Architecture: Fine - tuning the Similarity between Embeddings
182,Experiments,,,question-answering,0,['O'],['O'],0,0.0,181,0.7015503875968992,0,0.0,1,0,
183,Evaluation Protocols,,,question-answering,0,"['O', 'O']","['O', 'O']",0,0.0,182,0.7054263565891473,0,0.0,1,0,
185,Test Set,Evaluation Protocols,,question-answering,0,"['O', 'O']","['O', 'O']",2,0.1538461538461538,184,0.7131782945736435,0,0.0,1,0,Evaluation Protocols
186,The data set WikiAnswers + ReVerb contains no labeled examples but some are needed for evaluating models .,Evaluation Protocols,Test Set,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.2307692307692307,185,0.7170542635658915,1,0.1666666666666666,1,0,Evaluation Protocols: Test Set
187,"We used the test set which has been created by in the following way : ( 1 ) they identified 37 questions from a heldout portion of WikiAnswers which were likely to have at least one answer in ReVerb , ( 2 ) they added all valid paraphrases of these questions to obtain a set of 691 questions , ( 3 ) they ran various versions of their paralex system on them to gather candidate triples ( for a total of 48 k ) , which they finally hand - labeled .",Evaluation Protocols,Test Set,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.3076923076923077,186,0.7209302325581395,2,0.3333333333333333,1,0,Evaluation Protocols: Test Set
188,Reranking,Evaluation Protocols,,question-answering,0,['O'],['O'],5,0.3846153846153846,187,0.7248062015503876,3,0.5,1,0,Evaluation Protocols
192,Full Ranking,Evaluation Protocols,,question-answering,0,"['O', 'O']","['O', 'O']",9,0.6923076923076923,191,0.7403100775193798,0,0.0,1,0,Evaluation Protocols
193,"We hence decided to filter out some candidates before ranking by using a simple string matching strategy : after pos-tagging the question , we construct a set of candidate strings containing ( i ) all noun phrases that appear less than 1,000 .",Evaluation Protocols,Full Ranking,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.7692307692307693,192,0.7441860465116279,1,0.25,1,0,Evaluation Protocols: Full Ranking
195,"The prefix L : , resp.",Evaluation Protocols,,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O']",12,0.9230769230769232,194,0.751937984496124,3,0.75,1,0,Evaluation Protocols
196,"R : , indicates the embedding of an entity when appearing in left - hand side , resp. right - hand side , of triples .",Evaluation Protocols,"The prefix L : , resp.",question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,1.0,195,0.7558139534883721,4,1.0,1,0,"Evaluation Protocols: The prefix L : , resp."
197,Results,,,question-answering,0,['O'],['O'],0,0.0,196,0.7596899224806202,0,0.0,1,0,
203,"Note that the WikiAnswers data provides word alignment between paraphrases , which we did not use , unlike paralex .",Results,This section now discusses our empirical performance .,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.2608695652173913,202,0.7829457364341085,6,0.3,1,0,Results: This section now discusses our empirical performance .
204,"We also tried to use n-grams ( 2.5 M most frequent ) as well as the words to represent the question , but this did not bring any improvement , which might at first seem counter - intuitive .",Results,This section now discusses our empirical performance .,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.3043478260869565,203,0.7868217054263565,7,0.35,1,0,Results: This section now discusses our empirical performance .
205,"We believe this is due to two factors : it is hard to learn good embeddings for n-grams since their frequency is usually very low and ( 2 ) our automatically generated questions have a poor syntax and hence , many n-grams in this data set do not make sense .",Results,This section now discusses our empirical performance .,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.3478260869565217,204,0.7906976744186046,8,0.4,1,0,Results: This section now discusses our empirical performance .
210,Most of its predictions come from automatically acquired templates and rules : this allows fora good precision but it is not flexible enough across language variations to grant a satisfying recall .,Results,This section now discusses our empirical performance .,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.5652173913043478,209,0.810077519379845,13,0.65,1,0,Results: This section now discusses our empirical performance .
212,"However , as we said earlier , this reranking setting is detrimental for paralex because paralex was evaluated on the task of reranking some of its own predictions .",Results,This section now discusses our empirical performance .,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.6521739130434783,211,0.8178294573643411,15,0.75,1,0,Results: This section now discusses our empirical performance .
213,"The results provided for paralex , while not corresponding to those of a full ranking among all triples from ReVerb ( it is still reranking among a subset of candidates ) , concerns an evaluation setting more complicated than for our model .",Results,This section now discusses our empirical performance .,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.6956521739130435,212,0.8217054263565892,16,0.8,1,0,Results: This section now discusses our empirical performance .
217,But most of these can be discarded beforehand .,Results,,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.8695652173913043,216,0.8372093023255814,20,1.0,1,0,Results
218,Word,Results,,question-answering,0,['O'],['O'],21,0.9130434782608696,217,0.8410852713178295,0,0.0,1,0,Results
219,Closest entities or relationships from ReVerb in the embedding space get rid of get -rid - of.r be -get - rid - of.r rid-of.r can -get - rid - of.r will - get - rid - of.r should - get - rid - of.r have - to - get - rid - of.r want - to - get- rid- of.r will - not - get - rid-of.r,Results,Word,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.9565217391304348,218,0.8449612403100775,1,0.0526315789473684,1,0,Results: Word
220,help- get-rid-of.r useful be-useful - for.r be-useful - in.r,Results,Word,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",23,1.0,219,0.8488372093023255,2,0.1052631578947368,1,0,Results: Word
222,be-use-extensively-for. r be-not-very-useful-for.r,R:wide-range-of-application.e can-be-useful-for.r,R:wide-range-of-application.e can-be-useful-for.r,question-answering,0,"['O', 'O', 'O']","['O', 'O', 'O']",1,0.0625,221,0.8565891472868217,4,0.2105263157894736,1,0,R:wide-range-of-application.e can-be-useful-for.r
223,R:plex-or-technical-algorithm.e,R:wide-range-of-application.e can-be-useful-for.r,,question-answering,0,['O'],['O'],2,0.125,222,0.8604651162790697,5,0.2631578947368421,1,0,R:wide-range-of-application.e can-be-useful-for.r
224,R:internal-and-external-use.e,R:wide-range-of-application.e can-be-useful-for.r,,question-answering,0,['O'],['O'],3,0.1875,223,0.8643410852713178,6,0.3157894736842105,1,0,R:wide-range-of-application.e can-be-useful-for.r
225,R:authoring.e,R:wide-range-of-application.e can-be-useful-for.r,,question-answering,0,['O'],['O'],4,0.25,224,0.8682170542635659,7,0.3684210526315789,1,0,R:wide-range-of-application.e can-be-useful-for.r
226,R:good- or- bad- purpose.e radiation R:radiation.e,R:wide-range-of-application.e can-be-useful-for.r,,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O']",5,0.3125,225,0.872093023255814,8,0.4210526315789473,1,0,R:wide-range-of-application.e can-be-useful-for.r
227,L:radiation.e,R:wide-range-of-application.e can-be-useful-for.r,,question-answering,0,['O'],['O'],6,0.375,226,0.875968992248062,9,0.4736842105263157,1,0,R:wide-range-of-application.e can-be-useful-for.r
228,R:gamma-radiation.e,R:wide-range-of-application.e can-be-useful-for.r,,question-answering,0,['O'],['O'],7,0.4375,227,0.8798449612403101,10,0.5263157894736842,1,0,R:wide-range-of-application.e can-be-useful-for.r
229,L:gamma- radiation.e,R:wide-range-of-application.e can-be-useful-for.r,,question-answering,0,"['O', 'O']","['O', 'O']",8,0.5,228,0.8837209302325582,11,0.5789473684210527,1,0,R:wide-range-of-application.e can-be-useful-for.r
230,L:x - ray.e L:gamma - ray .,R:wide-range-of-application.e can-be-useful-for.r,,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.5625,229,0.8875968992248062,12,0.631578947368421,1,0,R:wide-range-of-application.e can-be-useful-for.r
236,"For instance , radiation is close to x - ray.e and iphone to smartphone .e.",R:wide-range-of-application.e can-be-useful-for.r,L:x - ray.e L:gamma - ray .,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.9375,235,0.9108527131782944,18,0.9473684210526316,1,0,R:wide-range-of-application.e can-be-useful-for.r: L:x - ray.e L:gamma - ray .
237,"This happens thanks to the multitasking with paraphrase data , since in our automatically generated ( q , t ) pairs , the words radiation and iphone are only used for entities with the strings radiation and iphone respectively in their names .",R:wide-range-of-application.e can-be-useful-for.r,L:x - ray.e L:gamma - ray .,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,1.0,236,0.9147286821705426,19,1.0,1,0,R:wide-range-of-application.e can-be-useful-for.r: L:x - ray.e L:gamma - ray .
238,Evaluation on WebQuestions,,,question-answering,0,"['O', 'O', 'O']","['O', 'O', 'O']",0,0.0,237,0.9186046511627908,0,0.0,1,0,
242,"We chose the data set WebQuestions , which consists of natural language questions matched with answers corresponding to entities of Freebase : in this case , no triple has to be returned , only a single entity .",Evaluation on WebQuestions,Evaluation on WebQuestions,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.3076923076923077,241,0.9341085271317828,4,0.3076923076923077,1,0,Evaluation on WebQuestions
246,"Top - 1 and Top - 10 are computed on questions for which the system returned at least one answer ( around 1,000 questions using string matching ) , while F1 is computed for all questions .",Evaluation on WebQuestions,Evaluation on WebQuestions,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.6153846153846154,245,0.9496124031007752,8,0.6153846153846154,1,0,Evaluation on WebQuestions
247,"Of course , performance is not great and can not be directly compared with that of the best system reported in ( more than 0.30 of F1 ) .",Evaluation on WebQuestions,Evaluation on WebQuestions,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.6923076923076923,246,0.9534883720930232,9,0.6923076923076923,1,0,Evaluation on WebQuestions
248,"One of the main reasons is that most questions of WebQuestions , such as Who was vice - president after Kennedy died ? , should be represented by multiple triples , a setting for which our system has not been designed .",Evaluation on WebQuestions,Evaluation on WebQuestions,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.7692307692307693,247,0.9573643410852714,10,0.7692307692307693,1,0,Evaluation on WebQuestions
250,"Besides , evaluation is broad since , in ReVerb , most entities actually appear many times under different names as explained in Section 3 .",Evaluation on WebQuestions,Evaluation on WebQuestions,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.9230769230769232,249,0.9651162790697676,12,0.9230769230769232,1,0,Evaluation on WebQuestions
251,"Hence , there might be higher ranked answers but they are missed by our evaluation script .",Evaluation on WebQuestions,Evaluation on WebQuestions,question-answering,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,1.0,250,0.9689922480620154,13,1.0,1,0,Evaluation on WebQuestions
252,Conclusion,,,question-answering,0,['O'],['O'],0,0.0,251,0.9728682170542636,0,0.0,1,0,
2,Convolutional Neural Network Architectures for Matching Natural Language Sentences,title,,question-answering,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0,1,0.005181347150259,1,0.0,1,0,title
3,abstract,,,question-answering,1,['O'],['O'],0,0.0,2,0.0103626943005181,0,0.0,1,0,
4,"Semantic matching is of central importance to many natural language tasks [ 2,28 ] .",abstract,abstract,question-answering,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.1666666666666666,3,0.0155440414507772,1,0.1666666666666666,1,0,abstract
5,A successful matching algorithm needs to adequately model the internal structures of language objects and the interaction between them .,abstract,abstract,question-answering,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.3333333333333333,4,0.0207253886010362,2,0.3333333333333333,1,0,abstract
13,"Examples include top -k re-ranking in machine translation ( e.g. , comparing the meanings of a French sentence and an English sentence ) and dialogue ( e.g. , evaluating the appropriateness of a response to a given utterance ) .",Introduction,Introduction,question-answering,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.1875,12,0.0621761658031088,3,0.1875,1,0,Introduction
14,"Natural language sentences have complicated structures , both sequential and hierarchical , that are essential for understanding them .",Introduction,Introduction,question-answering,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.25,13,0.0673575129533678,4,0.25,1,0,Introduction
15,A successful sentence - matching algorithm therefore needs to capture not only the internal structures of sentences but also the rich patterns in their interactions .,Introduction,Introduction,question-answering,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.3125,14,0.0725388601036269,5,0.3125,1,0,Introduction
20,Our main contributions can be summarized as follows .,Introduction,Introduction,question-answering,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.625,19,0.0984455958549222,10,0.625,1,0,Introduction
31,Convolution,Convolutional Sentence Model,,question-answering,1,['O'],['O'],4,0.125,30,0.155440414507772,4,0.2222222222222222,1,0,Convolutional Sentence Model
32,"As shown in , the convolution in Layer - 1 operates on sliding windows of words ( width k 1 ) , and the convolutions in deeper layers are defined in a similar way .",Convolutional Sentence Model,Convolution,question-answering,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.15625,31,0.160621761658031,5,0.2777777777777778,1,0,Convolutional Sentence Model: Convolution
33,"Generally , with sentence input x , the convolution unit for feature map of type -f ( among F of them ) on Layer - is",Convolutional Sentence Model,Convolution,question-answering,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.1875,32,0.1658031088082901,6,0.3333333333333333,1,0,Convolutional Sentence Model: Convolution
34,and it s matrix form is z,Convolutional Sentence Model,Convolution,question-answering,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.21875,33,0.1709844559585492,7,0.3888888888888889,1,0,Convolutional Sentence Model: Convolution
35,gives the output of feature map of type - f for location i in Layer - ;,Convolutional Sentence Model,Convolution,question-answering,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.25,34,0.1761658031088082,8,0.4444444444444444,1,0,Convolutional Sentence Model: Convolution
36,"w ( , f ) is the parameters for f on Layer - , with matrix form",Convolutional Sentence Model,Convolution,question-answering,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.28125,35,0.1813471502590673,9,0.5,1,0,Convolutional Sentence Model: Convolution
37,"? ( ) is the activation function ( e.g. , Sigmoid or Relu )",Convolutional Sentence Model,Convolution,question-answering,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.3125,36,0.1865284974093264,10,0.5555555555555556,1,0,Convolutional Sentence Model: Convolution
38,?,Convolutional Sentence Model,Convolution,question-answering,1,['O'],['O'],11,0.34375,37,0.1917098445595854,11,0.6111111111111112,1,0,Convolutional Sentence Model: Convolution
39,( ?1 ) i denotes the segment of Layer - ?,Convolutional Sentence Model,Convolution,question-answering,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.375,38,0.1968911917098445,12,0.6666666666666666,1,0,Convolutional Sentence Model: Convolution
40,"1 for the convolution at location i , whil",Convolutional Sentence Model,Convolution,question-answering,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.40625,39,0.2020725388601036,13,0.7222222222222222,1,0,Convolutional Sentence Model: Convolution
41,concatenates the vectors fork 1 ( width of sliding window ) words from sentence input x .,Convolutional Sentence Model,Convolution,question-answering,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.4375,40,0.2072538860103626,14,0.7777777777777778,1,0,Convolutional Sentence Model: Convolution
47,The variable length of sentences in a fairly broad range can be readily handled with the convolution and pooling strategy .,Convolutional Sentence Model,Length Variability,question-answering,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.625,46,0.238341968911917,1,0.0833333333333333,1,0,Convolutional Sentence Model: Length Variability
49,"To eliminate the boundary effect caused by the great variability of sentence lengths , we add to the convolutional unit agate which sets the output vectors to all - zeros if the input is all zeros .",Convolutional Sentence Model,Length Variability,question-answering,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.6875,48,0.2487046632124352,3,0.25,1,0,Convolutional Sentence Model: Length Variability
50,"For any given sentence input x , the output of type - f filter for location i in the th layer is given by",Convolutional Sentence Model,Length Variability,question-answering,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",23,0.71875,49,0.2538860103626943,4,0.3333333333333333,1,0,Convolutional Sentence Model: Length Variability
51,"( 2 ) where g ( v ) = 0 if all the elements in vector v equals 0 , otherwise g ( v ) = 1 . This gate , working with max - pooling and positive activation function ( e.g. , Sigmoid ) , keeps away the artifacts from padding in all layers .",Convolutional Sentence Model,Length Variability,question-answering,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.75,50,0.2590673575129533,5,0.4166666666666667,1,0,Convolutional Sentence Model: Length Variability
52,"Actually it creates a natural hierarchy of all - zero padding ( as illustrated in ) , consisting of nodes in the neural net that would not contribute in the forward process ( as in prediction ) and backward propagation ( as in learning ) .",Convolutional Sentence Model,Length Variability,question-answering,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.78125,51,0.2642487046632124,6,0.5,1,0,Convolutional Sentence Model: Length Variability
54,"gives an example on what could happen on the first two layers with input sentence "" The cat sat on the mat "" .",Convolutional Sentence Model,Length Variability,question-answering,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.84375,53,0.2746113989637305,8,0.6666666666666666,1,0,Convolutional Sentence Model: Length Variability
56,"For example , some feature maps ( group 2 ) give compositions for "" the cat "" and "" cat sat "" , each being a vector .",Convolutional Sentence Model,Length Variability,question-answering,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",29,0.90625,55,0.2849740932642487,10,0.8333333333333334,1,0,Convolutional Sentence Model: Length Variability
57,"Different feature maps offer a variety of compositions , with confidence encoded in the values ( color coded in output of convolution layer in ) .",Convolutional Sentence Model,Length Variability,question-answering,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",30,0.9375,56,0.2901554404145077,11,0.9166666666666666,1,0,Convolutional Sentence Model: Length Variability
58,"The pooling then chooses , for each composition type , between two adjacent sliding windows , e.g. , between "" on the "" and "" the mat "" for feature maps group 2 from the rightmost two sliding windows .",Convolutional Sentence Model,Length Variability,question-answering,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",31,0.96875,57,0.2953367875647668,12,1.0,1,0,Convolutional Sentence Model: Length Variability
62,"First , unlike RAE , it does not take a single path of word / phrase composition determined either by a separate gating function , an external parser , or just natural sequential order .",Relation to Recursive Models,Relation to Recursive Models,question-answering,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.1666666666666666,61,0.3160621761658031,3,0.2307692307692307,1,0,Relation to Recursive Models
64,"With any window width k ? 3 , the type of composition would be much richer than that of RAE .",Relation to Recursive Models,Relation to Recursive Models,question-answering,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.3333333333333333,63,0.3264248704663212,5,0.3846153846153846,1,0,Relation to Recursive Models
66,"However , unlike recursive models , the convolutional architecture has a fixed depth , which bounds the level of composition it could do .",Relation to Recursive Models,Relation to Recursive Models,question-answering,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.5,65,0.3367875647668393,7,0.5384615384615384,1,0,Relation to Recursive Models
67,"For tasks like matching , this limitation can be largely compensated with a network afterwards that can take a "" global "" synthesis on the learned sentence representation .",Relation to Recursive Models,Relation to Recursive Models,question-answering,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.5833333333333334,66,0.3419689119170984,8,0.6153846153846154,1,0,Relation to Recursive Models
70,"This type of models , with local convolutions and a global pooling , essentially do a "" soft "" local template matching and is able to detect local features useful fora certain task .",Relation to Recursive Models,"Relation to "" Shallow "" Convolutional Models",question-answering,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.8333333333333334,69,0.3575129533678756,11,0.8461538461538461,1,0,"Relation to Recursive Models: Relation to "" Shallow "" Convolutional Models"
71,"Since the sentencelevel sequential order is inevitably lost in the global pooling , the model is incapable of modeling more complicated structures .",Relation to Recursive Models,"Relation to "" Shallow "" Convolutional Models",question-answering,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.9166666666666666,70,0.3626943005181347,12,0.9230769230769232,1,0,"Relation to Recursive Models: Relation to "" Shallow "" Convolutional Models"
72,It is not hard to see that our convolutional model degenerates to the SENNA - type architecture if we limit the number of layers to be two and set the pooling window infinitely large .,Relation to Recursive Models,"Relation to "" Shallow "" Convolutional Models",question-answering,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,1.0,71,0.3678756476683937,13,1.0,1,0,"Relation to Recursive Models: Relation to "" Shallow "" Convolutional Models"
76,"Architecture - I ( ARC - I ) , as illustrated in , takes a conventional approach :",Convolutional Matching Models,Convolutional Matching Models,question-answering,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.09375,75,0.38860103626943,3,0.0681818181818181,1,0,Convolutional Matching Models
78,"It is essentially the Siamese architecture introduced in , which has been applied to different tasks as a nonlinear similarity function .",Convolutional Matching Models,Convolutional Matching Models,question-answering,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.15625,77,0.3989637305699481,5,0.1136363636363636,1,0,Convolutional Matching Models
79,"Although ARC - I enjoys the flexibility brought by the convolutional sentence model , it suffers from a drawback inherited from the Siamese architecture : it defers the interaction between two sentences ( in the final MLP ) to until their individual representation matures ( in the convolution model ) , therefore runs at the risk of losing details ( e.g. , a city name ) important for the matching task in representing the sentences .",Convolutional Matching Models,Convolutional Matching Models,question-answering,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.1875,78,0.4041450777202072,6,0.1363636363636363,1,0,Convolutional Matching Models
80,"In other words , in the forward phase ( prediction ) , the representation of each sentence is formed without knowledge of each other .",Convolutional Matching Models,Convolutional Matching Models,question-answering,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.21875,79,0.4093264248704663,7,0.1590909090909091,1,0,Convolutional Matching Models
81,"This can not be adequately circumvented in backward phase ( learning ) , when the convolutional model learns to extract structures informative for matching on a population level .",Convolutional Matching Models,Convolutional Matching Models,question-answering,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.25,80,0.4145077720207253,8,0.1818181818181818,1,0,Convolutional Matching Models
86,"For segment ion S X and segment j on S Y , we have the feature map",Convolutional Matching Models,Convolutional Matching Models,question-answering,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.40625,85,0.4404145077720207,13,0.2954545454545454,1,0,Convolutional Matching Models
87,where ?,Convolutional Matching Models,Convolutional Matching Models,question-answering,1,"['O', 'O']","['O', 'O']",14,0.4375,86,0.4455958549222797,14,0.3181818181818182,1,0,Convolutional Matching Models
88,"i , j ? R 2 k1 De simply concatenates the vectors for sentence segments for S X and S Y : z",Convolutional Matching Models,Convolutional Matching Models,question-answering,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.46875,87,0.4507772020725388,15,0.3409090909090909,1,0,Convolutional Matching Models
89,.,Convolutional Matching Models,Convolutional Matching Models,question-answering,1,['O'],['O'],16,0.5,88,0.4559585492227979,16,0.3636363636363636,1,0,Convolutional Matching Models
91,"After that in Layer - 2 , it performs a 2D max - pooling in non-overlapping 2 2 windows ( illustrated in )",Convolutional Matching Models,Convolutional Matching Models,question-answering,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.5625,90,0.466321243523316,18,0.4090909090909091,1,0,Convolutional Matching Models
92,"( 4 ) In Layer - 3 , we perform a 2D convolution on k 3 k 3 windows of output from Layer - 2 :",Convolutional Matching Models,Convolutional Matching Models,question-answering,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.59375,91,0.4715025906735751,19,0.4318181818181818,1,0,Convolutional Matching Models
94,"This could goon for more layers of 2D convolution and 2D max - pooling , analogous to that of convolutional architecture for image input .",Convolutional Matching Models,Convolutional Matching Models,question-answering,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.65625,93,0.4818652849740932,21,0.4772727272727273,1,0,Convolutional Matching Models
95,The 2D - Convolution,Convolutional Matching Models,,question-answering,1,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",22,0.6875,94,0.4870466321243523,22,0.5,1,0,Convolutional Matching Models
96,"After the first convolution , we obtain a low level representation of the interaction between the two sentences , and from then we obtain a high level representation z ( ) i , j which encodes the information from both sentences .",Convolutional Matching Models,The 2D - Convolution,question-answering,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",23,0.71875,95,0.4922279792746113,23,0.5227272727272727,1,0,Convolutional Matching Models: The 2D - Convolution
97,The general two - dimensional convolution is formulated as z ( ),Convolutional Matching Models,The 2D - Convolution,question-answering,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.75,96,0.4974093264248704,24,0.5454545454545454,1,0,Convolutional Matching Models: The 2D - Convolution
98,where ?,Convolutional Matching Models,The 2D - Convolution,question-answering,1,"['O', 'O']","['O', 'O']",25,0.78125,97,0.5025906735751295,25,0.5681818181818182,1,0,Convolutional Matching Models: The 2D - Convolution
99,concatenates the corresponding vectors from its 2 D receptive field in Layer -?1 .,Convolutional Matching Models,The 2D - Convolution,question-answering,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.8125,98,0.5077720207253886,26,0.5909090909090909,1,0,Convolutional Matching Models: The 2D - Convolution
100,"This pooling has different mechanism as in the 1D case , for it selects not only among compositions on different segments but also among different local matchings .",Convolutional Matching Models,The 2D - Convolution,question-answering,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.84375,99,0.5129533678756477,27,0.6136363636363636,1,0,Convolutional Matching Models: The 2D - Convolution
101,"This pooling strategy resembles the dynamic pooling in in a similarity learning context , but with two distinctions :",Convolutional Matching Models,The 2D - Convolution,question-answering,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",28,0.875,100,0.5181347150259067,28,0.6363636363636364,1,0,Convolutional Matching Models: The 2D - Convolution
102,1 ) it happens on a fixed architecture and 2 ) it has much richer structure than just similarity .,Convolutional Matching Models,The 2D - Convolution,question-answering,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",29,0.90625,101,0.5233160621761658,29,0.6590909090909091,1,0,Convolutional Matching Models: The 2D - Convolution
103,"contains information about the words in S X before those in z ( ) i + 1 , j , although they maybe generated with slightly different segments in S Y , due to the 2D pooling ( illustrated in .",Convolutional Matching Models,The 2D - Convolution,question-answering,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",30,0.9375,102,0.5284974093264249,30,0.6818181818181818,1,0,Convolutional Matching Models: The 2D - Convolution
104,"The orders is however retained in a "" conditional "" sense .",Convolutional Matching Models,The 2D - Convolution,question-answering,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",31,0.96875,103,0.533678756476684,31,0.7045454545454546,1,0,Convolutional Matching Models: The 2D - Convolution
106,Model Generality,,,question-answering,1,"['O', 'O']","['O', 'O']",0,0.0,105,0.5440414507772021,33,0.75,1,0,
107,It is not hard to show that ARC - II actually subsumes ARC - I as a special case .,Model Generality,Model Generality,question-answering,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0909090909090909,106,0.5492227979274611,34,0.7727272727272727,1,0,Model Generality
108,"Indeed , in ARC - II if we choose ( by turning off some parameters in W ( , ) ) to keep the representations of the two sentences separated until the final MLP , ARC - II can actually act fully like ARC - I , as illustrated in .",Model Generality,Model Generality,question-answering,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.1818181818181818,107,0.5544041450777202,35,0.7954545454545454,1,0,Model Generality
109,"More specifically , if we let the feature maps in the first convolution layer to be either devoted to S X or devoted to S Y ( instead of taking both as in general case ) , the output of each segment - pair is naturally divided into two corresponding groups .",Model Generality,Model Generality,question-answering,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.2727272727272727,108,0.5595854922279793,36,0.8181818181818182,1,0,Model Generality
110,"As a result , the output for each filter f , denoted z",Model Generality,Model Generality,question-answering,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.3636363636363636,109,0.5647668393782384,37,0.8409090909090909,1,0,Model Generality
111,"( 1 , f ) 1:n , 1:n ( n is the number of sliding windows ) , will be of rank - one , possessing essentially the same information as the result of the first convolution layer in ARC - I .",Model Generality,Model Generality,question-answering,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.4545454545454545,110,0.5699481865284974,38,0.8636363636363636,1,0,Model Generality
112,"Clearly the 2D pooling that follows will reduce to 1 D pooling , with this separateness preserved .",Model Generality,Model Generality,question-answering,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.5454545454545454,111,0.5751295336787565,39,0.8863636363636364,1,0,Model Generality
113,"If we further limit the parameters in the second convolution units ( more specifically w ( 2 , f ) ) to those for S X and S Y , we can ensure the individual development of different levels of abstraction on each side , and fully recover the functionality of ARC - I .",Model Generality,Model Generality,question-answering,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.6363636363636364,112,0.5803108808290155,40,0.9090909090909092,1,0,Model Generality
117,This intuition is verified by the superior performance of ARC - II in experiments ( Section 5 ) on different matching tasks .,Model Generality,Model Generality,question-answering,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,1.0,116,0.6010362694300518,44,1.0,1,0,Model Generality
118,Training,,,question-answering,1,['O'],['O'],0,0.0,117,0.6062176165803109,0,0.0,1,0,
120,"Suppose that we are given the following triples ( x , y + , y ? )",Training,Training,question-answering,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.1111111111111111,119,0.616580310880829,2,0.1111111111111111,1,0,Training
121,"from the oracle , with x matched with y + better than with y ? .",Training,Training,question-answering,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.1666666666666666,120,0.6217616580310881,3,0.1666666666666666,1,0,Training
123,"where s ( x , y) is predicted matching score for ( x , y ) , and ?",Training,Training,question-answering,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.2777777777777778,122,0.6321243523316062,5,0.2777777777777778,1,0,Training
126,The gating function ( see Section 2 ) can be easily adopted into the gradient by discounting the contribution from convolution units that have been turned off by the gating function .,Training,Training,question-answering,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.4444444444444444,125,0.6476683937823834,8,0.4444444444444444,1,0,Training
137,Experiments,,,question-answering,1,['O'],['O'],0,0.0,136,0.7046632124352331,0,0.0,1,0,
139,"Among them , the first two tasks ( namely , Sentence Completion and Tweet - Response Matching ) are about matching of language objects of heterogenous natures , while the third one ( paraphrase identification ) is a natural example of matching homogeneous objects .",Experiments,Experiments,question-answering,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.6666666666666666,138,0.7150259067357513,2,0.6666666666666666,1,0,Experiments
141,Competitor Methods,,,question-answering,1,"['O', 'O']","['O', 'O']",0,0.0,140,0.7253886010362695,0,0.0,1,0,
144,"We use the Unfolding Recursive Autoencoder to get a 100 dimensional vector representation of each sentence , and put an MLP on the top as in WORDEMBED ; SENNA + MLP / SIM :",Competitor Methods,Competitor Methods,question-answering,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.5,143,0.7409326424870466,3,0.1,1,0,Competitor Methods
149,Completion,Experiment I : Sentence,,question-answering,1,['O'],['O'],1,0.0303030303030303,148,0.7668393782383419,8,0.2666666666666666,1,0,Experiment I : Sentence
151,"Basically , we take a sentence from Reuterswith two "" balanced "" clauses ( with 8 ? 28 words ) divided by one comma , and use the first clause as S X and the second as S Y .",Experiment I : Sentence,Completion,question-answering,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.0909090909090909,150,0.7772020725388601,10,0.3333333333333333,1,0,Experiment I : Sentence: Completion
152,The task is then to recover the original second clause for any given first clause .,Experiment I : Sentence,Completion,question-answering,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.1212121212121212,151,0.7823834196891192,11,0.3666666666666666,1,0,Experiment I : Sentence: Completion
153,The matching here is considered heterogeneous since the relation between the two is nonsymmetrical on both lexical and semantic levels .,Experiment I : Sentence,Completion,question-answering,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.1515151515151515,152,0.7875647668393783,12,0.4,1,0,Experiment I : Sentence: Completion
154,"We deliberately make the task harder by using negative second clauses similar to the original ones 4 , both in training and testing .",Experiment I : Sentence,Completion,question-answering,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.1818181818181818,153,0.7927461139896373,13,0.4333333333333333,1,0,Experiment I : Sentence: Completion
155,One representative example is given as follows :,Experiment I : Sentence,Completion,question-answering,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.2121212121212121,154,0.7979274611398963,14,0.4666666666666667,1,0,Experiment I : Sentence: Completion
160,"It is a bit surprising that URAE comes last on this task , which might be caused by the facts that 1 ) the representation model ( including word - embedding ) is not trained on Reuters , and 2 ) the split - sentence setting hurts the parsing , which is vital to the quality of learned sentence representation ..",Experiment I : Sentence,Completion,question-answering,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.3636363636363636,159,0.8238341968911918,19,0.6333333333333333,1,0,Experiment I : Sentence: Completion
161,"This task is slightly easier than Experiment I , with more training instances and purely random negatives .",Experiment I : Sentence,Completion,question-answering,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.3939393939393939,160,0.8290155440414507,20,0.6666666666666666,1,0,Experiment I : Sentence: Completion
162,"It requires less about the grammatical rigor but more on detailed modeling of loose and local matching patterns ( e.g. , work - overtime ? rest ) .",Experiment I : Sentence,Completion,question-answering,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.4242424242424242,161,0.8341968911917098,21,0.7,1,0,Experiment I : Sentence: Completion
164,Experiment II : Matching A Response to A Tweet,Experiment I : Sentence,,question-answering,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.4848484848484848,163,0.844559585492228,23,0.7666666666666667,1,0,Experiment I : Sentence
166,"Paraphrase identification aims to determine whether two sentences have the same meaning , a problem considered a touchstone of natural language understanding .",Experiment I : Sentence,Experiment II : Matching A Response to A Tweet,question-answering,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.5454545454545454,165,0.8549222797927462,25,0.8333333333333334,1,0,Experiment I : Sentence: Experiment II : Matching A Response to A Tweet
170,"As stated earlier , our model is not specially tailored for modeling synonymy , and generally requires ? 100K instances to work favorably .",Experiment I : Sentence,Experiment II : Matching A Response to A Tweet,question-answering,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.6666666666666666,169,0.8756476683937824,29,0.9666666666666668,1,0,Experiment I : Sentence: Experiment II : Matching A Response to A Tweet
172,Discussions,Experiment I : Sentence,,question-answering,1,['O'],['O'],24,0.7272727272727273,171,0.8860103626943006,0,0.0,1,0,Experiment I : Sentence
174,"It s superiority over ARC - I , however , is less salient when the sentences have deep grammatical structures and the matching relies lesson the local matching patterns , as in Experiment - I .",Experiment I : Sentence,Discussions,question-answering,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.7878787878787878,173,0.8963730569948186,2,0.2222222222222222,1,0,Experiment I : Sentence: Discussions
175,"This therefore raises the interesting question about how to balance the representation of matching and the representations of objects , and whether we can guide the learning process through something like curriculum learning .",Experiment I : Sentence,Discussions,question-answering,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.8181818181818182,174,0.9015544041450776,3,0.3333333333333333,1,0,Experiment I : Sentence: Discussions
181,This is in contrast with other bag - of - words models like DEEPMATCH .,Experiment I : Sentence,Discussions,question-answering,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",33,1.0,180,0.932642487046632,9,1.0,1,0,Experiment I : Sentence: Discussions
182,Related Work,,,question-answering,1,"['O', 'O']","['O', 'O']",0,0.0,181,0.9378238341968912,0,0.0,1,0,
191,Conclusion,,,question-answering,1,['O'],['O'],0,0.0,190,0.9844559585492229,0,0.0,1,0,
2,Large - scale Simple Question Answering with Memory Networks,title,,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0,1,0.0036630036630036,1,0.0,1,0,title
3,abstract,,,question-answering,2,['O'],['O'],0,0.0,2,0.0073260073260073,0,0.0,1,0,
4,Training large - scale question answering systems is complicated because training sources usually cover a small portion of the range of possible questions .,abstract,abstract,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.25,3,0.0109890109890109,1,0.25,1,0,abstract
9,"Open-domain Question Answering ( QA ) systems aim at providing the exact answer ( s ) to questions formulated in natural language , without restriction of domain .",Introduction,Introduction,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0066666666666666,8,0.0293040293040293,1,0.0416666666666666,1,0,Introduction
10,"While there is along history of QA systems that search for textual documents or on the Web and extract answers from them ( see e.g. ) , recent progress has been made with the release of large Knowledge Bases ( KBs ) such as Freebase , which contain consolidated knowledge stored as atomic facts , and extracted from different sources , such as free text , tables in webpages or collaborative input .",Introduction,Introduction,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.0133333333333333,9,0.0329670329670329,2,0.0833333333333333,1,0,Introduction
12,"However , while most recent efforts have focused on designing systems with higher reasoning capabilities , that could jointly retrieve and use multiple facts to answer , the simpler problem of answering questions that refer to a single fact of the KB , which we call Simple Question Answering in this paper , is still far from solved .",Introduction,Introduction,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.0266666666666666,11,0.0402930402930402,4,0.1666666666666666,1,0,Introduction
13,"Hence , existing benchmarks are small ; they mostly cover the head of the distributions of facts , and are restricted in their question types and their syntactic and lexical variations .",Introduction,Introduction,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.0333333333333333,12,0.0439560439560439,5,0.2083333333333333,1,0,Introduction
14,"As such , it is still unknown how much the existing systems perform outside the range of the specific question templates of a few , small benchmark datasets , and it is also unknown whether learning on a single dataset transfers well on other ones , and whether such systems can learn from different training sources , which we believe is necessary to capture the whole range of possible questions .",Introduction,Introduction,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.04,13,0.0476190476190476,6,0.25,1,0,Introduction
15,"Besides , the actual need for reasoning , i.e. constructing the answer from more than a single fact from the KB , depends on the actual structure of the KB .",Introduction,Introduction,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.0466666666666666,14,0.0512820512820512,7,0.2916666666666667,1,0,Introduction
16,"As we shall see , for instance , a simple preprocessing of Freebase tremendously increases the coverage of simple QA in terms of possible questions that can be answered with a single fact , including list questions that expect more than a single answer .",Introduction,Introduction,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.0533333333333333,15,0.0549450549450549,8,0.3333333333333333,1,0,Introduction
17,"In fact , the task of simple QA itself might already cover a wide range of practical usages , if the KB is properly organized .",Introduction,Introduction,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.06,16,0.0586080586080586,9,0.375,1,0,Introduction
20,"This dataset , which is presented in Section 2 , contains more than 100 k questions written by human anno-What American cartoonist is the creator of Andy Lippincott ?",Introduction,Introduction,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.08,19,0.0695970695970696,12,0.5,1,0,Introduction
21,Which forest is Fires Creek in ?,Introduction,Introduction,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.0866666666666666,20,0.0732600732600732,13,0.5416666666666666,1,0,Introduction
23,"tators and associated to Freebase facts , while the largest existing benchmark , WebQuestions , contains less than 6 k questions created automatically using the Google suggest API .",Introduction,Introduction,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.1,22,0.0805860805860805,15,0.625,1,0,Introduction
25,"Memory Networks are learning systems centered around a memory component that can be read and written to , with a particular focus on cases where the relationship between the input and response languages ( here natural language ) and the storage language ( here , the facts from KBs ) is performed by embedding all of them in the same vector space .",Introduction,Introduction,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.1133333333333333,24,0.0879120879120879,17,0.7083333333333334,1,0,Introduction
26,The setting of the simple QA corresponds to the elementary operation of performing a single lookup in the memory .,Introduction,Introduction,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.12,25,0.0915750915750915,18,0.75,1,0,Introduction
33,Simple Question Answering,Introduction,,question-answering,2,"['O', 'O', 'O']","['O', 'O', 'O']",25,0.1666666666666666,32,0.1172161172161172,0,0.0,1,0,Introduction
34,Knowledge Bases contain facts expressed as triples where subject and object are entities and relationship describes the type of ( directed ) link between these entities .,Introduction,Simple Question Answering,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.1733333333333333,33,0.1208791208791208,1,0.1,1,0,Introduction: Simple Question Answering
35,"The simple QA prob - lem we address here consist in finding the answer to questions that can be rephrased as queries of the form , asking for all objects linked to subject by relationship .",Introduction,Simple Question Answering,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.18,34,0.1245421245421245,2,0.2,1,0,Introduction: Simple Question Answering
36,The question,Introduction,,question-answering,2,"['O', 'O']","['O', 'O']",28,0.1866666666666666,35,0.1282051282051282,3,0.3,1,0,Introduction
37,"What do Jamaican people speak ? , for instance , could be rephrased as the Freebase query ( jamaica , language spoken , ? ) .",Introduction,The question,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",29,0.1933333333333333,36,0.1318681318681318,4,0.4,1,0,Introduction: The question
38,"In other words , fetching a single fact from a KB is sufficient to answer correctly .",Introduction,The question,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",30,0.2,37,0.1355311355311355,5,0.5,1,0,Introduction: The question
39,"The term simple QA refers to the simplicity of the reasoning process needed to answer questions , since it involves a single fact .",Introduction,The question,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",31,0.2066666666666666,38,0.1391941391941392,6,0.6,1,0,Introduction: The question
40,"However , this does not mean that the QA problem is easy per se , since retrieving this single supporting fact can be very challenging as it involves to search over millions of alternatives given a query expressed in natural language .",Introduction,The question,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",32,0.2133333333333333,39,0.1428571428571428,7,0.7,1,0,Introduction: The question
41,"shows that , with a KB with many types of relationships like",Introduction,The question,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",33,0.22,40,0.1465201465201465,8,0.8,1,0,Introduction: The question
42,"Freebase , the range of questions that can be answered with a single fact is already very broad .",Introduction,The question,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",34,0.2266666666666666,41,0.1501831501831501,9,0.9,1,0,Introduction: The question
43,"Besides , as we shall see , modiying slightly the structure of the KB can make some QA problems simpler by adding direct connections between entities and hence allow to bypass the need for more complex reasoning .",Introduction,The question,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",35,0.2333333333333333,42,0.1538461538461538,10,1.0,1,0,Introduction: The question
44,Knowledge Bases,Introduction,,question-answering,2,"['O', 'O']","['O', 'O']",36,0.24,43,0.1575091575091575,0,0.0,1,0,Introduction
48,"Each entity has an internal identifier and a set of strings that are usually used to refer to that entity in text , termed aliases .",Introduction,Knowledge Bases,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",40,0.2666666666666666,47,0.1721611721611721,4,0.3076923076923077,1,0,Introduction: Knowledge Bases
50,"FB2M , which was used in , contains about 2 M entities and 5 k relationships .",Introduction,Knowledge Bases,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",42,0.28,49,0.1794871794871795,6,0.4615384615384615,1,0,Introduction: Knowledge Bases
51,"FB5M , is much larger with about 5 M entities and more than 7.5 k relationships .",Introduction,Knowledge Bases,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",43,0.2866666666666667,50,0.1831501831501831,7,0.5384615384615384,1,0,Introduction: Knowledge Bases
53,This is a pure setting of transfer learning .,Introduction,Knowledge Bases,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",45,0.3,52,0.1904761904761904,9,0.6923076923076923,1,0,Introduction: Knowledge Bases
54,Reverb is interesting for this experiment because it differs a lot from Freebase .,Introduction,Knowledge Bases,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",46,0.3066666666666666,53,0.1941391941391941,10,0.7692307692307693,1,0,Introduction: Knowledge Bases
55,Its data was extracted automatically from text with minimal human intervention and is highly unstructured : entities are unique strings and the lexicon for relationships is open .,Introduction,Knowledge Bases,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",47,0.3133333333333333,54,0.1978021978021978,11,0.8461538461538461,1,0,Introduction: Knowledge Bases
56,"This leads to many more relationships , but entities with multiple references are not deduplicated , ambiguous referents are not resolved , and the reliability of the stored facts is much lower than in Freebase .",Introduction,Knowledge Bases,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",48,0.32,55,0.2014652014652014,12,0.9230769230769232,1,0,Introduction: Knowledge Bases
58,The SimpleQuestions dataset,Introduction,Knowledge Bases,question-answering,2,"['O', 'O', 'O']","['O', 'O', 'O']",50,0.3333333333333333,57,0.2087912087912088,0,0.0,1,0,Introduction: Knowledge Bases
59,"Existing resources for QA such as WebQuestions are rather small ( few thousands questions ) and hence do not provide a very thorough coverage of the variety of questions that could be answered using a KB like Freebase , even in the context of simple QA .",Introduction,Knowledge Bases,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",51,0.34,58,0.2124542124542124,1,0.05,1,0,Introduction: Knowledge Bases
61,2,Introduction,Knowledge Bases,question-answering,2,['O'],['O'],53,0.3533333333333333,60,0.2197802197802197,3,0.15,1,0,Introduction: Knowledge Bases
63,"We randomly shuffle these questions and use 70 % of them ( 75910 ) as training set , 10 % as validation set , and the remaining 20 % as test set .",Introduction,Knowledge Bases,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",55,0.3666666666666666,62,0.2271062271062271,5,0.25,1,0,Introduction: Knowledge Bases
67,We used FB2M as background KB and removed all facts with undefined relationship type i.e. containing the word freebase .,Introduction,Knowledge Bases,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",59,0.3933333333333333,66,0.2417582417582417,9,0.45,1,0,Introduction: Knowledge Bases
68,"We also removed all facts for which the ( subject , relationship ) pair had more than a threshold number of objects .",Introduction,Knowledge Bases,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",60,0.4,67,0.2454212454212454,10,0.5,1,0,Introduction: Knowledge Bases
69,This filtering step is crucial to remove facts,Introduction,Knowledge Bases,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",61,0.4066666666666667,68,0.2490842490842491,11,0.55,1,0,Introduction: Knowledge Bases
70,"2 The dataset is available from http://fb.ai/babi. which would result in trivial uninformative questions , such as , Name a person who is an actor ?.",Introduction,Knowledge Bases,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",62,0.4133333333333333,69,0.2527472527472527,12,0.6,1,0,Introduction: Knowledge Bases
73,"For the sampling , each fact was associated with a probability which defined as a function of its relationship frequency in the KB : to favor variability , facts with relationship appearing more frequently were given lower probabilities .",Introduction,Knowledge Bases,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",65,0.4333333333333333,72,0.2637362637362637,15,0.75,1,0,Introduction: Knowledge Bases
75,"Given this information , annotators were asked to phrase a question involving the subject and the relationship of the fact , with the answer being the object .",Introduction,Knowledge Bases,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",67,0.4466666666666666,74,0.271062271062271,17,0.85,1,0,Introduction: Knowledge Bases
76,"The annotators were explicitly instructed to phrase the question differently as much as possible , if they encounter multiple facts with similar relationship .",Introduction,Knowledge Bases,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",68,0.4533333333333333,75,0.2747252747252747,18,0.9,1,0,Introduction: Knowledge Bases
78,This was very important to avoid the annotators to write a boilerplate questions when they had no background knowledge about some facts .,Introduction,Knowledge Bases,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",70,0.4666666666666667,77,0.282051282051282,20,1.0,1,0,Introduction: Knowledge Bases
80,A Memory Network consists of a memory ( an indexed array of objects ) and a neural network that is trained to query it given some inputs ( usually questions ) .,Introduction,Memory Networks for Simple QA,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",72,0.48,79,0.2893772893772894,1,0.0625,1,0,Introduction: Memory Networks for Simple QA
84,Storing Freebase : this first phase parses,Introduction,Memory Networks for Simple QA,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O']",76,0.5066666666666667,83,0.304029304029304,5,0.3125,1,0,Introduction: Memory Networks for Simple QA
87,2 .,Introduction,Memory Networks for Simple QA,question-answering,2,"['O', 'O']","['O', 'O']",79,0.5266666666666666,86,0.315018315018315,8,0.5,1,0,Introduction: Memory Networks for Simple QA
89,"This uses Input , Output and Response modules , the training concerns mainly the parameters of the embedding model at the core of the Output module .",Introduction,Memory Networks for Simple QA,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",81,0.54,88,0.3223443223443223,10,0.625,1,0,Introduction: Memory Networks for Simple QA
90,Connecting Reverb : this third phase adds new facts coming from,Introduction,Memory Networks for Simple QA,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",82,0.5466666666666666,89,0.326007326007326,11,0.6875,1,0,Introduction: Memory Networks for Simple QA
91,Reverb to the memory .,Introduction,Memory Networks for Simple QA,question-answering,2,"['O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O']",83,0.5533333333333333,90,0.3296703296703296,12,0.75,1,0,Introduction: Memory Networks for Simple QA
94,"After these three stages , the MemNN is ready to answer any question by running the I , O and R modules in turn .",Introduction,Memory Networks for Simple QA,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",86,0.5733333333333334,93,0.3406593406593406,15,0.9375,1,0,Introduction: Memory Networks for Simple QA
96,Input module,Introduction,,question-answering,2,"['O', 'O']","['O', 'O']",88,0.5866666666666667,95,0.3479853479853479,0,0.0,1,0,Introduction
97,This module preprocesses the three types of data that are input to the network :,Introduction,Input module,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",89,0.5933333333333334,96,0.3516483516483517,1,0.5,1,0,Introduction: Input module
98,"Freebase facts that are used to populate the memory , questions that the system need to answer , and Reverb facts that we use , in a second phase , to extend the memory .",Introduction,Input module,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",90,0.6,97,0.3553113553113553,2,1.0,1,0,Introduction: Input module
99,Preprocessing Freebase,Introduction,,question-answering,2,"['O', 'O']","['O', 'O']",91,0.6066666666666667,98,0.358974358974359,0,0.0,1,0,Introduction
100,"The Freebase data is initially stored as atomic facts involving single entities as subject and object , plus a relationship between them .",Introduction,Preprocessing Freebase,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",92,0.6133333333333333,99,0.3626373626373626,1,0.05,1,0,Introduction: Preprocessing Freebase
101,"However , this storage needs to be adapted to the QA task in two aspects .",Introduction,Preprocessing Freebase,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",93,0.62,100,0.3663003663003663,2,0.1,1,0,Introduction: Preprocessing Freebase
102,"First , in order to answer list questions , which expect more than one answer , we redefine a fact as being a triple containing a subject , a relationship , and the set of all objects linked to the subject by the relationship .",Introduction,Preprocessing Freebase,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",94,0.6266666666666667,101,0.36996336996337,3,0.15,1,0,Introduction: Preprocessing Freebase
103,"This grouping process transforms atomic facts into grouped facts , which we simply refer to as facts in the following .",Introduction,Preprocessing Freebase,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",95,0.6333333333333333,102,0.3736263736263736,4,0.2,1,0,Introduction: Preprocessing Freebase
104,"shows the impact of this grouping : on FB2M , this decreases the number of facts from 14 M to 11 M and , on FB5 M , from 22 M to 12M .",Introduction,Preprocessing Freebase,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",96,0.64,103,0.3772893772893773,5,0.25,1,0,Introduction: Preprocessing Freebase
105,"Second , the underlying structure of Freebase is a hypergraph , in which more than two entities can be linked .",Introduction,Preprocessing Freebase,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",97,0.6466666666666666,104,0.3809523809523809,6,0.3,1,0,Introduction: Preprocessing Freebase
108,"To obtain direct links between entities in such cases , we created a single fact for these facts by removing the intermediate node and using the second relationship as the relationship for the new condensed fact .",Introduction,Preprocessing Freebase,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",100,0.6666666666666666,107,0.3919413919413919,9,0.45,1,0,Introduction: Preprocessing Freebase
111,Preprocessing Freebase facts,Introduction,,question-answering,2,"['O', 'O', 'O']","['O', 'O', 'O']",103,0.6866666666666666,110,0.4029304029304029,12,0.6,1,0,Introduction
112,"A fact with k objects y = ( s , r , {o 1 , ... , o k } ) is represented by a bag - of - symbol vector f ( y ) in RN S , where NS is the number of entities and relationships .",Introduction,Preprocessing Freebase facts,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",104,0.6933333333333334,111,0.4065934065934066,13,0.65,1,0,Introduction: Preprocessing Freebase facts
113,Each dimension off ( y ) corresponds to a relationship or an entity ( independent of whether it appears as subject or object ) .,Introduction,Preprocessing Freebase facts,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",105,0.7,112,0.4102564102564102,14,0.7,1,0,Introduction: Preprocessing Freebase facts
114,"The entries of the subject and of the relationship have value 1 , and the entries of the objects are set to 1 / k .",Introduction,Preprocessing Freebase facts,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",106,0.7066666666666667,113,0.4139194139194139,15,0.75,1,0,Introduction: Preprocessing Freebase facts
115,All other entries are 0 .,Introduction,Preprocessing Freebase facts,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O']",107,0.7133333333333334,114,0.4175824175824176,16,0.8,1,0,Introduction: Preprocessing Freebase facts
116,Preprocessing questions,Introduction,,question-answering,2,"['O', 'O']","['O', 'O']",108,0.72,115,0.4212454212454212,17,0.85,1,0,Introduction
117,A question q is mapped to a bag - of - ngrams representation g ( q ) of dimension RN V where NV is the size of the vocabulary .,Introduction,Preprocessing questions,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",109,0.7266666666666667,116,0.4249084249084249,18,0.9,1,0,Introduction: Preprocessing questions
118,"The vocabulary contains all individual words that appear in the questions of our datasets , together with the aliases of Freebase entities , each alias being a single n-gram .",Introduction,Preprocessing questions,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",110,0.7333333333333333,117,0.4285714285714285,19,0.95,1,0,Introduction: Preprocessing questions
119,"The entries of g ( q ) that correspond to words and n-grams of q are equal to 1 , all other ones are set to 0 .",Introduction,Preprocessing questions,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",111,0.74,118,0.4322344322344322,20,1.0,1,0,Introduction: Preprocessing questions
120,Preprocessing Reverb facts,Introduction,,question-answering,2,"['O', 'O', 'O']","['O', 'O', 'O']",112,0.7466666666666667,119,0.4358974358974359,0,0.0,1,0,Introduction
121,"In our experiments with Reverb , each fact y = ( s , r , o) is represented as a vector h ( y ) ?",Introduction,Preprocessing Reverb facts,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",113,0.7533333333333333,120,0.4395604395604395,1,0.25,1,0,Introduction: Preprocessing Reverb facts
122,RN S +N V .,Introduction,Preprocessing Reverb facts,question-answering,2,"['O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O']",114,0.76,121,0.4432234432234432,2,0.5,1,0,Introduction: Preprocessing Reverb facts
123,"This vector is a bagof - symbol for the subject sand the object o , and a bag - of - words for the relationship r.",Introduction,Preprocessing Reverb facts,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",115,0.7666666666666667,122,0.4468864468864469,3,0.75,1,0,Introduction: Preprocessing Reverb facts
124,"The exact composition of h is provided by the Generalization module , which we describe now .",Introduction,Preprocessing Reverb facts,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",116,0.7733333333333333,123,0.4505494505494505,4,1.0,1,0,Introduction: Preprocessing Reverb facts
125,Generalization module,Introduction,,question-answering,2,"['O', 'O']","['O', 'O']",117,0.78,124,0.4542124542124542,0,0.0,1,0,Introduction
127,"In our case , the memory has a multigraph structure where each node is a Freebase entity and labeled arcs in the multigraph are Freebase relationships : after their preprocessing , all Freebase facts are stored using this structure .",Introduction,Generalization module,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",119,0.7933333333333333,126,0.4615384615384615,2,0.1818181818181818,1,0,Introduction: Generalization module
128,"We also consider the case where new facts , with a different structure ( i.e. new kinds of relationship ) , are provided to the MemNNs by using Reverb .",Introduction,Generalization module,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",120,0.8,127,0.4652014652014652,3,0.2727272727272727,1,0,Introduction: Generalization module
129,"In this case , the generalization module is then used to connect Reverb facts to the Freebase - based memory structure , in order to make them usable and searchable by the MemNN .",Introduction,Generalization module,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",121,0.8066666666666666,128,0.4688644688644688,4,0.3636363636363636,1,0,Introduction: Generalization module
131,"If such links do not give any result for an entity , we search for Freebase entities with at least one alias that matches the Reverb entity string .",Introduction,Generalization module,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",123,0.82,130,0.4761904761904761,6,0.5454545454545454,1,0,Introduction: Generalization module
133,"The remainder of entities were encoded using bag - of - words representation of their strings , since we had no other way of matching them to Freebase entities .",Introduction,Generalization module,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",125,0.8333333333333334,132,0.4835164835164835,8,0.7272727272727273,1,0,Introduction: Generalization module
136,We can then hope that what had been learned there could also be successfully used to query Reverb facts .,Introduction,Generalization module,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",128,0.8533333333333334,135,0.4945054945054945,11,1.0,1,0,Introduction: Generalization module
137,Output module,Introduction,,question-answering,2,"['O', 'O']","['O', 'O']",129,0.86,136,0.4981684981684982,0,0.0,1,0,Introduction
138,The output module performs the memory lookups given the input to return the supporting facts destined to eventually provide the answer given a question .,Introduction,Output module,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",130,0.8666666666666667,137,0.5018315018315018,1,0.25,1,0,Introduction: Output module
139,"In our case of simple QA , this module only returns a single supporting fact .",Introduction,Output module,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",131,0.8733333333333333,138,0.5054945054945055,2,0.5,1,0,Introduction: Output module
141,The supporting fact is the candidate fact that is most similar to the question according to an embedding model .,Introduction,Output module,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",133,0.8866666666666667,140,0.5128205128205128,4,1.0,1,0,Introduction: Output module
142,Candidate generation,Introduction,,question-answering,2,"['O', 'O']","['O', 'O']",134,0.8933333333333333,141,0.5164835164835165,0,0.0,1,0,Introduction
146,"We only keep the n-grams which are an alias of an entity , and then discard all n-grams that area subsequence of another n-gram , except if the longer n-gram only differs by in , of , for or the at the beginning .",Introduction,Candidate generation,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",138,0.92,145,0.5311355311355311,4,0.3076923076923077,1,0,Introduction: Candidate generation
149,Given two embedding matrices WV ?,Introduction,Candidate generation,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O']",141,0.94,148,0.5421245421245421,7,0.5384615384615384,1,0,Introduction: Candidate generation
150,R dN V and W S ?,Introduction,Candidate generation,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O']",142,0.9466666666666668,149,0.5457875457875457,8,0.6153846153846154,1,0,Introduction: Candidate generation
151,"R dN S , which respectively contain , in columns , the d-dimensional embeddings of the words / n - grams of the vocabulary and the embeddings of the Freebase entities and relationships , the similarity between question q and a Freebase candidate fact y is computed as :",Introduction,Candidate generation,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",143,0.9533333333333334,150,0.5494505494505495,9,0.6923076923076923,1,0,Introduction: Candidate generation
152,with cos ( ) the cosine similarity .,Introduction,Candidate generation,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",144,0.96,151,0.5531135531135531,10,0.7692307692307693,1,0,Introduction: Candidate generation
153,"When scoring a fact y from Reverb , we use the same embeddings and build the matrix WV S ? R d( N V +N S ) , which contains the concatenation in columns of WV and W S , and also compute the cosine similarity :",Introduction,Candidate generation,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",145,0.9666666666666668,152,0.5567765567765568,11,0.8461538461538461,1,0,Introduction: Candidate generation
154,"S RV B ( q , y ) = cos ( W V g ( q ) , WV S h ( y ) ) .",Introduction,Candidate generation,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",146,0.9733333333333334,153,0.5604395604395604,12,0.9230769230769232,1,0,Introduction: Candidate generation
155,"The dimension dis a hyperparameter , and the embedding matrices WV and W S are the parameters learned with the training algorithm of Section 4 .",Introduction,Candidate generation,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",147,0.98,154,0.5641025641025641,13,1.0,1,0,Introduction: Candidate generation
156,Response module,Introduction,,question-answering,2,"['O', 'O']","['O', 'O']",148,0.9866666666666668,155,0.5677655677655677,0,0.0,1,0,Introduction
157,"In Memory Networks , the Response module postprocesses the result of the Output module to compute the intended answer .",Introduction,Response module,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",149,0.9933333333333332,156,0.5714285714285714,1,0.5,1,0,Introduction: Response module
158,"In our case , it returns the set of objects of the selected supporting fact .",Introduction,Response module,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",150,1.0,157,0.575091575091575,2,1.0,1,0,Introduction: Response module
159,Training,,,question-answering,2,['O'],['O'],0,0.0,158,0.5787545787545788,0,0.0,1,0,
161,"First , in addition to the new SimpleQuestions dataset described in Section 2 , we also used We-bQuestions , a benchmark for QA introduced in : questions are labeled with answer strings from aliases of Freebase entities , and many questions expect multiple answers .",Training,Training,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.25,160,0.5860805860805861,2,0.25,1,0,Training
164,Statistics of FB2M or FB5M are given in ; we generated one training question per fact following the same process as that used in .,Training,Training,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.625,163,0.5970695970695971,5,0.625,1,0,Training
167,Our Paraphrases dataset is made of 15M clusters containing 2 or more paraphrases each .,Training,Training,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,1.0,166,0.608058608058608,8,1.0,1,0,Training
170,"For QA datasets the goal is that in the embedding space , a supporting fact is more similar to the question than any other non-supporting fact .",Multitask training,Multitask training,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.0555555555555555,169,0.6190476190476191,2,0.1538461538461538,1,0,Multitask training
171,"For the paraphrase dataset , a question should be more similar to one of its paraphrases than to any another question .",Multitask training,Multitask training,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.0833333333333333,170,0.6227106227106227,3,0.2307692307692307,1,0,Multitask training
173,"For the QA datasets , given a question / supporting fact pair ( q , y) and a non-supporting fact y ? , we perform a step to minimize the loss function ? QA ( q , y , y ? ) = ? ? S QA ( q , y ) + S QA ( q , y ? ) + , where [. ] + is the positive part and ?",Multitask training,Multitask training,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.1388888888888889,172,0.63003663003663,5,0.3846153846153846,1,0,Multitask training
174,is a margin hyperparameter .,Multitask training,Multitask training,question-answering,2,"['O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O']",6,0.1666666666666666,173,0.6336996336996337,6,0.4615384615384615,1,0,Multitask training
175,"For the paraphrase dataset , the similarity score between two questions q and q ?",Multitask training,Multitask training,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.1944444444444444,174,0.6373626373626373,7,0.5384615384615384,1,0,Multitask training
176,"is also the cosine between their embeddings , i.e. S QQ ( q , q ? ) = cos ( W V g ( q ) , WV g ( q ? ) ) , and given a paraphrase pair ( q , q ? ) and another question q ??",Multitask training,Multitask training,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.2222222222222222,175,0.6410256410256411,8,0.6153846153846154,1,0,Multitask training
177,", the loss is :",Multitask training,Multitask training,question-answering,2,"['O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O']",9,0.25,176,0.6446886446886447,9,0.6923076923076923,1,0,Multitask training
178,The embeddings ( i.e. the columns of WV and W S ) are projected onto the L 2 unit ball after each update .,Multitask training,Multitask training,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.2777777777777778,177,0.6483516483516484,10,0.7692307692307693,1,0,Multitask training
179,"At each time step , a sample from the paraphrase dataset is drawn with probability 0.2 ( this probability is arbitrary ) .",Multitask training,Multitask training,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.3055555555555556,178,0.652014652014652,11,0.8461538461538461,1,0,Multitask training
180,"Otherwise , a sample from one of the three QA datasets , chosen uniformly at random , is taken .",Multitask training,Multitask training,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.3333333333333333,179,0.6556776556776557,12,0.9230769230769232,1,0,Multitask training
183,"Unlike for SimpleQuestions or the synthetic QA data generated from Freebase , for WebQuestions only answer strings are provided for questions : the supporting facts are unknown .",Multitask training,Distant supervision,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.4166666666666667,182,0.6666666666666666,1,0.1,1,0,Multitask training: Distant supervision
187,"If multiple facts are obtained for the same question , the ones with the minimal number of objects are considered as supervision facts .",Multitask training,Distant supervision,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.5277777777777778,186,0.6813186813186813,5,0.5,1,0,Multitask training: Distant supervision
188,This last selection avoids favoring irrelevant relationships that would be kept only because they point to many objects but would not be specific enough .,Multitask training,Distant supervision,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.5555555555555556,187,0.684981684981685,6,0.6,1,0,Multitask training: Distant supervision
189,"If no answer string could be found from the objects of the initial candidates , the question is discarded from the training set .",Multitask training,Distant supervision,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.5833333333333334,188,0.6886446886446886,7,0.7,1,0,Multitask training: Distant supervision
190,Future work should investigate the process of weak supervised training of MemNNs recently introduced in that allows to train them without any supervision coming from the supporting facts . :,Multitask training,Distant supervision,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.6111111111111112,189,0.6923076923076923,8,0.8,1,0,Multitask training: Distant supervision
191,Training and evaluation datasets .,Multitask training,Distant supervision,question-answering,2,"['O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O']",23,0.6388888888888888,190,0.6959706959706959,9,0.9,1,0,Multitask training: Distant supervision
192,Questions automatically generated from the KB and paraphrases can also be used in training .,Multitask training,Distant supervision,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.6666666666666666,191,0.6996336996336996,10,1.0,1,0,Multitask training: Distant supervision
193,WebQuestions SimpleQuestions,Multitask training,Distant supervision,question-answering,2,"['O', 'O']","['O', 'O']",25,0.6944444444444444,192,0.7032967032967034,0,0.0,1,0,Multitask training: Distant supervision
194,Reverb,Multitask training,,question-answering,2,['O'],['O'],26,0.7222222222222222,193,0.706959706959707,1,0.0,1,0,Multitask training
195,Generating negative examples,Multitask training,Reverb,question-answering,2,"['O', 'O', 'O']","['O', 'O', 'O']",27,0.75,194,0.7106227106227107,0,0.0,1,0,Multitask training: Reverb
196,"As in , learning is performed with gradient descent , so that negative examples ( non - supporting facts or non-paraphrases ) are generated according to a randomized policy during training .",Multitask training,Reverb,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",28,0.7777777777777778,195,0.7142857142857143,1,0.1111111111111111,1,0,Multitask training: Reverb
197,"For paraphrases , given a pair ( q , q ? ) , a nonparaphrase pair is generated as ( q , q ?? )",Multitask training,Reverb,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",29,0.8055555555555556,196,0.717948717948718,2,0.2222222222222222,1,0,Multitask training: Reverb
198,where q ??,Multitask training,Reverb,question-answering,2,"['O', 'O', 'O']","['O', 'O', 'O']",30,0.8333333333333334,197,0.7216117216117216,3,0.3333333333333333,1,0,Multitask training: Reverb
199,"is a random question of the dataset , not belonging to the cluser of q .",Multitask training,Reverb,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",31,0.8611111111111112,198,0.7252747252747253,4,0.4444444444444444,1,0,Multitask training: Reverb
201,"The default policy to obtain a non-supporting fact is to corrupt the answer fact by exchanging it s subject , its relationship or its object ( s ) with that of another fact chosen uniformly at random from the KB .",Multitask training,Reverb,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",33,0.9166666666666666,200,0.7326007326007326,6,0.6666666666666666,1,0,Multitask training: Reverb
202,"In this policy , the element of the fact to corrupt is chosen randomly , with a small probability ( 0.3 ) of corrupting more than one element of the answer fact .",Multitask training,Reverb,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",34,0.9444444444444444,201,0.7362637362637363,7,0.7777777777777778,1,0,Multitask training: Reverb
203,"The second policy we propose , called candidates as negatives , is to take as non-supporting fact a randomly chosen fact from the set of candidate facts .",Multitask training,Reverb,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",35,0.9722222222222222,202,0.73992673992674,8,0.8888888888888888,1,0,Multitask training: Reverb
205,Related Work,,,question-answering,2,"['O', 'O']","['O', 'O']",0,0.0,204,0.7472527472527473,0,0.0,1,0,
215,Experiments,,,question-answering,2,['O'],['O'],0,0.0,214,0.7838827838827839,0,0.0,1,0,
218,Evaluation and baselines,,,question-answering,2,"['O', 'O', 'O']","['O', 'O', 'O']",0,0.0,217,0.7948717948717948,0,0.0,1,0,
220,"Since no previous result was published on SimpleQuestions , we only compare different versions of MemNNs .",Evaluation and baselines,Evaluation and baselines,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.2857142857142857,219,0.8021978021978022,2,0.2857142857142857,1,0,Evaluation and baselines
221,"SimpleQuestions questions are labeled with their entire Freebase fact , so we evaluate in terms of path - level accuracy , in which a prediction is correct if the subject and the relationship were correctly retrieved by the system .",Evaluation and baselines,Evaluation and baselines,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.4285714285714285,220,0.8058608058608059,3,0.4285714285714285,1,0,Evaluation and baselines
222,"The Reverb test set , based on the KB of the same name and introduced in is used for evaluation only .",Evaluation and baselines,Evaluation and baselines,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.5714285714285714,221,0.8095238095238095,4,0.5714285714285714,1,0,Evaluation and baselines
226,Experimental setup,,,question-answering,2,"['O', 'O']","['O', 'O']",0,0.0,225,0.8241758241758241,0,0.0,1,0,
233,"First , in the Candidates as Negatives setting ( negative facts are sampled from the candidate set , see Section 4 ) , abbreviated CANDS AS NEGS , the experimental protocol is the same as in the default setting but the embeddings are initialized with the best configuration of the default setup .",Experimental setup,Experimental setup,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.5833333333333334,232,0.8498168498168498,7,0.5833333333333334,1,0,Experimental setup
234,"Second , our model shares some similarities with an approach studied in , in which the authors noticed important gains using a subgraph representation of answers .",Experimental setup,Experimental setup,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.6666666666666666,233,0.8534798534798534,8,0.6666666666666666,1,0,Experimental setup
235,"For completeness , we also added such a subgraph representation of objects .",Experimental setup,Experimental setup,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.75,234,0.8571428571428571,9,0.75,1,0,Experimental setup
236,"In that setting , called Subgraph , each object o of a fact is itself represented as a bag - of - entities that encodes the immediate neighborhood of o .",Experimental setup,Experimental setup,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.8333333333333334,235,0.8608058608058609,10,0.8333333333333334,1,0,Experimental setup
238,We also report the results obtained by an ensemble of the 5 best models on validation ( subgraph excepted ) ; this is denoted 5 models .,Experimental setup,Experimental setup,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,1.0,237,0.8681318681318682,12,1.0,1,0,Experimental setup
239,Results,,,question-answering,2,['O'],['O'],0,0.0,238,0.8717948717948718,0,0.0,1,0,
240,Comparative results,,,question-answering,2,"['O', 'O']","['O', 'O']",0,0.0,239,0.8754578754578755,0,0.0,1,0,
244,WebQuestions SimpleQuestions,Comparative results,Comparative results,question-answering,2,"['O', 'O']","['O', 'O']",4,0.1428571428571428,243,0.8901098901098901,0,0.0,1,0,Comparative results
246,"This shows that MemNNs are effective at re-ranking the candidates , but also that simple QA is still not solved .",Comparative results,Comparative results,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.2142857142857142,245,0.8974358974358975,2,0.1818181818181818,1,0,Comparative results
248,"They use FB2M , and so their result ( 35.3 % F1 - score on WebQuestions ) should be compared to our 36.2 % .",Comparative results,Comparative results,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.2857142857142857,247,0.9047619047619048,4,0.3636363636363636,1,0,Comparative results
250,"Still , the major differences come from how we use Freebase .",Comparative results,Comparative results,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.3571428571428571,249,0.912087912087912,6,0.5454545454545454,1,0,Comparative results
253,Grouping facts also allows us to scale much better and to train on FB5M .,Comparative results,Comparative results,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.4642857142857143,252,0.9230769230769232,9,0.8181818181818182,1,0,Comparative results
264,Importance of data sources,Comparative results,Transfer learning on Reverb,question-answering,2,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",24,0.8571428571428571,263,0.9633699633699634,8,0.6666666666666666,1,0,Comparative results: Transfer learning on Reverb
265,The bottom half of,Comparative results,Transfer learning on Reverb,question-answering,2,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",25,0.8928571428571429,264,0.967032967032967,9,0.75,1,0,Comparative results: Transfer learning on Reverb
267,"This is because WebQuestions and SimpleQuestions questions follow simple patterns and are well formed , while Reverb questions have more syntactic and lexical variability .",Comparative results,Transfer learning on Reverb,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.9642857142857144,266,0.9743589743589745,11,0.9166666666666666,1,0,Comparative results: Transfer learning on Reverb
268,"Thus , paraphrases are important to avoid overfitting on specific question patterns of the training sets .",Comparative results,Transfer learning on Reverb,question-answering,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",28,1.0,267,0.978021978021978,12,1.0,1,0,Comparative results: Transfer learning on Reverb
269,Conclusion,,,question-answering,2,['O'],['O'],0,0.0,268,0.9816849816849816,0,0.0,1,0,
3,abstract,,,question-answering,3,['O'],['O'],0,0.0,2,0.0078740157480314,0,0.0,1,0,
4,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences .",abstract,abstract,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.02,3,0.0118110236220472,1,0.02,1,0,abstract
11,Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences .,abstract,abstract,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.16,10,0.0393700787401574,8,0.16,1,0,abstract
13,"For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) .",abstract,abstract,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.2,12,0.0472440944881889,10,0.2,1,0,abstract
14,"For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",abstract,abstract,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.22,13,0.0511811023622047,11,0.22,1,0,abstract
16,1 .,abstract,abstract,question-answering,3,"['O', 'O']","['O', 'O']",13,0.26,15,0.0590551181102362,13,0.26,1,0,abstract
17,There is a lexical gap between semantically equivalent sentences .,abstract,abstract,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.28,16,0.0629921259842519,14,0.28,1,0,abstract
18,"Take the E 1 and E 2 in Table 1 for example , they have the similar meaning but with different lexicons .",abstract,abstract,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.3,17,0.0669291338582677,15,0.3,1,0,abstract
20,"E.g. , "" not related "" in E 2 is an indivisible phrase when matching with "" irrelevant "" in E 1 ( shown in square brackets ) .",abstract,abstract,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.34,19,0.0748031496062992,17,0.34,1,0,abstract
21,3 .,abstract,abstract,question-answering,3,"['O', 'O']","['O', 'O']",18,0.36,20,0.0787401574803149,18,0.36,1,0,abstract
22,"The dissimilarity ( shown in angle brackets ) between two sentences is also a significant clue ( Qiu et al. , 2006 ) .",abstract,abstract,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.38,21,0.0826771653543307,19,0.38,1,0,abstract
23,"For example , by judging the dissimilar parts , we can easily identify that E 3 and E 5 share the similar meaning "" The study is about salmon "" , because "" sockeye "" belongs to the salmon family , and "" flounder "" does not .",abstract,abstract,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.4,22,0.0866141732283464,20,0.4,1,0,abstract
24,"Whereas the meaning of E 4 is quite different from E 3 , which emphasizes "" The study is about red ( a special kind of ) salmon "" , because both "" sockeye "" and "" coho "" are in the salmon family .",abstract,abstract,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.42,23,0.0905511811023622,21,0.42,1,0,abstract
25,How we can extract and utilize those information becomes another challenge .,abstract,abstract,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.44,24,0.0944881889763779,22,0.44,1,0,abstract
26,"In order to handle the above challenges , researchers have been working on sentence similarity algorithms fora longtime .",abstract,abstract,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",23,0.46,25,0.0984251968503937,23,0.46,1,0,abstract
27,"To bridge the lexical gap ( challenge 1 ) , some word similarity metrics were proposed to match different but semantically related words .",abstract,abstract,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.48,26,0.1023622047244094,24,0.48,1,0,abstract
28,"Examples include knowledge - based metrics ( Resnik , 1995 ) and corpus - based metrics ( Jiang and Conrath , 1997 ; Yin and Schtze , 2015 ; He et al. , 2015 ) .",abstract,abstract,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.5,27,0.1062992125984252,25,0.5,1,0,abstract
29,"To measure sentence similarity from various granularities ( challenge 2 ) , researchers have explored features extracted from n-grams , continuous phrases , discontinuous phrases , and parse trees ( Yin and Schtze , 2015 ; He et al. , 2015 ; Heilman and Smith , 2010 ) .",abstract,abstract,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.52,28,0.1102362204724409,26,0.52,1,0,abstract
31,narrative,abstract,abstract,question-answering,3,['O'],['O'],28,0.56,30,0.1181102362204724,28,0.56,1,0,abstract
32,E1,abstract,,question-answering,3,['O'],['O'],29,0.58,31,0.1220472440944882,29,0.58,1,0,abstract
33,The research is to sockeye .,abstract,E1,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O']",30,0.6,32,0.1259842519685039,30,0.6,1,0,abstract: E1
34,E2,abstract,,question-answering,3,['O'],['O'],31,0.62,33,0.1299212598425196,31,0.62,1,0,abstract
35,The study is [ not related ] to salmon .,abstract,E2,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",32,0.64,34,0.1338582677165354,32,0.64,1,0,abstract: E2
36,E3,abstract,,question-answering,3,['O'],['O'],33,0.66,35,0.1377952755905512,33,0.66,1,0,abstract
37,The research is relevant to salmon .,abstract,E3,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O']",34,0.68,36,0.1417322834645669,34,0.68,1,0,abstract: E3
38,E4,abstract,,question-answering,3,['O'],['O'],35,0.7,37,0.1456692913385826,35,0.7,1,0,abstract
39,"The study is relevant to sockeye , instead of coho .",abstract,E4,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",36,0.72,38,0.1496062992125984,36,0.72,1,0,abstract: E4
40,E5,abstract,,question-answering,3,['O'],['O'],37,0.74,39,0.1535433070866141,37,0.74,1,0,abstract
41,"The study is relevant to sockeye , rather than flounder .:",abstract,E5,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",38,0.76,40,0.1574803149606299,38,0.76,1,0,abstract: E5
42,"Examples for sentence similarity learning , where sockeye means "" red salmon "" , and coho means "" silver salmon "" .",abstract,E5,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",39,0.78,41,0.1614173228346456,39,0.78,1,0,abstract: E5
43,""" coho "" and "" sockeye "" are in the salmon family , while "" flounder "" is not .",abstract,E5,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",40,0.8,42,0.1653543307086614,40,0.8,1,0,abstract: E5
44,"attention in the past , the only related work of explored the dissimilarity between sentences in a pair for paraphrase identification task , but they require human annotations in order to train a classifier , and their performance is still below the state of the art .",abstract,E5,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",41,0.82,43,0.1692913385826771,41,0.82,1,0,abstract: E5
56,"To deal with the first challenge , we represent each word as a distributed vector , so that we can calculate similarities for formally different but semantically related words .",Model Overview,Model Overview,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.0202020202020202,55,0.2165354330708661,2,0.0555555555555555,1,0,Model Overview
66,"For example , in , to check whether E 2 is a paraphrase of E 1 , we need to know the single word "" irrelevant "" in E 1 is matched or covered by the phrase "" not related "" in E 2 .",Model Overview,Semantic Matching .,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.1212121212121212,65,0.2559055118110236,12,0.3333333333333333,1,0,Model Overview: Semantic Matching .
68,i for each word s i by composing part or full word vectors in the other sentence T .,Model Overview,Semantic Matching .,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.1414141414141414,67,0.2637795275590551,14,0.3888888888888889,1,0,Model Overview: Semantic Matching .
71,Decomposition .,Model Overview,,question-answering,3,"['O', 'O']","['O', 'O']",17,0.1717171717171717,70,0.2755905511811024,17,0.4722222222222222,1,0,Model Overview
72,"After the semantic matching phase , we have the semantic matching vectors of ?",Model Overview,Decomposition .,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.1818181818181818,71,0.2795275590551181,18,0.5,1,0,Model Overview: Decomposition .
73,i and t j .,Model Overview,Decomposition .,question-answering,3,"['O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O']",19,0.1919191919191919,72,0.2834645669291338,19,0.5277777777777778,1,0,Model Overview: Decomposition .
74,We interpret ?,Model Overview,Decomposition .,question-answering,3,"['O', 'O', 'O']","['O', 'O', 'O']",20,0.202020202020202,73,0.2874015748031496,20,0.5555555555555556,1,0,Model Overview: Decomposition .
75,i ( ort j ) as a semantic coverage of word s i ( or t j ) by the sentence T ( or S ) .,Model Overview,Decomposition .,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.2121212121212121,74,0.2913385826771653,21,0.5833333333333334,1,0,Model Overview: Decomposition .
76,"However , it is not necessary that all the semantics of s i ( or t j ) are fully covered by ?",Model Overview,Decomposition .,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.2222222222222222,75,0.2952755905511811,22,0.6111111111111112,1,0,Model Overview: Decomposition .
77,i ( ort j ) .,Model Overview,Decomposition .,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O']",23,0.2323232323232323,76,0.2992125984251969,23,0.6388888888888888,1,0,Model Overview: Decomposition .
78,"Take the E 1 and E 2 in for example , the word "" sockeye "" in E 1 is only partially matched by the word "" salmon "" ( the similar part ) in E 2 , as the full meaning of "" sockeye "" is "" red salmon "" ( the semantic meaning of "" red "" is the dissimilar part ) .",Model Overview,Decomposition .,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.2424242424242424,77,0.3031496062992126,24,0.6666666666666666,1,0,Model Overview: Decomposition .
79,"Motivated by this phenomenon , our model further decomposes word s i ( or t j ) , based on its semantic matching vector ?",Model Overview,Decomposition .,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.2525252525252525,78,0.3070866141732283,25,0.6944444444444444,1,0,Model Overview: Decomposition .
80,"i ( ort j ) , into two components : similar component s + i ( or t + j ) and dissimilar component s ? i ( or t ? j ) .",Model Overview,Decomposition .,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.2626262626262626,79,0.3110236220472441,26,0.7222222222222222,1,0,Model Overview: Decomposition .
81,"Formally , we define the decomposition function as :",Model Overview,Decomposition .,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.2727272727272727,80,0.3149606299212598,27,0.75,1,0,Model Overview: Decomposition .
83,"Besides the suggestion from that the significance of the dissimilar parts alone between two sentences has a great effect of their similarity , we also think that the dissimilar and similar components have strong connections .",Model Overview,Decomposition .,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",29,0.2929292929292929,82,0.3228346456692913,29,0.8055555555555556,1,0,Model Overview: Decomposition .
84,"For example , in , if we only look at the dissimilar or similar part alone , it is hard to judge which one between E 4 and E 5 is more similar to E 3 .",Model Overview,Decomposition .,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",30,0.303030303030303,83,0.326771653543307,30,0.8333333333333334,1,0,Model Overview: Decomposition .
85,"We can easily identify that E 5 is more similar to E 3 , when we consider both the similar and dissimilar parts .",Model Overview,Decomposition .,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",31,0.3131313131313131,84,0.3307086614173228,31,0.8611111111111112,1,0,Model Overview: Decomposition .
86,"Therefore , our model composes the similar component matrix and dissimilar component matrix into a feature vector S ( or T ) with the composition function :",Model Overview,Decomposition .,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",32,0.3232323232323232,85,0.3346456692913386,32,0.8888888888888888,1,0,Model Overview: Decomposition .
91,Semantic Matching Functions,Model Overview,,question-answering,3,"['O', 'O', 'O']","['O', 'O', 'O']",37,0.3737373737373737,90,0.3543307086614173,0,0.0,1,0,Model Overview
93,( 1 ) .,Model Overview,Semantic Matching Functions,question-answering,3,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",39,0.3939393939393939,92,0.3622047244094488,2,0.1333333333333333,1,0,Model Overview: Semantic Matching Functions
94,The goal off match is to generate a semantic matching vector ?,Model Overview,Semantic Matching Functions,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",40,0.404040404040404,93,0.3661417322834646,3,0.2,1,0,Model Overview: Semantic Matching Functions
95,i for s i by composing the vectors from T .,Model Overview,Semantic Matching Functions,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",41,0.4141414141414141,94,0.3700787401574803,4,0.2666666666666666,1,0,Model Overview: Semantic Matching Functions
96,"For a sentence pair Sand T , we first calculate a similarity matrix A mn , where each element a i , j ?",Model Overview,Semantic Matching Functions,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",42,0.4242424242424242,95,0.374015748031496,5,0.3333333333333333,1,0,Model Overview: Semantic Matching Functions
97,A mn computes the cosine similarity between words s i and t j as,Model Overview,Semantic Matching Functions,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",43,0.4343434343434343,96,0.3779527559055118,6,0.4,1,0,Model Overview: Semantic Matching Functions
99,"where k = argmax j a i , j .",Model Overview,Semantic Matching Functions,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",45,0.4545454545454545,98,0.3858267716535433,8,0.5333333333333333,1,0,Model Overview: Semantic Matching Functions
100,The idea of the global function is to consider all word vectors t j in T .,Model Overview,Semantic Matching Functions,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",46,0.4646464646464646,99,0.389763779527559,9,0.6,1,0,Model Overview: Semantic Matching Functions
102,"i is a weighted sum vector of all words t j in T , where each weight is the normalized word similarity a i , j .",Model Overview,Semantic Matching Functions,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",48,0.4848484848484848,101,0.3976377952755905,11,0.7333333333333333,1,0,Model Overview: Semantic Matching Functions
103,The max function moves to the other extreme .,Model Overview,Semantic Matching Functions,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",49,0.494949494949495,102,0.4015748031496063,12,0.8,1,0,Model Overview: Semantic Matching Functions
104,It generates the semantic matching vector by selecting the most similar word vector t k from T .,Model Overview,Semantic Matching Functions,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",50,0.5050505050505051,103,0.405511811023622,13,0.8666666666666667,1,0,Model Overview: Semantic Matching Functions
105,"The local -w function takes a compromise between global and max , where w indicates the size of the window to consider centered at k ( the most similar word position ) .",Model Overview,Semantic Matching Functions,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",51,0.5151515151515151,104,0.4094488188976378,14,0.9333333333333332,1,0,Model Overview: Semantic Matching Functions
106,So the semantic matching vector is a weighted average vector from t k?w tot k+w .,Model Overview,Semantic Matching Functions,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",52,0.5252525252525253,105,0.4133858267716535,15,1.0,1,0,Model Overview: Semantic Matching Functions
107,Decomposition Functions,Model Overview,,question-answering,3,"['O', 'O']","['O', 'O']",53,0.5353535353535354,106,0.4173228346456692,0,0.0,1,0,Model Overview
109,( 2 ) .,Model Overview,Decomposition Functions,question-answering,3,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",55,0.5555555555555556,108,0.4251968503937008,2,0.0666666666666666,1,0,Model Overview: Decomposition Functions
110,The intention off decomp is to decompose a word vector s j based on its semantic matching vector ?,Model Overview,Decomposition Functions,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",56,0.5656565656565656,109,0.4291338582677165,3,0.1,1,0,Model Overview: Decomposition Functions
113,i indicates the semantics of s i covered by ?,Model Overview,Decomposition Functions,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",59,0.5959595959595959,112,0.4409448818897638,6,0.2,1,0,Model Overview: Decomposition Functions
114,i and s ?,Model Overview,Decomposition Functions,question-answering,3,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",60,0.6060606060606061,113,0.4448818897637795,7,0.2333333333333333,1,0,Model Overview: Decomposition Functions
115,i indicates the uncovered part .,Model Overview,Decomposition Functions,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O']",61,0.6161616161616161,114,0.4488188976377952,8,0.2666666666666666,1,0,Model Overview: Decomposition Functions
118,"First , it detects whether there is an exactly matched word in the other sentence , or s i equal to ?",Model Overview,Decomposition Functions,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",64,0.6464646464646465,117,0.4606299212598425,11,0.3666666666666666,1,0,Model Overview: Decomposition Functions
120,"If yes , the vector s i is dispatched to the similar component s + i , and the dissimilar component is assigned with a zero vector",Model Overview,Decomposition Functions,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",66,0.6666666666666666,119,0.468503937007874,13,0.4333333333333333,1,0,Model Overview: Decomposition Functions
122,"Otherwise , the vector s i is assigned to the dissimilar component s ?",Model Overview,Decomposition Functions,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",68,0.6868686868686869,121,0.4763779527559055,15,0.5,1,0,Model Overview: Decomposition Functions
124,Eq. gives the formal definition :,Model Overview,Decomposition Functions,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O']",70,0.7070707070707071,123,0.484251968503937,17,0.5666666666666667,1,0,Model Overview: Decomposition Functions
125,The motivation for the linear decomposition is that the more similar between s i and ?,Model Overview,Decomposition Functions,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",71,0.7171717171717171,124,0.4881889763779528,18,0.6,1,0,Model Overview: Decomposition Functions
126,"i , the higher proportion of s i should be assigned to the similar component .",Model Overview,Decomposition Functions,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",72,0.7272727272727273,125,0.4921259842519685,19,0.6333333333333333,1,0,Model Overview: Decomposition Functions
128,between s i and ?,Model Overview,Decomposition Functions,question-answering,3,"['O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O']",74,0.7474747474747475,127,0.5,21,0.7,1,0,Model Overview: Decomposition Functions
131,Eq. gives the corresponding definition :,Model Overview,Decomposition Functions,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O']",77,0.7777777777777778,130,0.5118110236220472,24,0.8,1,0,Model Overview: Decomposition Functions
132,The orthogonal decomposition is to decompose a vector in the geometric space .,Model Overview,Decomposition Functions,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",78,0.7878787878787878,131,0.515748031496063,25,0.8333333333333334,1,0,Model Overview: Decomposition Functions
133,Based on the semantic matching vector ?,Model Overview,Decomposition Functions,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O']",79,0.797979797979798,132,0.5196850393700787,26,0.8666666666666667,1,0,Model Overview: Decomposition Functions
134,"i , our model decomposes s i into a parallel component and a perpendicular component .",Model Overview,Decomposition Functions,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",80,0.8080808080808081,133,0.5236220472440944,27,0.9,1,0,Model Overview: Decomposition Functions
135,"Then , the parallel component is viewed as the similar component s + i , and perpendicular component is taken as the dissimilar component s ?",Model Overview,Decomposition Functions,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",81,0.8181818181818182,134,0.5275590551181102,28,0.9333333333333332,1,0,Model Overview: Decomposition Functions
137,Eq. gives the concrete definitions .,Model Overview,Decomposition Functions,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O']",83,0.8383838383838383,136,0.5354330708661418,30,1.0,1,0,Model Overview: Decomposition Functions
138,Composition Functions,Model Overview,,question-answering,3,"['O', 'O']","['O', 'O']",84,0.8484848484848485,137,0.5393700787401575,0,0.0,1,0,Model Overview
139,The aim of composition function f comp in Eq. is to extract features from both the similar component matrix and the dissimilar component matrix .,Model Overview,Composition Functions,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",85,0.8585858585858586,138,0.5433070866141733,1,0.0833333333333333,1,0,Model Overview: Composition Functions
140,We also want to acquire similarities and dissimilarities of various granularity during the composition phase .,Model Overview,Composition Functions,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",86,0.8686868686868687,139,0.547244094488189,2,0.1666666666666666,1,0,Model Overview: Composition Functions
143,"For the convolution operation , we define a list of filters {w o }.",Model Overview,Composition Functions,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",89,0.898989898989899,142,0.5590551181102362,5,0.4166666666666667,1,0,Model Overview: Composition Functions
144,"The shape of each filter is d h , where dis the dimension of word vectors and h is the window size .",Model Overview,Composition Functions,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",90,0.9090909090909092,143,0.562992125984252,6,0.5,1,0,Model Overview: Composition Functions
146,Eq. ( 10 ) expresses this process .,Model Overview,Composition Functions,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",92,0.9292929292929292,145,0.5708661417322834,8,0.6666666666666666,1,0,Model Overview: Composition Functions
147,"To deal with variable feature size , we perform a max - pooling operation over co by selecting the maximum value co = max co .",Model Overview,Composition Functions,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",93,0.9393939393939394,146,0.5748031496062992,9,0.75,1,0,Model Overview: Composition Functions
148,"Therefore , after these two operations , each filter generates only one feature .",Model Overview,Composition Functions,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",94,0.9494949494949496,147,0.5787401574803149,10,0.8333333333333334,1,0,Model Overview: Composition Functions
150,"Eventually , a vector of features is captured by composing the two component matrixes , and the feature dimension is equal to the number of filters .",Model Overview,Composition Functions,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",96,0.9696969696969696,149,0.5866141732283464,12,1.0,1,0,Model Overview: Composition Functions
151,Similarity Assessment Function,Model Overview,,question-answering,3,"['O', 'O', 'O']","['O', 'O', 'O']",97,0.9797979797979798,150,0.5905511811023622,0,0.0,1,0,Model Overview
152,The similarity assessment function f sim in Eq. ( 4 ) predicts a similarity score by taking two feature vectors as input .,Model Overview,Similarity Assessment Function,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",98,0.98989898989899,151,0.594488188976378,1,0.5,1,0,Model Overview: Similarity Assessment Function
154,Training,,,question-answering,3,['O'],['O'],0,0.0,153,0.6023622047244095,0,0.0,1,0,
156,"Each training instance in the training set is represented as a triple ( S i , Ti , Li ) , where Si and Ti area pair of sentences , and Li ? { 0 , 1 } indicates the similarity between them .",Training,Training,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.4,155,0.610236220472441,2,0.4,1,0,Training
157,"We assign Li = 1 if Ti is a paraphrase of Si for the paraphrase identification task , or Ti is a correct answer for Si for the answer sentence selection task .",Training,Training,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.6,156,0.6141732283464567,3,0.6,1,0,Training
160,Experiment,,,question-answering,3,['O'],['O'],0,0.0,159,0.6259842519685039,0,0.0,1,0,
161,Experimental Setting,,,question-answering,3,"['O', 'O']","['O', 'O']",0,0.0,160,0.6299212598425197,0,0.0,1,0,
165,"The statistics of the two datasets can be found in , where QASent was created from the TREC QA track , and WikiQA ( Yang et al. , 2015 ) is constructed from real queries of Bing and Wikipedia .",Experimental Setting,Experimental Setting,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.4444444444444444,164,0.6456692913385826,4,0.4444444444444444,1,0,Experimental Setting
171,Model Properties,,,question-answering,3,"['O', 'O']","['O', 'O']",0,0.0,170,0.6692913385826772,0,0.0,1,0,
172,"There are several alternative options in our model , e.g. , the semantic matching functions , the decomposition operations , and the filter types .",Model Properties,Model Properties,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0144927536231884,171,0.6732283464566929,1,0.0144927536231884,1,0,Model Properties
177,"We switched the semantic matching functions among { max , global , local - l} , where l ? { 1 , 2 , 3 , 4 } , and fixed the other options as : the linear decomposition , the filter types including {unigram , bigram , trigram } , and 500 filters for each type .",Model Properties,Model Properties,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.0869565217391304,176,0.6929133858267716,6,0.0869565217391304,1,0,Model Properties
178,presents the results .,Model Properties,Model Properties,question-answering,3,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",7,0.1014492753623188,177,0.6968503937007874,7,0.1014492753623188,1,0,Model Properties
186,shows the performance .,Model Properties,Model Properties,question-answering,3,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",15,0.217391304347826,185,0.7283464566929134,15,0.217391304347826,1,0,Model Properties
188,"This is reasonable , because the rigid operation decomposes word vectors by exactly matching words .",Model Properties,Model Properties,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.2463768115942029,187,0.7362204724409449,17,0.2463768115942029,1,0,Model Properties
189,"The orthogonal operation got a similar MAP as the linear operation , and it worked better in term of MRR .",Model Properties,Model Properties,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.2608695652173913,188,0.7401574803149606,18,0.2608695652173913,1,0,Model Properties
190,"Therefore , we choose the orthogonal operation in the following experiments .",Model Properties,Model Properties,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.2753623188405797,189,0.7440944881889764,19,0.2753623188405797,1,0,Model Properties
200,Comparing with State - of - the - art Models,Model Properties,Model Properties,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",29,0.4202898550724637,199,0.7834645669291339,29,0.4202898550724637,1,0,Model Properties
204,1 .,Model Properties,Model Properties,question-answering,3,"['O', 'O']","['O', 'O']",33,0.4782608695652174,203,0.7992125984251969,33,0.4782608695652174,1,0,Model Properties
205,"Given a pair of sentences , Severyn and Moschitti ( 2015 ) employed a CNN model to compose each sentence into a vector separately , and joined the two sentence vectors to compute the sentence similarity .",Model Properties,Model Properties,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",34,0.4927536231884058,204,0.8031496062992126,34,0.4927536231884058,1,0,Model Properties
206,"Because only the sentencelevel granularity was used , the performance is much lower ( the second row of ) .",Model Properties,Model Properties,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",35,0.5072463768115942,205,0.8070866141732284,35,0.5072463768115942,1,0,Model Properties
210,"They got a slightly better performance ( the fourth row of ) , which indicates that the vector representation for words is helpful to bridging the lexical gap problem .",Model Properties,Model Properties,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",39,0.5652173913043478,209,0.8228346456692913,39,0.5652173913043478,1,0,Model Properties
215,Wiki QA dataset .,Model Properties,Model Properties,question-answering,3,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",44,0.6376811594202898,214,0.84251968503937,44,0.6376811594202898,1,0,Model Properties
220,The corresponding performance is given at the fourth row of .,Model Properties,Model Properties,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",49,0.7101449275362319,219,0.8622047244094488,49,0.7101449275362319,1,0,Model Properties
221,"introduced the attention mechanism into the CNN model , and captured the best performance ( the fifth row of ) .",Model Properties,Model Properties,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",50,0.7246376811594203,220,0.8661417322834646,50,0.7246376811594203,1,0,Model Properties
227,They obtained their best performance by pretraining the model on a language modeling task ( the 3rd row of ) .,Model Properties,Model Properties,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",56,0.8115942028985508,226,0.889763779527559,56,0.8115942028985508,1,0,Model Properties
228,"However , their model heavily depends on the pretraining strategy .",Model Properties,Model Properties,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",57,0.8260869565217391,227,0.8937007874015748,57,0.8260869565217391,1,0,Model Properties
229,"Without pretraining , they got a much worse performance ( the second row of ) .",Model Properties,Model Properties,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",58,0.8405797101449275,228,0.8976377952755905,58,0.8405797101449275,1,0,Model Properties
230,proposed a similar model to .,Model Properties,Model Properties,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O']",59,0.855072463768116,229,0.9015748031496064,59,0.855072463768116,1,0,Model Properties
231,"Similarly , they also used a CNN model to extract features at multiple levels of granularity .",Model Properties,Model Properties,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",60,0.8695652173913043,230,0.905511811023622,60,0.8695652173913043,1,0,Model Properties
232,"Differently , they utilized some extra annotated resources , e.g. , embeddings from part - of - speech ( POS ) tags and PARAGRAM vectors trained from the Paraphrase Database .",Model Properties,Model Properties,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",61,0.8840579710144928,231,0.9094488188976378,61,0.8840579710144928,1,0,Model Properties
234,"However , the performance was reduced after removing the extra resources ( the fourth and fifth rows of ) .",Model Properties,Model Properties,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",63,0.9130434782608696,233,0.9173228346456692,63,0.9130434782608696,1,0,Model Properties
235,applied their attention - based CNN model on this dataset .,Model Properties,Model Properties,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",64,0.927536231884058,234,0.9212598425196852,64,0.927536231884058,1,0,Model Properties
238,"However , the best performance so far on this dataset is obtained by .",Model Properties,Model Properties,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",67,0.9710144927536232,237,0.9330708661417324,67,0.9710144927536232,1,0,Model Properties
239,"In their model , they just utilized several hand - crafted features in a Support Vector Machine ( SVM ) model .",Model Properties,Model Properties,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",68,0.9855072463768116,238,0.937007874015748,68,0.9855072463768116,1,0,Model Properties
240,"Therefore , the deep learning methods still have along way to go for this task .",Model Properties,Model Properties,question-answering,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",69,1.0,239,0.9409448818897638,69,1.0,1,0,Model Properties
241,Related Work,,,question-answering,3,"['O', 'O']","['O', 'O']",0,0.0,240,0.9448818897637796,0,0.0,1,0,
249,Conclusion,,,question-answering,3,['O'],['O'],0,0.0,248,0.9763779527559056,0,0.0,1,0,
3,abstract,,,question-answering,4,['O'],['O'],0,0.0,2,0.0068728522336769,0,0.0,1,0,
7,"Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features .",abstract,abstract,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.5,6,0.0206185567010309,4,0.5,1,0,abstract
15,It has garnered significant attention from the machine learning research community in recent years .,Introduction,Introduction,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.0652173913043478,14,0.0481099656357388,3,0.0833333333333333,1,0,Introduction
22,"There are many use-cases in which comprehension over limited data would be handy : for example , user manuals , internal documentation , legal contracts , and soon .",Introduction,Introduction,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.217391304347826,21,0.0721649484536082,10,0.2777777777777778,1,0,Introduction
23,"Moreover , work towards more efficient learning from any quantity of data is important in its own right , for bringing machines more inline with the way humans learn .",Introduction,Introduction,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.2391304347826087,22,0.0756013745704467,11,0.3055555555555556,1,0,Introduction
24,"Typically , artificial neural networks require numerous parameters to capture complex patterns , and the more parameters , the more training data is required to tune them .",Introduction,Introduction,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.2608695652173913,23,0.0790378006872852,12,0.3333333333333333,1,0,Introduction
25,"Likewise , deep models learn to extract their own features , but this is a data - intensive process .",Introduction,Introduction,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.2826086956521739,24,0.0824742268041237,13,0.3611111111111111,1,0,Introduction
28,We refer to a question combined with one of its answer candidates as a hypothesis ( to be detailed below ) .,Introduction,Introduction,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.3478260869565217,27,0.0927835051546391,16,0.4444444444444444,1,0,Introduction
29,"The semantic perspective compares the hypothesis to sentences in the text viewed as single , self - contained thoughts ; these are represented using a sum and transformation of word embedding vectors , similarly to in .",Introduction,Introduction,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.3695652173913043,28,0.0962199312714776,17,0.4722222222222222,1,0,Introduction
31,"As in the semantic perspective , we consider matches over complete sentences .",Introduction,Introduction,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.4130434782608695,30,0.1030927835051546,19,0.5277777777777778,1,0,Introduction
35,These distinct perspectives naturally form a hierarchy that we depict in .,Introduction,Introduction,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",23,0.5,34,0.1168384879725085,23,0.6388888888888888,1,0,Introduction
37,The perspectives of our model can be considered a type of feature .,Introduction,Introduction,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.5434782608695652,36,0.1237113402061855,25,0.6944444444444444,1,0,Introduction
38,"However , they are implemented by parametric differentiable functions .",Introduction,Introduction,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.5652173913043478,37,0.127147766323024,26,0.7222222222222222,1,0,Introduction
39,"This is in contrast to most previous efforts on MCTest , whose numerous hand - engineered features can not be trained .",Introduction,Introduction,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.5869565217391305,38,0.1305841924398625,27,0.75,1,0,Introduction
44,We call this technique training wheels .,Introduction,Introduction,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O']",32,0.6956521739130435,43,0.1477663230240549,32,0.8888888888888888,1,0,Introduction
45,Computational models that comprehend ( insofar as they perform well on MC datasets ) have developed contemporaneously in several research groups .,Introduction,Introduction,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",33,0.717391304347826,44,0.1512027491408934,33,0.9166666666666666,1,0,Introduction
46,"Models designed specifically for MCTest include those of , and more recently , , and .",Introduction,Introduction,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",34,0.7391304347826086,45,0.1546391752577319,34,0.9444444444444444,1,0,Introduction
49,The Problem,Introduction,,question-answering,4,"['O', 'O']","['O', 'O']",37,0.8043478260869565,48,0.1649484536082474,0,0.0,1,0,Introduction
52,This can be viewed as selecting the best answer from a set of candidates .,Introduction,The Problem,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",40,0.8695652173913043,51,0.1752577319587628,3,0.3333333333333333,1,0,Introduction: The Problem
53,"In the multiple - choice case , candidate answers are predefined , but candidate answers may also be undefined yet restricted ( e.g. , to yes , no , or any noun phrase in the text ) .",Introduction,The Problem,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",41,0.8913043478260869,52,0.1786941580756013,4,0.4444444444444444,1,0,Introduction: The Problem
54,"For each question q , let T be the unstructured text and A = {a i } the set of candidate answers to q .",Introduction,The Problem,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",42,0.9130434782608696,53,0.1821305841924398,5,0.5555555555555556,1,0,Introduction: The Problem
55,The machine comprehension task reduces to selecting the answer that has the highest evidence given T .,Introduction,The Problem,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",43,0.9347826086956522,54,0.1855670103092783,6,0.6666666666666666,1,0,Introduction: The Problem
56,"As in , we combine an answer and a question into a hypothesis , hi = f ( q , a i ) .",Introduction,The Problem,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",44,0.9565217391304348,55,0.1890034364261168,7,0.7777777777777778,1,0,Introduction: The Problem
57,"To facilitate comparisons of the text with the hypotheses , we also breakdown the passage into sentences t j , T = {t j }.",Introduction,The Problem,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",45,0.9782608695652174,56,0.1924398625429553,8,0.8888888888888888,1,0,Introduction: The Problem
58,"In our setting , q , a i , and t j each represent a sequence of embedding vectors , one for each word and punctuation mark in the respective item .",Introduction,The Problem,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",46,1.0,57,0.1958762886597938,9,1.0,1,0,Introduction: The Problem
59,Related Work,,,question-answering,4,"['O', 'O']","['O', 'O']",0,0.0,58,0.1993127147766323,0,0.0,1,0,
85,This is because the dataset is sparse and complex .,Neural models,Neural models,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.018018018018018,84,0.2886597938144329,2,0.1176470588235294,1,0,Neural models
86,investigated deep - learning approaches concurrently with the present work .,Neural models,Neural models,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.027027027027027,85,0.2920962199312715,3,0.1764705882352941,1,0,Neural models
92,"As in the present work , matching scores are given by cosine similarity .",Neural models,Neural models,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.081081081081081,91,0.3127147766323024,9,0.5294117647058824,1,0,Neural models
99,"Below , we use subscripts to index elements of sequences , like word vectors , and superscripts to indicate whether elements come from the text , question , or answer .",Neural models,The Parallel - Hierarchical Model,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.1441441441441441,98,0.3367697594501718,16,0.9411764705882352,1,0,Neural models: The Parallel - Hierarchical Model
100,"In particular , we use the subscripts k , m , n , p to index sequences from the text , question , answer , and hypothesis , respectively , and superscripts t , q , a , h. We depict the model schematically in .",Neural models,The Parallel - Hierarchical Model,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.1531531531531531,99,0.3402061855670103,17,1.0,1,0,Neural models: The Parallel - Hierarchical Model
102,The semantic perspective is similar to the Memory Networks approach for embedding inputs into memory space .,Neural models,Semantic Perspective,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.1711711711711711,101,0.3470790378006873,1,0.0434782608695652,1,0,Neural models: Semantic Perspective
103,Each sen - tence of the text is a sequence of d-dimensional word vectors :,Neural models,Semantic Perspective,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.1801801801801801,102,0.3505154639175257,2,0.0869565217391304,1,0,Neural models: Semantic Perspective
104,"The semantic vector st is computed by embedding the word vectors into a D-dimensional space using a two - layer network that implements weighted sum followed by an affine tranformation and a nonlinearity ; i.e. ,",Neural models,Semantic Perspective,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.1891891891891892,103,0.3539518900343643,3,0.1304347826086956,1,0,Neural models: Semantic Perspective
105,"The matrix At ? R Dd , the bias vector b t A ? RD , and for f we use the leaky ReLU function .",Neural models,Semantic Perspective,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.1981981981981982,104,0.3573883161512027,4,0.1739130434782608,1,0,Neural models: Semantic Perspective
107,k is a trainable weight associated to each word in the vocabulary .,Neural models,Semantic Perspective,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.2162162162162162,106,0.3642611683848797,6,0.2608695652173913,1,0,Neural models: Semantic Perspective
109,"They can , for example , learn to perform the function of stopword lists in a soft , trainable way , to nullify the contribution of unimportant filler words .",Neural models,Semantic Perspective,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.2342342342342342,108,0.3711340206185567,8,0.3478260869565217,1,0,Neural models: Semantic Perspective
110,"The semantic representation of a hypothesis is formed analogously , except that we combine the question word vectors q m and answer word vectors an as a single sequence {h p } = {q m , an }.",Neural models,Semantic Perspective,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.2432432432432432,109,0.3745704467353952,9,0.391304347826087,1,0,Neural models: Semantic Perspective
111,"For semantic vector sh of the hypothesis , we use a unique transformation matrix A h ?",Neural models,Semantic Perspective,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",28,0.2522522522522522,110,0.3780068728522336,10,0.4347826086956521,1,0,Neural models: Semantic Perspective
112,R Dd and bias vector b h A ?,Neural models,Semantic Perspective,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",29,0.2612612612612612,111,0.3814432989690721,11,0.4782608695652174,1,0,Neural models: Semantic Perspective
115,"We compute the semantic match be-tween text sentence and hypothesis using the cosine similarity , M sem = cos ( s t , sh ) .",Neural models,Semantic Perspective,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",32,0.2882882882882883,114,0.3917525773195876,14,0.6086956521739131,1,0,Neural models: Semantic Perspective
116,( 2 ),Neural models,Semantic Perspective,question-answering,4,"['O', 'O', 'O']","['O', 'O', 'O']",33,0.2972972972972973,115,0.3951890034364261,15,0.6521739130434783,1,0,Neural models: Semantic Perspective
117,Word - by - Word Perspective,Neural models,,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O']",34,0.3063063063063063,116,0.3986254295532646,16,0.6956521739130435,1,0,Neural models
119,"For the text ,",Neural models,Word - by - Word Perspective,question-answering,4,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",36,0.3243243243243243,118,0.4054982817869416,18,0.782608695652174,1,0,Neural models: Word - by - Word Perspective
120,?,Neural models,Word - by - Word Perspective,question-answering,4,['O'],['O'],37,0.3333333333333333,119,0.40893470790378,19,0.8260869565217391,1,0,Neural models: Word - by - Word Perspective
121,RD and f is again the leaky ReLU .,Neural models,Word - by - Word Perspective,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",38,0.3423423423423423,120,0.4123711340206185,20,0.8695652173913043,1,0,Neural models: Word - by - Word Perspective
122,We transform the question and the answer toq m and n analogously using distinct matrices and bias vectors .,Neural models,Word - by - Word Perspective,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",39,0.3513513513513513,121,0.415807560137457,21,0.9130434782608696,1,0,Neural models: Word - by - Word Perspective
125,Sentential,Neural models,,question-answering,4,['O'],['O'],42,0.3783783783783784,124,0.4261168384879725,0,0.0,1,0,Neural models
127,This computation uses the cosine similarity as before :,Neural models,Sentential,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",44,0.3963963963963964,126,0.4329896907216495,2,0.2,1,0,Neural models: Sentential
128,"ca kn = cos (t k , n ) .",Neural models,Sentential,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",45,0.4054054054054054,127,0.436426116838488,3,0.3,1,0,Neural models: Sentential
130,"Here , ?",Neural models,Sentential,question-answering,4,"['O', 'O', 'O']","['O', 'O', 'O']",47,0.4234234234234234,129,0.4432989690721649,5,0.5,1,0,Neural models: Sentential
131,m is the word weight for the question word and Z normalizes these weights to sum to one over the question .,Neural models,Sentential,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",48,0.4324324324324324,130,0.4467353951890034,6,0.6,1,0,Neural models: Sentential
132,"We define the match between a sentence and answer candidate , M a , analogously .",Neural models,Sentential,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",49,0.4414414414414414,131,0.4501718213058419,7,0.7,1,0,Neural models: Sentential
134,Here the ?,Neural models,Sentential,question-answering,4,"['O', 'O', 'O']","['O', 'O', 'O']",51,0.4594594594594595,133,0.4570446735395189,9,0.9,1,0,Neural models: Sentential
136,Sequential Sliding Window,Neural models,,question-answering,4,"['O', 'O', 'O']","['O', 'O', 'O']",53,0.4774774774774775,135,0.4639175257731959,0,0.0,1,0,Neural models
137,The sequential sliding window is related to the original MCTest baseline by .,Neural models,Sequential Sliding Window,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",54,0.4864864864864865,136,0.4673539518900343,1,0.1,1,0,Neural models: Sequential Sliding Window
139,This modification enables the window to use information about the distance between word matches ; the original baseline used distance information through a predefined function .,Neural models,Sequential Sliding Window,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",56,0.5045045045045045,138,0.4742268041237113,3,0.3,1,0,Neural models: Sequential Sliding Window
141,"Each window is treated like a sentence in the previous subsection , but we include a location - based weight ?( k ) .",Neural models,Sequential Sliding Window,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",58,0.5225225225225225,140,0.4810996563573883,5,0.5,1,0,Neural models: Sequential Sliding Window
142,"This weight is based on a word 's position in the window , which , given a window , depends on its global position k.",Neural models,Sequential Sliding Window,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",59,0.5315315315315315,141,0.4845360824742268,6,0.6,1,0,Neural models: Sequential Sliding Window
143,The cosine similarity is adapted as,Neural models,Sequential Sliding Window,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O']",60,0.5405405405405406,142,0.4879725085910653,7,0.7,1,0,Neural models: Sequential Sliding Window
144,for the question and analogously for the answer .,Neural models,Sequential Sliding Window,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",61,0.5495495495495496,143,0.4914089347079037,8,0.8,1,0,Neural models: Sequential Sliding Window
146,"The final matching score , denoted as M sws , is computed as in and with sq km replacing c q km .",Neural models,Sequential Sliding Window,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",63,0.5675675675675675,145,0.4982817869415807,10,1.0,1,0,Neural models: Sequential Sliding Window
147,Dependency Sliding Window,Neural models,,question-answering,4,"['O', 'O', 'O']","['O', 'O', 'O']",64,0.5765765765765766,146,0.5017182130584192,0,0.0,1,0,Neural models
149,The output of this component is M swd and is formed analogously to M sws .,Neural models,Dependency Sliding Window,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",66,0.5945945945945946,148,0.5085910652920962,2,0.0512820512820512,1,0,Neural models: Dependency Sliding Window
151,"Thus , the dependency graph can be considered a fixed feature .",Neural models,Dependency Sliding Window,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",68,0.6126126126126126,150,0.5154639175257731,4,0.1025641025641025,1,0,Neural models: Dependency Sliding Window
152,"Moreover , linearization of the dependency graph , because it relies on an eigendecomposition , is not differentiable .",Neural models,Dependency Sliding Window,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",69,0.6216216216216216,151,0.5189003436426117,5,0.1282051282051282,1,0,Neural models: Dependency Sliding Window
155,"This graph has n w vertices , one for each word in the sentence .",Neural models,Dependency Sliding Window,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",72,0.6486486486486487,154,0.5292096219931272,8,0.2051282051282051,1,0,Neural models: Dependency Sliding Window
156,From the dependency graph we form the Laplacian matrix L ?,Neural models,Dependency Sliding Window,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",73,0.6576576576576577,155,0.5326460481099656,9,0.2307692307692307,1,0,Neural models: Dependency Sliding Window
157,R nwnw and determine its eigenvectors .,Neural models,Dependency Sliding Window,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O']",74,0.6666666666666666,156,0.5360824742268041,10,0.2564102564102564,1,0,Neural models: Dependency Sliding Window
158,The second eigenvector u 2 of the Laplacian is known as the Fiedler vector .,Neural models,Dependency Sliding Window,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",75,0.6756756756756757,157,0.5395189003436426,11,0.282051282051282,1,0,Neural models: Dependency Sliding Window
159,It is the solution to the minimization,Neural models,Dependency Sliding Window,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O']",76,0.6846846846846847,158,0.5429553264604811,12,0.3076923076923077,1,0,Neural models: Dependency Sliding Window
160,"where vi are the vertices of the graph , and ?",Neural models,Dependency Sliding Window,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",77,0.6936936936936937,159,0.5463917525773195,13,0.3333333333333333,1,0,Neural models: Dependency Sliding Window
161,ij is the weight of the edge from vertex i to vertex j.,Neural models,Dependency Sliding Window,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",78,0.7027027027027027,160,0.5498281786941581,14,0.358974358974359,1,0,Neural models: Dependency Sliding Window
163,1,Neural models,Dependency Sliding Window,question-answering,4,['O'],['O'],80,0.7207207207207207,162,0.5567010309278351,16,0.4102564102564102,1,0,Neural models: Dependency Sliding Window
165,The reordering of the words is given by the ordered index set I = arg sort ( u 2 ) .,Neural models,Dependency Sliding Window,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",82,0.7387387387387387,164,0.563573883161512,18,0.4615384615384615,1,0,Neural models: Dependency Sliding Window
166,"To give an example of how this works , consider the following sentence from MCTest and its dependency - based reordering :",Neural models,Dependency Sliding Window,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",83,0.7477477477477478,165,0.5670103092783505,19,0.4871794871794871,1,0,Neural models: Dependency Sliding Window
169,Sliding - window - based matching on the original sentence will answer the question,Neural models,Dependency Sliding Window,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",86,0.7747747747747747,168,0.5773195876288659,22,0.5641025641025641,1,0,Neural models: Dependency Sliding Window
170,Who called the police ? with Mrs. Mustard .,Neural models,Dependency Sliding Window,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",87,0.7837837837837838,169,0.5807560137457045,23,0.5897435897435898,1,0,Neural models: Dependency Sliding Window
171,"The dependency reordering enables the window to determine the correct answer , Jenny .",Neural models,Dependency Sliding Window,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",88,0.7927927927927928,170,0.584192439862543,24,0.6153846153846154,1,0,Neural models: Dependency Sliding Window
175,"MCTest was explicitly designed to ensure that it could not be solved by lexical techniques alone , but would instead require some form of inference or limited reasoning .",Neural models,Evidence,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",92,0.8288288288288288,174,0.5979381443298969,28,0.717948717948718,1,0,Neural models: Evidence
176,It therefore includes questions where the evidence for an answer spans several sentences .,Neural models,Evidence,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",93,0.8378378378378378,175,0.6013745704467354,29,0.7435897435897436,1,0,Neural models: Evidence
180,This is described in the next subsection .,Neural models,Evidence,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",97,0.8738738738738738,179,0.6151202749140894,33,0.8461538461538461,1,0,Neural models: Evidence
182,"In some cases , however , the required evidence is spread across distant sentences .",Neural models,Evidence,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",99,0.8918918918918919,181,0.6219931271477663,35,0.8974358974358975,1,0,Neural models: Evidence
183,"To give our model some capacity to deal with this scenario , we take the top N sentences as scored by all the preceding functions , and then repeat the scoring computations viewing these top N as a single sentence .",Neural models,Evidence,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",100,0.9009009009009008,182,0.6254295532646048,36,0.9230769230769232,1,0,Neural models: Evidence
184,The reasoning behind these approaches can be explained well in a probabilistic setting .,Neural models,Evidence,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",101,0.90990990990991,183,0.6288659793814433,37,0.9487179487179488,1,0,Neural models: Evidence
186,We can not model the joint probability as a product of individual terms ( score values ) because distributed pieces of evidence are likely not independent .,Neural models,Evidence,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",103,0.927927927927928,185,0.6357388316151202,39,1.0,1,0,Neural models: Evidence
191,"where is a constant margin , i * indexes the correct answer , and we take the maximum over i so that we are ranking the correct answer over the best - ranked incorrect answer ( of which there are three ) .",Neural models,Combining Perspectives,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",108,0.972972972972973,190,0.6529209621993127,4,0.5714285714285714,1,0,Neural models: Combining Perspectives
192,This approach worked better than comparing the correct answer to the incorrect answers individually as in .,Neural models,Combining Perspectives,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",109,0.981981981981982,191,0.6563573883161512,5,0.7142857142857143,1,0,Neural models: Combining Perspectives
194,2,Neural models,Combining Perspectives,question-answering,4,['O'],['O'],111,1.0,193,0.6632302405498282,7,1.0,1,0,Neural models: Combining Perspectives
195,Training Wheels,,,question-answering,4,"['O', 'O']","['O', 'O']",0,0.0,194,0.6666666666666666,0,0.0,1,0,
197,Training did not converge on the small MCTest without this vital approach .,Training Wheels,Training Wheels,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.2,196,0.6735395189003437,2,0.2,1,0,Training Wheels
199,"Therefore , we initialized the network for the semantic perspective to perform this sum , by initializing A x as the identity matrix and bx A as the zero vector , x ? {t , h} .",Training Wheels,Training Wheels,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.4,198,0.6804123711340206,4,0.4,1,0,Training Wheels
200,Recall that the activation function is a ReLU so that positive outputs are unchanged .,Training Wheels,Training Wheels,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.5,199,0.6838487972508591,5,0.5,1,0,Training Wheels
201,"We also found basic word - matching scores to be helpful , so we initialized the word - by - word networks likewise .",Training Wheels,Training Wheels,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.6,200,0.6872852233676976,6,0.6,1,0,Training Wheels
204,"For instance , proposed the identity - matrix initialization in the context of recurrent neural networks in order to preserve the error signal through backpropagation .",Training Wheels,Training Wheels,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.9,203,0.697594501718213,9,0.9,1,0,Training Wheels
205,"In residual networks , shortcut connections bypass certain layers in the network so that a simpler function can be trained in conjunction with the full model .",Training Wheels,Training Wheels,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,1.0,204,0.7010309278350515,10,1.0,1,0,Training Wheels
206,Experiments,,,question-answering,4,['O'],['O'],0,0.0,205,0.7044673539518901,0,0.0,1,0,
207,The Dataset,Experiments,,question-answering,4,"['O', 'O']","['O', 'O']",1,0.125,206,0.7079037800687286,0,0.0,1,0,Experiments
211,"Despite the elementary level , stories and questions are more natural and more complex than those found in synthetic MC datasets like bAb I and CNN .",Experiments,The Dataset,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.625,210,0.7216494845360825,4,0.5714285714285714,1,0,Experiments: The Dataset
212,MCTest is challenging because it is both complicated and small .,Experiments,The Dataset,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.75,211,0.7250859106529209,5,0.7142857142857143,1,0,Experiments: The Dataset
213,"As per , "" it is very difficult to train statistical models only on MCTest . """,Experiments,The Dataset,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.875,212,0.7285223367697594,6,0.8571428571428571,1,0,Experiments: The Dataset
214,"It s size limits the number of parameters that can be trained , and prevents learning any complex language modeling simultaneously with the capacity to answer questions .",Experiments,The Dataset,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,1.0,213,0.7319587628865979,7,1.0,1,0,Experiments: The Dataset
215,Training and Model Details,,,question-answering,4,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",0,0.0,214,0.7353951890034365,0,0.0,1,0,
219,"These vectors are kept fixed throughout training , since we found that training them was not helpful ( likely because of MCTest 's size ) .",Training and Model Details,Training and Model Details,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.16,218,0.7491408934707904,4,0.16,1,0,Training and Model Details
223,3,Training and Model Details,Training and Model Details,question-answering,4,['O'],['O'],8,0.32,222,0.7628865979381443,8,0.32,1,0,Training and Model Details
224,"However , we douse a short stopword list for questions .",Training and Model Details,Training and Model Details,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.36,223,0.7663230240549829,9,0.36,1,0,Training and Model Details
225,"This list nullifies query words such as { Who , what , when , where , how} , along with conjugations of the verbs to do and to be .",Training and Model Details,Training and Model Details,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.4,224,0.7697594501718213,10,0.4,1,0,Training and Model Details
227,"When a question contains the words which and not , we negate the hypothesis ranking scores so that the minimum becomes the maximum .",Training and Model Details,Training and Model Details,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.48,226,0.7766323024054983,12,0.48,1,0,Training and Model Details
230,The identity initialization requires that the network weight matrices are square ( d = D ) .,Training and Model Details,Training and Model Details,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.6,229,0.7869415807560137,15,0.6,1,0,Training and Model Details
232,"Dropout occurs after all neural - network transformations , if those transformations are allowed to change with training .",Training and Model Details,Training and Model Details,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.68,231,0.7938144329896907,17,0.68,1,0,Training and Model Details
237,"MCTest 's original validation set is too small for reliable hyperparameter tuning , so , following , we merged the training and validation sets of MCTest - 160 and MCTest - 500 , then split them randomly into a 250 - story training set and a 200 - story validation set .",Training and Model Details,Training and Model Details,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.88,236,0.8109965635738832,22,0.88,1,0,Training and Model Details
240,"Clearly , MCTest - 160 is easier .",Training and Model Details,Training and Model Details,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,1.0,239,0.8213058419243986,25,1.0,1,0,Training and Model Details
241,Results,,,question-answering,4,['O'],['O'],0,0.0,240,0.8247422680412371,0,0.0,1,0,
243,+,Results,Results,question-answering,4,['O'],['O'],2,0.0476190476190476,242,0.8316151202749141,2,0.1428571428571428,1,0,Results
249,We suspect this is because our neural method suffered from the relative lack of training data .,Results,Results,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.1904761904761904,248,0.852233676975945,8,0.5714285714285714,1,0,Results
253,"The Neural Reasoner and the Attentive Reader are large , deep models with hundreds of thousands of parameters , so it is unsurprising that they performed poorly on MCTest .",Results,Results,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.2857142857142857,252,0.865979381443299,12,0.8571428571428571,1,0,Results
255,"Because there are similarities between our model and the HABCNN , we hypothesize that much of the performance difference is attributable to our training wheels methodology .",Results,Results,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.3333333333333333,254,0.872852233676976,14,1.0,1,0,Results
256,Analysis and Discussion,Results,,question-answering,4,"['O', 'O', 'O']","['O', 'O', 'O']",15,0.3571428571428571,255,0.8762886597938144,0,0.0,1,0,Results
260,"Without this , the model has almost no Method MCTest - 160 accuracy ( % )",Results,Analysis and Discussion,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.4523809523809524,259,0.8900343642611683,4,0.1481481481481481,1,0,Results: Analysis and Discussion
261,MCTest - 500 accuracy ( % ) Single means for synthesizing distributed evidence .,Results,Analysis and Discussion,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.4761904761904761,260,0.8934707903780069,5,0.1851851851851851,1,0,Results: Analysis and Discussion
264,Simple word - by - word matching is obviously useful on MCTest .,Results,Analysis and Discussion,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",23,0.5476190476190477,263,0.9037800687285223,8,0.2962962962962963,1,0,Results: Analysis and Discussion
267,We found this surprising .,Results,Analysis and Discussion,question-answering,4,"['O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O']",26,0.6190476190476191,266,0.9140893470790378,11,0.4074074074074074,1,0,Results: Analysis and Discussion
268,It maybe that linearization of the dependency graph removes too much of its information .,Results,Analysis and Discussion,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.6428571428571429,267,0.9175257731958762,12,0.4444444444444444,1,0,Results: Analysis and Discussion
272,"This weakness is not unexpected , since our architecture lacks any capacity for counting or tracking temporal order .",Results,Analysis and Discussion,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",31,0.7380952380952381,271,0.9312714776632304,16,0.5925925925925926,1,0,Results: Analysis and Discussion
273,"Incorporating mechanisms for these forms of reasoning is a priority for future work ( in contrast , the Memory Network model is quite good at temporal reasoning ) .",Results,Analysis and Discussion,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",32,0.7619047619047619,272,0.9347079037800688,17,0.6296296296296297,1,0,Results: Analysis and Discussion
276,It s simplicity is a response to the limited data of MCTest .,Results,Analysis and Discussion,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",35,0.8333333333333334,275,0.9450171821305842,20,0.7407407407407407,1,0,Results: Analysis and Discussion
280,"This suggests that , to learn true reasoning abilities , MCTest is too simple a dataset - and it is almost certainly too small for this goal .",Results,Analysis and Discussion,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",39,0.9285714285714286,279,0.9587628865979382,24,0.8888888888888888,1,0,Results: Analysis and Discussion
281,"However , it maybe that human language processing can be factored into separate processes of comprehension and reasoning .",Results,Analysis and Discussion,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",40,0.9523809523809524,280,0.9621993127147768,25,0.925925925925926,1,0,Results: Analysis and Discussion
282,"If so , the Parallel - Hierarchical model is a good start on the former .",Results,Analysis and Discussion,question-answering,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",41,0.9761904761904762,281,0.9656357388316152,26,0.9629629629629628,1,0,Results: Analysis and Discussion
284,Conclusion,,,question-answering,4,['O'],['O'],0,0.0,283,0.972508591065292,0,0.0,1,0,
3,abstract,,,question-answering,5,['O'],['O'],0,0.0,2,0.0090497737556561,0,0.0,1,0,
8,"Recently , the idea of training machine comprehension models that can read , understand , and answer questions about a text has come closer to reality principally through two factors .",Introduction,Introduction,question-answering,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0109890109890109,7,0.0316742081447963,1,0.0454545454545454,1,0,Introduction
12,The task is to guess which word was deleted .,Introduction,Introduction,question-answering,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.0549450549450549,11,0.0497737556561085,5,0.2272727272727272,1,0,Introduction
13,"Ina pragmatic approach , recent work formed such questions by extracting a sentence from a larger document .",Introduction,Introduction,question-answering,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.0659340659340659,12,0.0542986425339366,6,0.2727272727272727,1,0,Introduction
14,"In contrast to considering a stand - alone statement , the system is now required to handle a larger amount of information that may possibly influence the prediction of the missing word .",Introduction,Introduction,question-answering,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.0769230769230769,13,0.0588235294117647,7,0.3181818181818182,1,0,Introduction
15,Such contextual dependencies may also be injected by removing a word from a short human - crafted summary of a larger body of text .,Introduction,Introduction,question-answering,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.0879120879120879,14,0.0633484162895927,8,0.3636363636363636,1,0,Introduction
16,The abstractive nature of the summary is likely to demand a higher level of comprehension of the original text .,Introduction,Introduction,question-answering,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.0989010989010989,15,0.0678733031674208,9,0.4090909090909091,1,0,Introduction
17,"In both cases , the machine comprehension system is presented with an ablated query and the document to which the original query refers .",Introduction,Introduction,question-answering,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.1098901098901098,16,0.0723981900452488,10,0.4545454545454545,1,0,Introduction
18,The missing word is assumed to appear in the document .,Introduction,Introduction,question-answering,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.1208791208791208,17,0.0769230769230769,11,0.5,1,0,Introduction
23,The result of this alternating search is fed back into the iterative inference process to seed the next search step .,Introduction,Introduction,question-answering,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.1758241758241758,22,0.0995475113122171,16,0.7272727272727273,1,0,Introduction
30,Task Description,Introduction,,question-answering,5,"['O', 'O']","['O', 'O']",23,0.2527472527472527,29,0.1312217194570135,0,0.0,1,0,Introduction
31,One of the advantages of using Cloze - style questions to evaluate machine comprehension systems is that a sufficient amount of training and test data can be obtained without human intervention .,Introduction,Task Description,question-answering,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.2637362637362637,30,0.1357466063348416,1,0.0769230769230769,1,0,Introduction: Task Description
32,The CBT and corpora are two such datasets .,Introduction,Task Description,question-answering,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.2747252747252747,31,0.1402714932126696,2,0.1538461538461538,1,0,Introduction: Task Description
33,The CBT 1 corpus was generated from well - known children 's books available through Project Gutenberg .,Introduction,Task Description,question-answering,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.2857142857142857,32,0.1447963800904977,3,0.2307692307692307,1,0,Introduction: Task Description
35,The related query is formed from an excerpt 's 21st sentence by replacing a single word with an anonymous placeholder token .,Introduction,Task Description,question-answering,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",28,0.3076923076923077,34,0.1538461538461538,5,0.3846153846153846,1,0,Introduction: Task Description
37,"The subsets are named entity , common noun , verb , and preposition .",Introduction,Task Description,question-answering,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",30,0.3296703296703296,36,0.1628959276018099,7,0.5384615384615384,1,0,Introduction: Task Description
38,"We will focus our evaluation solely on the first two subsets , i.e. CBT - NE ( named entity ) and CBT - CN ( common nouns ) , since the latter two are relatively simple as demonstrated by .",Introduction,Task Description,question-answering,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",31,0.3406593406593406,37,0.167420814479638,8,0.6153846153846154,1,0,Introduction: Task Description
42,"For both datasets , the training and evaluation data consist of tuples ( Q , D , A , a ) , where Q is the query ( represented as a sequence of words ) , Dis the document , A is the set of possible answers , and a ?",Introduction,Task Description,question-answering,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",35,0.3846153846153846,41,0.1855203619909502,12,0.9230769230769232,1,0,Introduction: Task Description
43,A is,Introduction,,question-answering,5,"['O', 'O']","['O', 'O']",36,0.3956043956043956,42,0.1900452488687782,13,1.0,1,0,Introduction
44,Alternating Iterative Attention,Introduction,,question-answering,5,"['O', 'O', 'O']","['O', 'O', 'O']",37,0.4065934065934066,43,0.1945701357466063,0,0.0,1,0,Introduction
45,Our model is represented in .,Introduction,Alternating Iterative Attention,question-answering,5,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O']",38,0.4175824175824176,44,0.1990950226244343,1,0.1428571428571428,1,0,Introduction: Alternating Iterative Attention
52,Bidirectional Encoding,Introduction,,question-answering,5,"['O', 'O']","['O', 'O']",45,0.4945054945054945,51,0.2307692307692307,0,0.0,1,0,Introduction
53,"The input to the encoding phase is a sequence of words X = ( x 1 , . . . , x | X | ) , such as a document or a query , drawn from a vocabulary V .",Introduction,Bidirectional Encoding,question-answering,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",46,0.5054945054945055,52,0.2352941176470588,1,0.0476190476190476,1,0,Introduction: Bidirectional Encoding
54,Each word is represented by a continuous word embedding x ?,Introduction,Bidirectional Encoding,question-answering,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",47,0.5164835164835165,53,0.2398190045248869,2,0.0952380952380952,1,0,Introduction: Bidirectional Encoding
55,Rd stored in a word embedding matrix X ? R | V |d .,Introduction,Bidirectional Encoding,question-answering,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",48,0.5274725274725275,54,0.2443438914027149,3,0.1428571428571428,1,0,Introduction: Bidirectional Encoding
63,"where hi , r i and u i ?",Introduction,Bidirectional Encoding,question-answering,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",56,0.6153846153846154,62,0.2805429864253393,11,0.5238095238095238,1,0,Introduction: Bidirectional Encoding
64,"Rh are the recurrent state , the reset gate and update gate respectively , I {r , u , h} ?",Introduction,Bidirectional Encoding,question-answering,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",57,0.6263736263736264,63,0.2850678733031674,12,0.5714285714285714,1,0,Introduction: Bidirectional Encoding
65,"R hd , H {r , u , h} ?",Introduction,Bidirectional Encoding,question-answering,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",58,0.6373626373626373,64,0.2895927601809955,13,0.6190476190476191,1,0,Introduction: Bidirectional Encoding
66,"R hh are the parameters of the GRU , ?",Introduction,Bidirectional Encoding,question-answering,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",59,0.6483516483516484,65,0.2941176470588235,14,0.6666666666666666,1,0,Introduction: Bidirectional Encoding
67,is the sigmoid function and is the elementwise multiplication .,Introduction,Bidirectional Encoding,question-answering,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",60,0.6593406593406593,66,0.2986425339366516,15,0.7142857142857143,1,0,Introduction: Bidirectional Encoding
68,The hidden state hi acts as a representation of the word x i in the context of the preceding sequence inputs x < i .,Introduction,Bidirectional Encoding,question-answering,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",61,0.6703296703296703,67,0.3031674208144796,16,0.7619047619047619,1,0,Introduction: Bidirectional Encoding
69,"In order to incorporate information from the future tokens x > i , we choose to process the sequence in reverse with an additional GRU .",Introduction,Bidirectional Encoding,question-answering,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",62,0.6813186813186813,68,0.3076923076923077,17,0.8095238095238095,1,0,Introduction: Bidirectional Encoding
73,R 2h the contextual encodings for word i in the query Q and the document D respectively .,Introduction,Bidirectional Encoding,question-answering,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",66,0.7252747252747253,72,0.3257918552036199,21,1.0,1,0,Introduction: Bidirectional Encoding
74,Iterative Alternating Attention,Introduction,,question-answering,5,"['O', 'O', 'O']","['O', 'O', 'O']",67,0.7362637362637363,73,0.3303167420814479,0,0.0,1,0,Introduction
75,This phase can be considered a means to uncover a possible inference chain that starts at the query and the document and leads to the answer .,Introduction,Iterative Alternating Attention,question-answering,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",68,0.7472527472527473,74,0.334841628959276,1,0.0625,1,0,Introduction: Iterative Alternating Attention
79,"In turn , both attentive reads are conditioned on the previous hidden state of the inference GRU s t ?1 , summarizing the information that has been gathered from the query and the document up to time t.",Introduction,Iterative Alternating Attention,question-answering,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",72,0.7912087912087912,78,0.3529411764705882,5,0.3125,1,0,Introduction: Iterative Alternating Attention
81,Query Attentive Read,Introduction,,question-answering,5,"['O', 'O', 'O']","['O', 'O', 'O']",74,0.8131868131868132,80,0.3619909502262443,7,0.4375,1,0,Introduction
82,"Given the query encodings {q i } , we formulate a query glimpse qt at timestep t by :",Introduction,Query Attentive Read,question-answering,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",75,0.8241758241758241,81,0.3665158371040724,8,0.5,1,0,Introduction: Query Attentive Read
83,"where q i , tare the query attention weights and A q ?",Introduction,Query Attentive Read,question-answering,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",76,0.8351648351648352,82,0.3710407239819004,9,0.5625,1,0,Introduction: Query Attentive Read
84,"R 2hs , where sis the dimensionality of the inference GRU state , and a q ?",Introduction,Query Attentive Read,question-answering,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",77,0.8461538461538461,83,0.3755656108597285,10,0.625,1,0,Introduction: Query Attentive Read
86,"The attention we use here is similar to the formulation used in , but with two differences .",Introduction,Query Attentive Read,question-answering,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",79,0.8681318681318682,85,0.3846153846153846,12,0.75,1,0,Introduction: Query Attentive Read
88,This simple bilinear attention has been successfully used in .,Introduction,Query Attentive Read,question-answering,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",81,0.8901098901098901,87,0.3936651583710407,14,0.875,1,0,Introduction: Query Attentive Read
89,"Second , we add a term a q that allows to bias the attention mechanism towards words which tend to be important across the questions independently of the search key s t?1 .",Introduction,Query Attentive Read,question-answering,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",82,0.9010989010989012,88,0.3981900452488687,15,0.9375,1,0,Introduction: Query Attentive Read
90,This is similar to what is achieved by the original attention mechanism proposed in without the burden of the additional tanh layer .,Introduction,Query Attentive Read,question-answering,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",83,0.912087912087912,89,0.4027149321266968,16,1.0,1,0,Introduction: Query Attentive Read
91,Document Attentive Read,Introduction,,question-answering,5,"['O', 'O', 'O']","['O', 'O', 'O']",84,0.9230769230769232,90,0.4072398190045249,0,0.0,1,0,Introduction
93,"In particular , the document attention weights are computed based on both the previous search state and the currently selected query glimpse qt :",Introduction,Document Attentive Read,question-answering,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",86,0.945054945054945,92,0.416289592760181,2,0.2857142857142857,1,0,Introduction: Document Attentive Read
94,"where d i , tare the attention weights for each word in the document and A d ?",Introduction,Document Attentive Read,question-answering,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",87,0.956043956043956,93,0.420814479638009,3,0.4285714285714285,1,0,Introduction: Document Attentive Read
95,R 2 h ( s + 2h ) and ad ?,Introduction,Document Attentive Read,question-answering,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",88,0.967032967032967,94,0.4253393665158371,4,0.5714285714285714,1,0,Introduction: Document Attentive Read
97,Note that the document attention is also conditioned on s t?1 .,Introduction,Document Attentive Read,question-answering,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",90,0.989010989010989,96,0.4343891402714932,6,0.8571428571428571,1,0,Introduction: Document Attentive Read
99,Gating Search Results,,,question-answering,5,"['O', 'O', 'O']","['O', 'O', 'O']",0,0.0,98,0.4434389140271493,0,0.0,1,0,
100,"In order to update its recurrent state , the inference GRU may evolve on the basis of the information gathered from the current inference step , i.e.",Gating Search Results,Gating Search Results,question-answering,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0588235294117647,99,0.4479638009049774,1,0.0833333333333333,1,0,Gating Search Results
101,"However , the current query glimpse maybe too general or the document may not contain the information specified in the query glimpse , i.e. the query or the document attention weights maybe nearly uniform .",Gating Search Results,Gating Search Results,question-answering,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.1176470588235294,100,0.4524886877828054,2,0.1666666666666666,1,0,Gating Search Results
103,"Formally , we implement a gating mech -",Gating Search Results,Gating Search Results,question-answering,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.2352941176470588,102,0.4615384615384615,4,0.3333333333333333,1,0,Gating Search Results
104,", where is the element - wise multiplication and g :",Gating Search Results,Gating Search Results,question-answering,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.2941176470588235,103,0.4660633484162896,5,0.4166666666666667,1,0,Gating Search Results
107,The gate g takes the form of a 2 - layer feed - forward network with sigmoid output unit activation .,Gating Search Results,Gating Search Results,question-answering,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.4705882352941176,106,0.4796380090497738,8,0.6666666666666666,1,0,Gating Search Results
109,"Given a query gate g q , producing r q , and a document gate g d , producing rd , the inputs of the inference GRU are given by the reset version of the query and document glimpses , i.e. ,",Gating Search Results,Gating Search Results,question-answering,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.5882352941176471,108,0.4886877828054298,10,0.8333333333333334,1,0,Gating Search Results
110,.,Gating Search Results,Gating Search Results,question-answering,5,['O'],['O'],11,0.6470588235294118,109,0.4932126696832579,11,0.9166666666666666,1,0,Gating Search Results
112,Answer Prediction,Gating Search Results,,question-answering,5,"['O', 'O']","['O', 'O']",13,0.7647058823529411,111,0.502262443438914,0,0.0,1,0,Gating Search Results
113,"After a fixed number of time - steps T , the document attention weights obtained in the last search step d i , T are used to predict the probability of the answer given the document and the query P ( a|Q , D ) .",Gating Search Results,Answer Prediction,question-answering,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.8235294117647058,112,0.5067873303167421,1,0.25,1,0,Gating Search Results: Answer Prediction
114,"Formally , we follow and apply the "" pointer - sum "" loss :",Gating Search Results,Answer Prediction,question-answering,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.8823529411764706,113,0.5113122171945701,2,0.5,1,0,Gating Search Results: Answer Prediction
115,"where I ( a , D ) is a set of positions where a occurs in the document .",Gating Search Results,Answer Prediction,question-answering,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.9411764705882352,114,0.5158371040723982,3,0.75,1,0,Gating Search Results: Answer Prediction
117,Training Details,,,question-answering,5,"['O', 'O']","['O', 'O']",0,0.0,116,0.5248868778280543,0,0.0,1,0,
121,"Following , the GRU recurrent weights are initialized to be orthogonal and biases are initialized to zero .",Training Details,Training Details,question-answering,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.3333333333333333,120,0.5429864253393665,4,0.3333333333333333,1,0,Training Details
122,"In order to stabilize the learning , we clip the gradients if their norm is greater than 5 and those marked with 2 are from .",Training Details,Training Details,question-answering,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.4166666666666667,121,0.5475113122171946,5,0.4166666666666667,1,0,Training Details
123,the inputs to both the query and the document attention mechanisms .,Training Details,Training Details,question-answering,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.5,122,0.5520361990950227,6,0.5,1,0,Training Details
126,"Computational Complexity Similar to previous state - of - the - art models which use a bidirectional encoder , the major bottleneck of our method is computing the document and query encodings .",Training Details,Training Details,question-answering,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.75,125,0.5656108597285068,9,0.75,1,0,Training Details
127,"The alternating attention mechanism runs only fora fixed number of steps ( T = 8 in our tests ) , which is orders of magnitude smaller than atypical document or query in our datasets ( see ) .",Training Details,Training Details,question-answering,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.8333333333333334,126,0.5701357466063348,10,0.8333333333333334,1,0,Training Details
130,Results,,,question-answering,5,['O'],['O'],0,0.0,129,0.583710407239819,0,0.0,1,0,
132,"The Humans , LSTMs and Memory Networks ( Mem NNs ) results are taken from and the Attention - Sum Reader ( AS Reader ) is a state - of - the - art result recently obtained by .",Results,Results,question-answering,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.6666666666666666,131,0.5927601809954751,2,1.0,1,0,Results
133,CBT,Results,,question-answering,5,['O'],['O'],3,1.0,132,0.5972850678733032,0,0.0,1,0,Results
134,Main result,,,question-answering,5,"['O', 'O']","['O', 'O']",0,0.0,133,0.6018099547511312,0,0.0,1,0,
136,This performance gap is only partially reflected on the CBT - NE dataset .,Main result,Main result,question-answering,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.1333333333333333,135,0.6108597285067874,2,0.1428571428571428,1,0,Main result
138,"In CBT - NE , the missing word is a named entity appearing in the story which is likely to be less frequent than a common noun .",Main result,Main result,question-answering,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.2666666666666666,137,0.6199095022624435,4,0.2857142857142857,1,0,Main result
140,"These numbers are considerably lower for the CBT - CN , for which only 2.5 % and 4.6 % of validation and test examples respectively contain an answer that has not been previously seen .",Main result,Main result,question-answering,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.4,139,0.6289592760180995,6,0.4285714285714285,1,0,Main result
145,"In order to measure the impact of the query attention step in our model , we constrain the query attention weights q i , t to be uniform , i.e. q i , t = 1 / | Q | , for all t = 1 , . . . , T ( line 6 ) .",Main result,Main result,question-answering,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.7333333333333333,144,0.6515837104072398,11,0.7857142857142857,1,0,Main result
148,A similar scenario was observed on the CNN dataset ..,Main result,Main result,question-answering,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.9333333333333332,147,0.665158371040724,14,1.0,1,0,Main result
149,CNN,Main result,,question-answering,5,['O'],['O'],15,1.0,148,0.669683257918552,0,0.0,1,0,Main result
150,Main result,,,question-answering,5,"['O', 'O']","['O', 'O']",0,0.0,149,0.6742081447963801,0,0.0,1,0,
152,We also report the very recent results of the Stanford AR system that came to our attention during the writeup of this article ) ( line 9 ) .,Main result,Main result,question-answering,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.0465116279069767,151,0.6832579185520362,2,0.1,1,0,Main result
154,We note that the latter comparison maybe influenced by different training and initialization strategies .,Main result,Main result,question-answering,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.0930232558139534,153,0.6923076923076923,4,0.2,1,0,Main result
157,Ensembles,Main result,,question-answering,5,['O'],['O'],7,0.1627906976744186,156,0.7058823529411765,7,0.35,1,0,Main result
161,"Categories that only require local context matching around the placeholder and the answer in the text are Exact Match , Paraphrasing , and Partial Clue , while those which require higher reasoning skills are Multiple Sentences and Ambiguous .",Main result,Ensembles,question-answering,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.2558139534883721,160,0.7239819004524887,11,0.55,1,0,Main result: Ensembles
162,"For example , in Exact Match examples , the question placeholder and the answer in the document share several neighboring exact words .",Main result,Ensembles,question-answering,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.2790697674418604,161,0.7285067873303167,12,0.6,1,0,Main result: Ensembles
164,"The first three categories require local context matching , the next two global context matching and coreference errors are unanswerable questions .",Main result,Ensembles,question-answering,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.3255813953488372,163,0.7375565610859729,14,0.7,1,0,Main result: Ensembles
167,"One hypothesis is that , in contrast to Stanford AR , which uses only one fixedquery attention step , our iterative attention may better explore the documents and queries .",Main result,Ensembles,question-answering,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.3953488372093023,166,0.751131221719457,17,0.85,1,0,Main result: Ensembles
168,"Finally , Coreference Errors ( ? 25 % of the corpus ) includes examples with critical coreference resolution errors which may make the questions "" unanswerable "" .",Main result,Ensembles,question-answering,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.4186046511627907,167,0.755656108597285,18,0.9,1,0,Main result: Ensembles
169,This is a barrier to achieving accuracies considerably above 75 % .,Main result,Ensembles,question-answering,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.4418604651162791,168,0.7601809954751131,19,0.95,1,0,Main result: Ensembles
171,Discussion,Main result,,question-answering,5,['O'],['O'],21,0.4883720930232558,170,0.7692307692307693,0,0.0,1,0,Main result
175,Each row corresponds to an inference timestep 1 ? t ?,Main result,Discussion,question-answering,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.5813953488372093,174,0.7873303167420814,4,0.1818181818181818,1,0,Main result: Discussion
177,"The model first focuses on @entity148 , which corresponds to "" Greek "" in this The approach to teaching @entity6 in @placeholder schools needs a makeover , she says : Visualization of the alternated attention mechanism for an article in CNN , treating about the decline of the Italian language in schools .",Main result,Discussion,question-answering,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.627906976744186,176,0.7963800904977375,6,0.2727272727272727,1,0,Main result: Discussion
178,The title of the plot is the query .,Main result,Discussion,question-answering,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",28,0.6511627906976745,177,0.8009049773755657,7,0.3181818181818182,1,0,Main result: Discussion
180,"The target is @entity3 which corresponds to the word "" Italian "" .",Main result,Discussion,question-answering,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",30,0.6976744186046512,179,0.8099547511312217,9,0.4090909090909091,1,0,Main result: Discussion
182,"At t = 2 , the query attention moves towards "" schools "" and the model hesitates between "" Italian "" and "" European Union "" ( @entity28 , see step 3 ) , both of which may satisfy the query .",Main result,Discussion,question-answering,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",32,0.7441860465116279,181,0.8190045248868778,11,0.5,1,0,Main result: Discussion
183,"At step 3 , the most likely candidates are "" European Union "" and "" Rome "" ( @entity159 ) .",Main result,Discussion,question-answering,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",33,0.7674418604651163,182,0.8235294117647058,12,0.5454545454545454,1,0,Main result: Discussion
187,"For these particular datasets , the majority of questions can be answered after attending only to the words directly neighbouring the placeholder .",Main result,Discussion,question-answering,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",37,0.8604651162790697,186,0.8416289592760181,16,0.7272727272727273,1,0,Main result: Discussion
189,"It would be worthwhile to formulate a dataset in which the placeholder is harder to infer using only local neighboring words , and thereby necessitates deeper query exploration .",Main result,Discussion,question-answering,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",39,0.9069767441860463,188,0.8506787330316742,18,0.8181818181818182,1,0,Main result: Discussion
193,A straight - forward extension of the model would be to dynamically select the number of inference steps conditioned on each example .,Main result,Discussion,question-answering,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",43,1.0,192,0.8687782805429864,22,1.0,1,0,Main result: Discussion
194,Related Works,,,question-answering,5,"['O', 'O']","['O', 'O']",0,0.0,193,0.8733031674208145,0,0.0,1,0,
214,Conclusion,,,question-answering,5,['O'],['O'],0,0.0,213,0.9638009049773756,0,0.0,1,0,
3,abstract,,,question-answering,6,['O'],['O'],0,0.0,2,0.006060606060606,0,0.0,1,0,
11,"For example , consider we know that Frogs eat insects and Flies are insects .",INTRODUCTION,INTRODUCTION,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.064516129032258,10,0.0303030303030303,2,0.064516129032258,1,0,INTRODUCTION
12,Then answering Do frogs eat flies ?,INTRODUCTION,INTRODUCTION,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.0967741935483871,11,0.0333333333333333,3,0.0967741935483871,1,0,INTRODUCTION
13,requires reasoning over both of the above facts .,INTRODUCTION,INTRODUCTION,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.1290322580645161,12,0.0363636363636363,4,0.1290322580645161,1,0,INTRODUCTION
14,"Question answering , more specifically context - based QA , has been extensively studied in machine comprehension tasks .",INTRODUCTION,INTRODUCTION,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.1612903225806451,13,0.0393939393939393,5,0.1612903225806451,1,0,INTRODUCTION
15,"However , most of the datasets are primarily focused on lexical and syntactic understanding , and hardly concentrate on inference over multiple facts .",INTRODUCTION,INTRODUCTION,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.1935483870967742,14,0.0424242424242424,6,0.1935483870967742,1,0,INTRODUCTION
16,"Recently , several datasets aimed for testing multi-hop reasoning have emerged ; among them are story - based QA and the dialog task .",INTRODUCTION,INTRODUCTION,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.2258064516129032,15,0.0454545454545454,7,0.2258064516129032,1,0,INTRODUCTION
17,"Recurrent Neural Network ( RNN ) and its variants , such as Long Short - Term Memory ( LSTM ) and Gated Recurrent Unit ( GRU ) , are popular choices for modeling natural language .",INTRODUCTION,INTRODUCTION,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.2580645161290322,16,0.0484848484848484,8,0.2580645161290322,1,0,INTRODUCTION
19,This is largely due to the fact that RNN 's internal memory is inherently unstable over along term .,INTRODUCTION,INTRODUCTION,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.3225806451612903,18,0.0545454545454545,10,0.3225806451612903,1,0,INTRODUCTION
20,"For this reason , most recent approaches in the literature have mainly relied on global attention mechanism and shared external memory .",INTRODUCTION,INTRODUCTION,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.3548387096774194,19,0.0575757575757575,11,0.3548387096774194,1,0,INTRODUCTION
23,"However , one major drawback of these standard attention mechanisms is that they are insensitive to the time step ( memory address ) of the sentences when accessing them .",INTRODUCTION,INTRODUCTION,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.4516129032258064,22,0.0666666666666666,14,0.4516129032258064,1,0,INTRODUCTION
26,"For instance in , the original question , Where is the apple ? , can not be directly answered by any single sentence from the story .",INTRODUCTION,INTRODUCTION,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.5483870967741935,25,0.0757575757575757,17,0.5483870967741935,1,0,INTRODUCTION
27,"After observing the first sentence , Sandra got the apple there , QRN transforms the original question to a reduced query",INTRODUCTION,INTRODUCTION,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.5806451612903226,26,0.0787878787878787,18,0.5806451612903226,1,0,INTRODUCTION
28,"Where is Sandra ? , which is presumably . ? and ?",INTRODUCTION,INTRODUCTION,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.6129032258064516,27,0.0818181818181818,19,0.6129032258064516,1,0,INTRODUCTION
29,"are update gate and reduce functions , respectively .?",INTRODUCTION,INTRODUCTION,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.6451612903225806,28,0.0848484848484848,20,0.6451612903225806,1,0,INTRODUCTION
30,"is assigned to be h 2 5 , the local query at the last time step in the last layer .",INTRODUCTION,INTRODUCTION,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.6774193548387096,29,0.0878787878787878,21,0.6774193548387096,1,0,INTRODUCTION
31,"Also , red-colored text is the inferred meanings of the vectors ( see. easier to answer than the original question given the context provided by the first sentence .",INTRODUCTION,INTRODUCTION,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.7096774193548387,30,0.0909090909090909,22,0.7096774193548387,1,0,INTRODUCTION
32,"2 Unlike RNN - based models , QRN 's candidate state ( h t in ) does not depend on the previous hidden state ( h t?1 ) .",INTRODUCTION,INTRODUCTION,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",23,0.7419354838709677,31,0.0939393939393939,23,0.7419354838709677,1,0,INTRODUCTION
37,"Hence it is well - suited for sequential data with both local and global interactions ( note that QRN is not the replacement of RNN , which is arguably better for modeling complex local interactions ) .",INTRODUCTION,INTRODUCTION,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",28,0.9032258064516128,36,0.109090909090909,28,0.9032258064516128,1,0,INTRODUCTION
38,"Third , unlike most RNN - based models , QRN can be parallelized overtime by computing candidate reduced queries ( h t ) directly from local input queries ( q t ) and context sentence vectors ( x t ) .",INTRODUCTION,INTRODUCTION,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",29,0.935483870967742,37,0.1121212121212121,29,0.935483870967742,1,0,INTRODUCTION
39,"In fact , the parallelizability of QRN implies that QRN does not suffer from the vanishing gradient problem of RNN , hence effectively addressing the long - term dependency .",INTRODUCTION,INTRODUCTION,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",30,0.967741935483871,38,0.1151515151515151,30,0.967741935483871,1,0,INTRODUCTION
41,MODEL,,,question-answering,6,['O'],['O'],0,0.0,40,0.1212121212121212,0,0.0,1,0,
42,"In story - based QA ( or dialog dataset ) , the input is the context as a sequence of sentences ( story or past conversations ) and a question in natural language ( equivalent to the user 's last utterance in the dialog ) .",MODEL,MODEL,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0083333333333333,41,0.1242424242424242,1,0.04,1,0,MODEL
44,The only supervision provided during training is the answer to the question .,MODEL,MODEL,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.025,43,0.1303030303030303,3,0.12,1,0,MODEL
47,"T denote the sequence of sentences , where T is the number of sentences in the story , and let q denote the question .",MODEL,"Let x 1 , . . . , x",question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.05,46,0.1393939393939394,6,0.24,1,0,"MODEL: Let x 1 , . . . , x"
48,Let ?,MODEL,"Let x 1 , . . . , x",question-answering,6,"['O', 'O']","['O', 'O']",7,0.0583333333333333,47,0.1424242424242424,7,0.28,1,0,"MODEL: Let x 1 , . . . , x"
49,"denote the predicted answer , and y denote the true answer .",MODEL,"Let x 1 , . . . , x",question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.0666666666666666,48,0.1454545454545454,8,0.32,1,0,"MODEL: Let x 1 , . . . , x"
51,Input module .,MODEL,,question-answering,6,"['O', 'O', 'O']","['O', 'O', 'O']",10,0.0833333333333333,50,0.1515151515151515,10,0.4,1,0,MODEL
52,"Input module maps each sentence x t and the question q to d-dimensional vector space , x t ?",MODEL,Input module .,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.0916666666666666,51,0.1545454545454545,11,0.44,1,0,MODEL: Input module .
53,Rd and qt ?,MODEL,Input module .,question-answering,6,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",12,0.1,52,0.1575757575757575,12,0.48,1,0,MODEL: Input module .
55,We adopt a previous solution for the input module ( details in Section 5 ) .,MODEL,Rd .,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.1166666666666666,54,0.1636363636363636,14,0.56,1,0,MODEL: Rd .
57,"QRN layers use the sentence vectors and the question vector from the input module to obtain the predicted answer in vector space , ? ?",MODEL,QRN layers .,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.1333333333333333,56,0.1696969696969697,16,0.64,1,0,MODEL: QRN layers .
58,"Rd . A QRN layer refers to the recurrent application of a QRN unit , which can be considered as a variant of RNN with two inputs , two outputs , and a hidden state ( reduced query ) , all of which operate in vector space .",MODEL,QRN layers .,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.1416666666666666,57,0.1727272727272727,17,0.68,1,0,MODEL: QRN layers .
60,Output module .,MODEL,,question-answering,6,"['O', 'O', 'O']","['O', 'O', 'O']",19,0.1583333333333333,59,0.1787878787878788,19,0.76,1,0,MODEL
61,Output module maps ?,MODEL,Output module .,question-answering,6,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",20,0.1666666666666666,60,0.1818181818181818,20,0.8,1,0,MODEL: Output module .
63,"Similar to the input module , we adopt a standard solution for the output module ( details in Section 5 ) .",MODEL,Output module .,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.1833333333333333,62,0.1878787878787878,22,0.88,1,0,MODEL: Output module .
67,QRN UNIT,MODEL,Output module .,question-answering,6,"['O', 'O']","['O', 'O']",26,0.2166666666666666,66,0.2,0,0.0,1,0,MODEL: Output module .
69,"depicts the schematic structure of a QRN unit , and demonstrates how layers are stacked .",MODEL,Output module .,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",28,0.2333333333333333,68,0.206060606060606,2,0.1111111111111111,1,0,MODEL: Output module .
70,A QRN unit accepts two inputs ( local query vector qt ?,MODEL,Output module .,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",29,0.2416666666666666,69,0.209090909090909,3,0.1666666666666666,1,0,MODEL: Output module .
71,"Rd and sentence vector x t ? Rd ) , and two outputs ( reduced query vector ht ?",MODEL,Output module .,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",30,0.25,70,0.2121212121212121,4,0.2222222222222222,1,0,MODEL: Output module .
72,"Rd , which is similar to the hidden state in RNN , and the sentence vector x t from the input without modification ) .",MODEL,Output module .,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",31,0.2583333333333333,71,0.2151515151515151,5,0.2777777777777778,1,0,MODEL: Output module .
73,The local query vector is not necessarily identical to the original query ( question ) vector q .,MODEL,Output module .,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",32,0.2666666666666666,72,0.2181818181818181,6,0.3333333333333333,1,0,MODEL: Output module .
74,"In order to compute the outputs , we use update gate function ? :",MODEL,Output module .,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",33,0.275,73,0.2212121212121212,7,0.3888888888888889,1,0,MODEL: Output module .
75,"Intuitively , the update gate function measures the relevance between the sentence and the local query and is used to update the hidden state .",MODEL,Output module .,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",34,0.2833333333333333,74,0.2242424242424242,8,0.4444444444444444,1,0,MODEL: Output module .
76,The reduce function transforms the local query input to a candidate state which is anew reduced ( easier ) query given the sentence .,MODEL,Output module .,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",35,0.2916666666666667,75,0.2272727272727272,9,0.5,1,0,MODEL: Output module .
77,The outputs are calculated with the following equations :,MODEL,Output module .,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",36,0.3,76,0.2303030303030303,10,0.5555555555555556,1,0,MODEL: Output module .
78,"where z t is the scalar update gate , h t is the candidate reduced query , and ht is the final reduced query at time step t , ? ( ) is sigmoid activation , tanh ( ) is hyperboolic tangent activation ( applied element - wise ) ,",MODEL,Output module .,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",37,0.3083333333333333,77,0.2333333333333333,11,0.6111111111111112,1,0,MODEL: Output module .
79,"is element - wise vector multiplication , and [ ; ] is vector concatenation along the row .",MODEL,Output module .,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",38,0.3166666666666666,78,0.2363636363636363,12,0.6666666666666666,1,0,MODEL: Output module .
80,"As abase case , h 0 = 0 .",MODEL,,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",39,0.325,79,0.2393939393939393,13,0.7222222222222222,1,0,MODEL
81,Here we have explicitly defined ?,MODEL,"As abase case , h 0 = 0 .",question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O']",40,0.3333333333333333,80,0.2424242424242424,14,0.7777777777777778,1,0,"MODEL: As abase case , h 0 = 0 ."
82,"and ? , but they can be any reasonable differentiable functions .",MODEL,"As abase case , h 0 = 0 .",question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",41,0.3416666666666667,81,0.2454545454545454,15,0.8333333333333334,1,0,"MODEL: As abase case , h 0 = 0 ."
83,The update gate is similar to the global attention mechanism in that it measures the similarity between the sentence ( a memory slot ) and the query .,MODEL,"As abase case , h 0 = 0 .",question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",42,0.35,82,0.2484848484848484,16,0.8888888888888888,1,0,"MODEL: As abase case , h 0 = 0 ."
84,"However , a significant difference is that the update gate is computed using sigmoid ( ? ) function on the current memory slot only ( hence internally embedded within the unit ) , whereas the global attention is computed using softmax function over the entire memory ( hence globally defined ) .",MODEL,"As abase case , h 0 = 0 .",question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",43,0.3583333333333333,83,0.2515151515151515,17,0.9444444444444444,1,0,"MODEL: As abase case , h 0 = 0 ."
85,The update gate can be rather considered as local sigmoid attention .,MODEL,"As abase case , h 0 = 0 .",question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",44,0.3666666666666666,84,0.2545454545454545,18,1.0,1,0,"MODEL: As abase case , h 0 = 0 ."
86,Stacking layers,MODEL,,question-answering,6,"['O', 'O']","['O', 'O']",45,0.375,85,0.2575757575757575,0,0.0,1,0,MODEL
87,"We just showed the single - layer case of QRN , but QRN with multiple layers is able to perform reasoning over multiple facts more effectively , as shown in the example of .",MODEL,Stacking layers,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",46,0.3833333333333333,86,0.2606060606060606,1,0.0476190476190476,1,0,MODEL: Stacking layers
88,"In order to stack several layers of QRN , the outputs of the current layer are used as the inputs to the next layer .",MODEL,Stacking layers,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",47,0.3916666666666666,87,0.2636363636363636,2,0.0952380952380952,1,0,MODEL: Stacking layers
89,"That is , using superscript k to denote the current layer 's index ( assuming 1 - based indexing ) , we let q k + 1 t = h kt .",MODEL,Stacking layers,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",48,0.4,88,0.2666666666666666,3,0.1428571428571428,1,0,MODEL: Stacking layers
90,"Note that x t is passed to the next layer without any modification , so we do not put a layer index on it .",MODEL,Stacking layers,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",49,0.4083333333333333,89,0.2696969696969697,4,0.1904761904761904,1,0,MODEL: Stacking layers
91,Bi-direction .,MODEL,,question-answering,6,"['O', 'O']","['O', 'O']",50,0.4166666666666667,90,0.2727272727272727,5,0.238095238095238,1,0,MODEL
92,"So far we have assumed that QRN only needs to look at past sentences , whereas oftentimes , query answers can depend on future sentences .",MODEL,Bi-direction .,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",51,0.425,91,0.2757575757575757,6,0.2857142857142857,1,0,MODEL: Bi-direction .
93,"For instance , consider a sentence "" John dropped the football . "" at time t.",MODEL,Bi-direction .,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",52,0.4333333333333333,92,0.2787878787878788,7,0.3333333333333333,1,0,MODEL: Bi-direction .
94,"Then , even if there is no mention about the "" football "" in the past ( at time i < t ) , it can be implied that "" John "" has the "" football "" at the current time t.",MODEL,Bi-direction .,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",53,0.4416666666666666,93,0.2818181818181818,8,0.3809523809523809,1,0,MODEL: Bi-direction .
95,"In order to incorporate the future dependency , we obtain ? ?",MODEL,Bi-direction .,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",54,0.45,94,0.2848484848484848,9,0.4285714285714285,1,0,MODEL: Bi-direction .
96,ht and ? ?,MODEL,Bi-direction .,question-answering,6,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",55,0.4583333333333333,95,0.2878787878787879,10,0.4761904761904761,1,0,MODEL: Bi-direction .
97,"ht in both forward and backward directions , respectively , using Equation",MODEL,Bi-direction .,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",56,0.4666666666666667,96,0.2909090909090909,11,0.5238095238095238,1,0,MODEL: Bi-direction .
98,3 .,MODEL,Bi-direction .,question-answering,6,"['O', 'O']","['O', 'O']",57,0.475,97,0.2939393939393939,12,0.5714285714285714,1,0,MODEL: Bi-direction .
100,"That is , are shared between the two directions .",MODEL,Bi-direction .,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",59,0.4916666666666666,99,0.3,14,0.6666666666666666,1,0,MODEL: Bi-direction .
101,Connecting input and output modules .,MODEL,,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O']",60,0.5,100,0.303030303030303,15,0.7142857142857143,1,0,MODEL
102,depicts how QRN is connected with the input and output modules .,MODEL,Connecting input and output modules .,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",61,0.5083333333333333,101,0.306060606060606,16,0.7619047619047619,1,0,MODEL: Connecting input and output modules .
103,"In the first layer of QRN , q 1 t = q for all t , where q is obtained from the input module by processing the natural language question input q. x t is also obtained from x t by the same input module .",MODEL,Connecting input and output modules .,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",62,0.5166666666666667,102,0.3090909090909091,17,0.8095238095238095,1,0,MODEL: Connecting input and output modules .
104,The output at the last time step in the last layer is passed to the output module .,MODEL,Connecting input and output modules .,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",63,0.525,103,0.3121212121212121,18,0.8571428571428571,1,0,MODEL: Connecting input and output modules .
105,"That is , y = h K t where K represent the number of layers in the network .",MODEL,Connecting input and output modules .,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",64,0.5333333333333333,104,0.3151515151515151,19,0.9047619047619048,1,0,MODEL: Connecting input and output modules .
106,Then the output module gives the predicted answer ?,MODEL,Connecting input and output modules .,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",65,0.5416666666666666,105,0.3181818181818182,20,0.9523809523809524,1,0,MODEL: Connecting input and output modules .
108,EXTENSIONS,MODEL,,question-answering,6,['O'],['O'],67,0.5583333333333333,107,0.3242424242424242,0,0.0,1,0,MODEL
110,Reset gate .,MODEL,,question-answering,6,"['O', 'O', 'O']","['O', 'O', 'O']",69,0.575,109,0.3303030303030303,2,0.1333333333333333,1,0,MODEL
111,"Inspired by GRU , we found that it is useful to allow the QRN unit to reset ( nullify ) the candidate reduced query ( i.e. , h t ) when necessary .",MODEL,Reset gate .,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",70,0.5833333333333334,110,0.3333333333333333,3,0.2,1,0,MODEL: Reset gate .
112,For this we use a reset gate function ? :,MODEL,Reset gate .,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",71,0.5916666666666667,111,0.3363636363636363,4,0.2666666666666666,1,0,MODEL: Reset gate .
113,", which can be defined similarly to the update gate function :",MODEL,Reset gate .,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",72,0.6,112,0.3393939393939394,5,0.3333333333333333,1,0,MODEL: Reset gate .
114,where W ( r ) ?,MODEL,Reset gate .,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O']",73,0.6083333333333333,113,0.3424242424242424,6,0.4,1,0,MODEL: Reset gate .
115,"R 1d is a weight matrix , and b ( r ) ?",MODEL,Reset gate .,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",74,0.6166666666666667,114,0.3454545454545454,7,0.4666666666666667,1,0,MODEL: Reset gate .
116,R is a bias term .,MODEL,,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O']",75,0.625,115,0.3484848484848485,8,0.5333333333333333,1,0,MODEL
117,Equation 3 is rewritten as,MODEL,R is a bias term .,question-answering,6,"['O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O']",76,0.6333333333333333,116,0.3515151515151515,9,0.6,1,0,MODEL: R is a bias term .
118,Note that we do not use the reset gate in the last layer .,MODEL,R is a bias term .,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",77,0.6416666666666667,117,0.3545454545454545,10,0.6666666666666666,1,0,MODEL: R is a bias term .
119,Vector gates .,MODEL,,question-answering,6,"['O', 'O', 'O']","['O', 'O', 'O']",78,0.65,118,0.3575757575757576,11,0.7333333333333333,1,0,MODEL
120,"As in LSTM and GRU , update and reset gates can be vectors instead of scalar values for fine - controlled gating .",MODEL,Vector gates .,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",79,0.6583333333333333,119,0.3606060606060606,12,0.8,1,0,MODEL: Vector gates .
121,"For vector gates , we modify the row dimension of weights and biases in Equation 1 and 5 from 1 to d .",MODEL,Vector gates .,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",80,0.6666666666666666,120,0.3636363636363636,13,0.8666666666666667,1,0,MODEL: Vector gates .
122,"Then we obtain z t , rt ?",MODEL,Vector gates .,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",81,0.675,121,0.3666666666666666,14,0.9333333333333332,1,0,MODEL: Vector gates .
123,"Rd ( instead of z t , rt ? R ) , and these can be element - wise multiplied ( ) instead of being broadcasted in Equation 3 and 6 .",MODEL,Vector gates .,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",82,0.6833333333333333,122,0.3696969696969697,15,1.0,1,0,MODEL: Vector gates .
124,PARALLELIZATION,MODEL,,question-answering,6,['O'],['O'],83,0.6916666666666667,123,0.3727272727272727,0,0.0,1,0,MODEL
125,An important advantage of QRN is that the recurrent updates in Equation 3 and 5 can be computed in parallel across time .,MODEL,PARALLELIZATION,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",84,0.7,124,0.3757575757575757,1,0.0833333333333333,1,0,MODEL: PARALLELIZATION
126,"This is in contrast with most RNN - based models that can not be parallelized , where computing the candidate hidden state at time t explicitly requires the previous hidden state .",MODEL,PARALLELIZATION,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",85,0.7083333333333334,125,0.3787878787878788,2,0.1666666666666666,1,0,MODEL: PARALLELIZATION
127,"In QRN , the final reduced queries ( h t ) can be decomposed into computing over candidate reduced queries ( h t ) , without looking at the previous reduced query .",MODEL,PARALLELIZATION,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",86,0.7166666666666667,126,0.3818181818181818,3,0.25,1,0,MODEL: PARALLELIZATION
129,The extension to Equation 5 is straightforward .,MODEL,,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",88,0.7333333333333333,128,0.3878787878787879,5,0.4166666666666667,1,0,MODEL
131,The recursive definition of Equation 3 can be explicitly written as,MODEL,The extension to Equation 5 is straightforward .,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",90,0.75,130,0.3939393939393939,7,0.5833333333333334,1,0,MODEL: The extension to Equation 5 is straightforward .
133,Then we can rewrite Equation 7 as the following equation :,MODEL,The extension to Equation 5 is straightforward .,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",92,0.7666666666666667,132,0.4,9,0.75,1,0,MODEL: The extension to Equation 5 is straightforward .
134,Figure,MODEL,,question-answering,6,['O'],['O'],93,0.775,133,0.403030303030303,10,0.8333333333333334,1,0,MODEL
136,"AGRU is a variant of GRU where the update gate is replaced with soft attention , proposed by .",MODEL,Figure,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",95,0.7916666666666666,135,0.4090909090909091,12,1.0,1,0,MODEL: Figure
137,RELATED WORK,MODEL,Figure,question-answering,6,"['O', 'O']","['O', 'O']",96,0.8,136,0.4121212121212121,0,0.0,1,0,MODEL: Figure
138,"QRN is inspired by RNN - based models with gating mechanism , such as LSTM and GRU .",MODEL,Figure,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",97,0.8083333333333333,137,0.4151515151515151,1,0.0416666666666666,1,0,MODEL: Figure
139,"While GRU and LSTM use the previous hidden state and the current input to obtain the candidate hidden state , QRN only uses the current two inputs to obtain the candidate reduced query ( equivalent to candidate hidden state ) .",MODEL,Figure,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",98,0.8166666666666667,138,0.4181818181818181,2,0.0833333333333333,1,0,MODEL: Figure
141,"The idea of structurally simplifying ( constraining ) RNNs for learning longer - term patterns has been explored in recent previous work , such as Structurally Constrained Recurrent Network and Strongly - Typed Recurrent Neural Network ( STRNN ) .",MODEL,Figure,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",100,0.8333333333333334,140,0.4242424242424242,4,0.1666666666666666,1,0,MODEL: Figure
142,"QRN is similar to STRNN in that both architectures use gating mechanism , and the gates and the candidate hidden states do not depend on the previous hidden states , which simplifies the recurrent relation .",MODEL,Figure,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",101,0.8416666666666667,141,0.4272727272727272,5,0.2083333333333333,1,0,MODEL: Figure
145,"On the other hand , the gates in STRNN can be considered as the simplification of LSTM / GRU by removing their dependency on previous hidden state .",MODEL,Figure,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",104,0.8666666666666667,144,0.4363636363636363,8,0.3333333333333333,1,0,MODEL: Figure
147,This is distinct from STRNN which has only one input .,MODEL,Figure,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",106,0.8833333333333333,146,0.4424242424242424,10,0.4166666666666667,1,0,MODEL: Figure
151,There are two key differences between N2N and our QRN .,MODEL,Our parallelization algorithm is also applicable to STRNN .,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",110,0.9166666666666666,150,0.4545454545454545,14,0.5833333333333334,1,0,MODEL: Our parallelization algorithm is also applicable to STRNN .
152,"First , N2N summarizes the entire memory in each layer to control the attention in the next layer ( circle nodes in ) .",MODEL,Our parallelization algorithm is also applicable to STRNN .,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",111,0.925,151,0.4575757575757576,15,0.625,1,0,MODEL: Our parallelization algorithm is also applicable to STRNN .
153,"Instead , QRN does not have any controller node ) and is able to focus on relevant sentences through the update gate that is internally embodied within its unit .",MODEL,Our parallelization algorithm is also applicable to STRNN .,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",112,0.9333333333333332,152,0.4606060606060606,16,0.6666666666666666,1,0,MODEL: Our parallelization algorithm is also applicable to STRNN .
155,QRN does not need such additional weights as its inherent RNN architecture allows QRN to effectively model the time dependency .,MODEL,Our parallelization algorithm is also applicable to STRNN .,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",114,0.95,154,0.4666666666666667,18,0.75,1,0,MODEL: Our parallelization algorithm is also applicable to STRNN .
156,Neural Reasoner and Gated End - toend Memory Network ) are variants of MemN2N that share its fundamental characteristics .,MODEL,Our parallelization algorithm is also applicable to STRNN .,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",115,0.9583333333333334,155,0.4696969696969697,19,0.7916666666666666,1,0,MODEL: Our parallelization algorithm is also applicable to STRNN .
158,"It consists of two distinct GRUs , one for the time axis ( rectangle nodes in ) and one for the layer axis ( circle nodes in ) .",MODEL,Our parallelization algorithm is also applicable to STRNN .,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",117,0.975,157,0.4757575757575757,21,0.875,1,0,MODEL: Our parallelization algorithm is also applicable to STRNN .
159,Note that the update gate of the GRU for the time axis is replaced with external softmax attention weights .,MODEL,Our parallelization algorithm is also applicable to STRNN .,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",118,0.9833333333333332,158,0.4787878787878787,22,0.9166666666666666,1,0,MODEL: Our parallelization algorithm is also applicable to STRNN .
161,"In contrast , QRN is simply a single recurrent unit without any controller node .",MODEL,Our parallelization algorithm is also applicable to STRNN .,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",120,1.0,160,0.4848484848484848,24,1.0,1,0,MODEL: Our parallelization algorithm is also applicable to STRNN .
162,EXPERIMENTS,,,question-answering,6,['O'],['O'],0,0.0,161,0.4878787878787878,0,0.0,1,0,
163,5.1 DATA bAb,EXPERIMENTS,EXPERIMENTS,question-answering,6,"['O', 'O', 'O']","['O', 'O', 'O']",1,0.025,162,0.4909090909090909,1,0.0555555555555555,1,0,EXPERIMENTS
165,A story can be as short as two sentences and as long as 200 + sentences .,EXPERIMENTS,EXPERIMENTS,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.075,164,0.4969696969696969,3,0.1666666666666666,1,0,EXPERIMENTS
167,"The answers are single words or lists ( e.g. "" football , apple "" ) .",EXPERIMENTS,EXPERIMENTS,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.125,166,0.503030303030303,5,0.2777777777777778,1,0,EXPERIMENTS
169,"The dataset also includes 10 k training data ( for each task ) , which allows training more complex models .",EXPERIMENTS,EXPERIMENTS,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.175,168,0.509090909090909,7,0.3888888888888889,1,0,EXPERIMENTS
170,Note that DMN + only reports on the 10k dataset .,EXPERIMENTS,EXPERIMENTS,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.2,169,0.5121212121212121,8,0.4444444444444444,1,0,EXPERIMENTS
171,bAb,EXPERIMENTS,EXPERIMENTS,question-answering,6,['O'],['O'],9,0.225,170,0.5151515151515151,9,0.5,1,0,EXPERIMENTS
174,"The authors also provide Out - Of - Vocabulary ( OOV ) version of the dataset , where many of the words and KB keywords in test data are not seen during training .",EXPERIMENTS,EXPERIMENTS,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.3,173,0.5242424242424243,12,0.6666666666666666,1,0,EXPERIMENTS
178,"Each dialog can be as long as 800 + utterances , and a system needs to choose from 2407 possible candidate responses for each utterance of the user .",EXPERIMENTS,EXPERIMENTS,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.4,177,0.5363636363636364,16,0.8888888888888888,1,0,EXPERIMENTS
179,"Note that the evaluation metric of the original DSTC2 is different from that of the transformed DSTC2 , so previous work on the original DSTC2 should not be directly compared to our work .",EXPERIMENTS,EXPERIMENTS,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.425,178,0.5393939393939394,17,0.9444444444444444,1,0,EXPERIMENTS
180,"We will refer to this transformed DSTC2 dataset by "" Task 6 "" of dialog dataset .",EXPERIMENTS,EXPERIMENTS,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.45,179,0.5424242424242425,18,1.0,1,0,EXPERIMENTS
182,Input Module .,EXPERIMENTS,,question-answering,6,"['O', 'O', 'O']","['O', 'O', 'O']",20,0.5,181,0.5484848484848485,1,0.0128205128205128,1,0,EXPERIMENTS
183,"In the input module , we are given sentences ( previous conversations in dialog ) x t and a question ( most recent user utterance ) q , and we want to obtain their vector representations , x t , q ?",EXPERIMENTS,Input Module .,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.525,182,0.5515151515151515,2,0.0256410256410256,1,0,EXPERIMENTS: Input Module .
185,We use a trainable embedding matrix A ?,EXPERIMENTS,Rd .,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",23,0.575,184,0.5575757575757576,4,0.0512820512820512,1,0,EXPERIMENTS: Rd .
186,R d V to encode the one - hot vector of each word x tj in each sentence x t into a d-dimensional vector x tj ?,EXPERIMENTS,Rd .,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.6,185,0.5606060606060606,5,0.0641025641025641,1,0,EXPERIMENTS: Rd .
188,Then the sentence representation x t is obtained by Position Encoder .,EXPERIMENTS,Rd .,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.65,187,0.5666666666666667,7,0.0897435897435897,1,0,EXPERIMENTS: Rd .
189,The same encoder with the same embedding matrix is also used to obtain the question vector q from q.,EXPERIMENTS,Rd .,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.675,188,0.5696969696969697,8,0.1025641025641025,1,0,EXPERIMENTS: Rd .
192,"and we want to obtain the natural language form of the answer , ?.",EXPERIMENTS,Output Module for story - based QA .,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",30,0.75,191,0.5787878787878787,11,0.141025641025641,1,0,EXPERIMENTS: Output Module for story - based QA .
194,"to a V - dimensional sparse vector , v = softmax W ( y ) ? ? RV , where W ( y ) ?",EXPERIMENTS,Output Module for story - based QA .,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",32,0.8,193,0.5848484848484848,13,0.1666666666666666,1,0,EXPERIMENTS: Output Module for story - based QA .
195,RV d is a weight matrix .,EXPERIMENTS,,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O']",33,0.825,194,0.5878787878787879,14,0.1794871794871795,1,0,EXPERIMENTS
197,is simply the argmax word inv .,EXPERIMENTS,RV d is a weight matrix .,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O']",35,0.875,196,0.593939393939394,16,0.2051282051282051,1,0,EXPERIMENTS: RV d is a weight matrix .
198,"To handle questions with multiple - word answers , we consider each of them as a single word that contains punctuations such as space and comma , and put it in the vocabulary .",EXPERIMENTS,RV d is a weight matrix .,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",36,0.9,197,0.5969696969696969,17,0.2179487179487179,1,0,EXPERIMENTS: RV d is a weight matrix .
199,Output Module for dialog .,EXPERIMENTS,,question-answering,6,"['O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O']",37,0.925,198,0.6,18,0.2307692307692307,1,0,EXPERIMENTS
201,"While it is similar in spirit to the RNN decoder , our output module does not have a recurrent hidden state or gating mechanism .",EXPERIMENTS,Output Module for dialog .,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",39,0.975,200,0.6060606060606061,20,0.2564102564102564,1,0,EXPERIMENTS: Output Module for dialog .
202,"Instead , it solely uses the final ouptut of the QRN , ? , and the current word output to influence the prediction of the next word among possible candidates .",EXPERIMENTS,Output Module for dialog .,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",40,1.0,201,0.6090909090909091,21,0.2692307692307692,1,0,EXPERIMENTS: Output Module for dialog .
203,Training .,,,question-answering,6,"['O', 'O']","['O', 'O']",0,0.0,202,0.6121212121212121,22,0.282051282051282,1,0,
209,Forget bias of 2.5 is used for update gates ( no bias for reset gates ) .,Training .,Training .,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.5454545454545454,208,0.6303030303030303,28,0.358974358974359,1,0,Training .
211,The loss function is the cross entropy betweenv and the one - hot vector of the true answer .,Training .,Training .,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.7272727272727273,210,0.6363636363636364,30,0.3846153846153846,1,0,Training .
215,RESULTS .,,,question-answering,6,"['O', 'O']","['O', 'O']",0,0.0,214,0.6484848484848484,34,0.4358974358974359,1,0,
217,"These include LSTM , End - to - end Memory Networks ( N2N ) , Dynamic Memory Networks ( DMN + ) , Gated End - to - end Memory Networks ( GMe m N2N ) , and Differentiable Neural Computer ( DNC ) .",RESULTS .,RESULTS .,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.2,216,0.6545454545454545,36,0.4615384615384615,1,0,RESULTS .
224,' Match ' is the extension to the model which additionally takes as input whether each answer candidate matches with context ( more details on Appendix ) .,RESULTS .,Story - based QA .,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.9,223,0.6757575757575758,43,0.5512820512820513,1,0,RESULTS .: Story - based QA .
226,Ablations .,,,question-answering,6,"['O', 'O']","['O', 'O']",0,0.0,225,0.6818181818181818,45,0.5769230769230769,1,0,
230,"In the case of 1 k dataset , when there are too many layers ( 6 ) , it seems correctly training the model becomes increasingly difficult .",Ablations .,Ablations .,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.1212121212121212,229,0.693939393939394,49,0.6282051282051282,1,0,Ablations .
231,"In the case of 10 k dataset , many layers ( 6 ) and hidden dimensions ( 200 ) helps reasoning , most notably in difficult task such as task 16 .",Ablations .,Ablations .,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.1515151515151515,230,0.696969696969697,50,0.6410256410256411,1,0,Ablations .
233,"( c ) Including vector gates hurts in 1 k datasets , as the model either overfits to the training data or converges to local minima .",Ablations .,Ablations .,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.2121212121212121,232,0.703030303030303,52,0.6666666666666666,1,0,Ablations .
234,"On the other hand , vector gates in bAbI story - based QA 10 k dataset sometimes help .",Ablations .,Ablations .,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.2424242424242424,233,0.706060606060606,53,0.6794871794871795,1,0,Ablations .
240,We expect that the speedup can be even higher for datasets with larger context .,Ablations .,Ablations .,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.4242424242424242,239,0.7242424242424242,59,0.7564102564102564,1,0,Ablations .
241,Interpretations .,Ablations .,,question-answering,6,"['O', 'O']","['O', 'O']",15,0.4545454545454545,240,0.7272727272727273,60,0.7692307692307693,1,0,Ablations .
242,An advantage of QRN is that the intermediate query updates are interpretable .,Ablations .,Interpretations .,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.4848484848484848,241,0.7303030303030303,61,0.782051282051282,1,0,Ablations .: Interpretations .
244,"In order to obtain these , we place a decoder on the input question embedding q and add it s loss for recovering the question to the classification loss ( similarly to ) .",Ablations .,Interpretations .,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.5454545454545454,243,0.7363636363636363,63,0.8076923076923077,1,0,Ablations .: Interpretations .
247,"In , the question Where is apple ?",Ablations .,Interpretations .,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.6363636363636364,246,0.7454545454545455,66,0.8461538461538461,1,0,Ablations .: Interpretations .
248,is transformed into,Ablations .,Interpretations .,question-answering,6,"['O', 'O', 'O']","['O', 'O', 'O']",22,0.6666666666666666,247,0.7484848484848485,67,0.8589743589743589,1,0,Ablations .: Interpretations .
249,"Where is Sandra ? at t = 1 . At t = 2 , as Sandra dropped the apple , the apple is no more relevant to Sandra .",Ablations .,Interpretations .,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",23,0.696969696969697,248,0.7515151515151515,68,0.8717948717948718,1,0,Ablations .: Interpretations .
250,We obtain Where is Daniel ?,Ablations .,Interpretations .,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O']",24,0.7272727272727273,249,0.7545454545454545,69,0.8846153846153846,1,0,Ablations .: Interpretations .
251,"at time t = 3 , and it is propagated until t = 5 , where we observe a sentence ( fact ) that can be used to answer the query .",Ablations .,Interpretations .,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.7575757575757576,250,0.7575757575757576,70,0.8974358974358975,1,0,Ablations .: Interpretations .
252,Visualization .,Ablations .,,question-answering,6,"['O', 'O']","['O', 'O']",26,0.7878787878787878,251,0.7606060606060606,71,0.9102564102564102,1,0,Ablations .
254,More visualizations are shown in Appendices : and .,Ablations .,,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",28,0.8484848484848485,253,0.7666666666666667,73,0.935897435897436,1,0,Ablations .
256,"In QA Task 2 example ( top left ) , we observe high update gate values in the first layer on facts that state who has the apple , and in the second layer , the high update gate values are on those that inform where that person went to .",Ablations .,More visualizations are shown in Appendices : and .,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",30,0.9090909090909092,255,0.7727272727272727,75,0.9615384615384616,1,0,Ablations .: More visualizations are shown in Appendices : and .
257,"We also observe that the forward reset gate at t = 2 in the first layer ( ? ? r 1 2 ) is low , which is signifying that apple no more belongs to Sandra .",Ablations .,More visualizations are shown in Appendices : and .,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",31,0.9393939393939394,256,0.7757575757575758,76,0.9743589743589745,1,0,Ablations .: More visualizations are shown in Appendices : and .
259,"In dialog Task 6 ( bottom ) , the model focuses on the sentences containing Spanish , and does not concentrate much on other facts such as I do n't care .",Ablations .,More visualizations are shown in Appendices : and .,question-answering,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",33,1.0,258,0.7818181818181819,78,1.0,1,0,Ablations .: More visualizations are shown in Appendices : and .
260,CONCLUSION,,,question-answering,6,['O'],['O'],0,0.0,259,0.7848484848484848,0,0.0,1,0,
3,abstract,,,question-answering,7,['O'],['O'],0,0.0,2,0.0072727272727272,0,0.0,1,0,
6,NSE can also access 1 multiple and shared memories .,abstract,abstract,question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.1071428571428571,5,0.0181818181818181,3,0.0434782608695652,1,0,abstract
9,Recurrent neural networks ( RNNs ) have been successful for modeling sequences,abstract,abstract,question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.2142857142857142,8,0.029090909090909,6,0.0869565217391304,1,0,abstract
10,[ 1 ] .,abstract,abstract,question-answering,7,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",7,0.25,9,0.0327272727272727,7,0.1014492753623188,1,0,abstract
11,"Particularly , RNNs equipped with internal short memories , such as long short - term memories ( LSTM )",abstract,abstract,question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.2857142857142857,10,0.0363636363636363,8,0.1159420289855072,1,0,abstract
12,"[ 2 ] have achieved a notable success in sequential tasks [ 3 , 4 ] .",abstract,abstract,question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.3214285714285714,11,0.04,9,0.1304347826086956,1,0,abstract
13,LSTM is powerful because it learns to control it s short term memories .,abstract,abstract,question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.3571428571428571,12,0.0436363636363636,10,0.144927536231884,1,0,abstract
14,"However , the short term memories in LSTM area part of the training parameters .",abstract,abstract,question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.3928571428571428,13,0.0472727272727272,11,0.1594202898550724,1,0,abstract
15,This imposes some practical difficulties in training and modeling long sequences with LSTM .,abstract,abstract,question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.4285714285714285,14,0.0509090909090909,12,0.1739130434782608,1,0,abstract
16,Recently several studies have explored ways of extending the neural networks with an external memory [ 5 ] [ 6 ] [ 7 ] .,abstract,abstract,question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.4642857142857143,15,0.0545454545454545,13,0.1884057971014492,1,0,abstract
17,"Unlike LSTM , the short term memories and the training parameters of such a neural network are no longer coupled and can be adapted .",abstract,abstract,question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.5,16,0.0581818181818181,14,0.2028985507246377,1,0,abstract
20,NSE has a variable sized encoding memory which allows the model to access entire input sequence during the reading process ; therefore efficiently delivering long - term dependencies overtime .,abstract,NSE offers several desirable properties .,question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.6071428571428571,19,0.069090909090909,17,0.2463768115942029,1,0,abstract: NSE offers several desirable properties .
21,"The encoding memory evolves overtime and maintains the memory of the input sequence through read , compose and write operations .",abstract,NSE offers several desirable properties .,question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.6428571428571429,20,0.0727272727272727,18,0.2608695652173913,1,0,abstract: NSE offers several desirable properties .
30,1,abstract,We evaluate NSE on five different real tasks .,question-answering,7,['O'],['O'],27,0.9642857142857144,29,0.1054545454545454,27,0.391304347826087,1,0,abstract: We evaluate NSE on five different real tasks .
31,"By access we mean changing the memory states by the read , compose and write operations .",abstract,We evaluate NSE on five different real tasks .,question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",28,1.0,30,0.109090909090909,28,0.4057971014492754,1,0,abstract: We evaluate NSE on five different real tasks .
32,Related Work,,,question-answering,7,"['O', 'O']","['O', 'O']",0,0.0,31,0.1127272727272727,29,0.4202898550724637,1,0,
73,Proposed Approach,,,question-answering,7,"['O', 'O']","['O', 'O']",0,0.0,72,0.2618181818181818,0,0.0,1,0,
74,Our training set consists,,,question-answering,7,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",0,0.0,73,0.2654545454545454,1,0.0277777777777777,1,0,
75,". . , w i Ti of tokens while the output Y i can be either a single target or a sequence .",Our training set consists,Our training set consists,question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0208333333333333,74,0.2690909090909091,2,0.0555555555555555,1,0,Our training set consists
76,We transform each input token wt to its word embedding x t .,Our training set consists,Our training set consists,question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.0416666666666666,75,0.2727272727272727,3,0.0833333333333333,1,0,Our training set consists
78,"R kl with a variable number of slots , where k is the embedding dimension and l is the length of the input sequence .",Our training set consists,Our training set consists,question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.0833333333333333,77,0.28,5,0.1388888888888889,1,0,Our training set consists
79,Each memory slot vector mt ?,Our training set consists,Our training set consists,question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O']",5,0.1041666666666666,78,0.2836363636363636,6,0.1666666666666666,1,0,Our training set consists
80,R k corresponds to the vector representation of information about word wt in memory .,Our training set consists,Our training set consists,question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.125,79,0.2872727272727273,7,0.1944444444444444,1,0,Our training set consists
81,"In particular , the memory is initialized by the embedding vectors {x t } l t=1 and is evolved overtime , through read , compose and write operations .",Our training set consists,Our training set consists,question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.1458333333333333,80,0.2909090909090909,8,0.2222222222222222,1,0,Our training set consists
82,"Read , Compose and Write",Our training set consists,,question-answering,7,"['O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O']",8,0.1666666666666666,81,0.2945454545454545,9,0.25,1,0,Our training set consists
84,"After initializing the memory slots with the corresponding input representations , NSE processes an embedding vector x t and retrieves a memory slot m r,t that is expected to be associatively coherent ( i.e. semantically associated ) with the current input word wt .",Our training set consists,"Read , Compose and Write",question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.2083333333333333,83,0.3018181818181818,11,0.3055555555555556,1,0,"Our training set consists: Read , Compose and Write"
85,The slot location r ( ranging from 1 to l ) is defined by a key vector z t which the read module emits by attending over the memory slots .,Our training set consists,"Read , Compose and Write",question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.2291666666666666,84,0.3054545454545455,12,0.3333333333333333,1,0,"Our training set consists: Read , Compose and Write"
87,The write module then transforms the composition output to the encoding memory space and writes the resulting new representation into the slot location of the memory .,Our training set consists,"Read , Compose and Write",question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.2708333333333333,86,0.3127272727272727,14,0.3888888888888889,1,0,"Our training set consists: Read , Compose and Write"
88,"Instead of composing the raw embedding vector x t , we use the hidden state o t produced by the read module at time t",Our training set consists,"Read , Compose and Write",question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.2916666666666667,87,0.3163636363636363,15,0.4166666666666667,1,0,"Our training set consists: Read , Compose and Write"
89,"where 1 is a matrix of ones , ?",Our training set consists,"Read , Compose and Write",question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.3125,88,0.32,16,0.4444444444444444,1,0,"Our training set consists: Read , Compose and Write"
90,denotes the outer product which duplicates it s left vector l or k times to form a matrix .,Our training set consists,"Read , Compose and Write",question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.3333333333333333,89,0.3236363636363636,17,0.4722222222222222,1,0,"Our training set consists: Read , Compose and Write"
91,The read function f LST,Our training set consists,,question-answering,7,"['O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O']",17,0.3541666666666667,90,0.3272727272727272,18,0.5,1,0,Our training set consists
92,Mr sequentially maps the word embeddings to the internal space of the memory M t?1 .,Our training set consists,The read function f LST,question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.375,91,0.3309090909090909,19,0.5277777777777778,1,0,Our training set consists: The read function f LST
93,Then Equation 2 looks for the slots related to the input by computing association degree between each memory slot and the hidden state o t .,Our training set consists,The read function f LST,question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.3958333333333333,92,0.3345454545454545,20,0.5555555555555556,1,0,Our training set consists: The read function f LST
94,We calculate the association degree by the dot product and transform this scores to the fuzzy key vector z t by normalizing with sof tmax function .,Our training set consists,The read function f LST,question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.4166666666666667,93,0.3381818181818182,21,0.5833333333333334,1,0,Our training set consists: The read function f LST
95,"Since our key vector is fuzzy , the slot to be composed is retrieved by taking weighted sum of the all slots as in Equation .",Our training set consists,The read function f LST,question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.4375,94,0.3418181818181818,22,0.6111111111111112,1,0,Our training set consists: The read function f LST
96,This process can also be seen as the soft attention mechanism .,Our training set consists,The read function f LST,question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.4583333333333333,95,0.3454545454545454,23,0.6388888888888888,1,0,Our training set consists: The read function f LST
97,"In Equation 4 and 5 , we compose and process the retrieved slot with the current hidden state and map the resulting vector to the encoder output space .",Our training set consists,The read function f LST,question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",23,0.4791666666666667,96,0.3490909090909091,24,0.6666666666666666,1,0,Our training set consists: The read function f LST
98,"Finally , we write the new representation to the memory location pointed by the key vector in where the key vector z t emitted by the read module is reused to inform the write module of the most recently read slots .",Our training set consists,The read function f LST,question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.5,97,0.3527272727272727,25,0.6944444444444444,1,0,Our training set consists: The read function f LST
99,First the slot information that was retrieved is erased and then the new representation is located .,Our training set consists,The read function f LST,question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.5208333333333334,98,0.3563636363636364,26,0.7222222222222222,1,0,Our training set consists: The read function f LST
100,NSE performs this iterative process until all words in the input sequence are read .,Our training set consists,The read function f LST,question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.5416666666666666,99,0.36,27,0.75,1,0,Our training set consists: The read function f LST
101,The encoding memories { M } T t=1 and output states {h} T t=1 are further used for the tasks .,Our training set consists,The read function f LST,question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.5625,100,0.3636363636363636,28,0.7777777777777778,1,0,Our training set consists: The read function f LST
102,"Although NSE reads a single word at a time , it has an anytime - access to the entire sequence stored in the encoding memory .",Our training set consists,The read function f LST,question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",28,0.5833333333333334,101,0.3672727272727272,29,0.8055555555555556,1,0,Our training set consists: The read function f LST
104,The memory is initialized with the raw embedding vector at time t = 0 .,Our training set consists,The read function f LST,question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",30,0.625,103,0.3745454545454545,31,0.8611111111111112,1,0,Our training set consists: The read function f LST
105,We term such a freshly initialized memory a baby memory .,Our training set consists,The read function f LST,question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",31,0.6458333333333334,104,0.3781818181818182,32,0.8888888888888888,1,0,Our training set consists: The read function f LST
107,functions are neural networks and are the training parameters in our NSE .,Our training set consists,The read function f LST,question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",33,0.6875,106,0.3854545454545454,34,0.9444444444444444,1,0,Our training set consists: The read function f LST
109,"Since NSE is fully differentiable , it can be trained with any gradient descent optimizer .",Our training set consists,The read function f LST,question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",35,0.7291666666666666,108,0.3927272727272727,36,1.0,1,0,Our training set consists: The read function f LST
111,"For sequence to sequence transduction tasks like question answering , natural language inference and machine translation , it is beneficial to access other relevant memories in addition to its own one .",Our training set consists,The read function f LST,question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",37,0.7708333333333334,110,0.4,1,0.0833333333333333,1,0,Our training set consists: The read function f LST
112,The shared or the multiple memory access allows a set of NSEs to exchange knowledge representations and to communicate with each other to accomplish a particular task throughout the encoding memory .,Our training set consists,The read function f LST,question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",38,0.7916666666666666,111,0.4036363636363636,2,0.1666666666666666,1,0,Our training set consists: The read function f LST
115,The first memory ( in green ) is the shared memory accessed by more than one NSEs .,Our training set consists,The read function f LST,question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",41,0.8541666666666666,114,0.4145454545454545,5,0.4166666666666667,1,0,Our training set consists: The read function f LST
117,"R kn that has been encoded by processing a relevant sequence with length n , MMA - NSE with the access to one relevant memory is defined as",Our training set consists,The read function f LST,question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",43,0.8958333333333334,116,0.4218181818181818,7,0.5833333333333334,1,0,Our training set consists: The read function f LST
118,and this is almost the same as standard NSE .,Our training set consists,The read function f LST,question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",44,0.9166666666666666,117,0.4254545454545455,8,0.6666666666666666,1,0,Our training set consists: The read function f LST
119,The read module now emits the additional key vector z n t for the shared memory and the composition function f M LP c combines more than one slots .,Our training set consists,The read function f LST,question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",45,0.9375,118,0.4290909090909091,9,0.75,1,0,Our training set consists: The read function f LST
121,They are then composed together with the current input and written back to their corresponding slots .,Our training set consists,The read function f LST,question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",47,0.9791666666666666,120,0.4363636363636363,11,0.9166666666666666,1,0,Our training set consists: The read function f LST
122,Note that MMA - NSE is capable of accessing a variable number of relevant shared memories once a composition function that takes in dynamic inputs is chosen .,Our training set consists,The read function f LST,question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",48,1.0,121,0.44,12,1.0,1,0,Our training set consists: The read function f LST
123,Experiments,,,question-answering,7,['O'],['O'],0,0.0,122,0.4436363636363636,0,0.0,1,0,
128,We chose two one - layer LSTM for read / write modules on the tasks other than QA on which we used two - layer LSTM .,Experiments,Experiments,question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.0357142857142857,127,0.4618181818181818,5,0.4545454545454545,1,0,Experiments
131,The embeddings for out - of - vocabulary words were set to zero vector .,Experiments,The word embeddings are fixed during training .,question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.0571428571428571,130,0.4727272727272727,8,0.7272727272727273,1,0,Experiments: The word embeddings are fixed during training .
133,A padding vector was inserted when padding .,Experiments,,question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.0714285714285714,132,0.48,10,0.9090909090909092,1,0,Experiments
135,Natural Language Inference,Experiments,A padding vector was inserted when padding .,question-answering,7,"['O', 'O', 'O']","['O', 'O', 'O']",12,0.0857142857142857,134,0.4872727272727272,0,0.0,1,0,Experiments: A padding vector was inserted when padding .
140,"Following the setting in the NSE output for each sentence was the input to a MLP , where the input layer computes the concatenation [ h pl ; h h l ] , absolute difference hp l ?",Experiments,A padding vector was inserted when padding .,question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.1214285714285714,139,0.5054545454545455,5,0.1923076923076923,1,0,Experiments: A padding vector was inserted when padding .
141,h h land elementwise product hp l h h l of the two sentence representations .,Experiments,A padding vector was inserted when padding .,question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.1285714285714285,140,0.509090909090909,6,0.2307692307692307,1,0,Experiments: A padding vector was inserted when padding .
153,The SPINN - PI model is similar to NSE in spirit that it also explicitly computes word composition .,Experiments,A padding vector was inserted when padding .,question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",30,0.2142857142857142,152,0.5527272727272727,18,0.6923076923076923,1,0,Experiments: A padding vector was inserted when padding .
154,"However , the composition in the SPINN - PI is guided by supervisions from a dependency parser .",Experiments,A padding vector was inserted when padding .,question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",31,0.2214285714285714,153,0.5563636363636364,19,0.7307692307692307,1,0,Experiments: A padding vector was inserted when padding .
159,"Particularly , it attends over the premise encoder outputs {h p } T t= 1 in respect to the final hypothesis representation h h land constructs an attentively blended vector of the premise .",Experiments,A padding vector was inserted when padding .,question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",36,0.2571428571428571,158,0.5745454545454546,24,0.9230769230769232,1,0,Experiments: A padding vector was inserted when padding .
162,Answer Sentence Selection,Experiments,This model obtained 85.4 % accuracy score .,question-answering,7,"['O', 'O', 'O']","['O', 'O', 'O']",39,0.2785714285714286,161,0.5854545454545454,0,0.0,1,0,Experiments: This model obtained 85.4 % accuracy score .
167,"The MLP setup used in the language inference task is kept same , except that we now replace the sof tmax layer with a sigmoid layer and model the following conditional probability distribution .",Experiments,We experiment on WikiQA dataset constructed from Wikipedia .,question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",44,0.3142857142857143,166,0.6036363636363636,5,0.217391304347826,1,0,Experiments: We experiment on WikiQA dataset constructed from Wikipedia .
168,where h q land ha l are the question and the answer encoded vectors and o QA denotes the output of the hidden layer of the MLP .,Experiments,We experiment on WikiQA dataset constructed from Wikipedia .,question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",45,0.3214285714285714,167,0.6072727272727273,6,0.2608695652173913,1,0,Experiments: We experiment on WikiQA dataset constructed from Wikipedia .
184,Bin :,Experiments,We experiment on WikiQA dataset constructed from Wikipedia .,question-answering,7,"['O', 'O']","['O', 'O']",61,0.4357142857142857,183,0.6654545454545454,22,0.9565217391304348,1,0,Experiments: We experiment on WikiQA dataset constructed from Wikipedia .
186,Sentence Classification,Experiments,,question-answering,7,"['O', 'O']","['O', 'O']",63,0.45,185,0.6727272727272727,0,0.0,1,0,Experiments
197,Most best performing methods exploited the parse tree provided in the treebank on this task with the exception of the DMN .,Experiments,The second layer is a sof tmax layer .,question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",74,0.5285714285714286,196,0.7127272727272728,11,0.8461538461538461,1,0,Experiments: The second layer is a sof tmax layer .
198,The Dynamic Memory Network ( DMN ) model is a memory - augmented network .,Experiments,The second layer is a sof tmax layer .,question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",75,0.5357142857142857,197,0.7163636363636363,12,0.9230769230769232,1,0,Experiments: The second layer is a sof tmax layer .
203,"Particularly , we used the pre-split datasets of .",Experiments,,question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",80,0.5714285714285714,202,0.7345454545454545,3,0.15,1,0,Experiments
211,"In order to speedup the training , we created document buckets by considering the number of sentences per document , i.e. , documents with the same number of sentences were put together in the same bucket .",Experiments,"Particularly , we used the pre-split datasets of .",question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",88,0.6285714285714286,210,0.7636363636363637,11,0.55,1,0,"Experiments: Particularly , we used the pre-split datasets of ."
213,"We did not use curriculum scheduling , although it is observed to help sequence training .",Experiments,The buckets were shuffled and updated per epoch .,question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",90,0.6428571428571429,212,0.7709090909090909,13,0.65,1,0,Experiments: The buckets were shuffled and updated per epoch .
216,"The best results on the task were previously obtained by Conv - GRNN and LSTM - GRNN , which are also stacked models .",Experiments,The buckets were shuffled and updated per epoch .,question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",93,0.6642857142857143,215,0.7818181818181819,16,0.8,1,0,Experiments: The buckets were shuffled and updated per epoch .
218,Yelp 13 dataset has five classes to distinguish .,Experiments,,question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",95,0.6785714285714286,217,0.7890909090909091,18,0.9,1,0,Experiments
220,This is possibly due to the encoding memory of the document level NSE that preserves the long dependency in documents with a large number of sentences .,Experiments,Yelp 13 dataset has five classes to distinguish .,question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",97,0.6928571428571428,219,0.7963636363636364,20,1.0,1,0,Experiments: Yelp 13 dataset has five classes to distinguish .
221,Machine Translation,Experiments,,question-answering,7,"['O', 'O']","['O', 'O']",98,0.7,220,0.8,0,0.0,1,0,Experiments
223,The NMT problem is mostly defined within the encoder - decoder framework .,Experiments,Machine Translation,question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",100,0.7142857142857143,222,0.8072727272727273,2,0.0869565217391304,1,0,Experiments: Machine Translation
227,The first model is a baseline model and is similar to the one proposed in ( RNNSearch ) .,Experiments,"For NTM , we implemented three different models .",question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",104,0.7428571428571429,226,0.8218181818181818,6,0.2608695652173913,1,0,"Experiments: For NTM , we implemented three different models ."
230,"The last model is an NSE - NSE setup , where the encoding part is the same as the NSE - LSTM while the decoder NSE now uses the output state and has an access to the encoder memory , i.e. , the encoder and the decoder NSEs access a shared memory .",Experiments,"For NTM , we implemented three different models .",question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",107,0.7642857142857142,229,0.8327272727272728,9,0.391304347826087,1,0,"Experiments: For NTM , we implemented three different models ."
231,The memory is encoded by the first NSEs and then read / written by the decoder NSEs .,Experiments,"For NTM , we implemented three different models .",question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",108,0.7714285714285715,230,0.8363636363636363,10,0.4347826086956521,1,0,"Experiments: For NTM , we implemented three different models ."
236,Word association or composition graphs produced by NSE memory access .,Experiments,"For NTM , we implemented three different models .",question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",113,0.8071428571428572,235,0.8545454545454545,15,0.6521739130434783,1,0,"Experiments: For NTM , we implemented three different models ."
237,The directed arcs connect the words that are composed via compose module .,Experiments,"For NTM , we implemented three different models .",question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",114,0.8142857142857143,236,0.8581818181818182,16,0.6956521739130435,1,0,"Experiments: For NTM , we implemented three different models ."
238,The source nodes are input words and the destination nodes ( pointed by the arrows ) correspond to the accessed memory slots .,Experiments,"For NTM , we implemented three different models .",question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",115,0.8214285714285714,237,0.8618181818181818,17,0.7391304347826086,1,0,"Experiments: For NTM , we implemented three different models ."
239,< S > denotes the beginning of sequence .,Experiments,"For NTM , we implemented three different models .",question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",116,0.8285714285714286,238,0.8654545454545455,18,0.782608695652174,1,0,"Experiments: For NTM , we implemented three different models ."
240,the number of parameters of the models is roughly the equal .,Experiments,"For NTM , we implemented three different models .",question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",117,0.8357142857142857,239,0.8690909090909091,19,0.8260869565217391,1,0,"Experiments: For NTM , we implemented three different models ."
246,NSE is capabable of performing multiscale composition by retrieving associative slots fora particular input at a time step .,Experiments,Memory Access and Compositionality,question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",123,0.8785714285714286,245,0.8909090909090909,1,0.0555555555555555,1,0,Experiments: Memory Access and Compositionality
249,The association graph was constructed by inspecting the key vector z .,Experiments,Memory Access and Compositionality,question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",126,0.9,248,0.901818181818182,4,0.2222222222222222,1,0,Experiments: Memory Access and Compositionality
250,"For an input word , we connect it to the most active slot pointed by z 12 .",Experiments,Memory Access and Compositionality,question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",127,0.9071428571428573,249,0.9054545454545454,5,0.2777777777777778,1,0,Experiments: Memory Access and Compositionality
251,"Note the graph components clustered around the semantically rich words : "" sits "" , "" wall "" and "" autumn "" ( a ) and "" Three "" , "" puppies "" , "" tub "" and "" vet "" ( b ) .",Experiments,Memory Access and Compositionality,question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",128,0.9142857142857144,250,0.9090909090909092,6,0.3333333333333333,1,0,Experiments: Memory Access and Compositionality
252,The memory slots corresponding to words that are semantically rich in the current context are the most frequently accessed .,Experiments,Memory Access and Compositionality,question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",129,0.9214285714285714,251,0.9127272727272728,7,0.3888888888888889,1,0,Experiments: Memory Access and Compositionality
255,"For example , NSE retrieved the memory slot corresponding to "" wall "" or "" Three "" when reading the input "" rock "" or "" are "" .",Experiments,Memory Access and Compositionality,question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",132,0.9428571428571428,254,0.9236363636363636,10,0.5555555555555556,1,0,Experiments: Memory Access and Compositionality
258,Note how the encoding memory is evolved overtime .,Experiments,,question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",135,0.9642857142857144,257,0.9345454545454546,13,0.7222222222222222,1,0,Experiments
259,"In time step four ( t = 4 ) , the memory slot for "" quietly "" encodes information about "" quiet ( ly ) little child "" .",Experiments,Note how the encoding memory is evolved overtime .,question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",136,0.9714285714285714,258,0.9381818181818182,14,0.7777777777777778,1,0,Experiments: Note how the encoding memory is evolved overtime .
260,"When t = 6 , the model forms another composition involving "" quietly "" , "" quietly sits "" .",Experiments,Note how the encoding memory is evolved overtime .,question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",137,0.9785714285714284,259,0.9418181818181818,15,0.8333333333333334,1,0,Experiments: Note how the encoding memory is evolved overtime .
261,"In the last time step , we are able to find the most or the least frequently accessed slots in the memory .",Experiments,Note how the encoding memory is evolved overtime .,question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",138,0.9857142857142858,260,0.9454545454545454,16,0.8888888888888888,1,0,Experiments: Note how the encoding memory is evolved overtime .
262,The least accessed slots correspond to function words while the frequently accessed slots are content words and tend to carryout rich semantics and intrinsic compositions found in the input sentence .,Experiments,Note how the encoding memory is evolved overtime .,question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",139,0.9928571428571428,261,0.9490909090909092,17,0.9444444444444444,1,0,Experiments: Note how the encoding memory is evolved overtime .
264,Conclusion,,,question-answering,7,['O'],['O'],0,0.0,263,0.9563636363636364,0,0.0,1,0,
2,MACHINE COMPREHENSION USING MATCH - LSTM AND ANSWER POINTER,,,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.2,1,0.0040160642570281,1,0.0,1,0,
3,abstract,,MACHINE COMPREHENSION USING MATCH - LSTM AND ANSWER POINTER,question-answering,8,['O'],['O'],3,0.3,2,0.0080321285140562,0,0.0,1,0,
4,Machine comprehension of text is an important problem in natural language processing .,,MACHINE COMPREHENSION USING MATCH - LSTM AND ANSWER POINTER,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.4,3,0.0120481927710843,1,0.1428571428571428,1,0,
6,"SQuAD provides a challenging testbed for evaluating machine comprehension algorithms , partly because compared with previous datasets , in SQuAD the answers do not come from a small set of candidate answers and they have variable lengths .",,MACHINE COMPREHENSION USING MATCH - LSTM AND ANSWER POINTER,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.6,5,0.0200803212851405,3,0.4285714285714285,1,0,
13,"While the ability of a machine to understand text can be assessed in many different ways , in recent years , several benchmark datasets have been created to focus on answering questions as away to evaluate machine comprehension .",INTRODUCTION,INTRODUCTION,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.0512820512820512,12,0.0481927710843373,2,0.0512820512820512,1,0,INTRODUCTION
14,"In this setup , typically the machine is first presented with apiece of text such as a news article or a story .",INTRODUCTION,INTRODUCTION,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.0769230769230769,13,0.0522088353413654,3,0.0769230769230769,1,0,INTRODUCTION
15,The machine is then expected to answer one or multiple questions related to the text .,INTRODUCTION,INTRODUCTION,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.1025641025641025,14,0.0562248995983935,4,0.1025641025641025,1,0,INTRODUCTION
16,"In most of the benchmark datasets , a question can be treated as a multiple choice question , whose correct answer is to be chosen from a set of provided candidate answers .",INTRODUCTION,INTRODUCTION,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.1282051282051282,15,0.0602409638554216,5,0.1282051282051282,1,0,INTRODUCTION
17,"Presumably , questions with more given candidate answers are more challenging .",INTRODUCTION,INTRODUCTION,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.1538461538461538,16,0.0642570281124498,6,0.1538461538461538,1,0,INTRODUCTION
18,The Stanford Question Answering Dataset ( SQuAD ) introduced recently by contains such more challenging questions whose correct answers can be any sequence of tokens from the given text .,INTRODUCTION,INTRODUCTION,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.1794871794871795,17,0.0682730923694779,7,0.1794871794871795,1,0,INTRODUCTION
19,"Moreover , unlike some other datasets whose questions and answers were created automatically in Cloze style , the questions and answers in SQu AD were created by humans through crowdsourcing , which makes the dataset more realistic .",INTRODUCTION,INTRODUCTION,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.2051282051282051,18,0.072289156626506,8,0.2051282051282051,1,0,INTRODUCTION
22,"Traditional solutions to this kind of question answering tasks rely on NLP pipelines that involve multiple steps of linguistic analyses and feature engineering , including syntactic parsing , named entity recognition , question classification , semantic parsing , etc .",INTRODUCTION,INTRODUCTION,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.282051282051282,21,0.0843373493975903,11,0.282051282051282,1,0,INTRODUCTION
23,"Recently , with the advances of applying neural network models in NLP , there has been much interest in building end - to - end neural architectures for various NLP tasks , including several pieces of work on machine comprehension .",INTRODUCTION,INTRODUCTION,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.3076923076923077,22,0.0883534136546184,12,0.3076923076923077,1,0,INTRODUCTION
24,"However , given the properties of previous machine comprehension datasets , existing end - to - end neural architectures for the task either rely on the candidate answers or assume that the In 1870 , Tesla moved to Karlovac , to attend school at the Higher Real Gymnasium , where he was profoundly influenced by a math teacher Martin Sekuli ?.",INTRODUCTION,INTRODUCTION,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.3333333333333333,23,0.0923694779116465,13,0.3333333333333333,1,0,INTRODUCTION
25,"The classes were held in German , as it was a school within the Austro-Hungarian Military Frontier .",INTRODUCTION,INTRODUCTION,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.358974358974359,24,0.0963855421686747,14,0.358974358974359,1,0,INTRODUCTION
26,"Tesla was able to perform integral calculus in his head , which prompted his teachers to believe that he was cheating .",INTRODUCTION,INTRODUCTION,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.3846153846153846,25,0.1004016064257028,15,0.3846153846153846,1,0,INTRODUCTION
27,"He finished a four - year term in three years , graduating in 1873 .",INTRODUCTION,INTRODUCTION,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.4102564102564102,26,0.1044176706827309,16,0.4102564102564102,1,0,INTRODUCTION
28,1 .,INTRODUCTION,INTRODUCTION,question-answering,8,"['O', 'O']","['O', 'O']",17,0.4358974358974359,27,0.108433734939759,17,0.4358974358974359,1,0,INTRODUCTION
30,German,INTRODUCTION,INTRODUCTION,question-answering,8,['O'],['O'],19,0.4871794871794871,29,0.1164658634538152,19,0.4871794871794871,1,0,INTRODUCTION
31,2 . Who was Tesla 's main influence in Karlovac ?,INTRODUCTION,INTRODUCTION,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.5128205128205128,30,0.1204819277108433,20,0.5128205128205128,1,0,INTRODUCTION
32,Martin Sekuli ?,INTRODUCTION,INTRODUCTION,question-answering,8,"['O', 'O', 'O']","['O', 'O', 'O']",21,0.5384615384615384,31,0.1244979919678714,21,0.5384615384615384,1,0,INTRODUCTION
36,The tokens in bold in the paragraph are our predicted answers while the texts next to the questions are the ground truth answers .,INTRODUCTION,INTRODUCTION,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.6410256410256411,35,0.1405622489959839,25,0.6410256410256411,1,0,INTRODUCTION
37,"answer is a single token , which make these methods unsuitable for the SQuAD dataset .",INTRODUCTION,INTRODUCTION,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.6666666666666666,36,0.144578313253012,26,0.6666666666666666,1,0,INTRODUCTION
45,Our contributions can be summarized as follows :,INTRODUCTION,INTRODUCTION,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",34,0.8717948717948718,44,0.1767068273092369,34,0.8717948717948718,1,0,INTRODUCTION
50,"Beisdes , we also made our code available online 1 .",INTRODUCTION,INTRODUCTION,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",39,1.0,49,0.1967871485943775,39,1.0,1,0,INTRODUCTION
51,METHOD,,,question-answering,8,['O'],['O'],0,0.0,50,0.2008032128514056,0,0.0,1,0,
55,MATCH - LSTM,METHOD,,question-answering,8,"['O', 'O', 'O']","['O', 'O', 'O']",4,0.2,54,0.216867469879518,4,0.2105263157894736,1,0,METHOD
57,"In textual entailment , two sentences are given where one is a premise and the other is a hypothesis .",METHOD,MATCH - LSTM,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.3,56,0.2248995983935743,6,0.3157894736842105,1,0,METHOD: MATCH - LSTM
60,"This weighted premise is then to be combined with a vector representation of the current token of the hypothesis and fed into an LSTM , which we call the match - LSTM .",METHOD,MATCH - LSTM,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.45,59,0.2369477911646586,9,0.4736842105263157,1,0,METHOD: MATCH - LSTM
64,"For each match - LSTM in a particular direction , h q i , which is defined as H q ?",METHOD,MATCH - LSTM,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.65,63,0.253012048192771,13,0.6842105263157895,1,0,METHOD: MATCH - LSTM
65,"i , is computed using the ?",METHOD,MATCH - LSTM,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.7,64,0.2570281124497992,14,0.7368421052631579,1,0,METHOD: MATCH - LSTM
66,"in the corresponding direction , as described in either Eqn. ( 2 ) or Eqn. ( 5 ) .",METHOD,MATCH - LSTM,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.75,65,0.2610441767068273,15,0.7894736842105263,1,0,METHOD: MATCH - LSTM
67,proposed a Pointer Network ( Ptr - Net ) model to solve a special kind of problems where we want to generate an output sequence whose tokens must come from the input sequence .,METHOD,MATCH - LSTM,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.8,66,0.2650602409638554,16,0.8421052631578947,1,0,METHOD: MATCH - LSTM
69,The pointer mechanism has inspired some recent work on language processing .,METHOD,MATCH - LSTM,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.9,68,0.2730923694779116,18,0.9473684210526316,1,0,METHOD: MATCH - LSTM
71,POINTER NET,METHOD,,question-answering,8,"['O', 'O']","['O', 'O']",20,1.0,70,0.2811244979919678,0,0.0,1,0,METHOD
72,OUR METHOD,,,question-answering,8,"['O', 'O']","['O', 'O']",0,0.0,71,0.285140562248996,0,0.0,1,0,
74,"We are given apiece of text , which we refer to as a passage , and a question related to the passage .",OUR METHOD,OUR METHOD,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.0217391304347826,73,0.2931726907630522,2,0.0327868852459016,1,0,OUR METHOD
75,The passage is represented by matrix P ?,OUR METHOD,OUR METHOD,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.0326086956521739,74,0.2971887550200803,3,0.0491803278688524,1,0,OUR METHOD
76,"R dP , where P is the length ( number of tokens ) of the passage and dis the dimensionality of word embeddings .",OUR METHOD,OUR METHOD,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.0434782608695652,75,0.3012048192771084,4,0.0655737704918032,1,0,OUR METHOD
77,"Similarly , the question is represented by matrix Q ?",OUR METHOD,OUR METHOD,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.0543478260869565,76,0.3052208835341365,5,0.081967213114754,1,0,OUR METHOD
78,R d Q where Q is the length of the question .,OUR METHOD,OUR METHOD,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.0652173913043478,77,0.3092369477911647,6,0.0983606557377049,1,0,OUR METHOD
80,"As pointed out earlier , since the output tokens are from the input , we would like to adopt the Pointer Net for this problem .",OUR METHOD,OUR METHOD,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.0869565217391304,79,0.3172690763052209,8,0.1311475409836065,1,0,OUR METHOD
81,"A straightforward way of applying Ptr - Net here is to treat an answer as a sequence of tokens from the input passage but ignore the fact that these tokens are consecutive in the original passage , because Ptr - Net does not make the consecutivity assumption .",OUR METHOD,OUR METHOD,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.0978260869565217,80,0.321285140562249,9,0.1475409836065573,1,0,OUR METHOD
82,"Specifically , we represent the answer as a sequence of integers a = ( a 1 , a 2 , . . . ) , where each a i is an integer between 1 and P , indicating a certain position in the passage .",OUR METHOD,OUR METHOD,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.108695652173913,81,0.3253012048192771,10,0.1639344262295081,1,0,OUR METHOD
83,"Alternatively , if we want to ensure consecutivity , that is , if we want to ensure that we indeed select a subsequence from the passage as an answer , we can use the Ptr-Net to predict only the start and the end of an answer .",OUR METHOD,OUR METHOD,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.1195652173913043,82,0.3293172690763052,11,0.180327868852459,1,0,OUR METHOD
84,"In this case , the Ptr - Net only needs to select two tokens from the input passage , and all the tokens between these two tokens in the passage are treated as the answer .",OUR METHOD,OUR METHOD,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.1304347826086956,83,0.3333333333333333,12,0.1967213114754098,1,0,OUR METHOD
85,"Specifically , we can represent the answer to be predicted as two integers a = ( a s , a e ) , where a s an a e are integers between 1 and P .",OUR METHOD,OUR METHOD,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.1413043478260869,84,0.3373493975903614,13,0.2131147540983606,1,0,OUR METHOD
86,We refer to the first setting above as a sequence model and the second setting above as a boundary model .,OUR METHOD,OUR METHOD,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.1521739130434782,85,0.3413654618473896,14,0.2295081967213114,1,0,OUR METHOD
87,"For either model , we assume that a set of training examples in the form of triplets {( P n , Q n , an ) } N n=1 are given .",OUR METHOD,OUR METHOD,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.1630434782608695,86,0.3453815261044177,15,0.2459016393442623,1,0,OUR METHOD
93,The difference between the two models only lies in the third layer .,OUR METHOD,OUR METHOD,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.2282608695652173,92,0.3694779116465863,21,0.3442622950819672,1,0,OUR METHOD
94,LSTM,OUR METHOD,,question-answering,8,['O'],['O'],22,0.2391304347826087,93,0.3734939759036144,22,0.360655737704918,1,0,OUR METHOD
95,Preprocessing Layer,OUR METHOD,LSTM,question-answering,8,"['O', 'O']","['O', 'O']",23,0.25,94,0.3775100401606425,23,0.3770491803278688,1,0,OUR METHOD: LSTM
100,"R lQ are hidden representations of the passage and the question , where l is the dimensionality of the hidden vectors .",OUR METHOD,LSTM,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",28,0.3043478260869565,99,0.3975903614457831,28,0.4590163934426229,1,0,OUR METHOD: LSTM
101,"In other words , the i th column vector hp i ( or h q i ) in H p ( or H q ) represents the i th token in the passage ( or the question ) together with some contextual information from the left .",OUR METHOD,LSTM,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",29,0.3152173913043478,100,0.4016064257028112,29,0.4754098360655737,1,0,OUR METHOD: LSTM
102,Match- LSTM,OUR METHOD,LSTM,question-answering,8,"['O', 'O']","['O', 'O']",30,0.3260869565217391,101,0.4056224899598393,30,0.4918032786885246,1,0,OUR METHOD: LSTM
103,Layer,OUR METHOD,LSTM,question-answering,8,['O'],['O'],31,0.3369565217391304,102,0.4096385542168674,31,0.5081967213114754,1,0,OUR METHOD: LSTM
105,The match - LSTM sequentially goes through the passage .,OUR METHOD,LSTM,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",33,0.358695652173913,104,0.4176706827309236,33,0.5409836065573771,1,0,OUR METHOD: LSTM
106,"At position i of the passage , it first uses the standard word - by - word attention mechanism to obtain attention weight vector ? ? ?",OUR METHOD,LSTM,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",34,0.3695652173913043,105,0.4216867469879518,34,0.5573770491803278,1,0,OUR METHOD: LSTM
107,i ?,OUR METHOD,LSTM,question-answering,8,"['O', 'O']","['O', 'O']",35,0.3804347826086957,106,0.4257028112449799,35,0.5737704918032787,1,0,OUR METHOD: LSTM
108,R Q as follows :,OUR METHOD,LSTM,question-answering,8,"['O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O']",36,0.391304347826087,107,0.429718875502008,36,0.5901639344262295,1,0,OUR METHOD: LSTM
109,"where W q , W p , W r ?",OUR METHOD,LSTM,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",37,0.4021739130434782,108,0.4337349397590361,37,0.6065573770491803,1,0,OUR METHOD: LSTM
111,R land b ?,OUR METHOD,LSTM,question-answering,8,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",39,0.4239130434782608,110,0.4417670682730923,39,0.639344262295082,1,0,OUR METHOD: LSTM
112,"R are parameters to be learned , ? ? hr i?1 ?",OUR METHOD,LSTM,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",40,0.4347826086956521,111,0.4457831325301205,40,0.6557377049180327,1,0,OUR METHOD: LSTM
113,"R l is the hidden vector of the one -directional match - LSTM ( to be explained below ) at position i ? 1 , and the outer product ( ? e Q ) produces a matrix or row vector by repeating the vector or scalar on the left for Q times .",OUR METHOD,LSTM,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",41,0.4456521739130434,112,0.4497991967871486,41,0.6721311475409836,1,0,OUR METHOD: LSTM
114,"Essentially , the resulting attention weight ? ? ?",OUR METHOD,LSTM,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",42,0.4565217391304347,113,0.4538152610441767,42,0.6885245901639344,1,0,OUR METHOD: LSTM
115,"i , j above indicates the degree of matching between the i th token in the passage with the j th token in the question .",OUR METHOD,LSTM,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",43,0.4673913043478261,114,0.4578313253012048,43,0.7049180327868853,1,0,OUR METHOD: LSTM
116,"Next , we use the attention weight vector ? ? ?",OUR METHOD,LSTM,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",44,0.4782608695652174,115,0.4618473895582329,44,0.7213114754098361,1,0,OUR METHOD: LSTM
117,i to obtain a weighted version of the question and combine it with the current token of the passage to form a vector ? ?,OUR METHOD,LSTM,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",45,0.4891304347826087,116,0.465863453815261,45,0.7377049180327869,1,0,OUR METHOD: LSTM
118,z i :,OUR METHOD,LSTM,question-answering,8,"['O', 'O', 'O']","['O', 'O', 'O']",46,0.5,117,0.4698795180722891,46,0.7540983606557377,1,0,OUR METHOD: LSTM
119,This vector ? ?,OUR METHOD,LSTM,question-answering,8,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",47,0.5108695652173914,118,0.4738955823293173,47,0.7704918032786885,1,0,OUR METHOD: LSTM
120,z i is fed into a standard one - directional LSTM to form our so - called match - LSTM :,OUR METHOD,LSTM,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",48,0.5217391304347826,119,0.4779116465863454,48,0.7868852459016393,1,0,OUR METHOD: LSTM
121,where ? ?,OUR METHOD,LSTM,question-answering,8,"['O', 'O', 'O']","['O', 'O', 'O']",49,0.532608695652174,120,0.4819277108433735,49,0.8032786885245902,1,0,OUR METHOD: LSTM
122,hr i ?,OUR METHOD,LSTM,question-answering,8,"['O', 'O', 'O']","['O', 'O', 'O']",50,0.5434782608695652,121,0.4859437751004016,50,0.819672131147541,1,0,OUR METHOD: LSTM
123,R l .,OUR METHOD,LSTM,question-answering,8,"['O', 'O', 'O']","['O', 'O', 'O']",51,0.5543478260869565,122,0.4899598393574297,51,0.8360655737704918,1,0,OUR METHOD: LSTM
126,"To build this reverse match - LSTM , we first define",OUR METHOD,LSTM,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",54,0.5869565217391305,125,0.5020080321285141,54,0.8852459016393442,1,0,OUR METHOD: LSTM
127,"Note that the parameters here ( W q , W p , W r , b p , wand b ) are the same as used in Eqn .",OUR METHOD,LSTM,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",55,0.5978260869565217,126,0.5060240963855421,55,0.9016393442622952,1,0,OUR METHOD: LSTM
128,( 2 ) .,OUR METHOD,LSTM,question-answering,8,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",56,0.6086956521739131,127,0.5100401606425703,56,0.918032786885246,1,0,OUR METHOD: LSTM
129,We then define ? ?,OUR METHOD,LSTM,question-answering,8,"['O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O']",57,0.6195652173913043,128,0.5140562248995983,57,0.9344262295081968,1,0,OUR METHOD: LSTM
130,z i in a similar way and finally define ? ?,OUR METHOD,LSTM,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",58,0.6304347826086957,129,0.5180722891566265,58,0.9508196721311476,1,0,OUR METHOD: LSTM
131,hr i to be the hidden representation at position i produced by the match - LSTM in the reverse direction .,OUR METHOD,LSTM,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",59,0.6413043478260869,130,0.5220883534136547,59,0.9672131147540984,1,0,OUR METHOD: LSTM
133,R 2 lP as the concatenation of the two :,OUR METHOD,LSTM,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",61,0.6630434782608695,132,0.5301204819277109,61,1.0,1,0,OUR METHOD: LSTM
134,Answer Pointer Layer,OUR METHOD,LSTM,question-answering,8,"['O', 'O', 'O']","['O', 'O', 'O']",62,0.6739130434782609,133,0.5341365461847389,0,0.0,1,0,OUR METHOD: LSTM
135,"The top layer , the Answer Pointer ( Ans - Ptr ) layer , is motivated by the Pointer Net introduced by .",OUR METHOD,LSTM,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",63,0.6847826086956522,134,0.5381526104417671,1,0.0333333333333333,1,0,OUR METHOD: LSTM
136,This layer uses the sequence H r as input .,OUR METHOD,LSTM,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",64,0.6956521739130435,135,0.5421686746987951,2,0.0666666666666666,1,0,OUR METHOD: LSTM
137,Recall that we have two different models :,OUR METHOD,LSTM,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",65,0.7065217391304348,136,0.5461847389558233,3,0.1,1,0,OUR METHOD: LSTM
138,The sequence model produces a sequence of answer tokens but these tokens may not be consecutive in the original passage .,OUR METHOD,LSTM,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",66,0.717391304347826,137,0.5502008032128514,4,0.1333333333333333,1,0,OUR METHOD: LSTM
139,"The boundary model produces only the start token and the end token of the answer , and then all the tokens between these two in the original passage are considered to be the answer .",OUR METHOD,LSTM,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",67,0.7282608695652174,138,0.5542168674698795,5,0.1666666666666666,1,0,OUR METHOD: LSTM
142,"Recall that in the sequence model , the answer is represented by a sequence of integers a = ( a 1 , a 2 , . . . ) indicating the positions of the selected tokens in the original passage .",OUR METHOD,LSTM,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",70,0.7608695652173914,141,0.5662650602409639,8,0.2666666666666666,1,0,OUR METHOD: LSTM
144,"Because the length of an answer is not fixed , in order to stop generating answer tokens at certain point , we allow each a k to take up an integer value between 1 and P + 1 , where P + 1 is a special value indicating the end of the answer .",OUR METHOD,LSTM,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",72,0.782608695652174,143,0.5742971887550201,10,0.3333333333333333,1,0,OUR METHOD: LSTM
145,"Once a k is set to be P + 1 , the generation of the answer stops .",OUR METHOD,LSTM,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",73,0.7934782608695652,144,0.5783132530120482,11,0.3666666666666666,1,0,OUR METHOD: LSTM
146,"In order to generate the k th answer token indicated by a k , first , the attention mechanism is used again to obtain an attention weight vector ? k ? R ( P + 1 ) , where ? k , j ( 1 ? j ? P + 1 ) is the probability of selecting the j th token from the passage as the k th token in the answer , and ? k , ( P + 1 ) is the probability of stopping the answer generation at position k. ?",OUR METHOD,LSTM,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",74,0.8043478260869565,145,0.5823293172690763,12,0.4,1,0,OUR METHOD: LSTM
147,k is modeled as follows :,OUR METHOD,LSTM,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O']",75,0.8152173913043478,146,0.5863453815261044,13,0.4333333333333333,1,0,OUR METHOD: LSTM
148,where H r ?,OUR METHOD,LSTM,question-answering,8,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",76,0.8260869565217391,147,0.5903614457831325,14,0.4666666666666667,1,0,OUR METHOD: LSTM
149,"R 2 l ( P + 1 ) is the concatenation of H r with a zero vector , defined as H r = [ H r ; 0 ] , V ? R l2 l , W a ?",OUR METHOD,LSTM,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",77,0.8369565217391305,148,0.5943775100401606,15,0.5,1,0,OUR METHOD: LSTM
151,R land c ?,OUR METHOD,LSTM,question-answering,8,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",79,0.8586956521739131,150,0.6024096385542169,17,0.5666666666666667,1,0,OUR METHOD: LSTM
152,"R are parameters to be learned , ( ? e ( P + 1 ) ) follows the same definition as before , and ha k?1 ?",OUR METHOD,LSTM,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",80,0.8695652173913043,151,0.606425702811245,18,0.6,1,0,OUR METHOD: LSTM
153,R l is the hidden vector at position k ?,OUR METHOD,LSTM,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",81,0.8804347826086957,152,0.6104417670682731,19,0.6333333333333333,1,0,OUR METHOD: LSTM
155,"We can then model the probability of generating the answer sequence as p ( a| H r ) = k p ( a k | a 1 , a 2 , . . . , a k?1 , H r ) ,",OUR METHOD,LSTM,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",83,0.9021739130434784,154,0.6184738955823293,21,0.7,1,0,OUR METHOD: LSTM
158,"log p ( a n | P n , Q n ) .",OUR METHOD,LSTM,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",86,0.9347826086956522,157,0.6305220883534136,24,0.8,1,0,OUR METHOD: LSTM
160,"The boundary model works in away very similar to the sequence model above , except that instead of predicting a sequence of indices a 1 , a 2 , . . . , we only need to predict two indices a sand a e .",OUR METHOD,LSTM,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",88,0.9565217391304348,159,0.6385542168674698,26,0.8666666666666667,1,0,OUR METHOD: LSTM
161,"So the main difference from the sequence model above is that in the boundary model we do not need to add the zero padding to H r , and the probability of generating an answer is simply modeled as p ( a| H r ) = p ( a s | H r ) p ( a e | a s , H r ) . Here "" Search "" refers to globally searching the spans with no more than 15 tokens , "" b "" refers to using bi-directional pre-processing LSTM , and "" en "" refers to ensemble method .",OUR METHOD,LSTM,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",89,0.967391304347826,160,0.642570281124498,27,0.9,1,0,OUR METHOD: LSTM
164,"Besides , as the boundary has a sequence of fixed number of values , bi-directional Ans - Ptr can be simply combined to fine - tune the correct span .",OUR METHOD,LSTM,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",92,1.0,163,0.6546184738955824,30,1.0,1,0,OUR METHOD: LSTM
165,EXPERIMENTS,,,question-answering,8,['O'],['O'],0,0.0,164,0.6586345381526104,0,0.0,1,0,
167,DATA,EXPERIMENTS,,question-answering,8,['O'],['O'],2,0.2857142857142857,166,0.6666666666666666,0,0.0,1,0,EXPERIMENTS
173,EXPERIMENT SETTINGS,,,question-answering,8,"['O', 'O']","['O', 'O']",0,0.0,172,0.6907630522088354,0,0.0,1,0,
177,Words not found in Glo Ve are initialized as zero vectors .,EXPERIMENT SETTINGS,EXPERIMENT SETTINGS,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.3333333333333333,176,0.7068273092369478,4,0.3333333333333333,1,0,EXPERIMENT SETTINGS
184,Note that in the development set and the test set each question has around three ground truth answers .,EXPERIMENT SETTINGS,EXPERIMENT SETTINGS,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.9166666666666666,183,0.7349397590361446,11,0.9166666666666666,1,0,EXPERIMENT SETTINGS
186,RESULTS,,,question-answering,8,['O'],['O'],0,0.0,185,0.7429718875502008,0,0.0,1,0,
188,We can see that both of our two models have clearly outper - :,RESULTS,RESULTS,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.0512820512820512,187,0.751004016064257,2,0.125,1,0,RESULTS
190,for three questions associated with the same passage .,RESULTS,RESULTS,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.1025641025641025,189,0.7590361445783133,4,0.25,1,0,RESULTS
196,"Observing that most of the answers are the spans with relatively small sizes , we simply limit the largest predicted span to have no more than 15 tokens and conducted experiment with span searching",RESULTS,RESULTS,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.2564102564102564,195,0.7831325301204819,10,0.625,1,0,RESULTS
198,"Besides , we tried to increase the memory dimension l in the model or add bi-directional pre-processing LSTM or add bi-directional Ans - Ptr .",RESULTS,RESULTS,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.3076923076923077,197,0.7911646586345381,12,0.75,1,0,RESULTS
203,FURTHER ANALYSES,RESULTS,,question-answering,8,"['O', 'O']","['O', 'O']",17,0.4358974358974359,202,0.8112449799196787,0,0.0,1,0,RESULTS
205,"First , we suspect that longer answers are harder to predict .",RESULTS,FURTHER ANALYSES,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.4871794871794871,204,0.8192771084337349,2,0.0909090909090909,1,0,RESULTS: FURTHER ANALYSES
208,And that supports our hypothesis .,RESULTS,FURTHER ANALYSES,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O']",22,0.5641025641025641,207,0.8313253012048193,5,0.2272727272727272,1,0,RESULTS: FURTHER ANALYSES
211,These different question words roughly refer to questions with different types of answers .,RESULTS,FURTHER ANALYSES,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.6410256410256411,210,0.8433734939759037,8,0.3636363636363636,1,0,RESULTS: FURTHER ANALYSES
212,"For example , "" when "" questions look for temporal expressions as answers , whereas "" where "" questions look for locations as answers .",RESULTS,FURTHER ANALYSES,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.6666666666666666,211,0.8473895582329317,9,0.4090909090909091,1,0,RESULTS: FURTHER ANALYSES
214,This maybe because in this dataset temporal expressions are relatively easier to recognize .,RESULTS,FURTHER ANALYSES,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",28,0.717948717948718,213,0.8554216867469879,11,0.5,1,0,RESULTS: FURTHER ANALYSES
216,"On the other hand , "" why "" questions are the hardest to answer .",RESULTS,FURTHER ANALYSES,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",30,0.7692307692307693,215,0.8634538152610441,13,0.5909090909090909,1,0,RESULTS: FURTHER ANALYSES
217,"This is not surprising because the answers to "" why "" questions can be very diverse , and they are not restricted to any certain type of phrases .",RESULTS,FURTHER ANALYSES,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",31,0.7948717948717948,216,0.8674698795180723,14,0.6363636363636364,1,0,RESULTS: FURTHER ANALYSES
218,"Finally , we would like to check whether the attention mechanism used in the match - LSTM layer is effective in helping the model locate the answer .",RESULTS,FURTHER ANALYSES,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",32,0.8205128205128205,217,0.8714859437751004,15,0.6818181818181818,1,0,RESULTS: FURTHER ANALYSES
219,We show the attention weights ?,RESULTS,FURTHER ANALYSES,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O']",33,0.8461538461538461,218,0.8755020080321285,16,0.7272727272727273,1,0,RESULTS: FURTHER ANALYSES
220,in .,RESULTS,FURTHER ANALYSES,question-answering,8,"['O', 'O']","['O', 'O']",34,0.8717948717948718,219,0.8795180722891566,17,0.7727272727272727,1,0,RESULTS: FURTHER ANALYSES
221,In the figure the darker the color is the higher the weight is .,RESULTS,FURTHER ANALYSES,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",35,0.8974358974358975,220,0.8835341365461847,18,0.8181818181818182,1,0,RESULTS: FURTHER ANALYSES
224,"For the question word "" who "" in the second question , the word "" teacher "" actually receives relatively higher attention weight , and the model has predicted the phrase "" Martin Sekulic "" after that as the answer , which is correct .",RESULTS,FURTHER ANALYSES,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",38,0.9743589743589745,223,0.8955823293172691,21,0.9545454545454546,1,0,RESULTS: FURTHER ANALYSES
225,"For the last question that starts with "" why "" , the attention weights are more evenly distributed and it is not clear which words have been aligned to "" why "" .",RESULTS,FURTHER ANALYSES,question-answering,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",39,1.0,224,0.8995983935742972,22,1.0,1,0,RESULTS: FURTHER ANALYSES
226,RELATED WORK,,,question-answering,8,"['O', 'O']","['O', 'O']",0,0.0,225,0.9036144578313252,0,0.0,1,0,
240,CONCLUSIONS,,,question-answering,8,['O'],['O'],0,0.0,239,0.9598393574297188,0,0.0,1,0,
3,abstract,,LEARNING RECURRENT SPAN REPRESENTATIONS FOR EXTRACTIVE QUESTION ANSWERING,question-answering,9,['O'],['O'],3,0.2,2,0.0112359550561797,0,0.0,1,0,
4,"The reading comprehension task , that asks questions about a given evidence document , is a central problem in natural language understanding .",,LEARNING RECURRENT SPAN REPRESENTATIONS FOR EXTRACTIVE QUESTION ANSWERING,question-answering,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.2666666666666666,3,0.0168539325842696,1,0.0833333333333333,1,0,
5,Recent formulations of this task have typically focused on answer selection from a set of candidates pre-defined manually or through the use of an external NLP pipeline .,,LEARNING RECURRENT SPAN REPRESENTATIONS FOR EXTRACTIVE QUESTION ANSWERING,question-answering,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.3333333333333333,4,0.0224719101123595,2,0.1666666666666666,1,0,
6,"However , Rajpurkar et al . ( 2016 ) recently released the SQUAD dataset in which the answers can be arbitrary strings from the supplied text .",,LEARNING RECURRENT SPAN REPRESENTATIONS FOR EXTRACTIVE QUESTION ANSWERING,question-answering,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.4,5,0.0280898876404494,3,0.25,1,0,
10,"Recently , Rajpurkar et al. ( 2016 ) released the less restricted SQUAD dataset 1 that does not place any constraints on the set of allowed answers , other than that they should be drawn from the evidence document .",,LEARNING RECURRENT SPAN REPRESENTATIONS FOR EXTRACTIVE QUESTION ANSWERING,question-answering,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.6666666666666666,9,0.0505617977528089,7,0.5833333333333334,1,0,
11,Rajpurkar et al. proposed a baseline system that chooses answers from the constituents identified by an existing syntactic parser .,,LEARNING RECURRENT SPAN REPRESENTATIONS FOR EXTRACTIVE QUESTION ANSWERING,question-answering,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.7333333333333333,10,0.0561797752808988,8,0.6666666666666666,1,0,
15,1,,LEARNING RECURRENT SPAN REPRESENTATIONS FOR EXTRACTIVE QUESTION ANSWERING,question-answering,9,['O'],['O'],15,1.0,14,0.0786516853932584,12,1.0,1,0,
17,A primary goal of natural language processing is to develop systems that can answer questions about the contents of documents .,INTRODUCTION,INTRODUCTION,question-answering,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0625,16,0.0898876404494382,1,0.0909090909090909,1,0,INTRODUCTION
18,"The reading comprehension task is of practical interest - we want computers to be able to read the world 's text and then answer our questions - and , since we believe it requires deep language understanding , it has also become a flagship task in NLP research .",INTRODUCTION,INTRODUCTION,question-answering,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.125,17,0.095505617977528,2,0.1818181818181818,1,0,INTRODUCTION
19,A number of reading comprehension datasets have been developed that focus on answer selection from a small set of alternatives defined by annotators or existing NLP pipelines that can not be trained end - to - end .,INTRODUCTION,INTRODUCTION,question-answering,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.1875,18,0.1011235955056179,3,0.2727272727272727,1,0,INTRODUCTION
20,"Subsequently , the models proposed for this task have tended to make use of the limited set of candidates , basing their predictions on mention - level attention weights , or centering classifiers , or network memories on candidate locations .",INTRODUCTION,INTRODUCTION,question-answering,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.25,19,0.1067415730337078,4,0.3636363636363636,1,0,INTRODUCTION
21,"In contrast , here we argue that it is beneficial to simplify the decoding procedure by enumerating all possible answer spans .",INTRODUCTION,INTRODUCTION,question-answering,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.3125,20,0.1123595505617977,5,0.4545454545454545,1,0,INTRODUCTION
23,"A naive approach to building the O ( N 2 ) spans of up to length N would require a network that is cubic in size with respect to the passage length , and such a network would be untrainable .",INTRODUCTION,INTRODUCTION,question-answering,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.4375,22,0.1235955056179775,7,0.6363636363636364,1,0,INTRODUCTION
29,TASK DEFINITION,INTRODUCTION,,question-answering,9,"['O', 'O']","['O', 'O']",13,0.8125,28,0.1573033707865168,0,0.0,1,0,INTRODUCTION
30,"Extractive question answering systems take as input a question q = {q 0 , . . . , q n } and a passage of text p = {p 0 , . . . , pm } from which they predict a single answer span a = a start , a end , represented as a pair of indices into p.",INTRODUCTION,TASK DEFINITION,question-answering,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.875,29,0.1629213483146067,1,0.3333333333333333,1,0,INTRODUCTION: TASK DEFINITION
31,"Machine learned extractive question answering systems , such as the one presented here , learn a predictor function f ( q , p ) ?",INTRODUCTION,TASK DEFINITION,question-answering,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.9375,30,0.1685393258426966,2,0.6666666666666666,1,0,INTRODUCTION: TASK DEFINITION
32,"a from a training dataset of q , p , a triples .",INTRODUCTION,TASK DEFINITION,question-answering,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,1.0,31,0.1741573033707865,3,1.0,1,0,INTRODUCTION: TASK DEFINITION
33,RELATED WORK,,,question-answering,9,"['O', 'O']","['O', 'O']",0,0.0,32,0.1797752808988764,0,0.0,1,0,
52,MODEL,,,question-answering,9,['O'],['O'],0,0.0,51,0.2865168539325842,0,0.0,1,0,
54,"In most structured prediction problems ( e.g. sequence labeling or parsing ) , the number of possible output structures is exponential in the input length , and computing representations for every candidate is prohibitively expensive .",MODEL,MODEL,question-answering,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.0384615384615384,53,0.297752808988764,2,0.1818181818181818,1,0,MODEL
58,"For the example in , RASOR computes an embedding for the candidate answer spans : fixed to , fixed to the , to the , etc .",MODEL,MODEL,question-answering,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.1153846153846153,57,0.3202247191011236,6,0.5454545454545454,1,0,MODEL
59,A naive approach for these aggregations would require a network that is cubic in size with respect to the passage length .,MODEL,MODEL,question-answering,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.1346153846153846,58,0.3258426966292135,7,0.6363636363636364,1,0,MODEL
64,SCORING ANSWER SPANS,MODEL,,question-answering,9,"['O', 'O', 'O']","['O', 'O', 'O']",12,0.2307692307692307,63,0.3539325842696629,0,0.0,1,0,MODEL
65,"The goal of our extractive question answering system is to predict the single best answer span among all candidates from the passage p , denoted as A ( p ) .",MODEL,SCORING ANSWER SPANS,question-answering,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.25,64,0.3595505617977528,1,0.027027027027027,1,0,MODEL: SCORING ANSWER SPANS
66,"Therefore , we define a probability distribution overall possible answer spans given the question q and passage p , and the predictor function finds the answer span with the maximum likelihood :",MODEL,SCORING ANSWER SPANS,question-answering,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.2692307692307692,65,0.3651685393258427,2,0.054054054054054,1,0,MODEL: SCORING ANSWER SPANS
67,One might be tempted to introduce independence assumptions that would enable cheaper decoding .,MODEL,SCORING ANSWER SPANS,question-answering,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.2884615384615384,66,0.3707865168539326,3,0.081081081081081,1,0,MODEL: SCORING ANSWER SPANS
68,"For example , this distribution can be modeled as ( 1 ) a product of conditionally independent distributions ( binary ) for every word or ( 2 ) a product of conditionally independent distributions ( over words ) for the start and end indices of the answer span .",MODEL,SCORING ANSWER SPANS,question-answering,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.3076923076923077,67,0.3764044943820224,4,0.1081081081081081,1,0,MODEL: SCORING ANSWER SPANS
69,"However , we show in Section 5.2 that such independence assumptions hurt the accuracy of the model , and instead we only assume a fixed - length representation ha of each candidate span that is scored and normalized with a softmax layer ( Span score and Softmax in ) :",MODEL,SCORING ANSWER SPANS,question-answering,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.3269230769230769,68,0.3820224719101123,5,0.1351351351351351,1,0,MODEL: SCORING ANSWER SPANS
70,where FFNN ( ) denotes a fully connected feed - forward neural network that provides a non-linear mapping of its input embedding .,MODEL,SCORING ANSWER SPANS,question-answering,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.3461538461538461,69,0.3876404494382022,6,0.1621621621621621,1,0,MODEL: SCORING ANSWER SPANS
72,"The previously defined probability distribution depends on the answer span representations , ha .",MODEL,RASOR : RECURRENT SPAN REPRESENTATION,question-answering,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.3846153846153846,71,0.398876404494382,8,0.2162162162162162,1,0,MODEL: RASOR : RECURRENT SPAN REPRESENTATION
73,"When computing ha , we assume access to representations of individual passage words that have been augmented with a representation of the question .",MODEL,RASOR : RECURRENT SPAN REPRESENTATION,question-answering,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.4038461538461538,72,0.4044943820224719,9,0.2432432432432432,1,0,MODEL: RASOR : RECURRENT SPAN REPRESENTATION
74,"We denote these question - focused passage word embeddings as {p * 1 , . . . , p * m } and describe their creation in Section 3.3 .",MODEL,RASOR : RECURRENT SPAN REPRESENTATION,question-answering,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.4230769230769231,73,0.4101123595505618,10,0.2702702702702703,1,0,MODEL: RASOR : RECURRENT SPAN REPRESENTATION
76,This allows us to simply concatenate the bidirectional LSTM ( BiLSTM ) outputs at the endpoints of a span to jointly encode its inside and outside information ( Span embedding in :,MODEL,RASOR : RECURRENT SPAN REPRESENTATION,question-answering,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.4615384615384615,75,0.4213483146067415,12,0.3243243243243243,1,0,MODEL: RASOR : RECURRENT SPAN REPRESENTATION
77,where BILSTM ( ) denotes a BiLSTM over it s input embedding sequence and p * i is the concatenation of forward and backward outputs at time - step i .,MODEL,RASOR : RECURRENT SPAN REPRESENTATION,question-answering,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.4807692307692308,76,0.4269662921348314,13,0.3513513513513513,1,0,MODEL: RASOR : RECURRENT SPAN REPRESENTATION
78,"While the visualization in shows a single layer BiLSTM for simplicity , we use a multi - layer BiLSTM in our experiments .",MODEL,RASOR : RECURRENT SPAN REPRESENTATION,question-answering,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.5,77,0.4325842696629213,14,0.3783783783783784,1,0,MODEL: RASOR : RECURRENT SPAN REPRESENTATION
80,QUESTION - FOCUSED PASSAGE WORD EMBEDDING,MODEL,,question-answering,9,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O']",28,0.5384615384615384,79,0.4438202247191011,16,0.4324324324324324,1,0,MODEL
81,"Computing the question - focused passage word embeddings {p * 1 , . . . , p * m } requires integrating question information into the passage .",MODEL,QUESTION - FOCUSED PASSAGE WORD EMBEDDING,question-answering,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",29,0.5576923076923077,80,0.449438202247191,17,0.4594594594594595,1,0,MODEL: QUESTION - FOCUSED PASSAGE WORD EMBEDDING
88,"The first component simply looks up the pretrained word embedding for the passage word , pi .",MODEL,QUESTION - FOCUSED PASSAGE WORD EMBEDDING,question-answering,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",36,0.6923076923076923,87,0.4887640449438202,24,0.6486486486486487,1,0,MODEL: QUESTION - FOCUSED PASSAGE WORD EMBEDDING
89,Passage - aligned question representation,MODEL,QUESTION - FOCUSED PASSAGE WORD EMBEDDING,question-answering,9,"['O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O']",37,0.7115384615384616,88,0.4943820224719101,25,0.6756756756756757,1,0,MODEL: QUESTION - FOCUSED PASSAGE WORD EMBEDDING
90,"In this dataset , the question - passage pairs often contain large lexical overlap or similarity near the correct answer span .",MODEL,QUESTION - FOCUSED PASSAGE WORD EMBEDDING,question-answering,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",38,0.7307692307692307,89,0.5,26,0.7027027027027027,1,0,MODEL: QUESTION - FOCUSED PASSAGE WORD EMBEDDING
93,q align i = n j=1 a ij q j,MODEL,QUESTION - FOCUSED PASSAGE WORD EMBEDDING,question-answering,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",41,0.7884615384615384,92,0.5168539325842697,29,0.7837837837837838,1,0,MODEL: QUESTION - FOCUSED PASSAGE WORD EMBEDDING
99,"Formally , the passage - independent question representation q indep is computed as follows :",MODEL,QUESTION - FOCUSED PASSAGE WORD EMBEDDING,question-answering,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",47,0.903846153846154,98,0.550561797752809,35,0.945945945945946,1,0,MODEL: QUESTION - FOCUSED PASSAGE WORD EMBEDDING
101,"Given the above three components , the complete question - focused passage word embedding for pi is their concatenation :",MODEL,QUESTION - FOCUSED PASSAGE WORD EMBEDDING,question-answering,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",49,0.9423076923076924,100,0.5617977528089888,37,1.0,1,0,MODEL: QUESTION - FOCUSED PASSAGE WORD EMBEDDING
102,LEARNING,MODEL,,question-answering,9,['O'],['O'],50,0.9615384615384616,101,0.5674157303370787,0,0.0,1,0,MODEL
103,"Given the above model specification , learning is straightforward .",MODEL,LEARNING,question-answering,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",51,0.9807692307692308,102,0.5730337078651685,1,0.5,1,0,MODEL: LEARNING
105,EXPERIMENTAL SETUP,,,question-answering,9,"['O', 'O']","['O', 'O']",0,0.0,104,0.5842696629213483,0,0.0,1,0,
110,Answer candidates are limited to spans with at most 30 words .,EXPERIMENTAL SETUP,EXPERIMENTAL SETUP,question-answering,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.625,109,0.6123595505617978,5,0.625,1,0,EXPERIMENTAL SETUP
114,RESULTS,,,question-answering,9,['O'],['O'],0,0.0,113,0.6348314606741573,0,0.0,1,0,
117,COMPARISONS TO OTHER WORK,RESULTS,,question-answering,9,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",3,0.2727272727272727,116,0.651685393258427,0,0.0,1,0,RESULTS
122,More closely related to RASOR is the boundary model with Match - LSTMs and Pointer Networks by .,RESULTS,COMPARISONS TO OTHER WORK,question-answering,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.7272727272727273,121,0.6797752808988764,5,0.625,1,0,RESULTS: COMPARISONS TO OTHER WORK
123,"Their model similarly uses recurrent networks to learn embeddings of each passage word in the context of the question , and it can also capture interactions between endpoints , since the end index probability distribution is conditioned on the start index .",RESULTS,COMPARISONS TO OTHER WORK,question-answering,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.8181818181818182,122,0.6853932584269663,6,0.75,1,0,RESULTS: COMPARISONS TO OTHER WORK
124,"However , both training and evaluation are greedy , making their system susceptible to search errors when decoding .",RESULTS,COMPARISONS TO OTHER WORK,question-answering,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.9090909090909092,123,0.6910112359550562,7,0.875,1,0,RESULTS: COMPARISONS TO OTHER WORK
126,MODEL VARIATIONS,,,question-answering,9,"['O', 'O']","['O', 'O']",0,0.0,125,0.702247191011236,0,0.0,1,0,
134,The input of both of these components is analyzed qualitatively in Section 6 .,MODEL VARIATIONS,MODEL VARIATIONS,question-answering,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.1739130434782608,133,0.7471910112359551,8,1.0,1,0,MODEL VARIATIONS
135,Question representation EM F1,MODEL VARIATIONS,MODEL VARIATIONS,question-answering,9,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",9,0.1956521739130435,134,0.7528089887640449,0,0.0,1,0,MODEL VARIATIONS
136,Only passage - independent 48.7 56.6 Only passage - aligned 63.1 71.3 RASOR 66.4 74.9,MODEL VARIATIONS,MODEL VARIATIONS,question-answering,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.217391304347826,135,0.7584269662921348,1,0.5,1,0,MODEL VARIATIONS
138,Learning objective EM F1,MODEL VARIATIONS,MODEL VARIATIONS,question-answering,9,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",12,0.2608695652173913,137,0.7696629213483146,0,0.0,1,0,MODEL VARIATIONS
141,"Given a fixed architecture that is capable of encoding the input questionpassage pairs , there are many ways of setting up a learning objective to encourage the model to predict the correct span .",MODEL VARIATIONS,MODEL VARIATIONS,question-answering,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.3260869565217391,140,0.7865168539325843,3,0.0909090909090909,1,0,MODEL VARIATIONS
143,"In order to provide clean comparisons , we restrict the alternatives to objectives that are trained and evaluated with exact decoding .",MODEL VARIATIONS,MODEL VARIATIONS,question-answering,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.3695652173913043,142,0.797752808988764,5,0.1515151515151515,1,0,MODEL VARIATIONS
144,The simplest alternative is to consider this task as binary classification for every word ( Membership prediction in ) .,MODEL VARIATIONS,MODEL VARIATIONS,question-answering,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.391304347826087,143,0.8033707865168539,6,0.1818181818181818,1,0,MODEL VARIATIONS
147,proposed a sequence - labeling scheme that is similar to the above baseline ( BIO sequence prediction in ) .,MODEL VARIATIONS,MODEL VARIATIONS,question-answering,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.4565217391304347,146,0.8202247191011236,9,0.2727272727272727,1,0,MODEL VARIATIONS
150,We also consider a model that independently predicts the two endpoints of the answer span ( Endpoints prediction in ) .,MODEL VARIATIONS,MODEL VARIATIONS,question-answering,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.5217391304347826,149,0.8370786516853933,12,0.3636363636363636,1,0,MODEL VARIATIONS
152,"When decoding , we only need to enforce the constraint that the start index is no greater than the end index .",MODEL VARIATIONS,MODEL VARIATIONS,question-answering,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.5652173913043478,151,0.848314606741573,14,0.4242424242424242,1,0,MODEL VARIATIONS
154,Note that this model has the same expressivity as RASOR if the span - level FFNN were removed .,MODEL VARIATIONS,MODEL VARIATIONS,question-answering,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",28,0.6086956521739131,153,0.8595505617977528,16,0.4848484848484848,1,0,MODEL VARIATIONS
158,"For example , the labels for membership prediction simply happens to provide single contiguous spans in the supervision .",MODEL VARIATIONS,MODEL VARIATIONS,question-answering,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",32,0.6956521739130435,157,0.8820224719101124,20,0.6060606060606061,1,0,MODEL VARIATIONS
159,The model must consider far more possible answers than it needs to ( the power set of all words ) .,MODEL VARIATIONS,MODEL VARIATIONS,question-answering,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",33,0.717391304347826,158,0.8876404494382022,21,0.6363636363636364,1,0,MODEL VARIATIONS
160,The same problem holds for BIO sequence predictionthe model must do additional work to learn the semantics of the BIO tags .,MODEL VARIATIONS,MODEL VARIATIONS,question-answering,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",34,0.7391304347826086,159,0.8932584269662921,22,0.6666666666666666,1,0,MODEL VARIATIONS
161,"On the other hand , in RASOR , the semantics of an answer span is naturally encoded by the set of labels .",MODEL VARIATIONS,MODEL VARIATIONS,question-answering,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",35,0.7608695652173914,160,0.898876404494382,23,0.696969696969697,1,0,MODEL VARIATIONS
165,shows how the performances of RASOR and the endpoint predictor introduced in Section 5.2 degrade as the lengths of their predictions increase .,MODEL VARIATIONS,MODEL VARIATIONS,question-answering,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",39,0.8478260869565217,164,0.9213483146067416,27,0.8181818181818182,1,0,MODEL VARIATIONS
166,It is clear that explicitly modeling interactions between end markers is increasingly important as the span grows in length .,MODEL VARIATIONS,MODEL VARIATIONS,question-answering,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",40,0.8695652173913043,165,0.9269662921348316,28,0.8484848484848485,1,0,MODEL VARIATIONS
167,"The passageindependent question representation pays most attention to the words that could attach to the answer in the passage ( "" brought "" , "" against "" ) or describe the answer category ( "" people "" ) .",MODEL VARIATIONS,MODEL VARIATIONS,question-answering,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",41,0.8913043478260869,166,0.9325842696629212,29,0.8787878787878788,1,0,MODEL VARIATIONS
170,"However , RASOR assigns almost as much probability mass to it 's incorrect third prediction "" British "" as it does to the top scoring correct prediction "" Egyptian "" .",MODEL VARIATIONS,MODEL VARIATIONS,question-answering,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",44,0.9565217391304348,169,0.949438202247191,32,0.9696969696969696,1,0,MODEL VARIATIONS
171,"This showcases a common failure case for RASOR , where it can find an answer of the correct type close to a phrase that overlaps with the question - but it can not accurately represent the semantic dependency on that phrase .",MODEL VARIATIONS,MODEL VARIATIONS,question-answering,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",45,0.9782608695652174,170,0.9550561797752808,33,1.0,1,0,MODEL VARIATIONS
172,ANALYSIS,MODEL VARIATIONS,,question-answering,9,['O'],['O'],46,1.0,171,0.9606741573033708,0,0.0,1,0,MODEL VARIATIONS
173,CONCLUSION,,,question-answering,9,['O'],['O'],0,0.0,172,0.9662921348314608,0,0.0,1,0,
3,abstract,,,relation-classification,0,['O'],['O'],0,0.0,2,0.0088495575221238,0,0.0,1,0,
14,"For instance , to learn that Toefting and Bolton have an Organization - Affiliation ( ORG - AFF ) relation in the sentence Toefting transferred to Bolton , the entity information that Toefting and Bolton are Person and Organization entities is important .",Introduction,Introduction,relation-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.1304347826086956,13,0.0575221238938053,3,0.1304347826086956,1,0,Introduction
15,"Extraction of these entities is in turn encouraged by the presence of the context words transferred to , which indicate an employment relation .",Introduction,Introduction,relation-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.1739130434782608,14,0.0619469026548672,4,0.1739130434782608,1,0,Introduction
16,Previous joint models have employed feature - based structured learning .,Introduction,Introduction,relation-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.217391304347826,15,0.0663716814159292,5,0.217391304347826,1,0,Introduction
17,An alternative approach to this end - to - end relation extraction task is to employ automatic feature learning via neural network ( NN ) based models .,Introduction,Introduction,relation-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.2608695652173913,16,0.0707964601769911,6,0.2608695652173913,1,0,Introduction
18,There are two ways to represent relations between entities using neural networks : recurrent / recursive neural networks ( RNNs ) and convolutional neural networks ( CNNs ) .,Introduction,Introduction,relation-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.3043478260869565,17,0.0752212389380531,7,0.3043478260869565,1,0,Introduction
19,"Among these , RNNs can directly represent essential linguistic structures , i.e. , word sequences and constituent / dependency trees .",Introduction,Introduction,relation-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.3478260869565217,18,0.079646017699115,8,0.3478260869565217,1,0,Introduction
20,"Despite this representation ability , for relation classification tasks , the previously reported performance using long short - term memory ( LSTM ) based RNNs is worse than one using CNNs .",Introduction,Introduction,relation-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.391304347826087,19,0.084070796460177,9,0.391304347826087,1,0,Introduction
21,"These previous LSTM - based systems mostly include limited linguistic structures and neural architectures , and do not model entities and relations jointly .",Introduction,Introduction,relation-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.4347826086956521,20,0.0884955752212389,10,0.4347826086956521,1,0,Introduction
23,Word sequence and tree structure are known to be complementary information for extracting relations .,Introduction,Introduction,relation-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.5217391304347826,22,0.0973451327433628,12,0.5217391304347826,1,0,Introduction
24,"For instance , dependencies between words are not enough to predict that source and U.S. have an ORG - AFF relation in the sentence "" This is ... "" , one U.S. source said , and the context word said is required for this prediction .",Introduction,Introduction,relation-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.5652173913043478,23,0.1017699115044247,13,0.5652173913043478,1,0,Introduction
25,"Many traditional , feature - based relation classification models extract features from both sequences and parse trees .",Introduction,Introduction,relation-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.6086956521739131,24,0.1061946902654867,14,0.6086956521739131,1,0,Introduction
26,"However , previous RNNbased models focus on only one of these linguistic structures .",Introduction,Introduction,relation-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.6521739130434783,25,0.1106194690265486,15,0.6521739130434783,1,0,Introduction
35,Related Work,,,relation-classification,0,"['O', 'O']","['O', 'O']",0,0.0,34,0.1504424778761062,0,0.0,1,0,
49,Model,,,relation-classification,0,['O'],['O'],0,0.0,48,0.2123893805309734,0,0.0,1,0,
56,Embedding Layer,Model,,relation-classification,0,"['O', 'O']","['O', 'O']",7,0.0921052631578947,55,0.243362831858407,0,0.0,1,0,Model
58,"n w , n p , n d and n e - dimensional vectors v , v ( p ) , v and v ( e ) are embedded to words , part - of - speech ( POS ) tags , dependency types , and entity labels , respectively .",Model,Embedding Layer,relation-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.1184210526315789,57,0.252212389380531,2,1.0,1,0,Model: Embedding Layer
59,Sequence Layer,Model,,relation-classification,0,"['O', 'O']","['O', 'O']",10,0.131578947368421,58,0.2566371681415929,0,0.0,1,0,Model
61,"This layer represents sentential context information and maintains entities , as shown in bottom - left part of .",Model,Sequence Layer,relation-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.1578947368421052,60,0.2654867256637168,2,0.1538461538461538,1,0,Model: Sequence Layer
63,"The LSTM unit at t-th word consists of a collection of n ls - dimensional vectors : an input gate it , a forget gate ft , an output gate o t , a memory cell ct , and a hidden state ht .",Model,Sequence Layer,relation-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.1842105263157894,62,0.2743362831858407,4,0.3076923076923077,1,0,Model: Sequence Layer
64,"The unit receives an n-dimensional input vector x t , the previous hidden state h t?1 , and the memory cell c t?1 , and calculates the new vectors using the following equations :",Model,Sequence Layer,relation-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.1973684210526315,63,0.2787610619469026,5,0.3846153846153846,1,0,Model: Sequence Layer
65,( 1 ),Model,Sequence Layer,relation-classification,0,"['O', 'O', 'O']","['O', 'O', 'O']",16,0.2105263157894736,64,0.2831858407079646,6,0.4615384615384615,1,0,Model: Sequence Layer
66,where ?,Model,Sequence Layer,relation-classification,0,"['O', 'O']","['O', 'O']",17,0.2236842105263158,65,0.2876106194690265,7,0.5384615384615384,1,0,Model: Sequence Layer
67,"denotes the logistic function , denotes element - wise multiplication , W and U are weight matrices , and bare bias vectors .",Model,Sequence Layer,relation-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.2368421052631578,66,0.2920353982300885,8,0.6153846153846154,1,0,Model: Sequence Layer
68,The LSTM unit at t-th word receives the concatenation of word and POS embeddings as its input vector :,Model,Sequence Layer,relation-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.25,67,0.2964601769911504,9,0.6923076923076923,1,0,Model: Sequence Layer
69,.,Model,Sequence Layer,relation-classification,0,['O'],['O'],20,0.2631578947368421,68,0.3008849557522124,10,0.7692307692307693,1,0,Model: Sequence Layer
70,We also concatenate the hidden state vectors of the two directions ' LSTM units corresponding to each word ( denoted as ? ?,Model,Sequence Layer,relation-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.2763157894736842,69,0.3053097345132743,11,0.8461538461538461,1,0,Model: Sequence Layer
71,"ht and ? ? ht ) as its output vector , st = ? ? ht ; ? ?",Model,Sequence Layer,relation-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.2894736842105263,70,0.3097345132743362,12,0.9230769230769232,1,0,Model: Sequence Layer
72,"ht , and pass it to the subsequent layers .",Model,Sequence Layer,relation-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",23,0.3026315789473684,71,0.3141592920353982,13,1.0,1,0,Model: Sequence Layer
73,Entity Detection,Model,,relation-classification,0,"['O', 'O']","['O', 'O']",24,0.3157894736842105,72,0.3185840707964602,0,0.0,1,0,Model
75,"We assign an entity tag to each word using a commonly used encoding scheme BILOU ( Begin , Inside , Last , Outside , Unit ) ( Ratinov and , where each entity tag represents the entity type and the position of a word in the entity .",Model,Entity Detection,relation-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.3421052631578947,74,0.3274336283185841,2,0.2222222222222222,1,0,Model: Entity Detection
76,"For example , in , we assign B - PER and L - PER ( which denote the beginning and last words of a person entity type , respectively ) to each word in Sidney Yates to represent this phrase as a PER ( person ) entity type .",Model,Entity Detection,relation-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.3552631578947368,75,0.331858407079646,3,0.3333333333333333,1,0,Model: Entity Detection
79,"Here , Ware weight matrices and bare bias vectors .",Model,Entity Detection,relation-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",30,0.3947368421052631,78,0.3451327433628318,6,0.6666666666666666,1,0,Model: Entity Detection
82,The NN above receives the concatenation of its corresponding outputs in the sequence layer and the label embedding for its previous word ) .,Model,Entity Detection,relation-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",33,0.4342105263157895,81,0.3584070796460177,9,1.0,1,0,Model: Entity Detection
83,Dependency Layer,Model,,relation-classification,0,"['O', 'O']","['O', 'O']",34,0.4473684210526316,82,0.3628318584070796,0,0.0,1,0,Model
84,"The dependency layer represents a relation between a pair of two target words ( corresponding to a relation candidate in relation classification ) in the dependency tree , and is in charge of relationspecific representations , as is shown in top - right part of .",Model,Dependency Layer,relation-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",35,0.4605263157894737,83,0.3672566371681416,1,0.0526315789473684,1,0,Model: Dependency Layer
86,"For example , we show the shortest path between Yates and Chicago in the bottom of , and this path well captures the key phrase of their relation , i.e. , born in .",Model,Dependency Layer,relation-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",37,0.4868421052631579,85,0.3761061946902654,3,0.1578947368421052,1,0,Model: Dependency Layer
90,2 Note that the two variants of tree - structured LSTM - RNNs by are notable to represent our target structures which have a variable number of typed children : the Child - Sum Tree - LSTM does not deal with types and the N - ary Tree assumes a fixed number of children .,Model,Dependency Layer,relation-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",41,0.5394736842105263,89,0.3938053097345133,7,0.3684210526315789,1,0,Model: Dependency Layer
92,"For this variant , we calculate n lt - dimensional vectors in the LSTM unit at t-th node with C ( t ) children using following equations :",Model,Dependency Layer,relation-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",43,0.5657894736842105,91,0.4026548672566372,9,0.4736842105263157,1,0,Model: Dependency Layer
93,where m ( ) is a type mapping function .,Model,Dependency Layer,relation-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",44,0.5789473684210527,92,0.4070796460176991,10,0.5263157894736842,1,0,Model: Dependency Layer
97,SubTree is the subtree under the lowest common ancestor of the target word pair .,Model,Dependency Layer,relation-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",48,0.631578947368421,96,0.4247787610619469,14,0.7368421052631579,1,0,Model: Dependency Layer
98,This provides additional modifier information to the path and the word pair in SPTree .,Model,Dependency Layer,relation-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",49,0.6447368421052632,97,0.4292035398230088,15,0.7894736842105263,1,0,Model: Dependency Layer
99,FullTree is the full dependency tree .,Model,Dependency Layer,relation-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O']",50,0.6578947368421053,98,0.4336283185840708,16,0.8421052631578947,1,0,Model: Dependency Layer
101,"While we use one node type for SPTree , we define two node types for SubTree and FullTree , i.e. , one for nodes on shortest paths and one for all other nodes .",Model,Dependency Layer,relation-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",52,0.6842105263157895,100,0.4424778761061947,18,0.9473684210526316,1,0,Model: Dependency Layer
102,We use the type mapping function m ( ) to distinguish these two nodes types .,Model,Dependency Layer,relation-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",53,0.6973684210526315,101,0.4469026548672566,19,1.0,1,0,Model: Dependency Layer
105,The dependency - layer LSTM unit at the t - th word receives as input,Model,Stacking Sequence and Dependency Layers,relation-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",56,0.7368421052631579,104,0.4601769911504424,2,0.6666666666666666,1,0,Model: Stacking Sequence and Dependency Layers
106,"i.e. , the concatenation of its corresponding hidden state vectors st in the sequence layer , dependency type embedding v",Model,Stacking Sequence and Dependency Layers,relation-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",57,0.75,105,0.4646017699115044,3,1.0,1,0,Model: Stacking Sequence and Dependency Layers
107,Relation Classification,Model,,relation-classification,0,"['O', 'O']","['O', 'O']",58,0.7631578947368421,106,0.4690265486725664,0,0.0,1,0,Model
109,"For instance , in , we build a relation candidate using Yates with an L - PER label and Chicago with an U - LOC label .",Model,Relation Classification,relation-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",60,0.7894736842105263,108,0.4778761061946903,2,0.1111111111111111,1,0,Model: Relation Classification
110,"For each relation candidate , we realize the dependency layer d p ( described above ) corresponding to the path between the word pair pin the relation candidate , and the NN receives a relation candidate vector constructed from the output of the dependency tree layer , and predicts its relation label .",Model,Relation Classification,relation-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",61,0.8026315789473685,109,0.4823008849557522,3,0.1666666666666666,1,0,Model: Relation Classification
111,We treat a pair as a negative relation when the detected entities are wrong or when the pair has no relation .,Model,Relation Classification,relation-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",62,0.8157894736842105,110,0.4867256637168141,4,0.2222222222222222,1,0,Model: Relation Classification
112,"We represent relation labels by type and direction , except for negative relations that have no direction .",Model,Relation Classification,relation-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",63,0.8289473684210527,111,0.4911504424778761,5,0.2777777777777778,1,0,Model: Relation Classification
113,"The relation candidate vector is constructed as the concatenation d p = [?h p A ; ?h p 1 ; ?h p 2 ] , where ?h p",Model,Relation Classification,relation-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",64,0.8421052631578947,112,0.495575221238938,6,0.3333333333333333,1,0,Model: Relation Classification
114,A is the hidden state vector of the top LSTM,Model,Relation Classification,relation-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",65,0.8552631578947368,113,0.5,7,0.3888888888888889,1,0,Model: Relation Classification
115,We use the dependency to the parent since the number of children varies .,Model,Relation Classification,relation-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",66,0.868421052631579,114,0.504424778761062,8,0.4444444444444444,1,0,Model: Relation Classification
116,"Dependency types can also be incorporated into m ( ) , but this did not help in initial experiments .",Model,Relation Classification,relation-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",67,0.881578947368421,115,0.5088495575221239,9,0.5,1,0,Model: Relation Classification
117,"unit in the bottom - up LSTM - RNN ( representing the lowest common ancestor of the target word pair p ) , and ?h p 1 , ?h p 2 are the hidden state vectors of the two LSTM units representing the first and second target words in the top - down LSTM - RNN .",Model,Relation Classification,relation-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",68,0.8947368421052632,116,0.5132743362831859,10,0.5555555555555556,1,0,Model: Relation Classification
118,All the corresponding arrows are shown in .,Model,Relation Classification,relation-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",69,0.9078947368421052,117,0.5176991150442478,11,0.6111111111111112,1,0,Model: Relation Classification
120,"We construct the input d p for relation classification from tree - structured LSTM - RNNs stacked on sequential LSTM - RNNs , so the contribution of sequence layer to the input is indirect .",Model,Relation Classification,relation-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",71,0.9342105263157896,119,0.5265486725663717,13,0.7222222222222222,1,0,Model: Relation Classification
121,"Furthermore , our model uses words for representing entities , so it can not fully use the entity information .",Model,Relation Classification,relation-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",72,0.9473684210526316,120,0.5309734513274337,14,0.7777777777777778,1,0,Model: Relation Classification
122,"To alleviate these problems , we directly concatenate the average of hidden state vectors for each entity from the sequence layer to the input d p to relation classification , i.e. , d p =",Model,Relation Classification,relation-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",73,0.9605263157894736,121,0.5353982300884956,15,0.8333333333333334,1,0,Model: Relation Classification
123,", where I p 1 and I p 2 represent sets of word indices in the first and second entities .",Model,Relation Classification,relation-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",74,0.9736842105263158,122,0.5398230088495575,16,0.8888888888888888,1,0,Model: Relation Classification
124,"Also , we assign two labels to each word pair in prediction since we consider both left - to - right and right - to - left directions .",Model,Relation Classification,relation-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",75,0.986842105263158,123,0.5442477876106194,17,0.9444444444444444,1,0,Model: Relation Classification
125,"When the predicted labels are inconsistent , we select the positive and more confident label , similar to .",Model,Relation Classification,relation-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",76,1.0,124,0.5486725663716814,18,1.0,1,0,Model: Relation Classification
126,Training,,,relation-classification,0,['O'],['O'],0,0.0,125,0.5530973451327433,0,0.0,1,0,
130,"In scheduled sampling , we use gold labels as prediction in the probability of i that depends on the number of epochs i during training if the gold labels are legal .",Training,Training,relation-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.5714285714285714,129,0.5707964601769911,4,0.5714285714285714,1,0,Training
131,"As for i , we choose the inverse sigmoid decay i = k / ( k + exp ( i / k ) ) , where k( ?",Training,Training,relation-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.7142857142857143,130,0.5752212389380531,5,0.7142857142857143,1,0,Training
132,1 ) is a hyper - parameter that adjusts how often we use the gold labels as prediction .,Training,Training,relation-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.8571428571428571,131,0.5796460176991151,6,0.8571428571428571,1,0,Training
134,Results and Discussion,,,relation-classification,0,"['O', 'O', 'O']","['O', 'O', 'O']",0,0.0,133,0.588495575221239,0,0.0,1,0,
135,Data and Task Settings,Results and Discussion,,relation-classification,0,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",1,0.0555555555555555,134,0.5929203539823009,0,0.0,1,0,Results and Discussion
141,We treat an entity as correct when it s type and the region of its head are correct .,Results and Discussion,Data and Task Settings,relation-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.3888888888888889,140,0.6194690265486725,6,0.3529411764705882,1,0,Results and Discussion: Data and Task Settings
142,We treat a relation as correct when it s type and argument entities are correct ; we thus treat all non-negative relations on wrong entities as false positives .,Results and Discussion,Data and Task Settings,relation-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.4444444444444444,141,0.6238938053097345,7,0.4117647058823529,1,0,Results and Discussion: Data and Task Settings
143,"ACE04 defines the same 7 coarse - grained entity types as ACE05 , but defines 7 coarse - grained relation types .",Results and Discussion,Data and Task Settings,relation-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.5,142,0.6283185840707964,8,0.4705882352941176,1,0,Results and Discussion: Data and Task Settings
147,Other when two nouns have none of these relations .,Results and Discussion,SemEval-2010,relation-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.7222222222222222,146,0.6460176991150443,12,0.7058823529411765,1,0,Results and Discussion: SemEval-2010
148,"We treat this Other type as a negative relation type , and no direction is considered .",Results and Discussion,SemEval-2010,relation-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.7777777777777778,147,0.6504424778761062,13,0.7647058823529411,1,0,Results and Discussion: SemEval-2010
153,Experimental Settings,,,relation-classification,0,"['O', 'O']","['O', 'O']",0,0.0,152,0.672566371681416,0,0.0,1,0,
159,"9 For ACE04 , we directly employed the best parameters for ACE05 .",Experimental Settings,Experimental Settings,relation-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.1428571428571428,158,0.6991150442477876,6,0.1428571428571428,1,0,Experimental Settings
161,For SemEval-2010,Experimental Settings,,relation-classification,0,"['O', 'O']","['O', 'O']",8,0.1904761904761904,160,0.7079646017699115,8,0.1904761904761904,1,0,Experimental Settings
162,"Task 8 , we also omitted the entity detection and label embeddings since only target nominals are annotated and the task defines no entity types .",Experimental Settings,For SemEval-2010,relation-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.2142857142857142,161,0.7123893805309734,9,0.2142857142857142,1,0,Experimental Settings: For SemEval-2010
168,"This is reasonable because the model can only create relation instances when both of the entities are found and , without these enhancements , it may get too late to find some relations .",Experimental Settings,For SemEval-2010,relation-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.3571428571428571,167,0.7389380530973452,15,0.3571428571428571,1,0,Experimental Settings: For SemEval-2010
170,7 http://nlp.stanford.edu/software/,Experimental Settings,For SemEval-2010,relation-classification,0,"['O', 'O']","['O', 'O']",17,0.4047619047619047,169,0.7477876106194691,17,0.4047619047619047,1,0,Experimental Settings: For SemEval-2010
171,stanford-corenlp-full-2015-04-20.zip,Experimental Settings,For SemEval-2010,relation-classification,0,['O'],['O'],18,0.4285714285714285,170,0.7522123893805309,18,0.4285714285714285,1,0,Experimental Settings: For SemEval-2010
173,9,Experimental Settings,For SemEval-2010,relation-classification,0,['O'],['O'],20,0.4761904761904761,172,0.7610619469026548,20,0.4761904761904761,1,0,Experimental Settings: For SemEval-2010
175,"Other work on ACE is not comparable or performs worse than the model by the entity detection performance , but this degraded the recall in relation classification .",Experimental Settings,For SemEval-2010,relation-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.5238095238095238,174,0.7699115044247787,22,0.5238095238095238,1,0,Experimental Settings: For SemEval-2010
178,This setting can be regarded as a pipeline model since two separate models are trained sequentially .,Experimental Settings,For SemEval-2010,relation-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.5952380952380952,177,0.7831858407079646,25,0.5952380952380952,1,0,Experimental Settings: For SemEval-2010
186,This maybe because the difference in the models appears only on nodes that have multiple children and all the nodes except for the least common node have one child .,Experimental Settings,For SemEval-2010,relation-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",33,0.7857142857142857,185,0.8185840707964602,33,0.7857142857142857,1,0,Experimental Settings: For SemEval-2010
191,SPX u is our adaptation of the shortest path LSTM - RNN proposed by to match our sequence - layer based model .,Experimental Settings,For SemEval-2010,relation-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",38,0.9047619047619048,190,0.8407079646017699,38,0.9047619047619048,1,0,Experimental Settings: For SemEval-2010
192,11 This has two LSTM - RNNs for the left and right subpaths of the shortest path .,Experimental Settings,For SemEval-2010,relation-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",39,0.9285714285714286,191,0.8451327433628318,39,0.9285714285714286,1,0,Experimental Settings: For SemEval-2010
196,Relation Classification Analysis Results,,,relation-classification,0,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",0,0.0,195,0.8628318584070797,0,0.0,1,0,
198,"This dataset , often used to evaluate NN models for relation classification , annotates only relation - related nominals ( unlike ACE datasets ) , so we can focus cleanly on the relation classification part .",Relation Classification Analysis Results,Relation Classification Analysis Results,relation-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.0869565217391304,197,0.8716814159292036,2,1.0,1,0,Relation Classification Analysis Results
199,Settings,Relation Classification Analysis Results,,relation-classification,0,['O'],['O'],3,0.1304347826086956,198,0.8761061946902655,0,0.0,1,0,Relation Classification Analysis Results
204,"As for the three input dependency structures ( SPTree , SubTree , FullTree ) , Full",Relation Classification Analysis Results,Settings,relation-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.3478260869565217,203,0.8982300884955752,5,0.25,1,0,Relation Classification Analysis Results: Settings
209,"0.848 produces different results on FullTree as compared to the results on ACE05 in , the trend still holds that selecting the appropriate tree structure representation of the input is more important than the choice of the LSTM - RNN structure on that input .",Relation Classification Analysis Results,Settings,relation-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.5652173913043478,208,0.9203539823008848,10,0.5,1,0,Relation Classification Analysis Results: Settings
214,This indicates that the sequence layer is necessary but the last words of nominals are almost enough for expressing the relations in this task .,Relation Classification Analysis Results,Settings,relation-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.782608695652174,213,0.9424778761061948,15,0.75,1,0,Relation Classification Analysis Results: Settings
216,This indicates that the selection of parsing models is not critical .,Relation Classification Analysis Results,Settings,relation-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.8695652173913043,215,0.9513274336283186,17,0.85,1,0,Relation Classification Analysis Results: Settings
218,"Lastly , for the generation of relation candidates , generating only leftto - right candidates slightly degraded the perfor- mance , but the difference was small and hence the creation of right - to - left candidates was not critical .",Relation Classification Analysis Results,Settings,relation-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.9565217391304348,217,0.9601769911504424,19,0.95,1,0,Relation Classification Analysis Results: Settings
220,Conclusion,,,relation-classification,0,['O'],['O'],0,0.0,219,0.9690265486725664,0,0.0,1,0,
3,abstract,,,relation-classification,1,['O'],['O'],0,0.0,2,0.008130081300813,0,0.0,1,0,
11,"Different from open information extraction ( Open IE ) ) whose relation words are extracted from the given sentence , in this task , relation words are extracted from a predefined relation set which may not appear in the given sentence .",Introduction,Introduction,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.05,10,0.040650406504065,2,0.05,1,0,Introduction
12,It is an important issue in knowledge extraction and automatic construction of knowledge base .,Introduction,Introduction,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.075,11,0.0447154471544715,3,0.075,1,0,Introduction
15,But it neglects the relevance between these two sub - tasks and each subtask is an independent model .,Introduction,Introduction,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.15,14,0.056910569105691,6,0.15,1,0,Introduction
16,The results of entity recognition may affect the performance of relation classification and lead to erroneous delivery .,Introduction,Introduction,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.175,15,0.0609756097560975,7,0.175,1,0,Introduction
19,"However , most existing joint methods are feature - based structured systems .",Introduction,Introduction,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.25,18,0.073170731707317,10,0.25,1,0,Introduction
20,"They need complicated feature engineering and heavily rely on the other NLP toolkits , which might also lead to error propagation .",Introduction,Introduction,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.275,19,0.0772357723577235,11,0.275,1,0,Introduction
23,"For instance , the sentence in contains three entities : "" United States "" , "" Trump "" and "" Apple Inc "" .",Introduction,Introduction,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.35,22,0.089430894308943,14,0.35,1,0,Introduction
24,"But only "" United States "" and "" Trump "" hold a fix relation "" Country - President "" .",Introduction,Introduction,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.375,23,0.0934959349593495,15,0.375,1,0,Introduction
25,"Entity "" Apple Inc "" has no obvious relationship with the other entities in this sentence .",Introduction,Introduction,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.4,24,0.0975609756097561,16,0.4,1,0,Introduction
26,"Hence , the extracted result from this sentence is { United States e 1 , Country - President r , Trump e 2 } , which called triplet here .",Introduction,Introduction,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.425,25,0.1016260162601626,17,0.425,1,0,Introduction
33,"Recently , end - to - end models based on LSTM have been successfully applied to various tagging tasks :",Introduction,Introduction,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.6,32,0.1300813008130081,24,0.6,1,0,Introduction
34,"Named Entity Recognition , CCG",Introduction,Introduction,relation-classification,1,"['O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O']",25,0.625,33,0.1341463414634146,25,0.625,1,0,Introduction
40,"In reality , however , the process of manually labeling a training set with a large number of entity and relation is too expensive and error-prone .",Introduction,Introduction,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",31,0.775,39,0.1585365853658536,31,0.775,1,0,Introduction
50,Related Works,,,relation-classification,1,"['O', 'O']","['O', 'O']",0,0.0,49,0.1991869918699187,0,0.0,1,0,
66,Method,,,relation-classification,1,['O'],['O'],0,0.0,65,0.2642276422764227,0,0.0,1,0,
70,is an example of how the results are tagged .,Method,Method,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.0579710144927536,69,0.2804878048780488,4,0.2857142857142857,1,0,Method
72,"Tag "" O "" represents the "" Other "" tag , which means that the corresponding word is independent of the extracted results .",Method,Method,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.0869565217391304,71,0.2886178861788618,6,0.4285714285714285,1,0,Method
73,"In addition to "" O "" , the other tags consist of three parts : the word position in the entity , the relation type , and the relation role .",Method,Method,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.1014492753623188,72,0.2926829268292683,7,0.5,1,0,Method
74,"We use the "" BIES "" ( Begin , Inside , End , Single ) signs to represent the position information of a word in the entity .",Method,Method,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.1159420289855072,73,0.2967479674796748,8,0.5714285714285714,1,0,Method
75,"The relation type information is obtained from a predefined set of relations and the relation role information is represented by the numbers "" 1 "" and "" 2 "" .",Method,Method,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.1304347826086956,74,0.3008130081300813,9,0.6428571428571429,1,0,Method
76,"An extracted result is represented by a triplet : ( Entity 1 , Relation T ype , Entity 2 ) .",Method,Method,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.144927536231884,75,0.3048780487804878,10,0.7142857142857143,1,0,Method
77,""" 1 "" means that the word belongs to the first entity in the triplet , while "" 2 "" belongs to second entity that behind the relation type .",Method,Method,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.1594202898550724,76,0.3089430894308943,11,0.7857142857142857,1,0,Method
78,"Thus , the total number of tags is N t = 2 * 4 * | R | + 1 , where | R | is the size of the predefined relation set .",Method,Method,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.1739130434782608,77,0.3130081300813008,12,0.8571428571428571,1,0,Method
79,is an example illustrating our tagging method .,Method,Method,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.1884057971014492,78,0.3170731707317073,13,0.9285714285714286,1,0,Method
80,"The input sentence contains two triplets : { United States , Country - President , Trump } and { Apple Inc , Company - Founder , Steven Paul Jobs } , where "" Country - President "" and "" Company - Founder "" are the predefined relation types .",Method,Method,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.2028985507246377,79,0.3211382113821138,14,1.0,1,0,Method
82,"The words "" United "" , "" States "" , "" Trump "" , "" Apple "" , "" Inc "" , "" Steven "" , "" Paul "" and "" Jobs "" are all related to the final extracted results .",Method,The Tagging Scheme,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.2318840579710145,81,0.3292682926829268,1,0.2,1,0,Method: The Tagging Scheme
83,Thus they are tagged based on our special tags .,Method,The Tagging Scheme,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.2463768115942029,82,0.3333333333333333,2,0.4,1,0,Method: The Tagging Scheme
84,"For example , the word of "" United "" is the first word of entity "" United States "" and is related to the relation "" Country - President "" , so its tag is "" B - CP - 1 "" .",Method,The Tagging Scheme,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.2608695652173913,83,0.3373983739837398,3,0.6,1,0,Method: The Tagging Scheme
85,"The other entity "" Trump "" , which is corresponding to "" United States "" , is labeled as "" S - CP - 2 "" .",Method,The Tagging Scheme,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.2753623188405797,84,0.3414634146341463,4,0.8,1,0,Method: The Tagging Scheme
86,"Besides , the other words irrelevant to the final result are labeled as "" O "" .",Method,The Tagging Scheme,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.2898550724637681,85,0.3455284552845528,5,1.0,1,0,Method: The Tagging Scheme
87,From Tag Sequence To Extracted Results,Method,,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O']",21,0.3043478260869565,86,0.3495934959349593,0,0.0,1,0,Method
88,"From the tag sequence in , we know that "" Trump "" and "" United States "" share the same relation type "" Country - President "" , "" Apple Inc "" and "" Steven Paul Jobs "" share the same relation type "" Company - Founder "" .",Method,From Tag Sequence To Extracted Results,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.3188405797101449,87,0.3536585365853658,1,0.0208333333333333,1,0,Method: From Tag Sequence To Extracted Results
90,"Accordingly , "" Trump "" and "" United States "" can be combined into a triplet whose relation type is "" Country - President "" .",Method,From Tag Sequence To Extracted Results,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.3478260869565217,89,0.3617886178861789,3,0.0625,1,0,Method: From Tag Sequence To Extracted Results
91,"Because , the relation role of "" Trump "" is "" 2 "" and "" United States "" is "" 1 "" , the final result is { United States , Country - President , Trump } .",Method,From Tag Sequence To Extracted Results,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.3623188405797101,90,0.3658536585365853,4,0.0833333333333333,1,0,Method: From Tag Sequence To Extracted Results
92,"The same applies to { Apple Inc , Company - Founder , Steven Paul Jobs } .",Method,From Tag Sequence To Extracted Results,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.3768115942028985,91,0.3699186991869919,5,0.1041666666666666,1,0,Method: From Tag Sequence To Extracted Results
94,"For example , if the relation type "" Country - President "" in is "" Company - Founder "" , then there will be four entities in the given sentence with the same relation type .",Method,From Tag Sequence To Extracted Results,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",28,0.4057971014492754,93,0.3780487804878049,7,0.1458333333333333,1,0,Method: From Tag Sequence To Extracted Results
95,""" United States "" is closest to entity "" Trump "" and the "" Apple Inc "" is closest to "" Jobs "" , so the results will be { United States , Company - Founder , Trump } and { Apple Inc , Company - Founder , Steven Paul Jobs } .",Method,From Tag Sequence To Extracted Results,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",29,0.4202898550724637,94,0.3821138211382114,8,0.1666666666666666,1,0,Method: From Tag Sequence To Extracted Results
96,"In this paper , we only consider the situation where an entity belongs to a triplet , and we leave identification of overlapping relations for future work .",Method,From Tag Sequence To Extracted Results,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",30,0.4347826086956521,95,0.3861788617886179,9,0.1875,1,0,Method: From Tag Sequence To Extracted Results
97,The End - to - end Model,Method,From Tag Sequence To Extracted Results,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O']",31,0.4492753623188406,96,0.3902439024390244,10,0.2083333333333333,1,0,Method: From Tag Sequence To Extracted Results
98,"In recent years , end - to - end model based on neural network is been widely used in sequence tagging task .",Method,From Tag Sequence To Extracted Results,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",32,0.463768115942029,97,0.3943089430894309,11,0.2291666666666666,1,0,Method: From Tag Sequence To Extracted Results
106,"Hence , a sequence of words can be represented as : Gold standard annotation for an example sentence based on our tagging scheme , where "" CP "" is short for "" Country - President "" and "" CF "" is short for "" Company - Founder "" .",Method,From Tag Sequence To Extracted Results,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",40,0.5797101449275363,105,0.4268292682926829,19,0.3958333333333333,1,0,Method: From Tag Sequence To Extracted Results
107,corresponding to the t - th word in the sentence and n is the length of the given sentence .,Method,From Tag Sequence To Extracted Results,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",41,0.5942028985507246,106,0.4308943089430894,20,0.4166666666666667,1,0,Method: From Tag Sequence To Extracted Results
109,"The LSTM architecture consists of a set of recurrently connected subnets , known as memory blocks .",Method,From Tag Sequence To Extracted Results,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",43,0.6231884057971014,108,0.4390243902439024,22,0.4583333333333333,1,0,Method: From Tag Sequence To Extracted Results
110,Each time - step is a LSTM memory block .,Method,From Tag Sequence To Extracted Results,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",44,0.6376811594202898,109,0.4430894308943089,23,0.4791666666666667,1,0,Method: From Tag Sequence To Extracted Results
111,"The LSTM memory block in Bi - LSTM encoding layer is used to compute current hidden vector ht based on the previous hidden vector h t?1 , the previous cell vector c t?1 and the current input word embedding wt .",Method,From Tag Sequence To Extracted Results,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",45,0.6521739130434783,110,0.4471544715447154,24,0.5,1,0,Method: From Tag Sequence To Extracted Results
112,"It s structure diagram is shown in , and detail operations are defined as follows :",Method,From Tag Sequence To Extracted Results,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",46,0.6666666666666666,111,0.4512195121951219,25,0.5208333333333334,1,0,Method: From Tag Sequence To Extracted Results
113,"where i , f and o are the input gate , forget gate and output gate respectively , b is the bias term , c is the cell memory , and W ( . ) are the parameters .",Method,From Tag Sequence To Extracted Results,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",47,0.6811594202898551,112,0.4552845528455284,26,0.5416666666666666,1,0,Method: From Tag Sequence To Extracted Results
114,"For each word wt , the forward LSTM layer will encode wt by considering the contextual information from word w 1 tow t , which is marked as ? ? ht .",Method,From Tag Sequence To Extracted Results,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",48,0.6956521739130435,113,0.4593495934959349,27,0.5625,1,0,Method: From Tag Sequence To Extracted Results
115,"In the similar way , the backward LSTM layer will encode wt based on the contextual information from w n tow t , which is marked as ? ? ht .",Method,From Tag Sequence To Extracted Results,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",49,0.7101449275362319,114,0.4634146341463415,28,0.5833333333333334,1,0,Method: From Tag Sequence To Extracted Results
116,"Finally , we concatenate ? ?",Method,From Tag Sequence To Extracted Results,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O']",50,0.7246376811594203,115,0.4674796747967479,29,0.6041666666666666,1,0,Method: From Tag Sequence To Extracted Results
117,ht and ? ?,Method,From Tag Sequence To Extracted Results,relation-classification,1,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",51,0.7391304347826086,116,0.4715447154471545,30,0.625,1,0,Method: From Tag Sequence To Extracted Results
118,"ht to represent word t 's encoding information , denoted as",Method,From Tag Sequence To Extracted Results,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",52,0.7536231884057971,117,0.4756097560975609,31,0.6458333333333334,1,0,Method: From Tag Sequence To Extracted Results
121,"When detecting the tag of word wt , the inputs of decoding layer are : ht obtained from Bi - LSTM encoding layer , former predicted tag embedding T t?1 , former cell value c ( 2 ) t? 1 , and the former hidden vector in decoding layer h ( 2 ) t?1 .",Method,From Tag Sequence To Extracted Results,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",55,0.7971014492753623,120,0.4878048780487805,34,0.7083333333333334,1,0,Method: From Tag Sequence To Extracted Results
122,"The structure diagram of the memory block in LSTM dis shown in , and detail operations are defined as follows :",Method,From Tag Sequence To Extracted Results,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",56,0.8115942028985508,121,0.491869918699187,35,0.7291666666666666,1,0,Method: From Tag Sequence To Extracted Results
123,The final softmax layer computes normalized entity tag probabilities based on the tag predicted vector T t :,Method,From Tag Sequence To Extracted Results,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",57,0.8260869565217391,122,0.4959349593495935,36,0.75,1,0,Method: From Tag Sequence To Extracted Results
124,"where W y is the softmax matrix , N t is the total number of tags .",Method,From Tag Sequence To Extracted Results,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",58,0.8405797101449275,123,0.5,37,0.7708333333333334,1,0,Method: From Tag Sequence To Extracted Results
126,The Bias Objective Function .,Method,,relation-classification,1,"['O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O']",60,0.8695652173913043,125,0.508130081300813,39,0.8125,1,0,Method
128,The objective function can be defined as :,Method,The Bias Objective Function .,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",62,0.8985507246376812,127,0.516260162601626,41,0.8541666666666666,1,0,Method: The Bias Objective Function .
129,"where | D| is the size of training set , L j is the length of sentence x j , y ( j ) t is the label of word tin sentence x j and p ( j ) t is the normalized probabilities of tags which defined in Formula 15 .",Method,The Bias Objective Function .,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",63,0.9130434782608696,128,0.5203252032520326,42,0.875,1,0,Method: The Bias Objective Function .
130,"Besides , I ( O ) is a switching function to distinguish the loss of tag ' O ' and relational tags that can indicate the results .",Method,The Bias Objective Function .,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",64,0.927536231884058,129,0.524390243902439,43,0.8958333333333334,1,0,Method: The Bias Objective Function .
131,It is defined as follows :,Method,The Bias Objective Function .,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O']",65,0.9420289855072465,130,0.5284552845528455,44,0.9166666666666666,1,0,Method: The Bias Objective Function .
132,?,Method,The Bias Objective Function .,relation-classification,1,['O'],['O'],66,0.9565217391304348,131,0.532520325203252,45,0.9375,1,0,Method: The Bias Objective Function .
133,is the bias weight .,Method,The Bias Objective Function .,relation-classification,1,"['O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O']",67,0.9710144927536232,132,0.5365853658536586,46,0.9583333333333334,1,0,Method: The Bias Objective Function .
134,The larger ?,Method,The Bias Objective Function .,relation-classification,1,"['O', 'O', 'O']","['O', 'O', 'O']",68,0.9855072463768116,133,0.540650406504065,47,0.9791666666666666,1,0,Method: The Bias Objective Function .
135,"is , the greater influence of relational tags on the model .",Method,The Bias Objective Function .,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",69,1.0,134,0.5447154471544715,48,1.0,1,0,Method: The Bias Objective Function .
136,Experiments,,,relation-classification,1,['O'],['O'],0,0.0,135,0.5487804878048781,0,0.0,1,0,
137,Experimental setting,,,relation-classification,1,"['O', 'O']","['O', 'O']",0,0.0,136,0.5528455284552846,0,0.0,1,0,
138,Dataset,Experimental setting,,relation-classification,1,['O'],['O'],1,0.1666666666666666,137,0.556910569105691,1,0.0238095238095238,1,0,Experimental setting
140,A large amount of training data can be obtained by means of distant supervision methods without manually labeling .,Experimental setting,Dataset,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.5,139,0.5650406504065041,3,0.0714285714285714,1,0,Experimental setting: Dataset
141,While the test set is manually labeled to ensure its quality .,Experimental setting,Dataset,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.6666666666666666,140,0.5691056910569106,4,0.0952380952380952,1,0,Experimental setting: Dataset
143,"Besides , the size of relation set is 24 .",Experimental setting,Dataset,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,1.0,142,0.5772357723577236,6,0.1428571428571428,1,0,Experimental setting: Dataset
144,Evaluation,,,relation-classification,1,['O'],['O'],0,0.0,143,0.5813008130081301,7,0.1666666666666666,1,0,
147,"In other words , we did not use the label of entity types to train the model , therefore we do not need to consider the entity types in the evaluation .",Evaluation,Evaluation,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.4285714285714285,146,0.5934959349593496,10,0.238095238095238,1,0,Evaluation
148,A triplet is regarded as correct when its relation type and the head offsets of two corresponding entities are both correct .,Evaluation,Evaluation,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.5714285714285714,147,0.5975609756097561,11,0.2619047619047619,1,0,Evaluation
149,"Besides , the groundtruth relation mentions are given and "" None "" label is excluded as did .",Evaluation,Evaluation,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.7142857142857143,148,0.6016260162601627,12,0.2857142857142857,1,0,Evaluation
150,We create a validation set by randomly sampling 10 % data from test set and use the remaining data as evaluation based on ) 's suggestion .,Evaluation,Evaluation,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.8571428571428571,149,0.6056910569105691,13,0.3095238095238095,1,0,Evaluation
158,The bias parameter ?,Hyperparameters,Hyperparameters,relation-classification,1,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",6,0.4,157,0.6382113821138211,21,0.5,1,0,Hyperparameters
159,corresponding to the results in is 10 .,Hyperparameters,Hyperparameters,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.4666666666666667,158,0.6422764227642277,22,0.5238095238095238,1,0,Hyperparameters
160,2,Hyperparameters,Hyperparameters,relation-classification,1,['O'],['O'],8,0.5333333333333333,159,0.6463414634146342,23,0.5476190476190477,1,0,Hyperparameters
162,There are three data sets in the public resource and we only use the NYT dataset .,Hyperparameters,Hyperparameters,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.6666666666666666,161,0.6544715447154471,25,0.5952380952380952,1,0,Hyperparameters
163,Because more than 50 % of the data in BioInfer has overlapping relations which is beyond the scope of this paper .,Hyperparameters,Hyperparameters,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.7333333333333333,162,0.6585365853658537,26,0.6190476190476191,1,0,Hyperparameters
164,"As for dataset Wiki - KBP , the number of relation type in the test set is more than that of the train set , which is also not suitable fora supervised training method .",Hyperparameters,Hyperparameters,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.8,163,0.6626016260162602,27,0.6428571428571429,1,0,Hyperparameters
165,Details of the data can be found in is the pipelined methods and the second part ( row 4 to 6 ) is the jointly extracting methods .,Hyperparameters,Hyperparameters,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.8666666666666667,164,0.6666666666666666,28,0.6666666666666666,1,0,Hyperparameters
168,Baselines,,,relation-classification,1,['O'],['O'],0,0.0,167,0.6788617886178862,31,0.7380952380952381,1,0,
172,These methods are :,Baselines,Baselines,relation-classification,1,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",4,0.3636363636363636,171,0.6951219512195121,35,0.8333333333333334,1,0,Baselines
180,Experimental Results,,,relation-classification,1,"['O', 'O']","['O', 'O']",0,0.0,179,0.7276422764227642,0,0.0,1,0,
188,The reason maybe that these end - to - end models all use a Bi - LSTM encoding input sentence and different neural networks to decode the results .,Experimental Results,Experimental Results,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.135593220338983,187,0.7601626016260162,8,0.4705882352941176,1,0,Experimental Results
190,"Therefore , they can learn the common features of the training set well and may lead to the lower expansibility .",Experimental Results,Experimental Results,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.1694915254237288,189,0.7682926829268293,10,0.5882352941176471,1,0,Experimental Results
192,"Because , LSTM is capable of learning long - term dependencies and CRF is good at capturing the joint probability of the entire sequence of labels .",Experimental Results,Experimental Results,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.2033898305084746,191,0.7764227642276422,12,0.7058823529411765,1,0,Experimental Results
193,The related tags may have along distance from each other .,Experimental Results,Experimental Results,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.2203389830508474,192,0.7804878048780488,13,0.7647058823529411,1,0,Experimental Results
194,"Hence ,",Experimental Results,,relation-classification,1,"['O', 'O']","['O', 'O']",14,0.2372881355932203,193,0.7845528455284553,14,0.8235294117647058,1,0,Experimental Results
195,LSTM decoding manner is a little better than CRF .,Experimental Results,"Hence ,",relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.2542372881355932,194,0.7886178861788617,15,0.8823529411764706,1,0,"Experimental Results: Hence ,"
198,Analysis and Discussion,Experimental Results,,relation-classification,1,"['O', 'O', 'O']","['O', 'O', 'O']",18,0.3050847457627119,197,0.8008130081300813,0,0.0,1,0,Experimental Results
199,Error Analysis,Experimental Results,,relation-classification,1,"['O', 'O']","['O', 'O']",19,0.3220338983050847,198,0.8048780487804879,0,0.0,1,0,Experimental Results
201,has shown the predict results of the task .,Experimental Results,Error Analysis,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.3559322033898305,200,0.8130081300813008,2,0.1538461538461538,1,0,Experimental Results: Error Analysis
203,"In order to find out the factors that affect the results of end - to - end models , we analyze the performance on predicting each element in the triplet as shows .",Experimental Results,Error Analysis,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",23,0.3898305084745763,202,0.8211382113821138,4,0.3076923076923077,1,0,Experimental Results: Error Analysis
205,"If the head offset of the first entity is correct , then the instance of E1 is correct , the same to E2 .",Experimental Results,Error Analysis,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.423728813559322,204,0.8292682926829268,6,0.4615384615384615,1,0,Experimental Results: Error Analysis
206,"Regardless of relation type , if the head offsets of two corresponding entities are both correct , the instance of ( E1 , E2 ) is correct .",Experimental Results,Error Analysis,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.4406779661016949,205,0.8333333333333334,7,0.5384615384615384,1,0,Experimental Results: Error Analysis
207,"As shown in , ( E1 , E2 ) has higher precision when compared with E1 and E2 .",Experimental Results,Error Analysis,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.4576271186440678,206,0.8373983739837398,8,0.6153846153846154,1,0,Experimental Results: Error Analysis
208,But its recall result is lower than E1 and E2 .,Experimental Results,Error Analysis,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",28,0.4745762711864407,207,0.8414634146341463,9,0.6923076923076923,1,0,Experimental Results: Error Analysis
209,It means that some of the predicted entities do not form a pair .,Experimental Results,Error Analysis,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",29,0.4915254237288136,208,0.8455284552845529,10,0.7692307692307693,1,0,Experimental Results: Error Analysis
210,"They only obtain E1 and do not find its corresponding E2 , or obtain E2 and do not find its corresponding E1 .",Experimental Results,Error Analysis,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",30,0.5084745762711864,209,0.8495934959349594,11,0.8461538461538461,1,0,Experimental Results: Error Analysis
211,"Thus it leads to the prediction of more single E and less ( E1 , E2 ) pairs .",Experimental Results,Error Analysis,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",31,0.5254237288135594,210,0.8536585365853658,12,0.9230769230769232,1,0,Experimental Results: Error Analysis
213,Analysis of Biased Loss,Experimental Results,,relation-classification,1,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",33,0.559322033898305,212,0.8617886178861789,0,0.0,1,0,Experimental Results
216,The single entities refer to those who can not find their corresponding entities .,Experimental Results,Analysis of Biased Loss,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",36,0.6101694915254238,215,0.8739837398373984,3,0.6,1,0,Experimental Results: Analysis of Biased Loss
217,"shows whether it is E1 or E2 , our method can get a relatively low ratio on the single entities .",Experimental Results,Analysis of Biased Loss,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",37,0.6271186440677966,216,0.8780487804878049,4,0.8,1,0,Experimental Results: Analysis of Biased Loss
219,Single E1,Experimental Results,,relation-classification,1,"['O', 'O']","['O', 'O']",39,0.6610169491525424,218,0.8861788617886179,0,0.0,1,0,Experimental Results
221,"from 1 to 20 , and the predicted results are shown in .",Experimental Results,Single E1,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",41,0.6949152542372882,220,0.8943089430894309,2,0.3333333333333333,1,0,Experimental Results: Single E1
222,If ?,Experimental Results,Single E1,relation-classification,1,"['O', 'O']","['O', 'O']",42,0.711864406779661,221,0.8983739837398373,3,0.5,1,0,Experimental Results: Single E1
223,"is too large , it will affect the accuracy of prediction and if ?",Experimental Results,Single E1,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",43,0.7288135593220338,222,0.902439024390244,4,0.6666666666666666,1,0,Experimental Results: Single E1
224,"is too small , the recall will decline .",Experimental Results,Single E1,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",44,0.7457627118644068,223,0.9065040650406504,5,0.8333333333333334,1,0,Experimental Results: Single E1
228,"Each example contains three row , the first row is the gold standard , the second and the third rows are the extracted results of model LSTM - LSTM and LSTM - LSTM - Bias respectively .",Experimental Results,Case Study,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",48,0.8135593220338984,227,0.9227642276422764,2,0.1538461538461538,1,0,Experimental Results: Case Study
229,"S1 represents the situation that the distance between the two interrelated entities is faraway from each other , which is more difficult to detect their relationships .",Experimental Results,Case Study,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",49,0.8305084745762712,228,0.926829268292683,3,0.2307692307692307,1,0,Experimental Results: Case Study
231,"Therefore , in this example , LSTM - LSTM - Bias can extract two related entities , while LSTM - LSTM can only extract one entity of "" Florida "" and can not detect entity "" Panama City Beach "" .",Experimental Results,Case Study,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",51,0.864406779661017,230,0.934959349593496,5,0.3846153846153846,1,0,Experimental Results: Case Study
232,S2 is a negative example that shows these methods may mistakenly predict one of the entity .,Experimental Results,Case Study,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",52,0.8813559322033898,231,0.9390243902439024,6,0.4615384615384615,1,0,Experimental Results: Case Study
233,There are no indicative words between entities Nuremberg and Germany .,Experimental Results,Case Study,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",53,0.8983050847457628,232,0.943089430894309,7,0.5384615384615384,1,0,Experimental Results: Case Study
234,"Besides , the patten "" a * of * "" between Germany and M iddle Ages maybe easy to mislead the models that there exists a relation of "" Contains "" between them .",Experimental Results,Case Study,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",54,0.9152542372881356,233,0.9471544715447154,8,0.6153846153846154,1,0,Experimental Results: Case Study
235,The problem can be solved by adding some samples of this kind of expression patterns to the training data .,Experimental Results,Case Study,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",55,0.9322033898305084,234,0.951219512195122,9,0.6923076923076923,1,0,Experimental Results: Case Study
236,"S3 is a case that models can predict the entities ' head offset right , but the relational role is wrong .",Experimental Results,Case Study,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",56,0.9491525423728814,235,0.9552845528455284,10,0.7692307692307693,1,0,Experimental Results: Case Study
237,"LSTM - LSTM treats both "" Stephen A. Schwarzman "" and "" Blackstone Group "" as entity E1 , and can not find its corresponding E2 .",Experimental Results,Case Study,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",57,0.9661016949152542,236,0.959349593495935,11,0.8461538461538461,1,0,Experimental Results: Case Study
238,"Although , LSTM - LSMT - Bias can find the entities pair ( E1 , E2 ) , it reverses the roles of "" Stephen A. Schwarzman "" and "" Blackstone Group "" .",Experimental Results,Case Study,relation-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",58,0.9830508474576272,237,0.9634146341463414,12,0.9230769230769232,1,0,Experimental Results: Case Study
240,Conclusion,,,relation-classification,1,['O'],['O'],0,0.0,239,0.9715447154471544,0,0.0,1,0,
2,Joint entity recognition and relation extraction as a multi-head selection problem,title,title,relation-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0,1,0.0033898305084745,1,0.0,1,0,title
3,abstract,,,relation-classification,2,['O'],['O'],0,0.0,2,0.0067796610169491,0,0.0,1,0,
5,"Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools .",abstract,abstract,relation-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.2857142857142857,4,0.0135593220338983,2,0.2857142857142857,1,0,abstract
6,"However , these features are not always accurate for various languages and contexts .",abstract,abstract,relation-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.4285714285714285,5,0.0169491525423728,3,0.4285714285714285,1,0,abstract
13,It is a central problem in information extraction since it is critical for tasks such as knowledge base population and question answering .,Introduction,Introduction,relation-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.064516129032258,12,0.0406779661016949,2,0.064516129032258,1,0,Introduction
15,"The main limitations of the pipeline models are : ( i ) error propagation between the components ( i.e. , NER and RE ) and ( ii ) possible useful information from the one task is not exploited by the other ( e.g. , identifying a Works for relation might be helpful for the NER module in detecting the type of the two entities , i.e. , PER , ORG and vice versa ) .",Introduction,Introduction,relation-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.1290322580645161,14,0.047457627118644,4,0.1290322580645161,1,0,Introduction
16,"On the other hand , more recent studies propose to use joint models to detect entities and their relations overcoming the aforementioned issues and achieving state - of - the - art performance .",Introduction,Introduction,relation-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.1612903225806451,15,0.0508474576271186,5,0.1612903225806451,1,0,Introduction
18,"Recent advances in neural networks alleviate the issue of manual feature engineering , but some of them still depend on NLP tools ( e.g. , POS taggers , dependency parsers ) .",Introduction,Introduction,relation-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.2258064516129032,17,0.0576271186440678,7,0.2258064516129032,1,0,Introduction
23,"Note that the aforementioned works examine pairs of entities for relation extraction , rather than modeling the whole sentence directly .",Introduction,Introduction,relation-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.3870967741935484,22,0.0745762711864406,12,0.3870967741935484,1,0,Introduction
24,This means that relations of other pairs of entities in the same sentence - which could be helpful in deciding on the relation type fora particular pair - are not taken into account .,Introduction,Introduction,relation-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.4193548387096774,23,0.0779661016949152,13,0.4193548387096774,1,0,Introduction
25,"propose a neural joint model based on LSTMs where they model the whole sentence at once , but still they do not have a principled way to deal with multiple relations .",Introduction,Introduction,relation-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.4516129032258064,24,0.0813559322033898,14,0.4516129032258064,1,0,Introduction
27,"The limitation of this approach is that only a single relation can be assigned to a token , while the time complexity for the entity recognition task is increased compared to the standard approaches with linear complexity .",Introduction,Introduction,relation-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.5161290322580645,26,0.0881355932203389,16,0.5161290322580645,1,0,Introduction
32,"Yet , our ambition is to develop a model that generalizes well in various setups , therefore using only automatically extracted features that are learned during training .",Introduction,Introduction,relation-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.6774193548387096,31,0.1050847457627118,21,0.6774193548387096,1,0,Introduction
33,"For instance , and use exactly the same model in different contexts , i.e. , news ( ACE04 ) and biomedical data ( ADE ) , respectively .",Introduction,Introduction,relation-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.7096774193548387,32,0.1084745762711864,22,0.7096774193548387,1,0,Introduction
43,Related work,,,relation-classification,2,"['O', 'O']","['O', 'O']",0,0.0,42,0.1423728813559322,0,0.0,1,0,
110,"By multi-head , we mean that any particular entity maybe the CoNLL04 dataset is presented .",Joint model,Joint model,relation-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.0625,109,0.3694915254237288,4,0.3333333333333333,1,0,Joint model
114,"The outputs for each token ( e.g. , Smith ) are twofold : ( i ) an entity recognition label ( e.g. , I - PER , denoting the token is inside a named entity of type PER ) and ( ii ) a set of tuples comprising the head tokens of the entity and the types of relations between them ( e.g. , {( Center , Works for ) , ( Atlanta , Lives in ) } ) .",Joint model,Joint model,relation-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.125,113,0.3830508474576271,8,0.6666666666666666,1,0,Joint model
115,"Since we assume token - based encoding , we consider only the last token of the entity as head of another token , eliminating redundant relations .",Joint model,Joint model,relation-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.140625,114,0.3864406779661017,9,0.75,1,0,Joint model
116,"For instance , there is a Works for relation between entities "" John Smith "" and "" Disease Control Center "" .",Joint model,Joint model,relation-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.15625,115,0.3898305084745763,10,0.8333333333333334,1,0,Joint model
117,"Instead of connecting all tokens of the entities , we connect only "" Smith "" with "" Center "" .",Joint model,Joint model,relation-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.171875,116,0.3932203389830508,11,0.9166666666666666,1,0,Joint model
118,"Also , for the case of no relation , we introduce the "" N "" label and we predict the token itself as the head .",Joint model,Joint model,relation-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.1875,117,0.3966101694915254,12,1.0,1,0,Joint model
119,Embedding layer,Joint model,,relation-classification,2,"['O', 'O']","['O', 'O']",13,0.203125,118,0.4,0,0.0,1,0,Joint model
124,"For instance , in the Adverse Drug Events ( ADE ) dataset , the suffix "" toxicity "" can specify an adverse drug event entity such as "" neurotoxicity "" or "" hepatotoxicity "" and thus it is very informative .",Joint model,Embedding layer,relation-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.28125,123,0.4169491525423728,5,0.4545454545454545,1,0,Joint model: Embedding layer
125,"Another example might be the Dutch suffix "" kamer "" ( "" room "" in English ) in the Dutch Real Estate Classifieds ( DREC ) dataset which is used to specify the space entities "" badkamer "" ( "" bathroom "" in English ) and "" slaapkamer "" ( "" bedroom "" in English ) .",Joint model,Embedding layer,relation-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.296875,124,0.4203389830508474,6,0.5454545454545454,1,0,Joint model: Embedding layer
131,Bidirectional LSTM encoding layer,Joint model,,relation-classification,2,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",25,0.390625,130,0.4406779661016949,0,0.0,1,0,Joint model
132,RNNs are commonly used in modeling sequential data and have been successfully applied in various NLP tasks .,Joint model,Bidirectional LSTM encoding layer,relation-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.40625,131,0.4440677966101695,1,0.2,1,0,Joint model: Bidirectional LSTM encoding layer
136,The BiLSTM output at timestep i can be written as :,Joint model,Bidirectional LSTM encoding layer,relation-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",30,0.46875,135,0.4576271186440678,5,1.0,1,0,Joint model: Bidirectional LSTM encoding layer
139,"In the general formulation of our method , each token w i can have multiple heads ( i.e. , multiple relations with other tokens ) .",Joint model,Relation extraction as multi-head selection,relation-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",33,0.515625,138,0.4677966101694915,2,0.0606060606060606,1,0,Joint model: Relation extraction as multi-head selection
140,"We predict the tuple (? i ,? i ) where ?",Joint model,Relation extraction as multi-head selection,relation-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",34,0.53125,139,0.4711864406779661,3,0.0909090909090909,1,0,Joint model: Relation extraction as multi-head selection
141,i is the vector of heads and ?,Joint model,Relation extraction as multi-head selection,relation-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",35,0.546875,140,0.4745762711864407,4,0.1212121212121212,1,0,Joint model: Relation extraction as multi-head selection
142,i is the vector of the corresponding relations for each token w i .,Joint model,Relation extraction as multi-head selection,relation-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",36,0.5625,141,0.4779661016949152,5,0.1515151515151515,1,0,Joint model: Relation extraction as multi-head selection
144,"Given as input a token sequence wand a set of relation labels R , our goal is to identify for each token w i , i ?",Joint model,Relation extraction as multi-head selection,relation-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",38,0.59375,143,0.4847457627118644,7,0.2121212121212121,1,0,Joint model: Relation extraction as multi-head selection
145,"{ 0 , ... , n} the vector of the most probable heads ?",Joint model,Relation extraction as multi-head selection,relation-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",39,0.609375,144,0.488135593220339,8,0.2424242424242424,1,0,Joint model: Relation extraction as multi-head selection
146,i ? wand the vector of the most probable corresponding relation labelsr i ?,Joint model,Relation extraction as multi-head selection,relation-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",40,0.625,145,0.4915254237288136,9,0.2727272727272727,1,0,Joint model: Relation extraction as multi-head selection
148,We calculate the score between tokens w i and w j given a label r k as follows :,Joint model,R .,relation-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",42,0.65625,147,0.4983050847457627,11,0.3333333333333333,1,0,Joint model: R .
149,"where the superscript ( r ) is used for the notation of the relation task , f ( ) is an elementwise activation function ( i.e. , relu , tanh ) ,",Joint model,R .,relation-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",43,0.671875,148,0.5016949152542373,12,0.3636363636363636,1,0,Joint model: R .
150,"is the hidden size of the LSTM , b is the size of the label embeddings and l the layer width .",Joint model,R .,relation-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",44,0.6875,149,0.5050847457627119,13,0.3939393939393939,1,0,Joint model: R .
151,We define,Joint model,,relation-classification,2,"['O', 'O']","['O', 'O']",45,0.703125,150,0.5084745762711864,14,0.4242424242424242,1,0,Joint model
152,"to be the probability of token w j to be selected as the head of token w i with the relation label r k between them , where ?( . ) stands for the sigmoid function .",Joint model,We define,relation-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",46,0.71875,151,0.511864406779661,15,0.4545454545454545,1,0,Joint model: We define
153,We minimize the cross - entropy loss L rel during training :,Joint model,We define,relation-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",47,0.734375,152,0.5152542372881356,16,0.4848484848484848,1,0,Joint model: We define
154,where y i ?,Joint model,We define,relation-classification,2,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",48,0.75,153,0.5186440677966102,17,0.5151515151515151,1,0,Joint model: We define
155,wand r i ?,Joint model,We define,relation-classification,2,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",49,0.765625,154,0.5220338983050847,18,0.5454545454545454,1,0,Joint model: We define
156,R are the ground truth vectors of heads and associated relation labels of w i and m is the number of relations ( heads ) for w i .,Joint model,We define,relation-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",50,0.78125,155,0.5254237288135594,19,0.5757575757575758,1,0,Joint model: We define
158,i and relation labelsr i exceeding a threshold based on the estimated joint probability as defined in Eq..,Joint model,We define,relation-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",52,0.8125,157,0.5322033898305085,21,0.6363636363636364,1,0,Joint model: We define
161,Edmonds ' algorithm,Joint model,,relation-classification,2,"['O', 'O', 'O']","['O', 'O', 'O']",55,0.859375,160,0.5423728813559322,24,0.7272727272727273,1,0,Joint model
164,"By using thresholded inference , a tree structure of relations is not guaranteed .",Joint model,Edmonds ' algorithm,relation-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",58,0.90625,163,0.5525423728813559,27,0.8181818181818182,1,0,Joint model: Edmonds ' algorithm
165,Thus we should enforce tree structure constraints to our model .,Joint model,Edmonds ' algorithm,relation-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",59,0.921875,164,0.5559322033898305,28,0.8484848484848485,1,0,Joint model: Edmonds ' algorithm
167,"A fully connected directed graph G = ( V , E ) is constructed ,",Joint model,Edmonds ' algorithm,relation-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",61,0.953125,166,0.5627118644067797,30,0.9090909090909092,1,0,Joint model: Edmonds ' algorithm
168,where the vertices,Joint model,Edmonds ' algorithm,relation-classification,2,"['O', 'O', 'O']","['O', 'O', 'O']",62,0.96875,167,0.5661016949152542,31,0.9393939393939394,1,0,Joint model: Edmonds ' algorithm
169,V represent the last tokens of the identified entities ( as predicted by NER ) and the edges E represent the highest scoring relations with their scores as weights .,Joint model,Edmonds ' algorithm,relation-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",63,0.984375,168,0.5694915254237288,32,0.9696969696969696,1,0,Joint model: Edmonds ' algorithm
171,Experimental setup,,,relation-classification,2,"['O', 'O']","['O', 'O']",0,0.0,170,0.576271186440678,0,0.0,1,0,
172,Datasets and evaluation metrics,,,relation-classification,2,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",0,0.0,171,0.5796610169491525,0,0.0,1,0,
174,We removed DISC and did 5 - fold cross -validation on the bnews and nwire subsets ( 348 documents ) .,Datasets and evaluation metrics,Datasets and evaluation metrics,relation-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.0588235294117647,173,0.5864406779661017,2,0.0666666666666666,1,0,Datasets and evaluation metrics
177,We treat an entity as correct when the entity type and the region of its head are correct .,Datasets and evaluation metrics,Datasets and evaluation metrics,relation-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.1470588235294117,176,0.5966101694915255,5,0.1666666666666666,1,0,Datasets and evaluation metrics
178,"We treat a relation as correct when it s type and argument entities are correct , similar to and .",Datasets and evaluation metrics,Datasets and evaluation metrics,relation-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.1764705882352941,177,0.6,6,0.2,1,0,Datasets and evaluation metrics
179,We refer to this type of evaluation as strict .,Datasets and evaluation metrics,Datasets and evaluation metrics,relation-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.2058823529411764,178,0.6033898305084746,7,0.2333333333333333,1,0,Datasets and evaluation metrics
180,"We select the best hyperparameter values on a randomly selected validation set for each fold , selected from the training set ( 15 % of the data ) since there are no official train and validation splits in the work of .",Datasets and evaluation metrics,Datasets and evaluation metrics,relation-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.2352941176470588,179,0.6067796610169491,8,0.2666666666666666,1,0,Datasets and evaluation metrics
181,"CoNLL04 : There are four entity types in the dataset ( Location , Organization , Person ,",Datasets and evaluation metrics,Datasets and evaluation metrics,relation-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.2647058823529412,180,0.6101694915254238,9,0.3,1,0,Datasets and evaluation metrics
182,"and Other ) and five relation types ( Kill , Live in , Located in , OrgBased in and Work for ) .",Datasets and evaluation metrics,Datasets and evaluation metrics,relation-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.2941176470588235,181,0.6135593220338983,10,0.3333333333333333,1,0,Datasets and evaluation metrics
183,We use the splits defined by Gupta et al. and .,Datasets and evaluation metrics,Datasets and evaluation metrics,relation-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.3235294117647059,182,0.6169491525423729,11,0.3666666666666666,1,0,Datasets and evaluation metrics
188,"To obtain comparable results , we omit the entity class "" Other "" when computing the EC score .",Datasets and evaluation metrics,Datasets and evaluation metrics,relation-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.4705882352941176,187,0.6338983050847458,16,0.5333333333333333,1,0,Datasets and evaluation metrics
189,We score a multi-token entity as correct if at least one of its comprising token types is correct assuming that the boundaries are given ; a relation is correct when the type of the relation and the argument entities are both correct .,Datasets and evaluation metrics,Datasets and evaluation metrics,relation-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.5,188,0.6372881355932203,17,0.5666666666666667,1,0,Datasets and evaluation metrics
194,"In this setting , we count an entity as correct if the boundaries of the entity are correct .",Datasets and evaluation metrics,Datasets and evaluation metrics,relation-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.6470588235294118,193,0.6542372881355932,22,0.7333333333333333,1,0,Datasets and evaluation metrics
195,A relation is correct when the relation is correct and the argument entities are both correct .,Datasets and evaluation metrics,Datasets and evaluation metrics,relation-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",23,0.6764705882352942,194,0.6576271186440678,23,0.7666666666666667,1,0,Datasets and evaluation metrics
197,ADE : There are two types of entities ( drugs and diseases ) in this dataset and the aim of the task is to identify the types of entities and relate each drug with a disease ( adverse drug events ) .,Datasets and evaluation metrics,Datasets and evaluation metrics,relation-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.7352941176470589,196,0.6644067796610169,25,0.8333333333333334,1,0,Datasets and evaluation metrics
198,"There are 6,821 sentences in total and similar to previous work , we remove ? 130 relations with overlapping entities ( e.g. , "" lithium "" is a drug which is related to "" lithium intoxication "" ) .",Datasets and evaluation metrics,Datasets and evaluation metrics,relation-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.7647058823529411,197,0.6677966101694915,26,0.8666666666666667,1,0,Datasets and evaluation metrics
199,"Since there are no official sets , we evaluate our model using 10 - fold cross- validation where 10 % of the data was used as validation and 10 % for test set similar to .",Datasets and evaluation metrics,Datasets and evaluation metrics,relation-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.7941176470588235,198,0.6711864406779661,27,0.9,1,0,Datasets and evaluation metrics
203,Word embeddings,Datasets and evaluation metrics,,relation-classification,2,"['O', 'O']","['O', 'O']",31,0.9117647058823528,202,0.6847457627118644,0,0.0,1,0,Datasets and evaluation metrics
206,We obtained the 50 - dimensional word embeddings used by,Datasets and evaluation metrics,Word embeddings,relation-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",34,1.0,205,0.6949152542372882,3,1.0,1,0,Datasets and evaluation metrics: Word embeddings
216,We experimented with tanh and relu activation functions ( recall that this is the function f ( ) from the model description relu activation only in the ACE04 and tanh in all other datasets .,Hyperparameters and implementation details,We use dropout to regularize our network .,relation-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.6923076923076923,215,0.7288135593220338,9,0.6923076923076923,1,0,Hyperparameters and implementation details: We use dropout to regularize our network .
220,For more details about the effect of each hyperparameter to the model performance seethe Appendix .,Hyperparameters and implementation details,We use dropout to regularize our network .,relation-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,1.0,219,0.7423728813559322,13,1.0,1,0,Hyperparameters and implementation details: We use dropout to regularize our network .
221,Results and discussion,,,relation-classification,2,"['O', 'O', 'O']","['O', 'O', 'O']",0,0.0,220,0.7457627118644068,0,0.0,1,0,
222,Results,,,relation-classification,2,['O'],['O'],0,0.0,221,0.7491525423728813,0,0.0,1,0,
224,The first column indicates the considered dataset .,Results,,relation-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.0434782608695652,223,0.7559322033898305,2,0.054054054054054,1,0,Results
225,"In the second column , we denote the model which is applied ( i.e. , previous work and the proposed models ) .",Results,The first column indicates the considered dataset .,relation-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.0652173913043478,224,0.7593220338983051,3,0.081081081081081,1,0,Results: The first column indicates the considered dataset .
227,"( i ) multi-head is the proposed model with the CRF layer for NER and the sigmoid loss for multiple head prediction , ( ii ) multi-head +",Results,The first column indicates the considered dataset .,relation-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.108695652173913,226,0.7661016949152543,5,0.1351351351351351,1,0,Results: The first column indicates the considered dataset .
228,"E is the proposed model with addition of Edmonds ' algorithm to guarantee a tree - structured output for the DREC dataset , ( iii ) single - head is the proposed method but it predicts only one head per token using a softmax loss instead of a sigmoid , and ( iv ) multi-head EC is the proposed method with a softmax to predict the entity classes assuming that the boundaries are given , and the sigmoid loss for multiple head selection . ( iii ) Relaxed : we score a multi-token entity as correct if at least one of its comprising token types is correct assuming that the boundaries are given ; a relation is correct when the type of the relation and the argument entities are both correct .",Results,The first column indicates the considered dataset .,relation-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.1304347826086956,227,0.7694915254237288,6,0.1621621621621621,1,0,Results: The first column indicates the considered dataset .
231,"We mark with bold font in , the class probabilities do not necessarily sum up to one since the classes are considered independent .",Results,The first column indicates the considered dataset .,relation-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.1956521739130435,230,0.7796610169491526,9,0.2432432432432432,1,0,Results: The first column indicates the considered dataset .
235,"for the RE task ) compared to For the CoNLL04 dataset , there are two different evaluation settings , namely relaxed and strict .",Results,The first column indicates the considered dataset .,relation-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.2826086956521739,234,0.7932203389830509,13,0.3513513513513513,1,0,Results: The first column indicates the considered dataset .
237,We adopt this setting to produce comparable results with previous studies ) .,Results,The first column indicates the considered dataset .,relation-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.3260869565217391,236,0.8,15,0.4054054054054054,1,0,Results: The first column indicates the considered dataset .
238,"Similar to , we present results of single models and no ensembles .",Results,The first column indicates the considered dataset .,relation-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.3478260869565217,237,0.8033898305084746,16,0.4324324324324324,1,0,Results: The first column indicates the considered dataset .
245,"We also report re-sults on the same dataset conducting NER ( i.e. , predicting entity types and boundaries ) and evaluating using the strict evaluation measure , similar to .",Results,The first column indicates the considered dataset .,relation-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",23,0.5,244,0.8271186440677966,23,0.6216216216216216,1,0,Results: The first column indicates the considered dataset .
246,Our results are not directly comparable to the work of because we use the splits provided by .,Results,The first column indicates the considered dataset .,relation-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.5217391304347826,245,0.8305084745762712,24,0.6486486486486487,1,0,Results: The first column indicates the considered dataset .
247,"However , in this setting we present the results from as reference .",Results,The first column indicates the considered dataset .,relation-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.5434782608695652,246,0.8338983050847457,25,0.6756756756756757,1,0,Results: The first column indicates the considered dataset .
251,"We transform the previous results from to the boundaries setting to make them comparable to our model since in their work , they report token - based F 1 score , which is not a common evaluation metric in relation extraction problems .",Results,The first column indicates the considered dataset .,relation-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",29,0.6304347826086957,250,0.847457627118644,29,0.7837837837837838,1,0,Results: The first column indicates the considered dataset .
252,"Also , in their work , they focus on identifying only the boundaries of the entities and not the types ( e.g. , Floor , Space ) .",Results,The first column indicates the considered dataset .,relation-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",30,0.6521739130434783,251,0.8508474576271187,30,0.8108108108108109,1,0,Results: The first column indicates the considered dataset .
254,"This is due to the fact that their quadratic scoring layer is beneficial for the RE task , yet complicates NER , which is usually modeled as a sequence labeling task .",Results,The first column indicates the considered dataset .,relation-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",32,0.6956521739130435,253,0.8576271186440678,32,0.8648648648648649,1,0,Results: The first column indicates the considered dataset .
269,Conclusion,,,relation-classification,2,['O'],['O'],0,0.0,268,0.9084745762711864,0,0.0,1,0,
3,abstract,,,relation-classification,3,['O'],['O'],0,0.0,2,0.0145985401459854,0,0.0,1,0,
8,"Many neural network methods have recently been exploited in various natural language processing ( NLP ) tasks , such as parsing , POS tagging , relation extraction , translation , and joint tasks .",Introduction,Introduction,relation-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.1,7,0.0510948905109489,1,0.1,1,0,Introduction
9,"However , observed that intentional small scale perturbations ( i.e. , adversarial examples ) to the input of such models may lead to incorrect decisions ( with high confidence ) .",Introduction,Introduction,relation-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.2,8,0.0583941605839416,2,0.2,1,0,Introduction
10,proposed adversarial training ( AT ) ( for image recognition ) as a regularization method which uses a mixture of clean and adversarial examples to enhance the robustness of the model .,Introduction,Introduction,relation-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.3,9,0.0656934306569343,3,0.3,1,0,Introduction
18,Related work,,,relation-classification,3,"['O', 'O']","['O', 'O']",0,0.0,17,0.1240875912408759,0,0.0,1,0,
34,Model,,,relation-classification,3,['O'],['O'],0,0.0,33,0.2408759124087591,0,0.0,1,0,
38,"The input is a sequence of tokens ( i.e. , sentence ) w = w 1 , ... , w n .",Model,Model,relation-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.0888888888888888,37,0.2700729927007299,3,0.0681818181818181,1,0,Model
44,"In , the B - PER tag is assigned to the beginning token of a ' person ' ( PER ) entity .",Model,Model,relation-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.2222222222222222,43,0.3138686131386861,9,0.2045454545454545,1,0,Model
47,"Although independent distribution of types is reasonable for EC tasks , this is not the case when there are strong correlations between neighboring tags .",Model,Model,relation-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.2888888888888888,46,0.3357664233576642,12,0.2727272727272727,1,0,Model
48,"For instance , the BIO encoding scheme imposes several constraints in the NER task ( e.g. , the B - PER and I - LOC tags can not be sequential ) .",Model,Model,relation-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.3111111111111111,47,0.3430656934306569,13,0.2954545454545454,1,0,Model
54,"In our model , each word w i can be involved in multiple relations with other words .",Model,Model,relation-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.4444444444444444,53,0.3868613138686131,19,0.4318181818181818,1,0,Model
55,"For instance , in the example illustrated in , "" Smith "" could be involved not only in a Lives in relation with the token "" California "" ( head ) but also in other relations simultaneously ( e.g. , Works for , Born In with some corresponding tokens ) .",Model,Model,relation-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.4666666666666667,54,0.3941605839416058,20,0.4545454545454545,1,0,Model
56,"The goal of the task is to predict for each word w i , a vector of heads ?",Model,Model,relation-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.4888888888888889,55,0.4014598540145985,21,0.4772727272727273,1,0,Model
57,i and the vector of corresponding relationsr i .,Model,Model,relation-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",23,0.5111111111111111,56,0.4087591240875912,22,0.5,1,0,Model
59,"The corresponding probability is defined as : P ( w j , r k | w i ; ? ) = ? ( s ( w j , w i , r k ) ) , where ?( . ) is the sigmoid function .",Model,Model,relation-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.5555555555555556,58,0.4233576642335766,24,0.5454545454545454,1,0,Model
60,"During training , we minimize the cross - entropy loss L rel as :",Model,Model,relation-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.5777777777777777,59,0.4306569343065693,25,0.5681818181818182,1,0,Model
61,where m is the number of associated heads ( and thus relations ) per word w i .,Model,Model,relation-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.6,60,0.437956204379562,26,0.5909090909090909,1,0,Model
63,The final objective for the joint task is computed as L JOINT ( w ; ? ) = L NER + L rel where ?,Model,Model,relation-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",29,0.6444444444444445,62,0.4525547445255474,28,0.6363636363636364,1,0,Model
64,is a set of parameters .,Model,Model,relation-classification,3,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O']",30,0.6666666666666666,63,0.4598540145985401,29,0.6590909090909091,1,0,Model
65,"In the case of multi-token entities , only the last token of the entity can serve as head of another token , to eliminate redundant relations .",Model,Model,relation-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",31,0.6888888888888889,64,0.4671532846715328,30,0.6818181818181818,1,0,Model
66,"If an entity is not involved in any relation , we predict the auxiliary "" N "" relation label and the token itself as head .",Model,Model,relation-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",32,0.7111111111111111,65,0.4744525547445255,31,0.7045454545454546,1,0,Model
67,Adversarial training ( AT ),Model,Model,relation-classification,3,"['O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O']",33,0.7333333333333333,66,0.4817518248175182,32,0.7272727272727273,1,0,Model
70,This is similar to the concept introduced by to improve the robustness of image recognition classifiers .,Model,Model,relation-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",36,0.8,69,0.5036496350364964,35,0.7954545454545454,1,0,Model
72,adv to the original embedding w that maximizes the loss function :,Model,Model,relation-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",38,0.8444444444444444,71,0.5182481751824818,37,0.8409090909090909,1,0,Model
73,where ?,Model,Model,relation-classification,3,"['O', 'O']","['O', 'O']",39,0.8666666666666667,72,0.5255474452554745,38,0.8636363636363636,1,0,Model
74,is a copy of the current model parameters .,Model,Model,relation-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",40,0.8888888888888888,73,0.5328467153284672,39,0.8863636363636364,1,0,Model
75,Since Eq.,Model,,relation-classification,3,"['O', 'O']","['O', 'O']",41,0.9111111111111112,74,0.5401459854014599,40,0.9090909090909092,1,0,Model
76,"( 2 ) is intractable in neural networks , we use the approximation proposed in defined as : ? adv = g/ g , with g = ? w L JOINT ( w ; ? ) , where is a small bounded norm treated as a hyperparameter .",Model,Since Eq.,relation-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",42,0.9333333333333332,75,0.5474452554744526,41,0.9318181818181818,1,0,Model: Since Eq.
77,"Similar to , we set to be ? ?",Model,Since Eq.,relation-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",43,0.9555555555555556,76,0.5547445255474452,42,0.9545454545454546,1,0,Model: Since Eq.
78,D ( where Dis the dimension of the embeddings ) .,Model,Since Eq.,relation-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",44,0.9777777777777776,77,0.5620437956204379,43,0.9772727272727272,1,0,Model: Since Eq.
79,"We train on the mixture of original and adversarial examples , so the final loss is computed as : L JOINT ( w ; ? ) + L JOINT ( w + ? adv ;? ) .",Model,Since Eq.,relation-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",45,1.0,78,0.5693430656934306,44,1.0,1,0,Model: Since Eq.
80,Experimental setup,,,relation-classification,3,"['O', 'O']","['O', 'O']",0,0.0,79,0.5766423357664233,0,0.0,1,0,
82,1,Experimental setup,Experimental setup,relation-classification,3,['O'],['O'],2,0.0434782608695652,81,0.5912408759124088,2,0.0434782608695652,1,0,Experimental setup
83,"Specifically , we follow the 5 - fold crossvalidation defined by for the ACE04 dataset .",Experimental setup,Experimental setup,relation-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.0652173913043478,82,0.5985401459854015,3,0.0652173913043478,1,0,Experimental setup
84,"For the CoNLL04 ) EC task ( assuming boundaries are given ) , we use the same splits as in ; .",Experimental setup,Experimental setup,relation-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.0869565217391304,83,0.6058394160583942,4,0.0869565217391304,1,0,Experimental setup
86,"For the Dutch Real Estate Classifieds , DREC ( Bekoulis et al. , 2017 ) dataset , we use train - test splits as in .",Experimental setup,Experimental setup,relation-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.1304347826086956,85,0.6204379562043796,6,0.1304347826086956,1,0,Experimental setup
87,"For the Adverse Drug Events , ADE , we perform 10 - fold cross -validation similar to .",Experimental setup,Experimental setup,relation-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.1521739130434782,86,0.6277372262773723,7,0.1521739130434782,1,0,Experimental setup
92,"is selected from { 5 e?2 , 1 e ? 2 , 1 e ? 3 , 1e?4 } .",Experimental setup,Experimental setup,relation-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.2608695652173913,91,0.6642335766423357,12,0.2608695652173913,1,0,Experimental setup
94,This can be explained from the fact that adding more noise can change the content of the sentence as also reported by .,Experimental setup,Experimental setup,relation-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.3043478260869565,93,0.6788321167883211,14,0.3043478260869565,1,0,Experimental setup
95,"We use three types of evaluation , namely : ( i ) S( trict ) : we score an entity as correct if both the entity boundaries and the entity type are correct ( ACE04 , ADE , CoNLL04 , DREC ) , ( ii ) B ( oundaries ) : we score an entity as correct if only the entity boundaries are correct while the entity type is not taken into account ( DREC ) and ( iii ) R( elaxed ) : a multi-token entity is considered correct if at least one correct type is assigned to the tokens comprising the entity , assuming that the : Comparison of our method with the stateof - the - art in terms of F 1 score .",Experimental setup,Experimental setup,relation-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.3260869565217391,94,0.6861313868613139,15,0.3260869565217391,1,0,Experimental setup
99,"boundaries are known ( CoNLL04 ) , to compare to previous works .",Experimental setup,Experimental setup,relation-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.4130434782608695,98,0.7153284671532847,19,0.4130434782608695,1,0,Experimental setup
100,"In all cases , a relation is considered as correct when both the relation type and the argument entities are correct .",Experimental setup,Experimental setup,relation-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.4347826086956521,99,0.7226277372262774,20,0.4347826086956521,1,0,Experimental setup
102,The name of the dataset is presented in the first column while the models are listed in the second column .,Experimental setup,Experimental setup,relation-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.4782608695652174,101,0.7372262773722628,22,0.4782608695652174,1,0,Experimental setup
110,This indicates that NLP tools are not always accurate for various contexts .,Experimental setup,Experimental setup,relation-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",30,0.6521739130434783,109,0.7956204379562044,30,0.6521739130434783,1,0,Experimental setup
127,Results,,,relation-classification,3,['O'],['O'],0,0.0,126,0.9197080291970804,0,0.0,1,0,
130,"This seemingly small performance increase is mainly due to the limited performance benefit for the NER component , which is in accordance with the recent advances in NER using neural networks that report similarly small gains ( e.g. , the performance improvement in and on the CoNLL - 2003 test set is 0.01 % and 0.17 % F 1 percentage points , while in the work of , a 0.07 % F 1 improvement on CoNLL - 2000 using AT for NER is reported ) .",Results,Results,relation-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.5,129,0.9416058394160584,3,0.5,1,0,Results
133,"This may indicate a correlation between the dataset size and the benefit of adversarial training in the context of joint models , but this needs further investigation in future work .",Results,Results,relation-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,1.0,132,0.9635036496350364,6,1.0,1,0,Results
134,Conclusion,,,relation-classification,3,['O'],['O'],0,0.0,133,0.9708029197080292,0,0.0,1,0,
3,abstract,,,relation-classification,4,['O'],['O'],0,0.0,2,0.0076045627376425,0,0.0,1,0,
5,"However , existing dependency - based models either neglect crucial information ( e.g. , negation ) by pruning the dependency trees too aggressively , or are computationally inefficient because it is difficult to parallelize over different tree structures .",abstract,abstract,relation-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.25,4,0.0152091254752851,2,0.25,1,0,abstract
10,* Equal contribution .,abstract,abstract,relation-classification,4,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",7,0.875,9,0.0342205323193916,7,0.875,1,0,abstract
11,The order of authorship was decided by a tossed coin .,abstract,abstract,relation-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,1.0,10,0.0380228136882129,8,1.0,1,0,abstract
13,"Relation extraction involves discerning whether a relation exists between two entities in a sentence ( often termed subject and object , respectively ) .",Introduction,Introduction,relation-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0434782608695652,12,0.0456273764258555,1,0.0434782608695652,1,0,Introduction
16,"A subtree of the original UD dependency tree between the subject ( "" he "" ) and object ( "" Mike Cane "" ) is also shown , where the shortest dependency path between the entities is highlighted in bold .",Introduction,Introduction,relation-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.1739130434782608,15,0.0570342205323193,4,0.1739130434782608,1,0,Introduction
17,"Note that negation ( "" not "" ) is off the dependency path .",Introduction,Introduction,relation-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.217391304347826,16,0.0608365019011406,5,0.217391304347826,1,0,Introduction
18,"have proven to be very effective in relation extraction , because they capture long - range syntactic relations that are obscure from the surface form alone ( e.g. , when long clauses or complex scoping are present ) .",Introduction,Introduction,relation-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.2608695652173913,17,0.0646387832699619,6,0.2608695652173913,1,0,Introduction
19,Traditional feature - based models are able to represent dependency information by featurizing dependency trees as overlapping paths along the trees .,Introduction,Introduction,relation-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.3043478260869565,18,0.0684410646387832,7,0.3043478260869565,1,0,Introduction
20,"However , these models face the challenge of sparse feature spaces and are brittle to lexical variations .",Introduction,Introduction,relation-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.3478260869565217,19,0.0722433460076045,8,0.3478260869565217,1,0,Introduction
21,More recent neural models address this problem with distributed representations built from their computation graphs formed along parse trees .,Introduction,Introduction,relation-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.391304347826087,20,0.0760456273764258,9,0.391304347826087,1,0,Introduction
22,One common approach to leverage dependency information is to perform bottom - up or top - down computation along the parse tree or the subtree below the lowest common ancestor ( LCA ) of the entities .,Introduction,Introduction,relation-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.4347826086956521,21,0.0798479087452471,10,0.4347826086956521,1,0,Introduction
23,"Another popular approach , inspired by , is to reduce the parse tree to the shortest dependency path between the entities .",Introduction,Introduction,relation-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.4782608695652174,22,0.0836501901140684,11,0.4782608695652174,1,0,Introduction
24,"However , these models suffer from several drawbacks .",Introduction,Introduction,relation-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.5217391304347826,23,0.0874524714828897,12,0.5217391304347826,1,0,Introduction
25,"Neural models operating directly on parse trees are usually difficult to parallelize and thus computationally inefficient , because aligning trees for efficient batch training is usually nontrivial .",Introduction,Introduction,relation-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.5652173913043478,24,0.091254752851711,13,0.5652173913043478,1,0,Introduction
26,"Models based on the shortest dependency path between the subject and object are computationally more efficient , but this simplifying assumption has major limitations as well .",Introduction,Introduction,relation-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.6086956521739131,25,0.0950570342205323,14,0.6086956521739131,1,0,Introduction
27,"shows a real - world example where crucial information ( i.e. , negation ) would be excluded when the model is restricted to only considering the dependency path .",Introduction,Introduction,relation-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.6521739130434783,26,0.0988593155893536,15,0.6521739130434783,1,0,Introduction
36,Models,,,relation-classification,4,['O'],['O'],0,0.0,35,0.1330798479087452,0,0.0,1,0,
40,"Given a graph with n nodes , we can represent the graph structure with an n n adjacency matrix A where A ij = 1 if there is an edge going from node i to node j.",Models,Graph Convolutional Networks over Dependency Trees,relation-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.0606060606060606,39,0.1482889733840304,2,0.0909090909090909,1,0,Models: Graph Convolutional Networks over Dependency Trees
41,"In an L-layer GCN , if we denote by h ( l?1 ) i the input vector and h ( l ) i the output vector of node i at the l - th layer , a graph convolution operation can be written as",Models,Graph Convolutional Networks over Dependency Trees,relation-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.0757575757575757,40,0.1520912547528517,3,0.1363636363636363,1,0,Models: Graph Convolutional Networks over Dependency Trees
42,"where W ( l ) is a linear transformation , b ( l ) a bias term , and ?",Models,Graph Convolutional Networks over Dependency Trees,relation-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.0909090909090909,41,0.155893536121673,4,0.1818181818181818,1,0,Models: Graph Convolutional Networks over Dependency Trees
43,"a nonlinear function ( e.g. , ReLU ) .",Models,Graph Convolutional Networks over Dependency Trees,relation-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.106060606060606,42,0.1596958174904943,5,0.2272727272727272,1,0,Models: Graph Convolutional Networks over Dependency Trees
45,"We adapt the graph convolution operation to model dependency trees by converting each tree into its corresponding adjacency matrix A , where A ij = 1 if there is a dependency edge between tokens i and j .",Models,Graph Convolutional Networks over Dependency Trees,relation-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.1363636363636363,44,0.1673003802281368,7,0.3181818181818182,1,0,Models: Graph Convolutional Networks over Dependency Trees
46,"However , naively applying the graph convolution operation in Equation ( 1 ) could lead to node representations with drastically different magnitudes , since the degree of a token varies a lot .",Models,Graph Convolutional Networks over Dependency Trees,relation-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.1515151515151515,45,0.1711026615969581,8,0.3636363636363636,1,0,Models: Graph Convolutional Networks over Dependency Trees
47,This could bias our sentence representation towards favoring high - degree nodes regardless of the information carried in the node ( see details in Section 2.2 ) .,Models,Graph Convolutional Networks over Dependency Trees,relation-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.1666666666666666,46,0.1749049429657794,9,0.4090909090909091,1,0,Models: Graph Convolutional Networks over Dependency Trees
48,"Furthermore , the information in h ( l?1 ) i is never carried over to hi , since nodes never connect to themselves in a dependency tree .",Models,Graph Convolutional Networks over Dependency Trees,relation-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.1818181818181818,47,0.1787072243346007,10,0.4545454545454545,1,0,Models: Graph Convolutional Networks over Dependency Trees
50,"where = A + I with I being then n identity matrix , and d i = n j=1 ij is the degree of token i in the resulting graph . :",Models,Graph Convolutional Networks over Dependency Trees,relation-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.2121212121212121,49,0.1863117870722433,12,0.5454545454545454,1,0,Models: Graph Convolutional Networks over Dependency Trees
52,"The left side shows the overall architecture , while on the right side , we only show the detailed graph convolution computation for the word "" relative "" for clarity .",Models,Graph Convolutional Networks over Dependency Trees,relation-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.2424242424242424,51,0.1939163498098859,14,0.6363636363636364,1,0,Models: Graph Convolutional Networks over Dependency Trees
54,Stacking this operation over,Models,Graph Convolutional Networks over Dependency Trees,relation-classification,4,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",18,0.2727272727272727,53,0.2015209125475285,16,0.7272727272727273,1,0,Models: Graph Convolutional Networks over Dependency Trees
55,"L layers gives us a deep GCN network , where we set h Moreover , the propagation of information between tokens occurs in parallel , and the runtime does not depend on the depth of the dependency tree .",Models,Graph Convolutional Networks over Dependency Trees,relation-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.2878787878787879,54,0.2053231939163498,17,0.7727272727272727,1,0,Models: Graph Convolutional Networks over Dependency Trees
56,Note that the GCN model presented above uses the same parameters for all edges in the dependency graph .,Models,Graph Convolutional Networks over Dependency Trees,relation-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.303030303030303,55,0.2091254752851711,18,0.8181818181818182,1,0,Models: Graph Convolutional Networks over Dependency Trees
57,"We also experimented with : ( 1 ) using different transformation matrices W for topdown , bottom - up , and self - loop edges ; and ( 2 ) adding dependency relation - specific parameters for edge - wise gating , similar to .",Models,Graph Convolutional Networks over Dependency Trees,relation-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.3181818181818182,56,0.2129277566539923,19,0.8636363636363636,1,0,Models: Graph Convolutional Networks over Dependency Trees
59,"We hypothesize that this is because the presented GCN model is usually already able to capture dependency edge patterns that are informative for classifying relations , and modeling edge directions and types does not offer additional discriminative power to the network before it leads to overfitting .",Models,Graph Convolutional Networks over Dependency Trees,relation-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",23,0.3484848484848485,58,0.2205323193916349,21,0.9545454545454546,1,0,Models: Graph Convolutional Networks over Dependency Trees
60,"For example , the relations entailed by "" A 's son , B "" and "" B 's son , A "" can be readily distinguished with "" 's "" attached to different entities , even when edge directionality is not considered .",Models,Graph Convolutional Networks over Dependency Trees,relation-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.3636363636363636,59,0.2243346007604562,22,1.0,1,0,Models: Graph Convolutional Networks over Dependency Trees
63,"Let X = [ x 1 , ... , x n ] denote a sentence , where xi is the i th token .",Models,Encoding Relations with GCN,relation-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.4090909090909091,62,0.2357414448669201,2,0.1333333333333333,1,0,Models: Encoding Relations with GCN
65,.,Models,Encoding Relations with GCN,relation-classification,4,['O'],['O'],29,0.4393939393939394,64,0.2433460076045627,4,0.2666666666666666,1,0,Models: Encoding Relations with GCN
66,"Given X , X s , and X o , the goal of relation extraction is to predict a relation r ?",Models,Encoding Relations with GCN,relation-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",30,0.4545454545454545,65,0.247148288973384,5,0.3333333333333333,1,0,Models: Encoding Relations with GCN
67,"R ( a predefined relation set ) that holds between the entities or "" no relation "" otherwise .",Models,Encoding Relations with GCN,relation-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",31,0.4696969696969697,66,0.2509505703422053,6,0.4,1,0,Models: Encoding Relations with GCN
70,"where h ( l ) denotes the collective hidden representations at layer l of the GCN , and f : R dn ?",Models,Encoding Relations with GCN,relation-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",34,0.5151515151515151,69,0.2623574144486692,9,0.6,1,0,Models: Encoding Relations with GCN
73,"Therefore , we also obtain a subject representation h s from h ( L ) as follows",Models,Encoding Relations with GCN,relation-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",37,0.5606060606060606,72,0.2737642585551331,12,0.8,1,0,Models: Encoding Relations with GCN
74,as well as an object representation ho similarly .,Models,Encoding Relations with GCN,relation-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",38,0.5757575757575758,73,0.2775665399239543,13,0.8666666666666667,1,0,Models: Encoding Relations with GCN
78,"The network architecture introduced so far learns effective representations for relation extraction , but it also leaves a few issues inadequately addressed .",Models,Contextualized GCN,relation-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",42,0.6363636363636364,77,0.2927756653992395,1,0.04,1,0,Models: Contextualized GCN
79,"First , the input word vectors do not contain contextual information about word order or disambiguation .",Models,Contextualized GCN,relation-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",43,0.6515151515151515,78,0.2965779467680608,2,0.08,1,0,Models: Contextualized GCN
80,"Second , the GCN highly depends on a correct parse tree to extract crucial information from the sentence ( especially when pruning is performed ) , while existing parsing algorithms produce imperfect trees in many cases .",Models,Contextualized GCN,relation-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",44,0.6666666666666666,79,0.3003802281368821,3,0.12,1,0,Models: Contextualized GCN
86,"Compared to tree - structured models ( e.g. , Tree - LSTM",Models,Contextualized GCN,relation-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",50,0.7575757575757576,85,0.3231939163498099,9,0.36,1,0,Models: Contextualized GCN
89,Incorporating Off - path Information with Path - centric Pruning,Models,Contextualized GCN,relation-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",53,0.803030303030303,88,0.3346007604562737,12,0.48,1,0,Models: Contextualized GCN
90,"Dependency trees provide rich structures that one can exploit in relation extraction , but most of the information pertinent to relations is usually contained within the subtree rooted at the lowest common ancestor ( LCA ) of the two entities .",Models,Contextualized GCN,relation-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",54,0.8181818181818182,89,0.338403041825095,13,0.52,1,0,Models: Contextualized GCN
91,Previous studies have shown that removing tokens outside this scope helps relation extraction by eliminating irrelevant information from the sentence .,Models,Contextualized GCN,relation-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",55,0.8333333333333334,90,0.3422053231939163,14,0.56,1,0,Models: Contextualized GCN
92,It is therefore desirable to combine our GCN models with tree pruning strategies to further improve performance .,Models,Contextualized GCN,relation-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",56,0.8484848484848485,91,0.3460076045627376,15,0.6,1,0,Models: Contextualized GCN
93,"However , pruning too aggressively ( e.g. , keeping only the dependency path ) could lead to loss of crucial information and conversely hurt robustness .",Models,Contextualized GCN,relation-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",57,0.8636363636363636,92,0.3498098859315589,16,0.64,1,0,Models: Contextualized GCN
94,"For instance , the negation in is neglected when a model is restricted to only looking at the dependency path between the entities .",Models,Contextualized GCN,relation-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",58,0.8787878787878788,93,0.3536121673003802,17,0.68,1,0,Models: Contextualized GCN
95,"Similarly , in the sentence "" She was diagnosed with cancer last year , and succumbed this June "" , the dependency path She?diagnosed ?",Models,Contextualized GCN,relation-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",59,0.8939393939393939,94,0.3574144486692015,18,0.72,1,0,Models: Contextualized GCN
96,cancer is not sufficient to establish that cancer is the cause of death for the subject unless the conjunction dependency to succumbed is also present .,Models,Contextualized GCN,relation-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",60,0.9090909090909092,95,0.3612167300380228,19,0.76,1,0,Models: Contextualized GCN
98,This is achieved by including tokens that are up to distance K away from the dependency path in the LCA subtree .,Models,Contextualized GCN,relation-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",62,0.9393939393939394,97,0.3688212927756654,21,0.84,1,0,Models: Contextualized GCN
99,"K = 0 , corresponds to pruning the tree down to the path , K = 1 keeps all nodes that are directly attached to the path , and K = ?",Models,Contextualized GCN,relation-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",63,0.9545454545454546,98,0.3726235741444867,22,0.88,1,0,Models: Contextualized GCN
103,Related Work,,,relation-classification,4,"['O', 'O']","['O', 'O']",0,0.0,102,0.3878326996197718,0,0.0,1,0,
121,Experiments,,,relation-classification,4,['O'],['O'],0,0.0,120,0.4562737642585551,0,0.0,1,0,
122,Baseline Models,,,relation-classification,4,"['O', 'O']","['O', 'O']",0,0.0,121,0.4600760456273764,0,0.0,1,0,
131,"Earlier , our group compared and with sequence models , and we report these results ; for ( 3 ) we report results with our own implementation .",Baseline Models,Baseline Models,relation-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.6923076923076923,130,0.494296577946768,9,0.6923076923076923,1,0,Baseline Models
135,3,Baseline Models,Baseline Models,relation-classification,4,['O'],['O'],13,1.0,134,0.5095057034220533,13,1.0,1,0,Baseline Models
136,Experimental Setup,,,relation-classification,4,"['O', 'O']","['O', 'O']",0,0.0,135,0.5133079847908745,0,0.0,1,0,
139,It represents 41 relation types and a special no relation class when the mention pair does not have a relation between them within these categories .,Experimental Setup,Experimental Setup,relation-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.0714285714285714,138,0.5247148288973384,3,0.25,1,0,Experimental Setup
140,"Mentions in TACRED are typed , with subjects categorized into person and organization , and objects into 16 fine - grained types ( e.g. , date and location ) .",Experimental Setup,Experimental Setup,relation-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.0952380952380952,139,0.5285171102661597,4,0.3333333333333333,1,0,Experimental Setup
141,We report micro-averaged F 1 scores on this dataset as is conventional .,Experimental Setup,Experimental Setup,relation-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.119047619047619,140,0.532319391634981,5,0.4166666666666667,1,0,Experimental Setup
142,"For fair comparisons on the TACRED dataset , we follow the evaluation protocol used in by selecting the model with the median dev F 1 from 5 independent runs and reporting its test F 1 .",Experimental Setup,Experimental Setup,relation-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.1428571428571428,141,0.5361216730038023,6,0.5,1,0,Experimental Setup
145,"Traditionally , evaluation on SemEval is conducted without entity mentions masked .",Experimental Setup,Experimental Setup,relation-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.2142857142857142,144,0.5475285171102662,9,0.75,1,0,Experimental Setup
146,"However , as we will discuss in Section 6.4 , this method encourages models to overfit to these mentions and fails to test their actual ability to generalize .",Experimental Setup,Experimental Setup,relation-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.238095238095238,145,0.5513307984790875,10,0.8333333333333334,1,0,Experimental Setup
158,"Given the output probabilities PG ( r|x ) from a GCN model and PS ( r|x ) from the sequence model for any relation r , we calculate the interpolated probability as",Experimental Setup,Results on the TACRED Dataset,relation-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.5238095238095238,157,0.596958174904943,9,0.6923076923076923,1,0,Experimental Setup: Results on the TACRED Dataset
159,where ? ?,Experimental Setup,Results on the TACRED Dataset,relation-classification,4,"['O', 'O', 'O']","['O', 'O', 'O']",23,0.5476190476190477,158,0.6007604562737643,10,0.7692307692307693,1,0,Experimental Setup: Results on the TACRED Dataset
163,Results on the SemEval Dataset,Experimental Setup,,relation-classification,4,"['O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O']",27,0.6428571428571429,162,0.6159695817490495,0,0.0,1,0,Experimental Setup
168,Effect of Path - centric,Experimental Setup,Results on the SemEval Dataset,relation-classification,4,"['O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O']",32,0.7619047619047619,167,0.6349809885931559,5,0.3571428571428571,1,0,Experimental Setup: Results on the SemEval Dataset
169,Pruning,Experimental Setup,,relation-classification,4,['O'],['O'],33,0.7857142857142857,168,0.6387832699619772,6,0.4285714285714285,1,0,Experimental Setup
174,Miwa and Bansal ( 2016 ) reported that a Tree - LSTM achieves similar performance when the dependency path and the LCA subtree are used respectively .,Experimental Setup,Pruning,relation-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",38,0.9047619047619048,173,0.6577946768060836,11,0.7857142857142857,1,0,Experimental Setup: Pruning
179,Ablation Study,,,relation-classification,4,"['O', 'O']","['O', 'O']",0,0.0,178,0.6768060836501901,0,0.0,1,0,
180,"To study the contribution of each component in the C - GCN model , we ran an ablation study on the TACRED dev set ) .",Ablation Study,Ablation Study,relation-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0526315789473684,179,0.6806083650190115,1,0.0526315789473684,1,0,Ablation Study
185,Complementary Strengths of GCNs and PA - LSTMs,Ablation Study,Ablation Study,relation-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.3157894736842105,184,0.6996197718631179,6,0.3157894736842105,1,0,Ablation Study
188,"For clarity , we removed edges which 1 ) connect to common punctuation ( i.e. , commas , periods , and quotation marks ) , 2 ) connect to common prepositions ( i.e. , of , to , by ) , and 3 ) connect between tokens within the same entity .",Ablation Study,Ablation Study,relation-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.4736842105263157,187,0.7110266159695817,9,0.4736842105263157,1,0,Ablation Study
197,"We further show that this difference is due to each model 's competitive advantage : dependency - based models are better at handling sentences with entities farther apart , while sequence models can better leverage local word patterns regardless of parsing quality ( see also .",Ablation Study,Ablation Study,relation-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.9473684210526316,196,0.7452471482889734,18,0.9473684210526316,1,0,Ablation Study
200,"To gain more insights into the C - GCN model 's behavior , we visualized the partial dependency tree it is processing and how much each token 's final representation contributed to h sent ( ) .",Understanding Model Behavior,Understanding Model Behavior,relation-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0476190476190476,199,0.7566539923954373,1,0.1428571428571428,1,0,Understanding Model Behavior
203,"To further understand what dependency edges contribute most to the classification of different relations , we scored each dependency edge by summing up the number of dimensions each of its connected nodes contributed to h sent .",Understanding Model Behavior,Understanding Model Behavior,relation-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.1904761904761904,202,0.7680608365019012,4,0.5714285714285714,1,0,Understanding Model Behavior
205,"As can be seen in the table , most of these edges are associated with indicative nouns or verbs of each relation .",Understanding Model Behavior,Understanding Model Behavior,relation-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.2857142857142857,204,0.7756653992395437,6,0.8571428571428571,1,0,Understanding Model Behavior
206,5,Understanding Model Behavior,Understanding Model Behavior,relation-classification,4,['O'],['O'],7,0.3333333333333333,205,0.779467680608365,7,1.0,1,0,Understanding Model Behavior
213,phenomenon .,Understanding Model Behavior,Entity Bias in the SemEval Dataset,relation-classification,4,"['O', 'O']","['O', 'O']",14,0.6666666666666666,212,0.8060836501901141,6,0.4615384615384615,1,0,Understanding Model Behavior: Entity Bias in the SemEval Dataset
214,"We started by simplifying every sentence in the SemEval training and dev sets to "" subject and object "" , where subject and object are the actual entities in the sentence .",Understanding Model Behavior,Entity Bias in the SemEval Dataset,relation-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.7142857142857143,213,0.8098859315589354,7,0.5384615384615384,1,0,Understanding Model Behavior: Entity Bias in the SemEval Dataset
219,This suggests that models trained without entities masked generalize poorly to new examples with unseen entities .,Understanding Model Behavior,Entity Bias in the SemEval Dataset,relation-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.9523809523809524,218,0.8288973384030418,12,0.9230769230769232,1,0,Understanding Model Behavior: Entity Bias in the SemEval Dataset
221,Conclusion,,,relation-classification,4,['O'],['O'],0,0.0,220,0.8365019011406845,0,0.0,1,0,
236,Sem Eval,A.1 Hyperparameters TACRED,,relation-classification,4,"['O', 'O']","['O', 'O']",10,0.7142857142857143,235,0.8935361216730038,11,0.7333333333333333,1,0,A.1 Hyperparameters TACRED
247,"In our experiments we found that the output vector h sent tends to have large magnitude , and : Aggregated 5 - run difference compared to PA - LSTM on the TACRED dev set .",A.2 Training,A.2 Training,relation-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.2727272727272727,246,0.935361216730038,6,0.2727272727272727,1,0,A.2 Training
248,"For each example , if X out of 5 GCN models predicted its label correctly and Y PA - LSTM models did , it is aggregated in the bar labeled X ?",A.2 Training,A.2 Training,relation-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.3181818181818182,247,0.9391634980988594,7,0.3181818181818182,1,0,A.2 Training
249,Y .,A.2 Training,,relation-classification,4,"['O', 'O']","['O', 'O']",8,0.3636363636363636,248,0.9429657794676806,8,0.3636363636363636,1,0,A.2 Training
251,therefore adding the following regularization term to the cross entropy loss of each example improves the results :,A.2 Training,Y .,relation-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.4545454545454545,250,0.9505703422053232,10,0.4545454545454545,1,0,A.2 Training: Y .
252,"Here , reg functions as an l 2 regularization on the learned sentence representations .",A.2 Training,Y .,relation-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.5,251,0.9543726235741444,11,0.5,1,0,A.2 Training: Y .
253,?,A.2 Training,Y .,relation-classification,4,['O'],['O'],12,0.5454545454545454,252,0.9581749049429658,12,0.5454545454545454,1,0,A.2 Training: Y .
262,This smaller difference is also reflected in the diminished gain from ensembling with the PA - LSTM shown in .,A.2 Training,Y .,relation-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.9545454545454546,261,0.9923954372623576,21,0.9545454545454546,1,0,A.2 Training: Y .
263,We hypoth -,A.2 Training,Y .,relation-classification,4,"['O', 'O', 'O']","['O', 'O', 'O']",22,1.0,262,0.9961977186311788,22,1.0,1,0,A.2 Training: Y .
3,abstract,,,relation-classification,5,['O'],['O'],0,0.0,2,0.0176991150442477,0,0.0,1,0,
9,"For example , given the sentence "" David Foster is the AP 's Northwest regional reporter , based in Seattle "" in the CoNLL04 dataset , our goal is to recognize "" David Foster "" as person , "" AP "" as organization , and "" Northwest "" and "" Seattle "" as location entities , then classifiy entity pairs to extract structured information : Work For ( David Foster , AP ) , OrgBased In ( AP , Northwest ) and OrgBased In ( AP , Seattle ) .",Introduction,Introduction,relation-classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.074074074074074,8,0.0707964601769911,2,0.074074074074074,1,0,Introduction
10,Such information is useful in many other NLP tasks .,Introduction,Introduction,relation-classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.1111111111111111,9,0.079646017699115,3,0.1111111111111111,1,0,Introduction
13,"More recently , end - to - end systems which jointly learn to extract entities and relations have been proposed with strong potential to obtain high performance .",Introduction,Introduction,relation-classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.2222222222222222,12,0.1061946902654867,6,0.2222222222222222,1,0,Introduction
14,Traditional joint approaches are feature - based supervised learning methods which employ numerous syntactic and lexical features based on external NLP tools as well as knowledge base resources .,Introduction,Introduction,relation-classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.2592592592592592,13,0.1150442477876106,7,0.2592592592592592,1,0,Introduction
18,Other neural models employ dependency parsing - based information .,Introduction,Their approach relies on various manually extracted features .,relation-classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.4074074074074074,17,0.1504424778761062,11,0.4074074074074074,1,0,Introduction: Their approach relies on various manually extracted features .
22,"Adel and Schtze ( 2017 ) assumed that entity boundaries are given , and trained a CNN to extract context features around the entities , and using these features for entity and relation classification .",Introduction,Their approach relies on various manually extracted features .,relation-classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.5555555555555556,21,0.1858407079646017,15,0.5555555555555556,1,0,Introduction: Their approach relies on various manually extracted features .
29,"In most previous neural joint models , the relation classification part relies on a common "" linear "" concatenation - based mechanism over the latent features associated with entity pairs , i.e. the latent features are first concatenated into a single feature vector which is then linearly transformed before being fed into a softmax classifier .",Introduction,Their approach relies on various manually extracted features .,relation-classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.8148148148148148,28,0.247787610619469,22,0.8148148148148148,1,0,Introduction: Their approach relies on various manually extracted features .
35,Our proposed model,,,relation-classification,5,"['O', 'O', 'O']","['O', 'O', 'O']",0,0.0,34,0.3008849557522124,0,0.0,1,0,
37,"Given an input sequence of n word tokens w 1 , w 2 , ... , w n , we use a vector vi to represent each i th word w i by concatenating word embedding e",Our proposed model,Our proposed model,relation-classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.0869565217391304,36,0.3185840707964602,2,0.0869565217391304,1,0,Our proposed model
39,Named entity recognition ( NER ) :,Our proposed model,Our proposed model,relation-classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.1739130434782608,38,0.336283185840708,4,0.1739130434782608,1,0,Our proposed model
42,The output layer size of FFNN NER is the number of BIOLU - based NER labels .,Our proposed model,Our proposed model,relation-classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.3043478260869565,41,0.3628318584070796,7,0.3043478260869565,1,0,Our proposed model
46,"Relation classification ( RC ) : Assume that t 1 , t 2 , ... , tn are NER labels predicted by the NER component for the input words .",Our proposed model,Our proposed model,relation-classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.4782608695652174,45,0.3982300884955752,11,0.4782608695652174,1,0,Our proposed model
47,We represent each i th predicted label by a vector embedding e ti .,Our proposed model,Our proposed model,relation-classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.5217391304347826,46,0.4070796460176991,12,0.5217391304347826,1,0,Our proposed model
48,We create a sequence of vectors x 1:n in which each x i is computed as :,Our proposed model,Our proposed model,relation-classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.5652173913043478,47,0.415929203539823,13,0.5652173913043478,1,0,Our proposed model
49,"As for NER , the RC component also uses a BiLSTM ( BiLSTM RC ) to learn another set of latent feature vectors , but from the sequence x 1:n :",Our proposed model,Our proposed model,relation-classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.6086956521739131,48,0.4247787610619469,14,0.6086956521739131,1,0,Our proposed model
50,The RC component further uses these latent vectors r i for relation classification .,Our proposed model,Our proposed model,relation-classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.6521739130434783,49,0.4336283185840708,15,0.6521739130434783,1,0,Our proposed model
55,We assign an entity pair to a negative relation class ( NEG ) when the pair has no relation or when the predicted entities are not correct .,Our proposed model,Our proposed model,relation-classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.8695652173913043,54,0.4778761061946903,20,0.8695652173913043,1,0,Our proposed model
56,"For example , for , we would have two relation candidates : NEG ( Paris , International ) and OrgBased In ( International , Paris ) .",Our proposed model,Our proposed model,relation-classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.9130434782608696,55,0.4867256637168141,21,0.9130434782608696,1,0,Our proposed model
57,"Then for each head - tail candidate pair ( w j , wk ) , we apply the biaffine attention operator :",Our proposed model,Our proposed model,relation-classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.9565217391304348,56,0.495575221238938,22,0.9565217391304348,1,0,Our proposed model
58,3 Experiments,Our proposed model,Our proposed model,relation-classification,5,"['O', 'O']","['O', 'O']",23,1.0,57,0.504424778761062,23,1.0,1,0,Our proposed model
59,Experimental setup,,,relation-classification,5,"['O', 'O']","['O', 'O']",0,0.0,58,0.5132743362831859,0,0.0,1,0,
62,( 1 ) NER&RC :,Experimental setup,Experimental setup,relation-classification,5,"['O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O']",3,0.1428571428571428,61,0.5398230088495575,3,0.1428571428571428,1,0,Experimental setup
63,A realistic scenario where entity boundaries are not given .,Experimental setup,Experimental setup,relation-classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.1904761904761904,62,0.5486725663716814,4,0.1904761904761904,1,0,Experimental setup
64,( 2 ) EC&RC : A less realistic scenario where the entity boundaries are given ] .,Experimental setup,Experimental setup,relation-classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.238095238095238,63,0.5575221238938053,5,0.238095238095238,1,0,Experimental setup
65,Thus the NER task which identifies both entity boundaries and classes reduces to the entity classification ( EC ) task .,Experimental setup,Experimental setup,relation-classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.2857142857142857,64,0.5663716814159292,6,0.2857142857142857,1,0,Experimental setup
68,"As a result , the vector vi in Equation 1 now also includes the boundary tag embedding in addition to the word embedding and character - level word embedding .",Experimental setup,Experimental setup,relation-classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.4285714285714285,67,0.5929203539823009,9,0.4285714285714285,1,0,Experimental setup
70,"Following , we use the 64%/16%/20 % training / development / test presplit available from Adel and Schtze ( 2017 ) , in which the test set was previously also used by .",Experimental setup,Experimental setup,relation-classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.5238095238095238,69,0.6106194690265486,11,0.5238095238095238,1,0,Experimental setup
76,More details of the implementation as well as optimal hyper - parameters are in the Appendix .,Experimental setup,Our model is implemented using DYNET v 2.0 .,relation-classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.8095238095238095,75,0.6637168141592921,17,0.8095238095238095,1,0,Experimental setup: Our model is implemented using DYNET v 2.0 .
79,More details of the metric are also in the Appendix .,Experimental setup,Our code is available at : https : //github.com/datquocnguyen/jointRE,relation-classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.9523809523809524,78,0.6902654867256637,20,0.9523809523809524,1,0,Experimental setup: Our code is available at : https : //github.com/datquocnguyen/jointRE
81,Main results,,,relation-classification,5,"['O', 'O']","['O', 'O']",0,0.0,80,0.7079646017699115,0,0.0,1,0,
86,"Note that Gupta et al. ( 2016 ) use the same test set as we do , however they report final results on a 80/0 / 20 training / development / test split rather than our 64/16 /20 , i.e. Gupta et al. ( 2016 ) use a larger training set , but producing about 1.5 % lower EC score and similar RC score against ours .",Main results,Main results,relation-classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.1785714285714285,85,0.7522123893805309,5,0.1785714285714285,1,0,Main results
88,"In , the last two rows present results reported in and on the dataset CoNLL04 .",Main results,Main results,relation-classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.25,87,0.7699115044247787,7,0.25,1,0,Main results
89,"However , these results are not comparable due to their random sampling of the test set , i.e. using different train - test splits .",Main results,Main results,relation-classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.2857142857142857,88,0.7787610619469026,8,0.2857142857142857,1,0,Main results
92,We plan to extend our model with their syntactic integration approach to further improve our model performance in future work .,Main results,Main results,relation-classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.3928571428571428,91,0.8053097345132744,11,0.3928571428571428,1,0,Main results
93,Ablation analysis :,Main results,Main results,relation-classification,5,"['O', 'O', 'O']","['O', 'O', 'O']",12,0.4285714285714285,92,0.8141592920353983,12,0.4285714285714285,1,0,Main results
95,"Here , the RC network uses gold NER labels when training , and uses predicted labels produced by the NER network when decoding .",Main results,Main results,relation-classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.5,94,0.831858407079646,14,0.5,1,0,Main results
97,"Ablation results on the development set . * and ** denote the statistically significant differences against the full results at p < 0.05 and p < 0.01 , respectively ( using the two - tailed paired t- test ) .",Main results,Main results,relation-classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.5714285714285714,96,0.8495575221238938,16,0.5714285714285714,1,0,Main results
98,differences are not significant .,Main results,Main results,relation-classification,5,"['O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O']",17,0.6071428571428571,97,0.8584070796460177,17,0.6071428571428571,1,0,Main results
99,A similar observation is also found in .,Main results,,relation-classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.6428571428571429,98,0.8672566371681416,18,0.6428571428571429,1,0,Main results
101,This is not surprising as the training NER score is at 99 +% .,Main results,A similar observation is also found in .,relation-classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.7142857142857143,100,0.8849557522123894,20,0.7142857142857143,1,0,Main results: A similar observation is also found in .
108,"This is reasonable because those 3 factors are part of the RC component only , thus helpful in predicting relations .",Main results,"However , they significantly decrease the RC score .",relation-classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.9642857142857144,107,0.9469026548672568,27,0.9642857142857144,1,0,"Main results: However , they significantly decrease the RC score ."
110,Conclusion,,,relation-classification,5,['O'],['O'],0,0.0,109,0.9646017699115044,0,0.0,1,0,
3,abstract,,,relation-classification,6,['O'],['O'],0,0.0,2,0.011049723756906,0,0.0,1,0,
5,"Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) .",abstract,abstract,relation-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.3333333333333333,4,0.0220994475138121,2,0.3333333333333333,1,0,abstract
6,"In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification .",abstract,abstract,relation-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.5,5,0.0276243093922651,3,0.5,1,0,abstract
13,"For example , given a sentence with tagged entity pair , crash and attack , this sentence is classified into the re-lation Cause - Effect ( e1 , e2 ) 1 between the entity pair like .",Introduction,Introduction,relation-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.1875,12,0.0662983425414364,3,0.1875,1,0,Introduction
14,"A first entity is surrounded by e 1 and / e 1 , and a second entity is surrounded by e 2 and / e 2 .",Introduction,Introduction,relation-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.25,13,0.0718232044198895,4,0.25,1,0,Introduction
15,"Most previous relation classification models rely heavily on high - level lexical and syntactic features obtained from NLP tools such as WordNet , dependency parser , part - of - speech ( POS ) tagger , and named entity recognizer ( NER ) .",Introduction,Introduction,relation-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.3125,14,0.0773480662983425,5,0.3125,1,0,Introduction
16,The classification models relying on such features suffer from propagation of implicit error of the tools and they are computationally expensive .,Introduction,Introduction,relation-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.375,15,0.0828729281767955,6,0.375,1,0,Introduction
17,"Recently , many studies therefore propose end - toend neural models without the high - level features .",Introduction,Introduction,relation-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.4375,16,0.0883977900552486,7,0.4375,1,0,Introduction
19,"Since these models are mainly proposed for solving translation and language modeling tasks , they could not fully utilize the information of tagged entities in relation classification task .",Introduction,Introduction,relation-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.5625,18,0.0994475138121546,9,0.5625,1,0,Introduction
20,"However , tagged entity pairs could be powerful hints for solving relation classification task .",Introduction,Introduction,relation-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.625,19,0.1049723756906077,10,0.625,1,0,Introduction
24,The contributions of our work are summarized as follows :,Introduction,Introduction,relation-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.875,23,0.1270718232044199,14,0.875,1,0,Introduction
27,Related Work,,,relation-classification,6,"['O', 'O']","['O', 'O']",0,0.0,26,0.143646408839779,0,0.0,1,0,
47,Model,,,relation-classification,6,['O'],['O'],0,0.0,46,0.2541436464088398,0,0.0,1,0,
51,Word Representation,Model,,relation-classification,6,"['O', 'O']","['O', 'O']",4,0.0526315789473684,50,0.2762430939226519,4,0.4,1,0,Model
52,Let a input sentence is denoted by,Model,Word Representation,relation-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.0657894736842105,51,0.281767955801105,5,0.5,1,0,Model: Word Representation
53,where n is the number of words .,Model,Word Representation,relation-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.0789473684210526,52,0.287292817679558,6,0.6,1,0,Model: Word Representation
54,We transform each word into vector representations by looking up word embedding matrix W word ?,Model,Word Representation,relation-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.0921052631578947,53,0.292817679558011,7,0.7,1,0,Model: Word Representation
55,"R dw |V | , where d w is the dimension of the vector and | V | is the size of vocabulary .",Model,Word Representation,relation-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.1052631578947368,54,0.2983425414364641,8,0.8,1,0,Model: Word Representation
56,"Then the word representations X = {x 1 , x 2 , ... , x n } are obtained by mapping w i , the i - th word , to a column vector xi ?",Model,Word Representation,relation-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.1184210526315789,55,0.3038674033149171,9,0.9,1,0,Model: Word Representation
57,R dw are fed into the next layer .,Model,Word Representation,relation-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.131578947368421,56,0.3093922651933701,10,1.0,1,0,Model: Word Representation
61,"The illustrates the results of the self attention in the sentence , "" the ?e1 ? pollution ? / e1 ?was caused by the ?e2 ? shipwrek ?/e2 ? "" , which is labeled Cause - Effect ( e1 , e2 ) .",Model,Self Attention,relation-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.1842105263157894,60,0.3314917127071823,3,0.081081081081081,1,0,Model: Self Attention
63,"The color density indicates the attention values , results of Equation 3.1 , which means how much an entity focuses on each word in a sentence .",Model,Self Attention,relation-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.2105263157894736,62,0.3425414364640884,5,0.1351351351351351,1,0,Model: Self Attention
64,"In , the left represents the words that pollution , the first entity , focuses on and the right represents the words that shipwreck , the second entity , focuses on .",Model,Self Attention,relation-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.2236842105263158,63,0.3480662983425414,6,0.1621621621621621,1,0,Model: Self Attention
65,"We can recognize that the entity pair is commonly concentrated on was , caused , and each other .",Model,Self Attention,relation-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.2368421052631578,64,0.3535911602209944,7,0.1891891891891892,1,0,Model: Self Attention
66,"Actually , these words play the most important role in semantically predicting the Cause - Effect ( e1 , e2 ) , which is the relation class of this entity pair .",Model,Self Attention,relation-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.25,65,0.3591160220994475,8,0.2162162162162162,1,0,Model: Self Attention
67,"shows where the model focuses on the sentence to compute relations between entity pairs , which is the result of visualizing the alpha vectors in Equation 3.9 .",Model,Self Attention,relation-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.2631578947368421,66,0.3646408839779005,9,0.2432432432432432,1,0,Model: Self Attention
68,"The important words in sentence are highlighted in yellow , which means that the more clearly the color is , the more important it is .",Model,Self Attention,relation-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.2763157894736842,67,0.3701657458563536,10,0.2702702702702703,1,0,Model: Self Attention
69,"For example , in the first sentence , the inside is strongly highlighted , which is actually the best word representing the relation Component - whole ( e 1 , e2 ) between the given entity pair .",Model,Self Attention,relation-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.2894736842105263,68,0.3756906077348066,11,0.2972972972972973,1,0,Model: Self Attention
70,"As another example , in the third sentence , the highlighted assess and using represent the relation , Instrument - Agency ( e2 , e1 ) between entity pair , analysts and frequency , well .",Model,Self Attention,relation-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",23,0.3026315789473684,69,0.3812154696132597,12,0.3243243243243243,1,0,Model: Self Attention
72,"visualizes latent type representation t j? { 1 , 2 } in Equation 3.12",Model,Self Attention,relation-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.3289473684210526,71,0.3922651933701657,14,0.3783783783783784,1,0,Model: Self Attention
74,"In , the red points represent latent type vectors c i?K and the rests are latent type representations t j , where the colors of points are determined by the closest of the latent type vectors in the vector space of the original dimensionality .",Model,Self Attention,relation-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.3552631578947368,73,0.4033149171270718,16,0.4324324324324324,1,0,Model: Self Attention
79,"In the type 1 , the words are related to human 's jobs and foods .",Model,Self Attention,relation-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",32,0.4210526315789473,78,0.430939226519337,21,0.5675675675675675,1,0,Model: Self Attention
80,"The type2 has a lot of entities related to machines and engineering like engine , woofer , and motor .",Model,Self Attention,relation-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",33,0.4342105263157895,79,0.43646408839779,22,0.5945945945945946,1,0,Model: Self Attention
81,"Finally , in type3 , there are many words with bad meanings related associated with disasters and :",Model,Self Attention,relation-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",34,0.4473684210526316,80,0.4419889502762431,23,0.6216216216216216,1,0,Model: Self Attention
84,Bidirectional LSTM,Model,,relation-classification,6,"['O', 'O']","['O', 'O']",37,0.4868421052631579,83,0.4585635359116022,26,0.7027027027027027,1,0,Model
85,Network,Model,,relation-classification,6,['O'],['O'],38,0.5,84,0.4640883977900552,27,0.7297297297297297,1,0,Model
88,The representation vectors M obtained from self attention layer are forwarded into to the network step by step .,Model,Network,relation-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",41,0.5394736842105263,87,0.4806629834254143,30,0.8108108108108109,1,0,Model: Network
89,"At the time step t , the hidden state",Model,Network,relation-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",42,0.5526315789473685,88,0.4861878453038674,31,0.8378378378378378,1,0,Model: Network
90,Entity - aware Attention Mechanism,Model,Network,relation-classification,6,"['O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O']",43,0.5657894736842105,89,0.4917127071823204,32,0.8648648648648649,1,0,Model: Network
91,Although many models with attention mechanism achieved state - of - the - art performance in many NLP tasks .,Model,Network,relation-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",44,0.5789473684210527,90,0.4972375690607735,33,0.8918918918918919,1,0,Model: Network
92,"However , for the relation classification task , these models lack of prior knowledge forgiven entity pairs , which could be powerful hints for solving the task .",Model,Network,relation-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",45,0.5921052631578947,91,0.5027624309392266,34,0.918918918918919,1,0,Model: Network
93,Relation classification differs from sentence classification in that information about entities is given along with sentences .,Model,Network,relation-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",46,0.6052631578947368,92,0.5082872928176796,35,0.945945945945946,1,0,Model: Network
95,"Entity - aware attention utilizes the two additional features except H = {h 1 , h 2 , ... , h n } , ( 1 ) relative position features , ( 2 ) entity features with LET , and the final sentence representation z , result of the attention , is computed as follows :",Model,Network,relation-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",48,0.631578947368421,94,0.5193370165745856,37,1.0,1,0,Model: Network
97,"In relation classification , the position of each word relative to entities has been widely used for word representations .",Model,Relative Position Features,relation-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",50,0.6578947368421053,96,0.5303867403314917,1,0.0833333333333333,1,0,Model: Relative Position Features
98,"Recently , position - aware attention is published as away to use the relative position features more effectively .",Model,Relative Position Features,relation-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",51,0.6710526315789473,97,0.5359116022099447,2,0.1666666666666666,1,0,Model: Relative Position Features
101,"In the equation , p e 1 i ?",Model,Relative Position Features,relation-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",54,0.7105263157894737,100,0.5524861878453039,5,0.4166666666666667,1,0,Model: Relative Position Features
102,R dp and p e 2 i ?,Model,Relative Position Features,relation-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",55,0.7236842105263158,101,0.5580110497237569,6,0.5,1,0,Model: Relative Position Features
103,"R dp corresponds to the position of the i - th word relative to the first entity ( e 1 - th word ) and second entity ( e 2 - th word ) in a sentence respectively , where e j ?{ 1 , 2 } is a index of j-th entity .",Model,Relative Position Features,relation-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",56,0.7368421052631579,102,0.56353591160221,7,0.5833333333333334,1,0,Model: Relative Position Features
104,"Similar to word embeddings , the relative positions are converted to vector representations by looking up learnable embedding matrix W pos ?",Model,Relative Position Features,relation-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",57,0.75,103,0.569060773480663,8,0.6666666666666666,1,0,Model: Relative Position Features
105,"R dp ( 2L?1 ) , where d p is the dimension of the relative position vectors and L is the maximum sentence length .",Model,Relative Position Features,relation-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",58,0.7631578947368421,104,0.574585635359116,9,0.75,1,0,Model: Relative Position Features
107,The representation is linearly transformed by W H ?,Model,Relative Position Features,relation-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",60,0.7894736842105263,106,0.585635359116022,11,0.9166666666666666,1,0,Model: Relative Position Features
108,R da ( 2 d h + 2 dp ) as in the Equation 3.8 .,Model,Relative Position Features,relation-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",61,0.8026315789473685,107,0.5911602209944752,12,1.0,1,0,Model: Relative Position Features
113,These are denoted by h ei ?,Model,Entity Features with Latent Type,relation-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O']",66,0.868421052631579,112,0.6187845303867403,4,0.2857142857142857,1,0,Model: Entity Features with Latent Type
114,"R 2d h , where e i is index of i - th entity .",Model,Entity Features with Latent Type,relation-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",67,0.881578947368421,113,0.6243093922651933,5,0.3571428571428571,1,0,Model: Entity Features with Latent Type
119,The mathematical formulation is the follows :,Model,Entity Features with Latent Type,relation-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O']",72,0.9473684210526316,118,0.6519337016574586,10,0.7142857142857143,1,0,Model: Entity Features with Latent Type
120,where c i is the i - th latent type vector and K is the number of latent entity types .,Model,Entity Features with Latent Type,relation-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",73,0.9605263157894736,119,0.6574585635359116,11,0.7857142857142857,1,0,Model: Entity Features with Latent Type
122,"After linear transformation of the entity features , they add up with the representations of BLSTM layer as in Equation 3.8 , and the representation of sentence z ?",Model,Entity Features with Latent Type,relation-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",75,0.986842105263158,121,0.6685082872928176,13,0.9285714285714286,1,0,Model: Entity Features with Latent Type
123,R 2 d h is computed by Equations from 3.8 to 3.10 .,Model,Entity Features with Latent Type,relation-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",76,1.0,122,0.6740331491712708,14,1.0,1,0,Model: Entity Features with Latent Type
124,Classification and Training,,,relation-classification,6,"['O', 'O', 'O']","['O', 'O', 'O']",0,0.0,123,0.6795580110497238,0,0.0,1,0,
127,where y is a target relation class and S is the input sentence .,Classification and Training,Classification and Training,relation-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.2727272727272727,126,0.6961325966850829,3,0.2727272727272727,1,0,Classification and Training
128,The ?,Classification and Training,Classification and Training,relation-classification,6,"['O', 'O']","['O', 'O']",4,0.3636363636363636,127,0.7016574585635359,4,0.3636363636363636,1,0,Classification and Training
129,is whole learnable parameters in the whole network including,Classification and Training,Classification and Training,relation-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.4545454545454545,128,0.7071823204419889,5,0.4545454545454545,1,0,Classification and Training
130,where | R| is the number of relation classes .,Classification and Training,Classification and Training,relation-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.5454545454545454,129,0.712707182320442,6,0.5454545454545454,1,0,Classification and Training
131,"A loss function L is the cross entropy between the predictions and the ground truths , which is defined as :",Classification and Training,Classification and Training,relation-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.6363636363636364,130,0.7182320441988951,7,0.6363636363636364,1,0,Classification and Training
132,"where | D| is the size of training dataset and ( S ( i ) , y ( i ) ) is the i - th sample in the dataset .",Classification and Training,Classification and Training,relation-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.7272727272727273,131,0.7237569060773481,8,0.7272727272727273,1,0,Classification and Training
136,Experiments,,,relation-classification,6,['O'],['O'],0,0.0,135,0.7458563535911602,0,0.0,1,0,
137,Dataset and Evaluation Metrics,,,relation-classification,6,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",0,0.0,136,0.7513812154696132,0,0.0,1,0,
140,"The former 9 relations have two directions , whereas Other is not directional , so the total number of relations is 19 .",Dataset and Evaluation Metrics,Dataset and Evaluation Metrics,relation-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.6,139,0.7679558011049724,3,0.6,1,0,Dataset and Evaluation Metrics
143,Implementation Details,,,relation-classification,6,"['O', 'O']","['O', 'O']",0,0.0,142,0.7845303867403315,0,0.0,1,0,
146,Hyperparameter,Implementation Details,,relation-classification,6,['O'],['O'],3,0.2142857142857142,145,0.8011049723756906,0,0.0,1,0,Implementation Details
151,They used many handcraft feature and SVM classifier .,Implementation Details,Hyperparameter,relation-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.5714285714285714,150,0.8287292817679558,5,0.4545454545454545,1,0,Implementation Details: Hyperparameter
155,"Actually , the SDP - based models show high performance , but SDP may not always be accurate and the parsing time is exponentially increased by long sentences .",Implementation Details,Hyperparameter,relation-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.8571428571428571,154,0.850828729281768,9,0.8181818181818182,1,0,Implementation Details: Hyperparameter
156,The last model is End - to - End Model automatically learned internal representations can occur between the original inputs and the final outputs in deep learning .,Implementation Details,Hyperparameter,relation-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.9285714285714286,155,0.856353591160221,10,0.9090909090909092,1,0,Implementation Details: Hyperparameter
157,"There are CNN - based models such as CNN , CR - CNN , and Attention - CNN and RNN - based models such as BLSTM , Attention - BLSTM , and Hierarchical - BLSTM ( Hier - BLSTM ) for this task .",Implementation Details,Hyperparameter,relation-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,1.0,156,0.861878453038674,11,1.0,1,0,Implementation Details: Hyperparameter
158,Experimental Results,,,relation-classification,6,"['O', 'O']","['O', 'O']",0,0.0,157,0.8674033149171271,0,0.0,1,0,
159,Model F1,,,relation-classification,6,"['O', 'O']","['O', 'O']",0,0.0,158,0.8729281767955801,0,0.0,1,0,
161,"However , they rely on high - level lexical features such as WordNet , dependency parse trees , POS tags , and NER tags from NLP tools .",Model F1,Model F1,relation-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.1428571428571428,160,0.8839779005524862,2,0.4,1,0,Model F1
165,Visualization,Model F1,,relation-classification,6,['O'],['O'],6,0.4285714285714285,164,0.9060773480662984,0,0.0,1,0,Model F1
174,Conclusion,,,relation-classification,6,['O'],['O'],0,0.0,173,0.9558011049723756,0,0.0,1,0,
3,abstract,,,relation-classification,7,['O'],['O'],0,0.0,2,0.0100502512562814,0,0.0,1,0,
4,Motivation :,abstract,abstract,relation-classification,7,"['O', 'O']","['O', 'O']",1,0.0833333333333333,3,0.0150753768844221,1,0.0833333333333333,1,0,abstract
5,Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows .,abstract,abstract,relation-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.1666666666666666,4,0.0201005025125628,2,0.1666666666666666,1,0,abstract
6,"With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models .",abstract,abstract,relation-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.25,5,0.0251256281407035,3,0.25,1,0,abstract
7,"However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora .",abstract,abstract,relation-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.3333333333333333,6,0.0301507537688442,4,0.3333333333333333,1,0,abstract
9,Results :,abstract,abstract,relation-classification,7,"['O', 'O']","['O', 'O']",6,0.5,8,0.0402010050251256,6,0.5,1,0,abstract
17,The volume of biomedical literature continues to rapidly increase .,Introduction,Introduction,relation-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0625,16,0.0804020100502512,1,0.0625,1,0,Introduction
21,"Consequently , there is increasingly more demand for accurate biomedical text mining tools for extracting information from the literature .",Introduction,Introduction,relation-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.3125,20,0.100502512562814,5,0.3125,1,0,Introduction
22,Recent progress of biomedical text mining models was made possible by the advancements of deep learning techniques used in natural language processing ( NLP ) .,Introduction,Introduction,relation-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.375,21,0.1055276381909547,6,0.375,1,0,Introduction
24,Other deep learning based models have made improvements in biomedical text mining tasks such as relation extraction ( RE ) and question answering ( QA ) .,Introduction,Introduction,relation-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.5,23,0.1155778894472361,8,0.5,1,0,Introduction
25,"However , directly applying state - of - the - art NLP methodologies to biomedical text mining has limitations .",Introduction,Introduction,relation-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.5625,24,0.1206030150753768,9,0.5625,1,0,Introduction
26,"First , as recent word representation models such as Word2 Vec , ELMo and BERT are trained and tested mainly on datasets containing general domain texts ( e.g. Wikipedia ) , it is difficult to estimate their performance on datasets containing biomedical texts .",Introduction,Introduction,relation-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.625,25,0.1256281407035175,10,0.625,1,0,Introduction
27,"Also , the word distributions of general and biomedical corpora are quite different , which can often be a problem for biomedical text mining models .",Introduction,Introduction,relation-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.6875,26,0.1306532663316583,11,0.6875,1,0,Introduction
28,"As a result , recent models in biomedical text mining rely largely on adapted versions of word representations .",Introduction,Introduction,relation-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.75,27,0.135678391959799,12,0.75,1,0,Introduction
30,"Previously , Word2 Vec , which is one of the most widely known context independent word representation models , was trained on biomedical corpora which contain terms and expressions that are usually not included in a general domain corpus .",Introduction,Introduction,relation-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.875,29,0.1457286432160804,14,0.875,1,0,Introduction
31,"While ELMo and BERT have proven the effectiveness of contextualized word representations , they can not obtain high performance on biomedical corpora because they are pre-trained on only general domain corpora .",Introduction,Introduction,relation-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.9375,30,0.1507537688442211,15,0.9375,1,0,Introduction
33,Approach,,,relation-classification,7,['O'],['O'],0,0.0,32,0.1608040201005025,0,0.0,1,0,
41,The contributions of our paper are as follows :,Approach,Approach,relation-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.6153846153846154,40,0.2010050251256281,8,0.6153846153846154,1,0,Approach
47,Materials and methods,,,relation-classification,7,"['O', 'O', 'O']","['O', 'O', 'O']",0,0.0,46,0.2311557788944723,0,0.0,1,0,
48,BioBERT basically has the same structure as BERT .,Materials and methods,Materials and methods,relation-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0204081632653061,47,0.236180904522613,1,0.0769230769230769,1,0,Materials and methods
51,Learning word representations from a large amount of unannotated text is a long - established method .,Materials and methods,Materials and methods,relation-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.0816326530612244,50,0.2512562814070351,4,0.3076923076923077,1,0,Materials and methods
52,"While previous models ( e.g. Word2 Vec , GloVe ) focused on learning context independent word representations , recent works have focused on learning context dependent word representations .",Materials and methods,Materials and methods,relation-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.1020408163265306,51,0.2562814070351759,5,0.3846153846153846,1,0,Materials and methods
53,"For instance , ELMo uses a bidirectional language model , while uses machine translation to embed context information into word representations .",Materials and methods,Materials and methods,relation-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.1224489795918367,52,0.2613065326633166,6,0.4615384615384615,1,0,Materials and methods
55,"Due to the nature of language modeling where future words can not be seen , previous language models were limited to a combination of two unidirectional language models ( i.e. left - to - right and right - toleft ) .",Materials and methods,Materials and methods,relation-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.1632653061224489,54,0.271356783919598,8,0.6153846153846154,1,0,Materials and methods
58,"According to the authors of BERT , incorporating information from bidirectional representations , rather than unidirectional representations , is crucial for representing words in natural language .",Materials and methods,Materials and methods,relation-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.2244897959183673,57,0.2864321608040201,11,0.8461538461538461,1,0,Materials and methods
60,"Due to the space limitations , we refer readers to fora more detailed description of BERT .",Materials and methods,Materials and methods,relation-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.2653061224489796,59,0.2964824120603015,13,1.0,1,0,Materials and methods
61,Pre-training BioBERT,Materials and methods,Materials and methods,relation-classification,7,"['O', 'O']","['O', 'O']",14,0.2857142857142857,60,0.3015075376884422,0,0.0,1,0,Materials and methods
63,"However , biomedical domain texts contain a considerable number of domain - specific .",Materials and methods,Materials and methods,relation-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.3265306122448979,62,0.3115577889447236,2,0.1538461538461538,1,0,Materials and methods
64,"Overview of the pre-training and fine - tuning of BioBERT proper nouns ( e.g. BRCA1 , c.248T > C ) and terms ( e.g. transcriptional , antimicrobial ) , which are understood mostly by biomedical researchers .",Materials and methods,Materials and methods,relation-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.3469387755102041,63,0.3165829145728643,3,0.2307692307692307,1,0,Materials and methods
65,"As a result , NLP models designed for general purpose language understanding often obtains poor performance in biomedical text mining tasks .",Materials and methods,Materials and methods,relation-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.3673469387755102,64,0.321608040201005,4,0.3076923076923077,1,0,Materials and methods
67,"The text corpora used for pre-training of BioBERT are listed in , and the tested combinations of text corpora are listed in .",Materials and methods,Materials and methods,relation-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.4081632653061224,66,0.3316582914572864,6,0.4615384615384615,1,0,Materials and methods
68,"For computational efficiency , whenever the Wiki Books corpora were used for pre-training , we initialized BioBERT with the pre-trained BERT model provided by .",Materials and methods,Materials and methods,relation-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.4285714285714285,67,0.3366834170854271,7,0.5384615384615384,1,0,Materials and methods
71,"With WordPiece tokenization , any new words can be represented by frequent subwords ( e.g. Immunoglobulin >",Materials and methods,Materials and methods,relation-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.4897959183673469,70,0.3517587939698492,10,0.7692307692307693,1,0,Materials and methods
72,I ##mm ##uno ##g ##lo # #bul # #in ) .,Materials and methods,Materials and methods,relation-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.5102040816326531,71,0.3567839195979899,11,0.8461538461538461,1,0,Materials and methods
74,"Although we could have constructed new WordPiece vocabulary based on biomedical corpora , we used the original vocabulary of BERT BASE for the following reasons : ( i ) compatibility of BioBERT with BERT , which allows BERT pre-trained on general domain corpora to be re-used , and makes it easier to interchangeably use existing models based on BERT and BioBERT and ( ii ) any new words may still be represented and fine - tuned for the biomedical domain using the original WordPiece vocabulary of BERT .",Materials and methods,Materials and methods,relation-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.5510204081632653,73,0.3668341708542713,13,1.0,1,0,Materials and methods
75,Fine-tuning BioBERT,Materials and methods,Materials and methods,relation-classification,7,"['O', 'O']","['O', 'O']",28,0.5714285714285714,74,0.371859296482412,0,0.0,1,0,Materials and methods
78,"Named entity recognition is one of the most fundamental biomedical text mining tasks , which involves recognizing numerous domain - specific proper nouns in a biomedical corpus .",Materials and methods,Materials and methods,relation-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",31,0.6326530612244898,77,0.3869346733668342,3,0.1428571428571428,1,0,Materials and methods
79,"While most previous works were built upon different combinations of LSTMs and CRFs , BERT has a simple architecture based on bidirectional transformers .",Materials and methods,Materials and methods,relation-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",32,0.6530612244897959,78,0.3919597989949748,4,0.1904761904761904,1,0,Materials and methods
81,"Note that while previous works in biomedical NER often used word embeddings trained on PubMed or PMC corpora , BioBERT directly learns WordPiece embeddings during pre-training and fine - tuning .",Materials and methods,Materials and methods,relation-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",34,0.6938775510204082,80,0.4020100502512563,6,0.2857142857142857,1,0,Materials and methods
84,"We utilized the sentence classifier of the original version of BERT , which uses a [ CLS ] token for the classification of relations .",Materials and methods,Materials and methods,relation-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",37,0.7551020408163265,83,0.4170854271356783,9,0.4285714285714285,1,0,Materials and methods
86,We anonymized target named entities in a sentence using pre-defined tags such as @ GENE $ or @DISEASE $ .,Materials and methods,Materials and methods,relation-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",39,0.7959183673469388,85,0.4271356783919598,11,0.5238095238095238,1,0,Materials and methods
87,"For instance , a sentence with two target entities ( gene and disease in this case ) is represented as "" Serine at position 986 of @GENE $ maybe an independent genetic predictor of angiographic @DISEASE $ . """,Materials and methods,Materials and methods,relation-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",40,0.8163265306122449,86,0.4321608040201005,12,0.5714285714285714,1,0,Materials and methods
89,Question answering is a task of answering questions posed in natural language given related passages .,Materials and methods,Materials and methods,relation-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",42,0.8571428571428571,88,0.4422110552763819,14,0.6666666666666666,1,0,Materials and methods
91,We used the BioASQ factoid datasets because their format is similar to that of SQuAD .,Materials and methods,Materials and methods,relation-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",44,0.8979591836734694,90,0.4522613065326633,16,0.7619047619047619,1,0,Materials and methods
93,"However , we observed that about 30 % of the BioASQ factoid questions were unanswerable in an extractive QA setting as the exact answers did not appear in the given passages .",Materials and methods,Materials and methods,relation-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",46,0.9387755102040816,92,0.4623115577889447,18,0.8571428571428571,1,0,Materials and methods
94,"Like , we excluded the samples with unanswerable questions from the training sets .",Materials and methods,Materials and methods,relation-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",47,0.9591836734693876,93,0.4673366834170854,19,0.9047619047619048,1,0,Materials and methods
97,Results,,,relation-classification,7,['O'],['O'],0,0.0,96,0.4824120603015075,0,0.0,1,0,
98,Datasets,Results,,relation-classification,7,['O'],['O'],1,0.05,97,0.4874371859296482,0,0.0,1,0,Results
99,The statistics of biomedical NER datasets are listed in .,Results,Datasets,relation-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.1,98,0.4924623115577889,1,0.0526315789473684,1,0,Results: Datasets
101,The pre-processed NCBI Disease dataset has fewer annotations than the original dataset due to the removal of duplicate articles from its training set .,Results,Datasets,relation-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.2,100,0.5025125628140703,3,0.1578947368421052,1,0,Results: Datasets
104,"We did not use alternate annotations for the BC2 GM dataset , and all NER evaluations are based on entity - level exact matches .",Results,Datasets,relation-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.35,103,0.5175879396984925,6,0.3157894736842105,1,0,Results: Datasets
105,"Note that although there are several other recently introduced high quality biomedical NER datasets , we use datasets that are frequently used by many biomedical NLP researchers , which makes it much easier to compare our work with theirs .",Results,Datasets,relation-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.4,104,0.5226130653266332,7,0.3684210526315789,1,0,Results: Datasets
106,The RE datasets contain gene - disease relations and protein - chemical relations ) .,Results,Datasets,relation-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.45,105,0.5276381909547738,8,0.4210526315789473,1,0,Results: Datasets
108,"For the CHEMPROT dataset , we used the same pre-processing procedure described in .",Results,Datasets,relation-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.55,107,0.5376884422110553,10,0.5263157894736842,1,0,Results: Datasets
109,"We used the BioASQ factoid datasets , which can be converted into the same format as the SQuAD dataset ) .",Results,Datasets,relation-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.6,108,0.542713567839196,11,0.5789473684210527,1,0,Results: Datasets
112,"For all the datasets , we used the same dataset splits used in previous works ) fora fair evaluation ; however , the splits of LINAAEUS and Species - 800 could not be found from and maybe different .",Results,Datasets,relation-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.75,111,0.5577889447236181,14,0.7368421052631579,1,0,Results: Datasets
113,"Like previous work , we reported the performance of 10 - fold cross-validation on datasets that do not have separate test sets ( e.g. GAD , EU - ADR ) .",Results,Datasets,relation-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.8,112,0.5628140703517588,15,0.7894736842105263,1,0,Results: Datasets
115,Note that the state - of - the - art models each have a different architecture and training procedure .,Results,Datasets,relation-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.9,114,0.5728643216080402,17,0.8947368421052632,1,0,Results: Datasets
116,"For instance , the state - of - the - art model by trained on the JNLPBA dataset is based on multiple Bi - LSTM CRF models with character level CNNs , while the state - of - the - art model by trained on the LINNAEUS dataset uses a Bi - LSTM CRF model with character level LSTMs and is additionally trained on silver - standard datasets .",Results,Datasets,relation-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.95,115,0.5778894472361809,18,0.9473684210526316,1,0,Results: Datasets
117,"On the other hand , BERT and BioBERT have exactly the same structure , and use only the gold standard datasets and not any additional datasets .",Results,Datasets,relation-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,1.0,116,0.5829145728643216,19,1.0,1,0,Results: Datasets
118,Experimental setups,,,relation-classification,7,"['O', 'O']","['O', 'O']",0,0.0,117,0.5879396984924623,0,0.0,1,0,
120,BioBERT v1.0 ( PubMed PMC ) is the version of BioBERT ( PubMed PMC ) trained for 470 K steps .,Experimental setups,Experimental setups,relation-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.125,119,0.5979899497487438,2,0.125,1,0,Experimental setups
123,"After our initial release of BioBERT v 1.0 , we pre-trained BioBERT on PubMed for 1 M steps , and we refer to this version as BioBERT v 1.1 ( PubMed ) .",Experimental setups,Experimental setups,relation-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.3125,122,0.6130653266331658,5,0.3125,1,0,Experimental setups
124,Other hyper - parameters such as batch size and learning rate scheduling for pre-training BioBERT are the same as those for pre-training BERT unless stated otherwise .,Experimental setups,Experimental setups,relation-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.375,123,0.6180904522613065,6,0.375,1,0,Experimental setups
128,It takes more than 10 days to pre-train BioBERT v 1.0 ( PubMed PMC ) nearly 23 days for BioBERT v 1.1 ( PubMed ) in this setting .,Experimental setups,Experimental setups,relation-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.625,127,0.6381909547738693,10,0.625,1,0,Experimental setups
129,"Despite our best efforts to use BERT LARGE , we used only BERT BASE due to the computational complexity of BERT LARGE .",Experimental setups,Experimental setups,relation-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.6875,128,0.6432160804020101,11,0.6875,1,0,Experimental setups
131,Note that the fine - tuning process is more computationally efficient than pre-training BioBERT .,Experimental setups,Experimental setups,relation-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.8125,130,0.6532663316582915,13,0.8125,1,0,Experimental setups
133,Fine - tuning BioBERT on QA and RE tasks took less than an hour as the size of the training data is much smaller than that of the training data used by .,Experimental setups,Experimental setups,relation-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.9375,132,0.6633165829145728,15,0.9375,1,0,Experimental setups
135,Experimental results,,,relation-classification,7,"['O', 'O']","['O', 'O']",0,0.0,134,0.6733668341708543,0,0.0,1,0,
140,"The relatively low scores on the LINNAEUS dataset can be attributed to the following : ( i ) the lack of a silver - standard dataset for training previous state - of - the - art models and ( ii ) different training / test set splits used in previous work , which were unavailable .",Experimental results,Experimental results,relation-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.1612903225806451,139,0.6984924623115578,5,0.3571428571428571,1,0,Experimental results
150,Discussion,Experimental results,,relation-classification,7,['O'],['O'],15,0.4838709677419355,149,0.7487437185929648,0,0.0,1,0,Experimental results
153,"shows that the performance of BioBERT v 1.0 ( PubMed ) on three NER datasets ( NCBI Disease , BC2GM , BC4CHEMD ) changes in relation to the size of the PubMed corpus .",Experimental results,Discussion,relation-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.5806451612903226,152,0.7638190954773869,3,0.1875,1,0,Experimental results: Discussion
161,"As shown in , we sampled predictions from BERT and BioBERT v 1.1 ( PubMed ) to seethe effect of pre-training on downstream tasks .",Experimental results,Discussion,relation-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.8387096774193549,160,0.8040201005025126,11,0.6875,1,0,Experimental results: Discussion
162,BioBERT can recognize biomedical named entities that BERT can not and can find the exact boundaries of named Disease 19 665 BC5 CDR Disease 12 694 BC5CDR Drug / Chem. 15 411 BC4CHEMD Drug / Chem. 79 842 BC2 GM Gene / Protein 20 703 JNLPBA Gene / Protein 35 460 LINNAEUS Species 4077 Species - 800 Species 3708,Experimental results,Discussion,relation-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.8709677419354839,161,0.8090452261306532,12,0.75,1,0,Experimental results: Discussion
163,"Note Note : For the CHEMPROT dataset , the number of relations in the training , validation and test sets was summed . 486 150 BioASQ 6 b-factoid 618 161",Experimental results,Discussion,relation-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",28,0.9032258064516128,162,0.8140703517587939,13,0.8125,1,0,Experimental results: Discussion
164,entities .,Experimental results,Discussion,relation-classification,7,"['O', 'O']","['O', 'O']",29,0.935483870967742,163,0.8190954773869347,14,0.875,1,0,Experimental results: Discussion
165,"While BERT often gives incorrect answers to simple biomedical questions , BioBERT provides correct answers to such questions .",Experimental results,Discussion,relation-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",30,0.967741935483871,164,0.8241206030150754,15,0.9375,1,0,Experimental results: Discussion
166,"Also , BioBERT can provide longer named entities as answers .",Experimental results,Discussion,relation-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",31,1.0,165,0.8291457286432161,16,1.0,1,0,Experimental results: Discussion
167,Conclusion,,,relation-classification,7,['O'],['O'],0,0.0,166,0.8341708542713567,0,0.0,1,0,
3,abstract,,,relation-classification,8,['O'],['O'],0,0.0,2,0.0145985401459854,0,0.0,1,0,
4,The state - of - the - art solutions for extracting multiple entity - relations from an input paragraph always require a multiple - pass encoding on the input .,abstract,abstract,relation-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.2,3,0.0218978102189781,1,0.2,1,0,abstract
8,1,abstract,abstract,relation-classification,8,['O'],['O'],5,1.0,7,0.0510948905109489,5,1.0,1,0,abstract
11,"A solution to this task is essential for many downstream NLP applications such as automatic knowledge - base completion , knowledge base question answering , and symbolic approaches for visual question answering , etc .",Introduction,Introduction,relation-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.1818181818181818,10,0.072992700729927,2,0.1818181818181818,1,0,Introduction
13,"Because in real - world applications , whose input paragraphs dominantly contain multiple pairs of entities , an efficient and effective solution for MRE has more important and more practical implications .",Introduction,Introduction,relation-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.3636363636363636,12,0.0875912408759124,4,0.3636363636363636,1,0,Introduction
14,"However , nearly all existing approaches for MRE tasks 2014 ; adopt some variations of the singlerelation extraction ( SRE ) approach , which treats each pair of entity mentions as an independent instance , and requires multiple passes of encoding for the multiple pairs of entities .",Introduction,Introduction,relation-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.4545454545454545,13,0.0948905109489051,5,0.4545454545454545,1,0,Introduction
15,"The drawback of this approach is obvious - it is computationally expensive and this issue becomes more severe when the input paragraph is large , making this solution impossible to implement when the encoding step involves deep models .",Introduction,Introduction,relation-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.5454545454545454,14,0.1021897810218978,6,0.5454545454545454,1,0,Introduction
21,Background,,,relation-classification,8,['O'],['O'],0,0.0,20,0.145985401459854,0,0.0,1,0,
28,Proposed Approach,,,relation-classification,8,"['O', 'O']","['O', 'O']",0,0.0,27,0.1970802919708029,0,0.0,1,0,
32,"It is worth mentioning that our solution can easily use other transformer - based encoders besides BERT , e.g..",Proposed Approach,The framework is illustrated in 1 .,relation-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.1176470588235294,31,0.2262773722627737,4,1.0,1,0,Proposed Approach: The framework is illustrated in 1 .
35,"However , the final prediction layers used in the original model is not applicable to MRE tasks .",Proposed Approach,The framework is illustrated in 1 .,relation-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.2058823529411764,34,0.2481751824817518,2,0.0689655172413793,1,0,Proposed Approach: The framework is illustrated in 1 .
38,"Thus , the representation fora pair of entity mentions ( e i , e j ) can be denoted as oi and o j respectively .",Proposed Approach,The framework is illustrated in 1 .,relation-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.2941176470588235,37,0.2700729927007299,5,0.1724137931034483,1,0,Proposed Approach: The framework is illustrated in 1 .
39,"In the case of a mention e i consist of multiple hidden states ( due to the byte pair encoding ) , oi is aggregated via average - pooling over the hidden states of the corresponding tokens in the last BERT layer .",Proposed Approach,The framework is illustrated in 1 .,relation-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.3235294117647059,38,0.2773722627737226,6,0.2068965517241379,1,0,Proposed Approach: The framework is illustrated in 1 .
40,"We then concatenate oi and o j denoted as [ o i : o j ] , and pass it to a linear classifier 2 to predict the relation",Proposed Approach,The framework is illustrated in 1 .,relation-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.3529411764705882,39,0.2846715328467153,7,0.2413793103448276,1,0,Proposed Approach: The framework is illustrated in 1 .
41,where W L ?,Proposed Approach,The framework is illustrated in 1 .,relation-classification,8,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",13,0.3823529411764705,40,0.291970802919708,8,0.2758620689655172,1,0,Proposed Approach: The framework is illustrated in 1 .
43,"dz is the dimension of BERT embedding at each token position , and l is the number of relation labels .",Proposed Approach,R 2 dzl .,relation-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.4411764705882353,42,0.3065693430656934,10,0.3448275862068966,1,0,Proposed Approach: R 2 dzl .
48,"Following , for each pair of word tokens ( x i , x j ) with the input representations from the previous layer ash i and h j , we extend the computation of self - attention z i as :",Proposed Approach,R 2 dzl .,relation-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.5882352941176471,47,0.3430656934306569,15,0.5172413793103449,1,0,Proposed Approach: R 2 dzl .
49,"are the parameters of the model , and dz is the dimension of the output from the self - attention layer .",Proposed Approach,R 2 dzl .,relation-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.6176470588235294,48,0.3503649635036496,16,0.5517241379310345,1,0,Proposed Approach: R 2 dzl .
50,"Compared to standard BERT 's self - attention , a V ij , a K ij ?",Proposed Approach,R 2 dzl .,relation-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.6470588235294118,49,0.3576642335766423,17,0.5862068965517241,1,0,Proposed Approach: R 2 dzl .
51,"R dz are extra , which could be viewed as the edge representation between the input element x i and x j .",Proposed Approach,R 2 dzl .,relation-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",23,0.6764705882352942,50,0.364963503649635,18,0.6206896551724138,1,0,Proposed Approach: R 2 dzl .
53,"Adapted from , we argue that the relative distance information will not help if the distance is beyond a certain threshold .",Proposed Approach,R 2 dzl .,relation-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.7352941176470589,52,0.3795620437956204,20,0.6896551724137931,1,0,Proposed Approach: R 2 dzl .
54,Hence we first define the distance function as :,Proposed Approach,R 2 dzl .,relation-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.7647058823529411,53,0.3868613138686131,21,0.7241379310344828,1,0,Proposed Approach: R 2 dzl .
55,"This distance definition clips all distances to a region [?k , k].",Proposed Approach,R 2 dzl .,relation-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.7941176470588235,54,0.3941605839416058,22,0.7586206896551724,1,0,Proposed Approach: R 2 dzl .
57,We can now define a V ij and a K ij formally as :,Proposed Approach,R 2 dzl .,relation-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",29,0.8529411764705882,56,0.4087591240875912,24,0.8275862068965517,1,0,Proposed Approach: R 2 dzl .
58,"As defined above , if either token xi or x j belongs to an entity , we will introduce a relative positional representation according to their distance .",Proposed Approach,R 2 dzl .,relation-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",30,0.8823529411764706,57,0.4160583941605839,25,0.8620689655172413,1,0,Proposed Approach: R 2 dzl .
59,The distance is defined in an entity - centric way as we always compute the distance from the entity mention to the other token .,Proposed Approach,R 2 dzl .,relation-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",31,0.9117647058823528,58,0.4233576642335766,26,0.896551724137931,1,0,Proposed Approach: R 2 dzl .
60,"If neither xi nor x j are entity mentions , we explicitly assign a zero vector to a K ij and a V ij .",Proposed Approach,R 2 dzl .,relation-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",32,0.9411764705882352,59,0.4306569343065693,27,0.9310344827586208,1,0,Proposed Approach: R 2 dzl .
61,"When both xi and x j are inside entity mentions , we take the distance as d ( i , j ) to make row - wise attention computation coherent as depicted in .",Proposed Approach,R 2 dzl .,relation-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",33,0.9705882352941176,60,0.437956204379562,28,0.9655172413793104,1,0,Proposed Approach: R 2 dzl .
63,Experiments,,,relation-classification,8,['O'],['O'],0,0.0,62,0.4525547445255474,0,0.0,1,0,
66,Settings,Experiments,,relation-classification,8,['O'],['O'],3,0.2727272727272727,65,0.4744525547445255,0,0.0,1,0,Experiments
68,For Se-m,Experiments,,relation-classification,8,"['O', 'O']","['O', 'O']",5,0.4545454545454545,67,0.4890510948905109,2,0.25,1,0,Experiments
69,Eval 2018,Experiments,,relation-classification,8,"['O', 'O']","['O', 'O']",6,0.5454545454545454,68,0.4963503649635036,3,0.375,1,0,Experiments
72,The passages in this task is usually much longer compared to ACE .,Experiments,Eval 2018,relation-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.8181818181818182,71,0.5182481751824818,6,0.75,1,0,Experiments: Eval 2018
73,"Therefore we adopt the following pre-processing step - for the entity pair in each relation , we assume the tokens related to their relation labeling are always within a range from the fifth token ahead of the pair to the fifth token after it .",Experiments,Eval 2018,relation-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.9090909090909092,72,0.5255474452554745,7,0.875,1,0,Experiments: Eval 2018
74,"Therefore , the tokens in the original passage that are not covered by the range of ANY input relations , will be removed from the input .",Experiments,Eval 2018,relation-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,1.0,73,0.5328467153284672,8,1.0,1,0,Experiments: Eval 2018
75,Methods,,,relation-classification,8,['O'],['O'],0,0.0,74,0.5401459854014599,0,0.0,1,0,
79,BERT SP with position embedding on the final attention layer .,Methods,Methods,relation-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.5,78,0.5693430656934306,4,0.5,1,0,Methods
80,This is a more straightforward way to achieve MRE in one - pass derived from previous works using position embeddings .,Methods,Methods,relation-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.625,79,0.5766423357664233,5,0.625,1,0,Methods
84,Results on ACE 2005,,,relation-classification,8,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",0,0.0,83,0.6058394160583942,0,0.0,1,0,
91,Summing up the vectors confuses this information .,Results on ACE 2005,,relation-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.3684210526315789,90,0.656934306569343,7,0.28,1,0,Results on ACE 2005
96,Performance Gap between MRE in One - Pass and Multi - Pass,Results on ACE 2005,Summing up the vectors confuses this information .,relation-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.631578947368421,95,0.6934306569343066,12,0.48,1,0,Results on ACE 2005: Summing up the vectors confuses this information .
97,The MRE - in - one - pass models can also be used to train and test with one entity pair per pass ( Single - Relation per Pass results in ) .,Results on ACE 2005,Summing up the vectors confuses this information .,relation-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.6842105263157895,96,0.7007299270072993,13,0.52,1,0,Results on ACE 2005: Summing up the vectors confuses this information .
99,"For BERT SP with entity indicators on inputs , it is expected to perform slightly better in the single - relation setting , because of the mixture of information from multiple pairs .",Results on ACE 2005,Summing up the vectors confuses this information .,relation-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.7894736842105263,98,0.7153284671532847,15,0.6,1,0,Results on ACE 2005: Summing up the vectors confuses this information .
100,A 2 % gap is observed as expected .,Results on ACE 2005,,relation-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.8421052631578947,99,0.7226277372262774,16,0.64,1,0,Results on ACE 2005
102,The BERT SP is not expected to have a gap as shown in the table .,Results on ACE 2005,A 2 % gap is observed as expected .,relation-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.9473684210526316,101,0.7372262773722628,18,0.72,1,0,Results on ACE 2005: A 2 % gap is observed as expected .
103,"For BERT SP with position embeddings on the final attention layer , we train the model in the single - relation setting and test with two different settings , so the results are the same .",Results on ACE 2005,A 2 % gap is observed as expected .,relation-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,1.0,102,0.7445255474452555,19,0.76,1,0,Results on ACE 2005: A 2 % gap is observed as expected .
104,Training and Inference Time,,,relation-classification,8,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",0,0.0,103,0.7518248175182481,20,0.8,1,0,
109,"evaluates the usage of different prediction layers , including replacing our linear layer in Eq .",Training and Inference Time,Training and Inference Time,relation-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.5555555555555556,108,0.7883211678832117,25,1.0,1,0,Training and Inference Time
110,Prediction Module Selection,Training and Inference Time,Training and Inference Time,relation-classification,8,"['O', 'O', 'O']","['O', 'O', 'O']",6,0.6666666666666666,109,0.7956204379562044,0,0.0,1,0,Training and Inference Time
113,This is consistent with the motivation of the pre-trained encoders : by unsupervised pre-training the encoders are expected to be sufficiently powerful thus adding more complex layers on top does not improve the capacity but leads to more free parameters and higher risk of over-fitting .,Training and Inference Time,Training and Inference Time,relation-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,1.0,112,0.8175182481751825,3,0.2307692307692307,1,0,Training and Inference Time
114,Results on SemEval 2018,,,relation-classification,8,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",0,0.0,113,0.8248175182481752,4,0.3076923076923077,1,0,
119,"Note that the system ( Rotsztejn et al. , 2018 ) integrates many techniques like feature - engineering , model combination , pretraining embeddings on in - domain data , and artificial data generation , while our model is almost a direct adaption from the ACE architecture .",Results on SemEval 2018,Task 7,relation-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.7142857142857143,118,0.8613138686131386,9,0.6923076923076923,1,0,Results on SemEval 2018: Task 7
122,Results,,,relation-classification,8,['O'],['O'],0,0.0,121,0.8832116788321168,12,0.9230769230769232,1,0,
123,"We conduct additional experiments on the relation classification task , SemEval 2010 Task 8 , to com -",Results,Results,relation-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0,122,0.8905109489051095,13,1.0,1,0,Results
124,Method,,,relation-classification,8,['O'],['O'],0,0.0,123,0.8978102189781022,0,0.0,1,0,
125,Averaged F1 Macro Micro,Method,Method,relation-classification,8,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",1,0.125,124,0.9051094890510948,1,0.125,1,0,Method
132,"This is likely because of the bias of data distribution : the assumption that only two target entities exist , makes the two techniques have similar effects .",Method,Method,relation-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,1.0,131,0.9562043795620438,8,1.0,1,0,Method
133,Conclusion,,,relation-classification,8,['O'],['O'],0,0.0,132,0.9635036496350364,0,0.0,1,0,
3,abstract,,,relation-classification,9,['O'],['O'],0,0.0,2,0.0136054421768707,0,0.0,1,0,
4,Obtaining large - scale annotated data for NLP tasks in the scientific domain is challenging and expensive .,abstract,abstract,relation-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.1666666666666666,3,0.0204081632653061,1,0.1666666666666666,1,0,abstract
12,"Recent progress in NLP has been driven by the adoption of deep neural models , but training such models often requires large amounts of labeled data .",Introduction,Introduction,relation-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.1666666666666666,11,0.0748299319727891,2,0.1666666666666666,1,0,Introduction
13,"In general domains , large - scale training data is often possible to obtain through crowdsourcing , but in scientific domains , annotated data is difficult and expensive to collect due to the expertise required for quality annotation .",Introduction,Introduction,relation-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.25,12,0.0816326530612244,3,0.25,1,0,Introduction
17,"Yet while both BERT and ELMo have released pretrained models , they are still trained on general domain corpora such as news articles and Wikipedia .",Introduction,Introduction,relation-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.5833333333333334,16,0.1088435374149659,7,0.5833333333333334,1,0,Introduction
23,Methods,,,relation-classification,9,['O'],['O'],0,0.0,22,0.1496598639455782,0,0.0,1,0,
24,Background,,,relation-classification,9,['O'],['O'],0,0.0,23,0.1564625850340136,1,0.1,1,0,
105,Results,,,relation-classification,9,['O'],['O'],0,0.0,104,0.7074829931972789,0,0.0,1,0,
106,Biomedical Domain,Results,,relation-classification,9,"['O', 'O']","['O', 'O']",1,0.032258064516129,105,0.7142857142857143,0,0.0,1,0,Results
112,"The SOTA result for GENIA is in Nguyen and Verspoor ( 2019 ) which uses the model from with partof - speech ( POS ) features , which we do not use .",Results,Biomedical Domain,relation-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.2258064516129032,111,0.7551020408163265,6,0.5454545454545454,1,0,Results: Biomedical Domain
116,"8 Forrest of this paper , all results reported in this manner are averaged over datasets excluding UAS for DEP since we already include LAS .",Results,Biomedical Domain,relation-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.3548387096774194,115,0.782312925170068,10,0.9090909090909092,1,0,Results: Biomedical Domain
118,Computer Science Domain,Results,,relation-classification,9,"['O', 'O', 'O']","['O', 'O', 'O']",13,0.4193548387096774,117,0.7959183673469388,0,0.0,1,0,Results
121,"For relations in Sci - ERC , our results are not comparable with those in because we are performing relation classification given gold entities , while they perform joint entity and relation extraction .",Results,Computer Science Domain,relation-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.5161290322580645,120,0.8163265306122449,3,1.0,1,0,Results: Computer Science Domain
122,Multiple Domains,Results,,relation-classification,9,"['O', 'O']","['O', 'O']",17,0.5483870967741935,121,0.8231292517006803,0,0.0,1,0,Results
125,No prior published SOTA results exist for the Paper Field dataset .,Results,Multiple Domains,relation-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.6451612903225806,124,0.8435374149659864,3,1.0,1,0,Results: Multiple Domains
126,Discussion,Results,,relation-classification,9,['O'],['O'],21,0.6774193548387096,125,0.8503401360544217,0,0.0,1,0,Results
127,Effect of Finetuning,Results,,relation-classification,9,"['O', 'O', 'O']","['O', 'O', 'O']",22,0.7096774193548387,126,0.8571428571428571,0,0.0,1,0,Results
131,Effect of SCIVOCAB,Results,,relation-classification,9,"['O', 'O', 'O']","['O', 'O', 'O']",26,0.8387096774193549,130,0.8843537414965986,0,0.0,1,0,Results
136,"Given the disjoint vocabularies ( Section 2 ) and the magnitude of improvement over BERT - Base ( Section 4 ) , we suspect that while an in - domain vocabulary is helpful , SCIBERT benefits most from the scientific corpus pretraining .",Results,Effect of SCIVOCAB,relation-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",31,1.0,135,0.9183673469387756,5,1.0,1,0,Results: Effect of SCIVOCAB
137,Related Work,,,relation-classification,9,"['O', 'O']","['O', 'O']",0,0.0,136,0.9251700680272108,0,0.0,1,0,
142,Conclusion and Future Work,,,relation-classification,9,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",0,0.0,141,0.9591836734693876,0,0.0,1,0,
3,*,title,Character - level Convolutional Networks for Text Classification,text-classification,0,['O'],['O'],2,1.0,2,0.0088105726872246,2,1.0,1,0,title: Character - level Convolutional Networks for Text Classification
4,abstract,,,text-classification,0,['O'],['O'],0,0.0,3,0.013215859030837,0,0.0,1,0,
8,There are also related works that use character - level features for language processing .,abstract,abstract,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.5714285714285714,7,0.0308370044052863,4,0.5714285714285714,1,0,abstract
10,"In particular , these ConvNet approaches use words as a basis , in which character - level features extracted at word [ 28 ] or word n-gram [ 29 ] level form a distributed representation .",abstract,abstract,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.8571428571428571,9,0.039647577092511,6,0.8571428571428571,1,0,abstract
13,"Text classification is a classic topic for natural language processing , in which one needs to assign predefined categories to free - text documents .",Introduction,Introduction,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0204081632653061,12,0.052863436123348,1,0.0555555555555555,1,0,Introduction
14,The range of text classification research goes from designing the best features to choosing the best possible machine learning classifiers .,Introduction,Introduction,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.0408163265306122,13,0.0572687224669603,2,0.1111111111111111,1,0,Introduction
15,"To date , almost all techniques of text classification are based on words , in which simple statistics of some ordered word combinations ( such as n-grams ) usually perform the best .",Introduction,Introduction,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.0612244897959183,14,0.0616740088105726,3,0.1666666666666666,1,0,Introduction
16,"On the other hand , many researchers have found convolutional networks ( ConvNets ) are useful in extracting information from raw signals , ranging from computer vision applications to speech recognition and others .",Introduction,Introduction,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.0816326530612244,15,0.066079295154185,4,0.2222222222222222,1,0,Introduction
17,"In particular , time - delay networks used in the early days of deep learning research are essentially convolutional networks that model sequential data .",Introduction,Introduction,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.1020408163265306,16,0.0704845814977973,5,0.2777777777777778,1,0,Introduction
19,For this article we only used a classification task as away to exemplify ConvNets ' ability to understand texts .,Introduction,Introduction,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.1428571428571428,18,0.079295154185022,7,0.3888888888888889,1,0,Introduction
20,"Historically we know that ConvNets usually require large - scale datasets to work , therefore we also build several of them .",Introduction,Introduction,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.1632653061224489,19,0.0837004405286343,8,0.4444444444444444,1,0,Introduction
22,Applying convolutional networks to text classification or natural language processing at large was explored in literature .,Introduction,Introduction,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.2040816326530612,21,0.092511013215859,10,0.5555555555555556,1,0,Introduction
24,These approaches have been proven to be competitive to traditional models .,Introduction,Introduction,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.2448979591836734,23,0.1013215859030837,12,0.6666666666666666,1,0,Introduction
25,from previous research that ConvNets do not require the knowledge about the syntactic or semantic structure of a language .,Introduction,Introduction,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.2653061224489796,24,0.105726872246696,13,0.7222222222222222,1,0,Introduction
26,"This simplification of engineering could be crucial fora single system that can work for different languages , since characters always constitute a necessary construct regardless of whether segmentation into words is possible .",Introduction,Introduction,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.2857142857142857,25,0.1101321585903083,14,0.7777777777777778,1,0,Introduction
27,Working on only characters also has the advantage that abnormal character combinations such as misspellings and emoticons maybe naturally learnt .,Introduction,Introduction,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.3061224489795918,26,0.1145374449339207,15,0.8333333333333334,1,0,Introduction
33,Suppose we have a discrete input function g ( x ) ?,Introduction,Key Modules,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.4285714285714285,32,0.1409691629955947,2,0.0869565217391304,1,0,Introduction: Key Modules
34,"[ 1 , l ] ?",Introduction,Key Modules,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O']",22,0.4489795918367347,33,0.145374449339207,3,0.1304347826086956,1,0,Introduction: Key Modules
35,Rand a discrete kernel function,Introduction,Key Modules,text-classification,0,"['O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O']",23,0.4693877551020408,34,0.1497797356828193,4,0.1739130434782608,1,0,Introduction: Key Modules
36,where c = k ? d + 1 is an offset constant .,Introduction,Key Modules,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.4897959183673469,35,0.1541850220264317,5,0.217391304347826,1,0,Introduction: Key Modules
37,"Just as in traditional convolutional networks in vision , the module is parameterized by a set of such kernel functions f ij ( x ) ( i = 1 , 2 , . . . , m and j = 1 , 2 , . . . , n) which we call weights , on a set of inputs g i ( x ) and outputs h j ( y ) .",Introduction,Key Modules,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.5102040816326531,36,0.158590308370044,6,0.2608695652173913,1,0,Introduction: Key Modules
38,"We call each g i ( or h j ) input ( or output ) features , and m ( or n) input ( or output ) feature size .",Introduction,Key Modules,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.5306122448979592,37,0.1629955947136563,7,0.3043478260869565,1,0,Introduction: Key Modules
39,The outputs h j ( y ) is obtained by a sum over i of the convolutions between g i ( x ) and f ij ( x ) .,Introduction,Key Modules,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.5510204081632653,38,0.1674008810572687,8,0.3478260869565217,1,0,Introduction: Key Modules
40,One key module that helped us to train deeper models is temporal max - pooling .,Introduction,Key Modules,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",28,0.5714285714285714,39,0.171806167400881,9,0.391304347826087,1,0,Introduction: Key Modules
41,It is the 1 - D version of the max - pooling module used in computer vision .,Introduction,Key Modules,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",29,0.5918367346938775,40,0.1762114537444934,10,0.4347826086956521,1,0,Introduction: Key Modules
42,Given a discrete input function g ( x ) ?,Introduction,Key Modules,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",30,0.6122448979591837,41,0.1806167400881057,11,0.4782608695652174,1,0,Introduction: Key Modules
43,"[ 1 , l ] ?",Introduction,Key Modules,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O']",31,0.6326530612244898,42,0.185022026431718,12,0.5217391304347826,1,0,Introduction: Key Modules
44,"R , the max - pooling function h ( y ) ?",Introduction,Key Modules,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",32,0.6530612244897959,43,0.1894273127753304,13,0.5652173913043478,1,0,Introduction: Key Modules
45,"[ 1 , ( l ? k) / d + 1 ] ?",Introduction,Key Modules,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",33,0.673469387755102,44,0.1938325991189427,14,0.6086956521739131,1,0,Introduction: Key Modules
46,R of g ( x ) is defined as,Introduction,Key Modules,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",34,0.6938775510204082,45,0.198237885462555,15,0.6521739130434783,1,0,Introduction: Key Modules
47,where c = k ? d + 1 is an offset constant .,Introduction,Key Modules,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",35,0.7142857142857143,46,0.2026431718061674,16,0.6956521739130435,1,0,Introduction: Key Modules
49,The analysis by might shed some light on this .,Introduction,Key Modules,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",37,0.7551020408163265,48,0.211453744493392,18,0.782608695652174,1,0,Introduction: Key Modules
50,"The non-linearity used in our model is the rectifier or thresholding function h ( x ) = max {0 , x} , which makes our convolutional layers similar to rectified linear units ( ReLUs ) .",Introduction,Key Modules,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",38,0.7755102040816326,49,0.2158590308370044,19,0.8260869565217391,1,0,Introduction: Key Modules
53,This number will later be detailed for each dataset sparately .,Introduction,Key Modules,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",41,0.8367346938775511,52,0.2290748898678414,22,0.9565217391304348,1,0,Introduction: Key Modules
55,Character quantization,Introduction,,text-classification,0,"['O', 'O']","['O', 'O']",43,0.8775510204081632,54,0.237885462555066,0,0.0,1,0,Introduction
58,"Then , the sequence of characters is transformed to a sequence of such m sized vectors with fixed length l 0 .",Introduction,Character quantization,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",46,0.9387755102040816,57,0.2511013215859031,3,0.5,1,0,Introduction: Character quantization
59,"Any character exceeding length l 0 is ignored , and any characters that are not in the alphabet including blank characters are quantized as all - zero vectors .",Introduction,Character quantization,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",47,0.9591836734693876,58,0.2555066079295154,4,0.6666666666666666,1,0,Introduction: Character quantization
62,Model Design,,,text-classification,0,"['O', 'O']","['O', 'O']",0,0.0,61,0.2687224669603524,0,0.0,1,0,
65,gives an illustration .,Model Design,Model Design,text-classification,0,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",3,0.081081081081081,64,0.2819383259911894,3,1.0,1,0,Model Design
67,Convolutions,Model Design,,text-classification,0,['O'],['O'],5,0.1351351351351351,66,0.2907488986784141,0,0.0,1,0,Model Design
70,Fully - connected :,Model Design,Conv. and Pool. layers,text-classification,0,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",8,0.2162162162162162,69,0.3039647577092511,3,0.1666666666666666,1,0,Model Design: Conv. and Pool. layers
72,"The input have number of features equal to 70 due to our character quantization method , and the input feature length is 1014 .",Model Design,Conv. and Pool. layers,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.2702702702702703,71,0.3127753303964757,5,0.2777777777777778,1,0,Model Design: Conv. and Pool. layers
73,It seems that 1014 characters could already capture most of the texts of interest .,Model Design,Conv. and Pool. layers,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.2972972972972973,72,0.3171806167400881,6,0.3333333333333333,1,0,Model Design: Conv. and Pool. layers
80,The number of output units for the last layer is determined by the problem .,Model Design,Conv. and Pool. layers,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.4864864864864865,79,0.3480176211453744,13,0.7222222222222222,1,0,Model Design: Conv. and Pool. layers
81,"For example , fora 10 - class classification problem it will be 10 .",Model Design,Conv. and Pool. layers,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.5135135135135135,80,0.3524229074889868,14,0.7777777777777778,1,0,Model Design: Conv. and Pool. layers
82,Depends on the problem,Model Design,,text-classification,0,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",20,0.5405405405405406,81,0.3568281938325991,15,0.8333333333333334,1,0,Model Design
83,"For different problems the input lengths maybe different ( for example in our case l 0 = 1014 ) , and so are the frame lengths .",Model Design,Depends on the problem,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.5675675675675675,82,0.3612334801762114,16,0.8888888888888888,1,0,Model Design: Depends on the problem
84,"From our model design , it is easy to know that given input length l 0 , the output frame length after the last convolutional layer ( but before any of the fully - connected layers ) isl 6 = ( l 0 ? 96 ) / 27 .",Model Design,Depends on the problem,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.5945945945945946,83,0.3656387665198238,17,0.9444444444444444,1,0,Model Design: Depends on the problem
85,This number multiplied with the frame size at layer 6 will give the input dimension the first fully - connected layer accepts .,Model Design,Depends on the problem,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",23,0.6216216216216216,84,0.3700440528634361,18,1.0,1,0,Model Design: Depends on the problem
86,Layer Output Units Large Output Units,Model Design,,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O']",24,0.6486486486486487,85,0.3744493392070485,0,0.0,1,0,Model Design
88,Many researchers have found that appropriate data augmentation techniques are useful for controlling generalization error for deep learning models .,Model Design,Data Augmentation using Thesaurus,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.7027027027027027,87,0.3832599118942731,1,0.0833333333333333,1,0,Model Design: Data Augmentation using Thesaurus
89,These techniques usually work well when we could find appropriate invariance properties that the model should possess .,Model Design,Data Augmentation using Thesaurus,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.7297297297297297,88,0.3876651982378855,2,0.1666666666666666,1,0,Model Design: Data Augmentation using Thesaurus
90,"In terms of texts , it is not reasonable to augment the data using signal transformations as done in image or speech recognition , because the exact order of characters may form rigorous syntactic and semantic meaning .",Model Design,Data Augmentation using Thesaurus,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",28,0.7567567567567568,89,0.3920704845814978,3,0.25,1,0,Model Design: Data Augmentation using Thesaurus
91,"Therefore , the best way to do data augmentation would have been using human rephrases of sentences , but this is unrealistic and expensive due the large volume of samples in our datasets .",Model Design,Data Augmentation using Thesaurus,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",29,0.7837837837837838,90,0.3964757709251101,4,0.3333333333333333,1,0,Model Design: Data Augmentation using Thesaurus
92,"As a result , the most natural choice in data augmentation for us is to replace words or phrases with their synonyms .",Model Design,Data Augmentation using Thesaurus,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",30,0.8108108108108109,91,0.4008810572687225,5,0.4166666666666667,1,0,Model Design: Data Augmentation using Thesaurus
94,"That thesaurus in turn was obtained from Word - Net , where every synonym to a word or phrase is ranked by the semantic closeness to the most frequently seen meaning .",Model Design,Data Augmentation using Thesaurus,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",32,0.8648648648648649,93,0.4096916299559471,7,0.5833333333333334,1,0,Model Design: Data Augmentation using Thesaurus
96,The probability of number r is determined by a geometric distribution with parameter pin which P [ r ] ? pr .,Model Design,Data Augmentation using Thesaurus,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",34,0.918918918918919,95,0.4185022026431718,9,0.75,1,0,Model Design: Data Augmentation using Thesaurus
97,The index s of the synonym chosen given a word is also determined by a another geometric distribution in which P [ s ] ? q s .,Model Design,Data Augmentation using Thesaurus,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",35,0.945945945945946,96,0.4229074889867841,10,0.8333333333333334,1,0,Model Design: Data Augmentation using Thesaurus
98,"This way , the probability of a synonym chosen becomes smaller when it moves distant from the most frequently seen meaning .",Model Design,Data Augmentation using Thesaurus,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",36,0.972972972972973,97,0.4273127753303964,11,0.9166666666666666,1,0,Model Design: Data Augmentation using Thesaurus
100,Comparison Models,,,text-classification,0,"['O', 'O']","['O', 'O']",0,0.0,99,0.4361233480176211,0,0.0,1,0,
104,We refer to traditional methods as those that using a hand - crafted feature extractor and a linear classifier .,Traditional Methods,Traditional Methods,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0588235294117647,103,0.4537444933920704,1,0.0588235294117647,1,0,Traditional Methods
109,"For the TFIDF ( term-frequency inverse - document - frequency ) version , we use the counts as the term-frequency .",Traditional Methods,Traditional Methods,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.3529411764705882,108,0.4757709251101321,6,0.3529411764705882,1,0,Traditional Methods
110,The inverse document frequency is the logarithm of the division between total number of samples and number of samples with the word in the training subset .,Traditional Methods,Traditional Methods,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.4117647058823529,109,0.4801762114537445,7,0.4117647058823529,1,0,Traditional Methods
112,Bag - of - ngrams and its TFIDF .,Traditional Methods,Traditional Methods,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.5294117647058824,111,0.4889867841409692,9,0.5294117647058824,1,0,Traditional Methods
117,We take into consideration all the words that appeared more than 5 times in the training subset .,Traditional Methods,Traditional Methods,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.8235294117647058,116,0.5110132158590308,14,0.8235294117647058,1,0,Traditional Methods
122,Recently deep learning methods have started to be applied to text classification .,Deep Learning Methods,Deep Learning Methods,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.01,121,0.5330396475770925,1,0.0625,1,0,Deep Learning Methods
125,"Among the large number of recent works on word - based ConvNets for text classification , one of the differences is the choice of using pretrained or end - to - end learned word representations .",Deep Learning Methods,Deep Learning Methods,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.04,124,0.5462555066079295,4,0.25,1,0,Deep Learning Methods
128,"To ensure fair comparison , the models for each case are of the same size as our character - level ConvNets , in terms of both the number of layers and each layer 's output size .",Deep Learning Methods,Deep Learning Methods,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.07,127,0.5594713656387665,7,0.4375,1,0,Deep Learning Methods
130,LSTM LSTM LSTM ... : long - short term memory,Deep Learning Methods,Deep Learning Methods,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.09,129,0.5682819383259912,9,0.5625,1,0,Deep Learning Methods
136,"The variant of LSTM we used is the common "" vanilla "" architecture [ 8 ] .",Deep Learning Methods,Deep Learning Methods,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.15,135,0.5947136563876652,15,0.9375,1,0,Deep Learning Methods
138,Mean,Deep Learning Methods,,text-classification,0,['O'],['O'],17,0.17,137,0.6035242290748899,0,0.0,1,0,Deep Learning Methods
139,Choice of Alphabet,Deep Learning Methods,,text-classification,0,"['O', 'O', 'O']","['O', 'O', 'O']",18,0.18,138,0.6079295154185022,0,0.0,1,0,Deep Learning Methods
140,"For the alphabet of English , one apparent choice is whether to distinguish between upper-case and lower - case letters .",Deep Learning Methods,Choice of Alphabet,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.19,139,0.6123348017621145,1,0.0121951219512195,1,0,Deep Learning Methods: Choice of Alphabet
142,"One possible explanation might be that semantics do not change with different letter cases , therefore there is a benefit of regularization .",Deep Learning Methods,Choice of Alphabet,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.21,141,0.6211453744493393,3,0.0365853658536585,1,0,Deep Learning Methods: Choice of Alphabet
144,"Previous research on ConvNets in different areas has shown that they usually work well with largescale datasets , especially when the model takes in low - level raw features like characters in our case .",Deep Learning Methods,Large - scale Datasets and Results,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",23,0.23,143,0.6299559471365639,5,0.0609756097560975,1,0,Deep Learning Methods: Large - scale Datasets and Results
145,"However , most open datasets for text classification are quite small , and large - scale datasets are splitted with a significantly smaller training set than testing .",Deep Learning Methods,Large - scale Datasets and Results,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.24,144,0.6343612334801763,6,0.073170731707317,1,0,Deep Learning Methods: Large - scale Datasets and Results
146,"Therefore , instead of confusing our community more by using them , we built several large - scale datasets for our experiments , ranging from hundreds of thousands to several millions of samples .",Deep Learning Methods,Large - scale Datasets and Results,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.25,145,0.6387665198237885,7,0.0853658536585365,1,0,Deep Learning Methods: Large - scale Datasets and Results
147,is a summary .,Deep Learning Methods,Large - scale Datasets and Results,text-classification,0,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",26,0.26,146,0.6431718061674009,8,0.0975609756097561,1,0,Deep Learning Methods: Large - scale Datasets and Results
152,There area large number categories but most of them contain only few articles .,Deep Learning Methods,Large - scale Datasets and Results,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",31,0.31,151,0.6651982378854625,13,0.1585365853658536,1,0,Deep Learning Methods: Large - scale Datasets and Results
153,"We choose 5 categories - "" sports "" , "" finance "" , "" entertainment "" , "" automobile "" and "" technology "" .",Deep Learning Methods,Large - scale Datasets and Results,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",32,0.32,152,0.6696035242290749,14,0.1707317073170731,1,0,Deep Learning Methods: Large - scale Datasets and Results
154,"The number of training samples selected for each class is 90,000 and testing 12,000 .",Deep Learning Methods,Large - scale Datasets and Results,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",33,0.33,153,0.6740088105726872,15,0.1829268292682926,1,0,Deep Learning Methods: Large - scale Datasets and Results
155,"Although this is a dataset in Chinese , we used pypinyin package combined with jieba Chinese segmentation system to produce Pinyin - a phonetic romanization of Chinese .",Deep Learning Methods,Large - scale Datasets and Results,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",34,0.34,154,0.6784140969162996,16,0.1951219512195122,1,0,Deep Learning Methods: Large - scale Datasets and Results
157,The fields used are title and content . :,Deep Learning Methods,Large - scale Datasets and Results,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",36,0.36,156,0.6872246696035242,18,0.2195121951219512,1,0,Deep Learning Methods: Large - scale Datasets and Results
159,Numbers are in percentage .,Deep Learning Methods,Large - scale Datasets and Results,text-classification,0,"['O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O']",38,0.38,158,0.6960352422907489,20,0.2439024390243902,1,0,Deep Learning Methods: Large - scale Datasets and Results
160,""" Lg "" stands for "" large "" and "" Sm "" stands for "" small "" .",Deep Learning Methods,Large - scale Datasets and Results,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",39,0.39,159,0.7004405286343612,21,0.2560975609756097,1,0,Deep Learning Methods: Large - scale Datasets and Results
161,""" w2 v "" is an abbreviation for "" word2vec "" , and "" Lk "" for "" lookup DBPedia ontology dataset .",Deep Learning Methods,Large - scale Datasets and Results,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",40,0.4,160,0.7048458149779736,22,0.2682926829268293,1,0,Deep Learning Methods: Large - scale Datasets and Results
164,"From each of these 14 ontology classes , we randomly choose 40,000 training samples and 5,000 testing samples .",Deep Learning Methods,Large - scale Datasets and Results,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",43,0.43,163,0.7180616740088106,25,0.3048780487804878,1,0,Deep Learning Methods: Large - scale Datasets and Results
165,The fields we used for this dataset contain title and abstract of each Wikipedia article .,Deep Learning Methods,Large - scale Datasets and Results,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",44,0.44,164,0.7224669603524229,26,0.3170731707317073,1,0,Deep Learning Methods: Large - scale Datasets and Results
170,"The full dataset has 130,000 training samples and 10,000 testing samples in each star , and the polarity dataset has 280,000 training samples and 19,000 test samples in each polarity .",Deep Learning Methods,Large - scale Datasets and Results,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",49,0.49,169,0.7444933920704846,31,0.3780487804878049,1,0,Deep Learning Methods: Large - scale Datasets and Results
171,Yahoo!,Deep Learning Methods,,text-classification,0,['O'],['O'],50,0.5,170,0.748898678414097,32,0.3902439024390244,1,0,Deep Learning Methods
172,Answers dataset .,Deep Learning Methods,Yahoo!,text-classification,0,"['O', 'O', 'O']","['O', 'O', 'O']",51,0.51,171,0.7533039647577092,33,0.4024390243902439,1,0,Deep Learning Methods: Yahoo!
173,We obtained Yahoo!,Deep Learning Methods,,text-classification,0,"['O', 'O', 'O']","['O', 'O', 'O']",52,0.52,172,0.7577092511013216,34,0.4146341463414634,1,0,Deep Learning Methods
183,"The full dataset contains 600,000 training samples and 130,000 testing samples in each class , whereas the polarity dataset contains 1,800,000 training samples and 200,000 testing samples in each polarity sentiment .",Deep Learning Methods,We obtained Yahoo!,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",62,0.62,182,0.801762114537445,44,0.5365853658536586,1,0,Deep Learning Methods: We obtained Yahoo!
186,"Note that since we do not have a Chinese thesaurus , the Sogou News dataset does not have any results using thesaurus augmentation .",Deep Learning Methods,We obtained Yahoo!,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",65,0.65,185,0.8149779735682819,47,0.573170731707317,1,0,Deep Learning Methods: We obtained Yahoo!
187,We labeled the best result in blue and worse result in red .,Deep Learning Methods,We obtained Yahoo!,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",66,0.66,186,0.8193832599118943,48,0.5853658536585366,1,0,Deep Learning Methods: We obtained Yahoo!
188,Figure 3 : Relative errors with comparison models,Deep Learning Methods,We obtained Yahoo!,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",67,0.67,187,0.8237885462555066,49,0.5975609756097561,1,0,Deep Learning Methods: We obtained Yahoo!
191,"Each of these plots is computed by taking the difference between errors on comparison model and our character - level ConvNet model , then divided by the comparison model error .",Deep Learning Methods,We obtained Yahoo!,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",70,0.7,190,0.8370044052863436,52,0.6341463414634146,1,0,Deep Learning Methods: We obtained Yahoo!
193,Character - level,Deep Learning Methods,We obtained Yahoo!,text-classification,0,"['O', 'O', 'O']","['O', 'O', 'O']",72,0.72,192,0.8458149779735683,54,0.6585365853658537,1,0,Deep Learning Methods: We obtained Yahoo!
196,This is a strong indication that language could also bethought of as a signal no different from any other kind .,Deep Learning Methods,We obtained Yahoo!,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",75,0.75,195,0.8590308370044053,57,0.6951219512195121,1,0,Deep Learning Methods: We obtained Yahoo!
198,Dataset size forms a dichotomy between traditional and ConvNets models .,Deep Learning Methods,We obtained Yahoo!,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",77,0.77,197,0.8678414096916299,59,0.7195121951219512,1,0,Deep Learning Methods: We obtained Yahoo!
201,Conv Nets may work well for user - generated data .,Deep Learning Methods,We obtained Yahoo!,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",80,0.8,200,0.8810572687224669,62,0.7560975609756098,1,0,Deep Learning Methods: We obtained Yahoo!
203,"For example , in our million scale datasets , Amazon reviews tend to be raw user-inputs , whereas users might be extra careful in their writings on Yahoo !",Deep Learning Methods,We obtained Yahoo!,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",82,0.82,202,0.8898678414096917,64,0.7804878048780488,1,0,Deep Learning Methods: We obtained Yahoo!
204,Answers .,Deep Learning Methods,,text-classification,0,"['O', 'O']","['O', 'O']",83,0.83,203,0.8942731277533039,65,0.7926829268292683,1,0,Deep Learning Methods
206,This property suggests that ConvNets may have better applicability to real - world scenarios .,Deep Learning Methods,Answers .,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",85,0.85,205,0.9030837004405288,67,0.8170731707317073,1,0,Deep Learning Methods: Answers .
207,"However , further analysis is needed to validate the hypothesis that ConvNets are truly good at identifying exotic character combinations such as misspellings and emoticons , as our experiments alone do not show any explicit evidence .",Deep Learning Methods,Answers .,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",86,0.86,206,0.9074889867841408,68,0.8292682926829268,1,0,Deep Learning Methods: Answers .
208,Choice of alphabet makes a difference .,Deep Learning Methods,Answers .,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O']",87,0.87,207,0.9118942731277532,69,0.8414634146341463,1,0,Deep Learning Methods: Answers .
209,shows that changing the alphabet by distinguishing between uppercase and lowercase letters could make a difference .,Deep Learning Methods,Answers .,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",88,0.88,208,0.9162995594713657,70,0.8536585365853658,1,0,Deep Learning Methods: Answers .
211,"One possible explanation is that there is a regularization effect , but this is to be validated .",Deep Learning Methods,Answers .,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",90,0.9,210,0.9251101321585904,72,0.8780487804878049,1,0,Deep Learning Methods: Answers .
212,Semantics of tasks may not matter .,Deep Learning Methods,Answers .,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O']",91,0.91,211,0.9295154185022028,73,0.8902439024390244,1,0,Deep Learning Methods: Answers .
214,This dichotomy in task semantics does not seem to play a role in deciding which method is better .,Deep Learning Methods,Answers .,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",93,0.93,213,0.9383259911894272,75,0.9146341463414634,1,0,Deep Learning Methods: Answers .
215,Bag - of - means is a misuse of word2vec .,Deep Learning Methods,Answers .,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",94,0.94,214,0.9427312775330396,76,0.926829268292683,1,0,Deep Learning Methods: Answers .
217,"Comparing with traditional models , this suggests such a simple use of a distributed word representation may not give us an advantage to text classification .",Deep Learning Methods,Answers .,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",96,0.96,216,0.9515418502202644,78,0.951219512195122,1,0,Deep Learning Methods: Answers .
218,"However , our experiments does not speak for any other language processing tasks or use of word2vec in any other way .",Deep Learning Methods,Answers .,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",97,0.97,217,0.9559471365638766,79,0.9634146341463414,1,0,Deep Learning Methods: Answers .
219,There is no free lunch .,Deep Learning Methods,Answers .,text-classification,0,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O']",98,0.98,218,0.960352422907489,80,0.975609756097561,1,0,Deep Learning Methods: Answers .
3,abstract,,,text-classification,1,['O'],['O'],0,0.0,2,0.0078125,0,0.0,1,0,
7,"LSTM can embed text regions of variable ( and possibly large ) sizes , whereas the region size needs to be fixed in a CNN .",abstract,abstract,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.5,6,0.0234375,4,0.5,1,0,abstract
13,"Text categorization is the task of assigning labels to documents written in a natural language , and it has numerous real - world applications including sentiment analysis as well as traditional topic assignment tasks .",Introduction,Introduction,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.009009009009009,12,0.046875,1,0.0294117647058823,1,0,Introduction
14,"The state - of - the art methods for text categorization had long been linear predictors ( e.g. , SVM with a linear kernel ) with either bag - ofword or bag - of - n- gram vectors ( hereafter bow ) as input , e.g. , .",Introduction,Introduction,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.018018018018018,13,0.05078125,2,0.0588235294117647,1,0,Introduction
15,"This , however , A convolutional neural network ( CNN ) ) is a feedforward neural network with convolution layers interleaved with pooling layers , originally developed for image processing .",Introduction,Introduction,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.027027027027027,14,0.0546875,3,0.088235294117647,1,0,Introduction
22,"The specific region embedding function of one - hot CNN takes the simple form v ( x ) = max (0 , Wx + b ) ,",Introduction,Introduction,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.09009009009009,21,0.08203125,10,0.2941176470588235,1,0,Introduction
23,"where x is a concatenation of one - hot vectors ( therefore , ' one - hot ' in the name ) of the words in the - th region ( of a fixed size ) , and the weight matrix W and the bias vector b need to be trained .",Introduction,Introduction,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.0990990990990991,22,0.0859375,11,0.3235294117647059,1,0,Introduction
25,"However , there are also potential shortcomings .",Introduction,,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.1171171171171171,24,0.09375,13,0.3823529411764705,1,0,Introduction
26,"The region size must be fixed , which may not be optimal as the size of relevant regions may vary .",Introduction,"However , there are also potential shortcomings .",text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.1261261261261261,25,0.09765625,14,0.4117647058823529,1,0,"Introduction: However , there are also potential shortcomings ."
27,"Practically , the region size can not be very large as the number of parameters to be learned ( components of W ) depends on it .",Introduction,"However , there are also potential shortcomings .",text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.1351351351351351,26,0.1015625,15,0.4411764705882353,1,0,"Introduction: However , there are also potential shortcomings ."
28,JZ15 proposed variations to alleviate these issues .,Introduction,,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.1441441441441441,27,0.10546875,16,0.4705882352941176,1,0,Introduction
29,"For example , a bow - input variation allows x above to be a bow vector of the region .",Introduction,JZ15 proposed variations to alleviate these issues .,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.1531531531531531,28,0.109375,17,0.5,1,0,Introduction: JZ15 proposed variations to alleviate these issues .
30,"This enables a larger region , but at the expense of losing word order in the region and so its use maybe limited .",Introduction,JZ15 proposed variations to alleviate these issues .,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.1621621621621621,29,0.11328125,18,0.5294117647058824,1,0,Introduction: JZ15 proposed variations to alleviate these issues .
32,LSTM ) is a recurrent neural network .,Introduction,,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.1801801801801801,31,0.12109375,20,0.5882352941176471,1,0,Introduction
33,"In its typical applications to text , an LSTM takes words in a sequence one by one ; i.e. , at time t , it takes as input the t- th word and the output from time t ?",Introduction,LSTM ) is a recurrent neural network .,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.1891891891891892,32,0.125,21,0.6176470588235294,1,0,Introduction: LSTM ) is a recurrent neural network .
36,"That is , an LSTM can be used to embed text regions of variable ( and possibly large ) sizes .",Introduction,LSTM ) is a recurrent neural network .,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.2162162162162162,35,0.13671875,24,0.7058823529411765,1,0,Introduction: LSTM ) is a recurrent neural network .
38,"Our strategy is to simplify the model as much as possible , including elimination of a word embedding layer routinely used to produce input to LSTM .",Introduction,LSTM ) is a recurrent neural network .,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.2342342342342342,37,0.14453125,26,0.7647058823529411,1,0,Introduction: LSTM ) is a recurrent neural network .
47,Preliminary,Introduction,,text-classification,1,['O'],['O'],35,0.3153153153153153,46,0.1796875,0,0.0,1,0,Introduction
48,"On text , LSTM has been used for labeling or generating words .",Introduction,Preliminary,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",36,0.3243243243243243,47,0.18359375,1,0.0357142857142857,1,0,Introduction: Preliminary
49,"It has been also used for representing short sentences mostly for sentiment analysis , and some of them rely on syntactic parse trees ; see e.g. , .",Introduction,Preliminary,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",37,0.3333333333333333,48,0.1875,2,0.0714285714285714,1,0,Introduction: Preliminary
50,"Unlike these studies , this work as well as JZ15 focuses on classifying general full - length documents without any special linguistic knowledge .",Introduction,Preliminary,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",38,0.3423423423423423,49,0.19140625,3,0.1071428571428571,1,0,Introduction: Preliminary
51,"Similarly , DL15 applied LSTM to categorizing general full - length documents .",Introduction,Preliminary,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",39,0.3513513513513513,50,0.1953125,4,0.1428571428571428,1,0,Introduction: Preliminary
52,"Therefore , our empirical comparisons will focus on DL15 and JZ15 , both of which reported new state of the art results .",Introduction,Preliminary,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",40,0.3603603603603603,51,0.19921875,5,0.1785714285714285,1,0,Introduction: Preliminary
53,"Let us first introduce the general LSTM formulation , and then briefly describe DL15 's model as it illustrates the challenges in using LSTMs for this task .",Introduction,Preliminary,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",41,0.3693693693693693,52,0.203125,6,0.2142857142857142,1,0,Introduction: Preliminary
54,LSTM,Introduction,,text-classification,1,['O'],['O'],42,0.3783783783783784,53,0.20703125,7,0.25,1,0,Introduction
55,"While several variations exist , we base our work on the following LSTM formulation , which was used in , e.g. ,",Introduction,LSTM,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",43,0.3873873873873873,54,0.2109375,8,0.2857142857142857,1,0,Introduction: LSTM
56,where denotes element - wise multiplication and ?,Introduction,LSTM,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",44,0.3963963963963964,55,0.21484375,9,0.3214285714285714,1,0,Introduction: LSTM
57,"is an element - wise squash function to make the gating values in [ 0 , 1 ] .",Introduction,LSTM,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",45,0.4054054054054054,56,0.21875,10,0.3571428571428571,1,0,Introduction: LSTM
58,We fix ? to sigmoid .,Introduction,LSTM,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O']",46,0.4144144144144144,57,0.22265625,11,0.3928571428571428,1,0,Introduction: LSTM
59,x t ?,Introduction,LSTM,text-classification,1,"['O', 'O', 'O']","['O', 'O', 'O']",47,0.4234234234234234,58,0.2265625,12,0.4285714285714285,1,0,Introduction: LSTM
60,"Rd is the input from the lower layer at time step t , where d would be , for example , size of vocabulary if the input was a one - hot vector representing a word , or the dimensionality of word vector if the lower layer was a word embedding layer .",Introduction,LSTM,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",48,0.4324324324324324,59,0.23046875,13,0.4642857142857143,1,0,Introduction: LSTM
61,"With q LSTM units , the dimensionality of the weight matrices and bias vectors , which need to be trained , are W ( ) ? R qd , U ( ) ? R qq , and b ( ) ?",Introduction,LSTM,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",49,0.4414414414414414,60,0.234375,14,0.5,1,0,Introduction: LSTM
62,R q for all types .,Introduction,,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O']",50,0.4504504504504504,61,0.23828125,15,0.5357142857142857,1,0,Introduction
63,"The centerpiece of LSTM is the memory cells ct , designed to counteract the risk of vanishing / exploding gradients , thus enabling learning of dependencies over larger time lags than feasible with traditional recurrent networks .",Introduction,R q for all types .,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",51,0.4594594594594595,62,0.2421875,16,0.5714285714285714,1,0,Introduction: R q for all types .
64,The forget gate ft is for resetting the memory cells .,Introduction,R q for all types .,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",52,0.4684684684684684,63,0.24609375,17,0.6071428571428571,1,0,Introduction: R q for all types .
65,The input gate it and output gate o t control the input and output of the memory cells .,Introduction,R q for all types .,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",53,0.4774774774774775,64,0.25,18,0.6428571428571429,1,0,Introduction: R q for all types .
66,Word - vector LSTM ( wv - LSTM ) [ DL15 ] DL15 's application of LSTM to text categorization is straightforward .,Introduction,R q for all types .,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",54,0.4864864864864865,65,0.25390625,19,0.6785714285714286,1,0,Introduction: R q for all types .
67,"As illustrated in , for each document , the output of the LSTM layer is the output of the last time step ( corresponding to the last word of the document ) , which represents the whole document ( document embedding ) .",Introduction,R q for all types .,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",55,0.4954954954954955,66,0.2578125,20,0.7142857142857143,1,0,Introduction: R q for all types .
68,"Like many other studies of LSTM on text , words are first converted to low - dimensional dense word vectors via a word embedding layer ; therefore , we call it word - vector LSTM or wv - LSTM .",Introduction,R q for all types .,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",56,0.5045045045045045,67,0.26171875,21,0.75,1,0,Introduction: R q for all types .
69,DL15 observed that wv - LSTM underperformed linear predictors and its training was unstable .,Introduction,R q for all types .,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",57,0.5135135135135135,68,0.265625,22,0.7857142857142857,1,0,Introduction: R q for all types .
70,This was attributed to the fact that documents are long .,Introduction,R q for all types .,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",58,0.5225225225225225,69,0.26953125,23,0.8214285714285714,1,0,Introduction: R q for all types .
71,"In addition , we found that training and testing of wv - LSTM is time / resource consuming .",Introduction,R q for all types .,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",59,0.5315315315315315,70,0.2734375,24,0.8571428571428571,1,0,Introduction: R q for all types .
72,"To put it into perspective , using a GPU , one epoch of wv - LSTM training takes nearly 20 times longer than that of one - hot CNN training even though it achieves poorer accuracy ( the first two rows of ) .",Introduction,R q for all types .,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",60,0.5405405405405406,71,0.27734375,25,0.8928571428571429,1,0,Introduction: R q for all types .
73,"This is due to the sequential nature of LSTM , i.e. , computation at time t requires the output of time t ? 1 , whereas modern computation depends on parallelization for speedup .",Introduction,R q for all types .,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",61,0.5495495495495496,72,0.28125,26,0.9285714285714286,1,0,Introduction: R q for all types .
75,It was shown in DL15 that training becomes stable and accuracy improves drastically when LSTM and the word embedding layer are jointly pre-trained with either the language model learning objective ( predicting the next word ) or autoencoder objective ( memorizing the document ) .,Introduction,R q for all types .,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",63,0.5675675675675675,74,0.2890625,28,1.0,1,0,Introduction: R q for all types .
82,Facts : A word embedding is a linear operation that can be written as Vx t with x t being a one - hot vector and columns of V being word vectors .,Introduction,Elimination of the word embedding layer,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",70,0.6306306306306306,81,0.31640625,1,0.1111111111111111,1,0,Introduction: Elimination of the word embedding layer
83,"Therefore , by replacing the LSTM weights W ( ) with W ( ) V and removing the word embedding layer , a word - vector LSTM can be turned into a one - hot LSTM without changing the model behavior .",Introduction,Elimination of the word embedding layer,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",71,0.6396396396396397,82,0.3203125,2,0.2222222222222222,1,0,Introduction: Elimination of the word embedding layer
84,"Thus , word - vector LSTM is not more expressive than one - hot LSTM ; rather , a merit , if any , of training with a word embedding layer would be through imposing restrictions ( e.g. , a low - rank V makes a less expressive model ) to achieve good prior / regularization effects .",Introduction,Elimination of the word embedding layer,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",72,0.6486486486486487,83,0.32421875,3,0.3333333333333333,1,0,Introduction: Elimination of the word embedding layer
85,"In the end - to - end supervised setting , a word embedding matrix V would need to be initialized randomly and trained as part of the model .",Introduction,Elimination of the word embedding layer,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",73,0.6576576576576577,84,0.328125,4,0.4444444444444444,1,0,Introduction: Elimination of the word embedding layer
87,"Instead , demerits were evident - more meta-parameters to tune , poor accuracy with lowdimensional word vectors , and slow training / testing with high - dimensional word vectors as they are dense .",Introduction,Elimination of the word embedding layer,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",75,0.6756756756756757,86,0.3359375,6,0.6666666666666666,1,0,Introduction: Elimination of the word embedding layer
88,"If a word embedding is appropriately pre-trained with unlabeled data , its inclusion is a form of semi-supervised learning and could be useful .",Introduction,Elimination of the word embedding layer,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",76,0.6846846846846847,87,0.33984375,7,0.7777777777777778,1,0,Introduction: Elimination of the word embedding layer
89,"We will show later , however , that this type of approach falls behind our approach of learning region embeddings through training one - hot LSTM on unlabeled data .",Introduction,Elimination of the word embedding layer,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",77,0.6936936936936937,88,0.34375,8,0.8888888888888888,1,0,Introduction: Elimination of the word embedding layer
91,More simplifications,Introduction,,text-classification,1,"['O', 'O']","['O', 'O']",79,0.7117117117117117,90,0.3515625,0,0.0,1,0,Introduction
95,"In wv - LSTM , the sub-problem that LSTM needs to solve is to represent the entire document by one vector ( document embedding ) .",Introduction,Pooling : simplifying sub - problems,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",83,0.7477477477477478,94,0.3671875,4,0.056338028169014,1,0,Introduction: Pooling : simplifying sub - problems
97,"As illustrated in , we let the LSTM layer emit vectors ht at each time step , and let pooling aggregate them into a document vector .",Introduction,Pooling : simplifying sub - problems,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",85,0.7657657657657657,96,0.375,6,0.0845070422535211,1,0,Introduction: Pooling : simplifying sub - problems
98,"With wv - LSTM , LSTM has to remember relevant information until it gets to the end of the document even if relevant information was observed 10 K words away .",Introduction,Pooling : simplifying sub - problems,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",86,0.7747747747747747,97,0.37890625,7,0.0985915492957746,1,0,Introduction: Pooling : simplifying sub - problems
100,A related architecture appears in the Deep Learning Tutorials 2 though it uses a word embedding .,Introduction,Pooling : simplifying sub - problems,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",88,0.7927927927927928,99,0.38671875,9,0.1267605633802817,1,0,Introduction: Pooling : simplifying sub - problems
101,"Another related work is , which combined pooling with non -LSTM recurrent networks and a word embedding .",Introduction,Pooling : simplifying sub - problems,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",89,0.8018018018018018,100,0.390625,10,0.1408450704225352,1,0,Introduction: Pooling : simplifying sub - problems
104,"Since we set the goal of LSTM to embedding text regions instead of documents , it is no longer crucial to go through the document from the beginning to the end sequentially .",Introduction,Chopping for speeding up training,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",92,0.8288288288288288,103,0.40234375,13,0.1830985915492957,1,0,Introduction: Chopping for speeding up training
105,"At the time of training , we can chop each document into segments of a fixed length that is sufficiently long ( e.g. , 50 or 100 ) and process all the segments in a mini batch in parallel as if these segments were individual documents .",Introduction,Chopping for speeding up training,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",93,0.8378378378378378,104,0.40625,14,0.1971830985915492,1,0,Introduction: Chopping for speeding up training
106,( Note that this is done only in the LSTM layer and pooling is done over the entire document . ),Introduction,Chopping for speeding up training,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",94,0.8468468468468469,105,0.41015625,15,0.2112676056338028,1,0,Introduction: Chopping for speeding up training
109,"There is a risk of chopping important phrases ( e.g. , "" do n't | like it "" ) , and this can be easily avoided by having segments slightly overlap .",Introduction,We perform testing without chopping .,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",97,0.8738738738738738,108,0.421875,18,0.2535211267605634,1,0,Introduction: We perform testing without chopping .
110,"However , we found that gains from overlapping segments tend to be small and so our experiments reported below were done without overlapping .",Introduction,We perform testing without chopping .,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",98,0.8828828828828829,109,0.42578125,19,0.2676056338028169,1,0,Introduction: We perform testing without chopping .
111,Removing the input / output gates,Introduction,,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O']",99,0.8918918918918919,110,0.4296875,20,0.2816901408450704,1,0,Introduction
113,"It is intuitive , in particular , that pooling can make the output gate unnecessary ; the role of the output gate is to prevent undesirable information from entering the output ht , and such irrelevant information can be filtered out by max - pooling .",Introduction,Removing the input / output gates,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",101,0.90990990990991,112,0.4375,22,0.3098591549295774,1,0,Introduction: Removing the input / output gates
114,"Without the input and output gates , the LSTM formulation can be simplified to :",Introduction,Removing the input / output gates,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",102,0.918918918918919,113,0.44140625,23,0.323943661971831,1,0,Introduction: Removing the input / output gates
115,( 2 ),Introduction,Removing the input / output gates,text-classification,1,"['O', 'O', 'O']","['O', 'O', 'O']",103,0.927927927927928,114,0.4453125,24,0.3380281690140845,1,0,Introduction: Removing the input / output gates
116,This is equivalent to fixing it and o t to all ones .,Introduction,Removing the input / output gates,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",104,0.9369369369369368,115,0.44921875,25,0.352112676056338,1,0,Introduction: Removing the input / output gates
117,"It is in spirit similar to Gated Recurrent Units but simpler , having fewer gates .",Introduction,Removing the input / output gates,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",105,0.945945945945946,116,0.453125,26,0.3661971830985915,1,0,Introduction: Removing the input / output gates
119,The changes from wv - LSTM above substantially reduce the time and 2 http://deeplearning.net/tutorial/lstm.html,Introduction,Bidirectional LSTM for better accuracy,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",107,0.963963963963964,118,0.4609375,28,0.3943661971830985,1,0,Introduction: Bidirectional LSTM for better accuracy
120,One - hot vectors memory required for training and make it practical to add one more layer of LSTM going in the opposite direction for accuracy improvement .,Introduction,Bidirectional LSTM for better accuracy,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",108,0.972972972972973,119,0.46484375,29,0.4084507042253521,1,0,Introduction: Bidirectional LSTM for better accuracy
121,"As shown in , we concatenate the output of a forward LSTM ( left to right ) and a backward LSTM ( right to left ) , which is referred to as bidirectional LSTM in the literature .",Introduction,Bidirectional LSTM for better accuracy,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",109,0.981981981981982,120,0.46875,30,0.4225352112676056,1,0,Introduction: Bidirectional LSTM for better accuracy
122,"The resulting model is a one - hot bidirectional LSTM with pooling , and we abbreviate it to oh - 2 LSTMp .",Introduction,Bidirectional LSTM for better accuracy,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",110,0.990990990990991,121,0.47265625,31,0.4366197183098591,1,0,Introduction: Bidirectional LSTM for better accuracy
124,Experiments ( supervised ),,,text-classification,1,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",0,0.0,123,0.48046875,33,0.4647887323943662,1,0,
126,The first three were used in JZ15 .,Experiments ( supervised ),,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.0285714285714285,125,0.48828125,35,0.4929577464788732,1,0,Experiments ( supervised )
128,The datasets are summarized in .,Experiments ( supervised ),,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O']",4,0.0571428571428571,127,0.49609375,37,0.5211267605633803,1,0,Experiments ( supervised )
129,The data was converted to lower - case letters .,Experiments ( supervised ),The datasets are summarized in .,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.0714285714285714,128,0.5,38,0.5352112676056338,1,0,Experiments ( supervised ): The datasets are summarized in .
130,"In the neural network experiments , vocabulary was reduced to the most frequent 30 K words of the training data to reduce computational burden ; square loss was minimized with dropout applied to the input to the top layer ; weights were initialized by the .",Experiments ( supervised ),The datasets are summarized in .,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.0857142857142857,129,0.50390625,39,0.5492957746478874,1,0,Experiments ( supervised ): The datasets are summarized in .
138,"The pooling settings chosen based on the performance on the development data are the same as JZ15a , which are max - pooling with k= 1 on IMDB and Elec and average - pooling with k=10 on RCV1 ; on 20 NG , max - pooling with k = 10 was chosen .",Experiments ( supervised ),The datasets are summarized in .,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.2,137,0.53515625,47,0.6619718309859155,1,0,Experiments ( supervised ): The datasets are summarized in .
145,We conjecture that this is because strict word order is not very useful on RCV1 .,Experiments ( supervised ),"However , on RCV1 , it underperforms both .",text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.3,144,0.5625,54,0.7605633802816901,1,0,"Experiments ( supervised ): However , on RCV1 , it underperforms both ."
146,This point can also be observed in the SVM and CNN performances .,Experiments ( supervised ),"However , on RCV1 , it underperforms both .",text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.3142857142857143,145,0.56640625,55,0.7746478873239436,1,0,"Experiments ( supervised ): However , on RCV1 , it underperforms both ."
148,"That is , on RCV1 , bags of words in a window of 20 at every location are more useful than words in strict order .",Experiments ( supervised ),"However , on RCV1 , it underperforms both .",text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.3428571428571428,147,0.57421875,57,0.8028169014084507,1,0,"Experiments ( supervised ): However , on RCV1 , it underperforms both ."
149,This is presumably because the former can more easily cover variability of expressions indicative of topics .,Experiments ( supervised ),"However , on RCV1 , it underperforms both .",text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.3571428571428571,148,0.578125,58,0.8169014084507042,1,0,"Experiments ( supervised ): However , on RCV1 , it underperforms both ."
150,"Thus , LSTM , which does not have an ability to put words into bags , loses to bow - CNN .",Experiments ( supervised ),"However , on RCV1 , it underperforms both .",text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.3714285714285714,149,0.58203125,59,0.8309859154929577,1,0,"Experiments ( supervised ): However , on RCV1 , it underperforms both ."
151,More on one - hot CNN vs. one - hot LSTM LSTM can embed regions of variable ( and possibly large ) sizes whereas CNN requires the region size to be fixed .,Experiments ( supervised ),"However , on RCV1 , it underperforms both .",text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.3857142857142857,150,0.5859375,60,0.8450704225352113,1,0,"Experiments ( supervised ): However , on RCV1 , it underperforms both ."
152,We attribute to this fact the small improvements of oh - 2 LSTMp over oh - CNN in .,Experiments ( supervised ),"However , on RCV1 , it underperforms both .",text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",28,0.4,151,0.58984375,61,0.8591549295774648,1,0,"Experiments ( supervised ): However , on RCV1 , it underperforms both ."
153,"However , this shortcoming of CNN can be alleviated by having multiple convolution layers with distinct region sizes .",Experiments ( supervised ),"However , on RCV1 , it underperforms both .",text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",29,0.4142857142857143,152,0.59375,62,0.8732394366197183,1,0,"Experiments ( supervised ): However , on RCV1 , it underperforms both ."
155,"Although these models are larger than those in , training / testing is still faster than the LSTM models due to simplicity of the region embeddings .",Experiments ( supervised ),"However , on RCV1 , it underperforms both .",text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",31,0.4428571428571428,154,0.6015625,64,0.9014084507042254,1,0,"Experiments ( supervised ): However , on RCV1 , it underperforms both ."
157,This maybe because the amount of training data is not sufficient enough to learn the relevance of longer word sequences .,Experiments ( supervised ),"However , on RCV1 , it underperforms both .",text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",33,0.4714285714285714,156,0.609375,66,0.9295774647887324,1,0,"Experiments ( supervised ): However , on RCV1 , it underperforms both ."
159,Comparison with the previous best results on 20 NG,Experiments ( supervised ),,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",35,0.5,158,0.6171875,68,0.9577464788732394,1,0,Experiments ( supervised )
160,"The previous best performance on 20NG is 15.3 ( not shown in the table ) of DL15 , obtained by pre-training wv - LSTM of 1024 units with labeled training data .",Experiments ( supervised ),Comparison with the previous best results on 20 NG,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",36,0.5142857142857142,159,0.62109375,69,0.971830985915493,1,0,Experiments ( supervised ): Comparison with the previous best results on 20 NG
162,"The previous best results on the other datasets use unlabeled data , and we will review them with our semi-supervised results .",Experiments ( supervised ),Comparison with the previous best results on 20 NG,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",38,0.5428571428571428,161,0.62890625,71,1.0,1,0,Experiments ( supervised ): Comparison with the previous best results on 20 NG
163,Semi-supervised LSTM,Experiments ( supervised ),Comparison with the previous best results on 20 NG,text-classification,1,"['O', 'O']","['O', 'O']",39,0.5571428571428572,162,0.6328125,0,0.0,1,0,Experiments ( supervised ): Comparison with the previous best results on 20 NG
165,This was used in JZ15 b to learn from unlabeled data a region embedding embodied by a convolution layer .,Experiments ( supervised ),Comparison with the previous best results on 20 NG,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",41,0.5857142857142857,164,0.640625,2,0.125,1,0,Experiments ( supervised ): Comparison with the previous best results on 20 NG
168,Two - view embedding ( tv-embedding ) [ JZ15 b ],Experiments ( supervised ),Comparison with the previous best results on 20 NG,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",44,0.6285714285714286,167,0.65234375,5,0.3125,1,0,Experiments ( supervised ): Comparison with the previous best results on 20 NG
171,An embedding is called a tv-embedding if the embedded view is as good as the original view for the purpose of predicting the other view .,Experiments ( supervised ),Consider two views of the input .,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",47,0.6714285714285714,170,0.6640625,8,0.5,1,0,Experiments ( supervised ): Consider two views of the input .
173,Such an embedding is useful provided that its dimensionality is much lower than the original view .,Experiments ( supervised ),Consider two views of the input .,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",49,0.7,172,0.671875,10,0.625,1,0,Experiments ( supervised ): Consider two views of the input .
174,JZ15 b applied this idea by regarding text regions embedded by the convolution layer as one view and their surrounding context as the other view and training a tv-embedding ( embodied by a convolution layer ) on unlabeled data .,Experiments ( supervised ),Consider two views of the input .,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",50,0.7142857142857143,173,0.67578125,11,0.6875,1,0,Experiments ( supervised ): Consider two views of the input .
183,We use the tv-embeddings obtained from unlabeled data to produce additional input to LSTM by replacing and ( 3 ) by the following :,Experiments ( supervised ),Learning LSTM tv-embeddings,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",59,0.8428571428571429,182,0.7109375,3,0.5,1,0,Experiments ( supervised ): Learning LSTM tv-embeddings
184,) .,Experiments ( supervised ),Learning LSTM tv-embeddings,text-classification,1,"['O', 'O']","['O', 'O']",60,0.8571428571428571,183,0.71484375,4,0.6666666666666666,1,0,Experiments ( supervised ): Learning LSTM tv-embeddings
185,"x j t is the output of a tv-embedding ( an LSTM trained with unlabeled data ) indexed by j at time step t , and S is a set of tv-embeddings which contains the two LSTMs going forward and backward as in .",Experiments ( supervised ),Learning LSTM tv-embeddings,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",61,0.8714285714285714,184,0.71875,5,0.8333333333333334,1,0,Experiments ( supervised ): Learning LSTM tv-embeddings
188,"It is easy to see that the set S above can be expanded with any tv-embeddings , not only those in the form of LSTM ( LSTM tv-embeddings ) but also with the tv-embeddings in the form of convolution layers ( CNN tv-embeddings ) such as those obtained in JZ15 b .",Experiments ( supervised ),Combining LSTM tv-embeddings and CNN tv-embeddings,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",64,0.9142857142857144,187,0.73046875,1,0.1428571428571428,1,0,Experiments ( supervised ): Combining LSTM tv-embeddings and CNN tv-embeddings
189,"Similarly , it is possible to use LSTM tv-embeddings to produce additional input to CNN .",Experiments ( supervised ),Combining LSTM tv-embeddings and CNN tv-embeddings,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",65,0.9285714285714286,188,0.734375,2,0.2857142857142857,1,0,Experiments ( supervised ): Combining LSTM tv-embeddings and CNN tv-embeddings
192,Note that being able to naturally combine several tv-embeddings is a strength of 2100 - dim LSTM tv-embed .,Experiments ( supervised ),Combining LSTM tv-embeddings and CNN tv-embeddings,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",68,0.9714285714285714,191,0.74609375,5,0.7142857142857143,1,0,Experiments ( supervised ): Combining LSTM tv-embeddings and CNN tv-embeddings
195,Semi-supervised experiments,,,text-classification,1,"['O', 'O']","['O', 'O']",0,0.0,194,0.7578125,0,0.0,1,0,
196,"We used IMDB , Elec , and RCV1 for our semi-supervised experiments ; 20 NG was excluded due to the absence of standard unlabeled data .",Semi-supervised experiments,Semi-supervised experiments,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0196078431372549,195,0.76171875,1,0.0357142857142857,1,0,Semi-supervised experiments
200,"Similar to JZ15 b , we minimized weighted square",Semi-supervised experiments,,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.0980392156862745,199,0.77734375,5,0.1785714285714285,1,0,Semi-supervised experiments
201,") 2 where i goes through the time steps , z represents the next k words by a bow vector , and p is the model output ; ?",Semi-supervised experiments,"Similar to JZ15 b , we minimized weighted square",text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.1176470588235294,200,0.78125,6,0.2142857142857142,1,0,"Semi-supervised experiments: Similar to JZ15 b , we minimized weighted square"
202,"i , j were set to achieve negative sampling effect for speed - up ; vocabulary control was performed for reducing undesirable relations between views , which sets the vocabulary of the target ( i.e. , the k words ) to the 30 K most frequent words excluding function words ( or stop words on RCV1 ) .",Semi-supervised experiments,"Similar to JZ15 b , we minimized weighted square",text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.1372549019607843,201,0.78515625,7,0.25,1,0,"Semi-supervised experiments: Similar to JZ15 b , we minimized weighted square"
209,"We used the same configuration on Elec as DL15 ; however , on RCV1 , which has 55 classes , 30 hidden units turned out to be too few and we changed it to 1000 .",Semi-supervised experiments,Other details followed the supervised experiments .,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.2745098039215686,208,0.8125,14,0.5,1,0,Semi-supervised experiments: Other details followed the supervised experiments .
211,"Previous studies on LSTM for text often convert words into pre-trained word vectors , and word2vec is a popular choice for this purpose .",Semi-supervised experiments,Other details followed the supervised experiments .,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.3137254901960784,210,0.8203125,16,0.5714285714285714,1,0,Semi-supervised experiments: Other details followed the supervised experiments .
212,"Therefore , we tested wv - 2 LSTMp ( word - vector bidirectional LSTM with pooling ) , whose only difference from oh - 2 LSTMp is that the input to the LSTM layers is the pre-trained word vectors .",Semi-supervised experiments,Other details followed the supervised experiments .,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.3333333333333333,211,0.82421875,17,0.6071428571428571,1,0,Semi-supervised experiments: Other details followed the supervised experiments .
228,Error rates ( % ) .,Semi-supervised experiments,,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O']",33,0.6470588235294118,227,0.88671875,4,0.2666666666666666,1,0,Semi-supervised experiments
229,""" U "" : Was unlabeled data used ?",Semi-supervised experiments,Error rates ( % ) .,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",34,0.6666666666666666,228,0.890625,5,0.3333333333333333,1,0,Semi-supervised experiments: Error rates ( % ) .
234,"In the second setting , we trained one - hot CNN with these five types of tv-embeddings by replacing ( 1 ) max ( 0 , Wx + b ) by max ( 0 , Wx + j W ( j ) x j + b ) where x j is the output of the j - th tv-embedding with the same alignment as above .",Semi-supervised experiments,Error rates ( % ) .,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",39,0.7647058823529411,233,0.91015625,10,0.6666666666666666,1,0,Semi-supervised experiments: Error rates ( % ) .
240,Comparison with the previous best results,Semi-supervised experiments,,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O']",45,0.8823529411764706,239,0.93359375,0,0.0,1,0,Semi-supervised experiments
241,The previous best results in the literature are shown in Table 7 .,Semi-supervised experiments,Comparison with the previous best results,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",46,0.9019607843137256,240,0.9375,1,0.1666666666666666,1,0,Semi-supervised experiments: Comparison with the previous best results
242,"More results of previous semi-supervised models can be found in JZ15b , all of which clearly underperform the semi-supervised one - hot CNN of .",Semi-supervised experiments,Comparison with the previous best results,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",47,0.9215686274509804,241,0.94140625,2,0.3333333333333333,1,0,Semi-supervised experiments: Comparison with the previous best results
244,"Many more of the previous results on IMDB can be found in , all of which are over 10 % except for 8.78 by bi-gram NBSVM .",Semi-supervised experiments,Comparison with the previous best results,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",49,0.9607843137254902,243,0.94921875,4,0.6666666666666666,1,0,Semi-supervised experiments: Comparison with the previous best results
245,7.42 by paragraph vectors ) and 6.51 by JZ15 b were considered to be large improvements .,Semi-supervised experiments,Comparison with the previous best results,text-classification,1,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",50,0.9803921568627452,244,0.953125,5,0.8333333333333334,1,0,Semi-supervised experiments: Comparison with the previous best results
247,Conclusion,,,text-classification,1,['O'],['O'],0,0.0,246,0.9609375,0,0.0,1,0,
3,abstract,,,text-classification,2,['O'],['O'],0,0.0,2,0.021505376344086,0,0.0,1,0,
8,"Text classification is an important task in Natural Language Processing with many applications , such as web search , information retrieval , ranking and document classification .",Introduction,Introduction,text-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.1111111111111111,7,0.075268817204301,1,0.1111111111111111,1,0,Introduction
9,"Recently , models based on neural networks have become increasingly popular .",Introduction,Introduction,text-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.2222222222222222,8,0.086021505376344,2,0.2222222222222222,1,0,Introduction
11,"Meanwhile , linear classifiers are often considered as strong baselines for text classification problems .",Introduction,Introduction,text-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.4444444444444444,10,0.1075268817204301,4,0.4444444444444444,1,0,Introduction
13,They also have the potential to scale to very large corpus .,Introduction,Introduction,text-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.6666666666666666,12,0.1290322580645161,6,0.6666666666666666,1,0,Introduction
17,Model architecture,,,text-classification,2,"['O', 'O']","['O', 'O']",0,0.0,16,0.1720430107526881,0,0.0,1,0,
18,"A simple and efficient baseline for sentence classification is to represent sentences as bag of words ( BoW ) and train a linear classifier , e.g. , a logistic regression or an SVM .",Model architecture,Model architecture,text-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0333333333333333,17,0.1827956989247312,1,0.0769230769230769,1,0,Model architecture
19,"However , linear classifiers do not share parameters among features and classes .",Model architecture,Model architecture,text-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.0666666666666666,18,0.1935483870967742,2,0.1538461538461538,1,0,Model architecture
20,This possibly limits their generalization in the context of large output space where some classes have very few examples .,Model architecture,Model architecture,text-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.1,19,0.2043010752688172,3,0.2307692307692307,1,0,Model architecture
21,Common solutions to this problem are to factorize the linear classifier into low rank matrices or to use multilayer neural networks .,Model architecture,Model architecture,text-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.1333333333333333,20,0.2150537634408602,4,0.3076923076923077,1,0,Model architecture
23,The first weight matrix A is a look - up table over the words .,Model architecture,Model architecture,text-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.2,22,0.2365591397849462,6,0.4615384615384615,1,0,Model architecture
25,The text representa - tion is an hidden variable which can be potentially be reused .,Model architecture,Model architecture,text-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.2666666666666666,24,0.2580645161290322,8,0.6153846153846154,1,0,Model architecture
26,"This architecture is similar to the cbow model of , where the middle word is replaced by a label .",Model architecture,Model architecture,text-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.3,25,0.2688172043010752,9,0.6923076923076923,1,0,Model architecture
28,"For a set of N documents , this leads to minimizing the negative loglikelihood over the classes :",Model architecture,Model architecture,text-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.3666666666666666,27,0.2903225806451613,11,0.8461538461538461,1,0,Model architecture
29,"where x n is the normalized bag of features of the nth document , y n the label , A and B the weight matrices .",Model architecture,Model architecture,text-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.4,28,0.3010752688172043,12,0.9230769230769232,1,0,Model architecture
32,"When the number of classes is large , computing the linear classifier is computationally expensive .",Model architecture,Hierarchical softmax,text-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.5,31,0.3333333333333333,1,0.0625,1,0,Model architecture: Hierarchical softmax
33,"More precisely , the computational complexity is O ( kh ) where k is the number of classes and h the dimension of the text representation .",Model architecture,Hierarchical softmax,text-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.5333333333333333,32,0.3440860215053763,2,0.125,1,0,Model architecture: Hierarchical softmax
36,The hierarchical softmax is also advantageous attest time when searching for the most likely class .,Model architecture,Hierarchical softmax,text-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.6333333333333333,35,0.3763440860215054,5,0.3125,1,0,Model architecture: Hierarchical softmax
37,Each node is associated with a probability that is the probability of the path from the root to that node .,Model architecture,Hierarchical softmax,text-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.6666666666666666,36,0.3870967741935484,6,0.375,1,0,Model architecture: Hierarchical softmax
38,"If the node is at depth l + 1 with parents n 1 , . . . , n l , it s probability is",Model architecture,Hierarchical softmax,text-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.7,37,0.3978494623655914,7,0.4375,1,0,Model architecture: Hierarchical softmax
39,This means that the probability of anode is always lower than the one of its parent .,Model architecture,Hierarchical softmax,text-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.7333333333333333,38,0.4086021505376344,8,0.5,1,0,Model architecture: Hierarchical softmax
44,Bag of words is invariant to word order but taking explicitly this order into account is often computationally very expensive .,Model architecture,N - gram features,text-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.9,43,0.4623655913978494,13,0.8125,1,0,Model architecture: N - gram features
48,Experiments,,,text-classification,2,['O'],['O'],0,0.0,47,0.5053763440860215,0,0.0,1,0,
52,"Note that our model could be implemented with the Vowpal Wabbit library , 2 but we observe in practice , that our tailored implementation is at least 2 - 5 faster .",Experiments,We evaluate fastText on two different tasks .,text-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.8,51,0.5483870967741935,4,1.0,1,0,Experiments: We evaluate fastText on two different tasks .
58,Results .,,,text-classification,2,"['O', 'O']","['O', 'O']",0,0.0,57,0.6129032258064516,5,0.3333333333333333,1,0,
66,"Unlike , fastText does not use pre-trained word embeddings , which can be explained the 1 % difference inaccuracy .",Results .,We present the results in .,text-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.7272727272727273,65,0.6989247311827957,13,0.8666666666666667,1,0,Results .: We present the results in .
69,Tag prediction,Results .,,text-classification,2,"['O', 'O']","['O', 'O']",11,1.0,68,0.7311827956989247,0,0.0,1,0,Results .
82,Results and training time . and 200 .,Dataset and baselines .,,text-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.7058823529411765,81,0.8709677419354839,13,0.7222222222222222,1,0,Dataset and baselines .
87,shows some qualitative examples .,Dataset and baselines .,Results and training time . and 200 .,text-classification,2,"['O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O']",17,1.0,86,0.9247311827956988,18,1.0,1,0,Dataset and baselines .: Results and training time . and 200 .
88,Discussion and conclusion,,,text-classification,2,"['O', 'O', 'O']","['O', 'O', 'O']",0,0.0,87,0.935483870967742,0,0.0,1,0,
3,abstract,,,text-classification,3,['O'],['O'],0,0.0,2,0.016260162601626,0,0.0,1,0,
4,"Text preprocessing is often the first step in the pipeline of a Natural Language Processing ( NLP ) system , with potential impact in its final performance .",abstract,abstract,text-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.1428571428571428,3,0.024390243902439,1,0.1428571428571428,1,0,abstract
5,"Despite its importance , text preprocessing has not received much attention in the deep learning literature .",abstract,abstract,text-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.2857142857142857,4,0.032520325203252,2,0.2857142857142857,1,0,abstract
12,"Words are often considered as the basic constituents of texts for many languages , including English .",Introduction,Introduction,text-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.02,11,0.089430894308943,1,0.0476190476190476,1,0,Introduction
14,"However , in practise , other preprocessing techniques can be ( and are ) further used together with tokenization .",Introduction,Introduction,text-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.06,13,0.1056910569105691,3,0.1428571428571428,1,0,Introduction
15,"These include lemmatization , lowercasing and 1 Note that although word - based models are mainstream in NLP in general and text classification in particular , recent work has also considered other linguistic units , such as characters or word senses .",Introduction,Introduction,text-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.08,14,0.1138211382113821,4,0.1904761904761904,1,0,Introduction
16,"These techniques require a different kind of preprocessing and , while they have been shown effective in various settings , in this work we only focus on the mainstream word - based models .",Introduction,Introduction,text-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.1,15,0.1219512195121951,5,0.238095238095238,1,0,Introduction
18,"Although these preprocessing decisions have been studied in the context of conventional text classification techniques , little attention has been paid to them in the more recent neural - based models .",Introduction,Introduction,text-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.14,17,0.1382113821138211,7,0.3333333333333333,1,0,Introduction
19,"The most similar study to ours is , which analyzed different encoding levels for English and Asian languages such as Chinese , Japanese and Korean .",Introduction,Introduction,text-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.16,18,0.1463414634146341,8,0.3809523809523809,1,0,Introduction
20,"As opposed to our work , their analysis was focused on UTF - 8 bytes , characters , words , romanized characters and romanized words as encoding levels , rather than the preprocessing techniques analyzed in this paper .",Introduction,Introduction,text-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.18,19,0.1544715447154471,9,0.4285714285714285,1,0,Introduction
21,"Additionally , word embeddings have been shown to play an important role in boosting the generalization capabilities of neural systems .",Introduction,Introduction,text-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.2,20,0.1626016260162601,10,0.4761904761904761,1,0,Introduction
22,"However , while some studies have focused on intrinsically analyzing the role of lemmatization in their underlying training corpus , the impact on their extrinsic performance when integrated into a neural network architecture has remained understudied .",Introduction,Introduction,text-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.22,21,0.1707317073170731,11,0.5238095238095238,1,0,Introduction
26,1 .,Introduction,Introduction,text-classification,3,"['O', 'O']","['O', 'O']",15,0.3,25,0.2032520325203252,15,0.7142857142857143,1,0,Introduction
28,2 .,Introduction,Introduction,text-classification,3,"['O', 'O']","['O', 'O']",17,0.34,27,0.2195121951219512,17,0.8095238095238095,1,0,Introduction
33,Text Preprocessing,Introduction,,text-classification,3,"['O', 'O']","['O', 'O']",22,0.44,32,0.2601626016260163,0,0.0,1,0,Introduction
35,We refer to the corpus which is only tokenized as vanilla .,Introduction,Text Preprocessing,text-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.48,34,0.2764227642276423,2,0.4,1,0,Introduction: Text Preprocessing
36,"For example , given the sentence "" Apple is asking its manufacturers to move Mac - Book Air production to the United States . "" ( running example ) , the vanilla tokenized text would be as follows ( white spaces delimiting different word units ) :",Introduction,Text Preprocessing,text-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.5,35,0.2845528455284553,3,0.6,1,0,Introduction: Text Preprocessing
37,Apple is asking its manufacturers to move MacBook Air production to the United States .,Introduction,Text Preprocessing,text-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.52,36,0.2926829268292683,4,0.8,1,0,Introduction: Text Preprocessing
39,Lowercasing,Introduction,,text-classification,3,['O'],['O'],28,0.56,38,0.3089430894308943,0,0.0,1,0,Introduction
40,This is the simplest preprocessing technique which consists of lowercasing each single token of the input text :,Introduction,Lowercasing,text-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",29,0.58,39,0.3170731707317073,1,0.2,1,0,Introduction: Lowercasing
41,apple is asking its manufacturers to move macbook air production to the united states .,Introduction,Lowercasing,text-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",30,0.6,40,0.3252032520325203,2,0.4,1,0,Introduction: Lowercasing
42,"Due to its simplicity , lowercasing has been a popular practice in modules of deep learning libraries and word embedding packages .",Introduction,Lowercasing,text-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",31,0.62,41,0.3333333333333333,3,0.6,1,0,Introduction: Lowercasing
43,"Despite its desirable property of reducing sparsity and vocabulary size , lowercasing may negatively impact system 's performance by increasing ambiguity .",Introduction,Lowercasing,text-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",32,0.64,42,0.3414634146341463,4,0.8,1,0,Introduction: Lowercasing
44,"For instance , the Apple company in our example and the apple fruit would be considered as identical entities .",Introduction,Lowercasing,text-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",33,0.66,43,0.3495934959349593,5,1.0,1,0,Introduction: Lowercasing
45,Lemmatizing,Introduction,,text-classification,3,['O'],['O'],34,0.68,44,0.3577235772357723,0,0.0,1,0,Introduction
46,The process of lemmatizing consists of replacing a given token with its corresponding lemma :,Introduction,Lemmatizing,text-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",35,0.7,45,0.3658536585365853,1,0.1666666666666666,1,0,Introduction: Lemmatizing
48,Lemmatization has been traditionally a standard preprocessing technique for linear text classification systems .,Introduction,Lemmatizing,text-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",37,0.74,47,0.3821138211382114,3,0.5,1,0,Introduction: Lemmatizing
49,"However , it is rarely used as a preprocessing stage in neural - based systems .",Introduction,Lemmatizing,text-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",38,0.76,48,0.3902439024390244,4,0.6666666666666666,1,0,Introduction: Lemmatizing
50,"The main idea behind lemmatization is to reduce sparsity , as different inflected forms of the same lemma may occur infrequently ( or not at all ) during training .",Introduction,Lemmatizing,text-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",39,0.78,49,0.3983739837398374,5,0.8333333333333334,1,0,Introduction: Lemmatizing
51,"However , this may come at the cost of neglecting important syntactic nuances .",Introduction,Lemmatizing,text-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",40,0.8,50,0.4065040650406504,6,1.0,1,0,Introduction: Lemmatizing
52,Multiword grouping,Introduction,,text-classification,3,"['O', 'O']","['O', 'O']",41,0.82,51,0.4146341463414634,0,0.0,1,0,Introduction
53,This last preprocessing technique consists of grouping consecutive tokens together into a single token if found in a given inventory :,Introduction,Multiword grouping,text-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",42,0.84,52,0.4227642276422764,1,0.1111111111111111,1,0,Introduction: Multiword grouping
54,Apple is asking its manufacturers to move MacBook Air production to the United States .,Introduction,Multiword grouping,text-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",43,0.86,53,0.4308943089430894,2,0.2222222222222222,1,0,Introduction: Multiword grouping
55,"The motivation behind this step lies in the idiosyncratic nature of multiword expressions , e.g. United States in the example .",Introduction,Multiword grouping,text-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",44,0.88,54,0.4390243902439024,3,0.3333333333333333,1,0,Introduction: Multiword grouping
56,The meaning of these multiword expressions are often hardly traceable from their individual tokens .,Introduction,Multiword grouping,text-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",45,0.9,55,0.4471544715447154,4,0.4444444444444444,1,0,Introduction: Multiword grouping
57,"As a result , treating multiwords as single units may lead to better training of a given model .",Introduction,Multiword grouping,text-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",46,0.92,56,0.4552845528455284,5,0.5555555555555556,1,0,Introduction: Multiword grouping
58,"Because of this , word embedding toolkits such as Word2vec propose statistical approaches for extracting these multiwords , or directly include multiwords along with single words in their pretrained embedding spaces .",Introduction,Multiword grouping,text-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",47,0.94,57,0.4634146341463415,6,0.6666666666666666,1,0,Introduction: Multiword grouping
62,Experimental setup,,,text-classification,3,"['O', 'O']","['O', 'O']",0,0.0,61,0.4959349593495935,0,0.0,1,0,
69,4 .,Experimental setup,We tried with two classification models .,text-classification,3,"['O', 'O']","['O', 'O']",7,1.0,68,0.5528455284552846,7,0.14,1,0,Experimental setup: We tried with two classification models .
70,Evaluation datasets .,,,text-classification,3,"['O', 'O', 'O']","['O', 'O', 'O']",0,0.0,69,0.5609756097560976,8,0.16,1,0,
73,5 http://mlg.ucd.ie/datasets/bbc.html,Evaluation datasets .,Evaluation datasets .,text-classification,3,"['O', 'O']","['O', 'O']",3,0.1153846153846153,72,0.5853658536585366,11,0.22,1,0,Evaluation datasets .
75,Preprocessing .,Evaluation datasets .,,text-classification,3,"['O', 'O']","['O', 'O']",5,0.1923076923076923,74,0.6016260162601627,13,0.26,1,0,Evaluation datasets .
82,9 http://www.rottentomatoes.com,Evaluation datasets .,Preprocessing .,text-classification,3,"['O', 'O']","['O', 'O']",12,0.4615384615384615,81,0.6585365853658537,20,0.4,1,0,Evaluation datasets .: Preprocessing .
83,"10 We mapped the numerical value of phrases to either negative ( from 0 to 0.4 ) or positive ( from 0.6 to 1 ) , removing the neutral phrases according to the scale ( from 0.4 to 0.6 ) .",Evaluation datasets .,Preprocessing .,text-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.5,82,0.6666666666666666,21,0.42,1,0,Evaluation datasets .: Preprocessing .
84,"For the datasets with train - test partitions , the sizes of the test sets are the following : 7,532 for 20 News ; 12,733 for Ohsumed ; 25,000 for IMDb ; and 1,000 for RTC .",Evaluation datasets .,Preprocessing .,text-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.5384615384615384,83,0.6747967479674797,22,0.44,1,0,Evaluation datasets .: Preprocessing .
85,For future work it would be interesting to explore more complex methods to learn embeddings for multiword expressions .,Evaluation datasets .,Preprocessing .,text-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.5769230769230769,84,0.6829268292682927,23,0.46,1,0,Evaluation datasets .: Preprocessing .
89,"This suggests that the preprocessing decisions are not so important when the training data is large enough , but they are indeed relevant in benchmarks where the training data is limited .",Evaluation datasets .,Computed by averaging accuracy of two different runs .,text-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.7307692307692307,88,0.7154471544715447,27,0.54,1,0,Evaluation datasets .: Computed by averaging accuracy of two different runs .
91,"The only topic categorization dataset in which tokenization does not seem enough is Ohsumed , which , unlike the more general nature of other categorization datasets ( news ) , belongs to a specialized domain ( medical ) for which fine - grained distinctions are required to classify cardiovascular diseases .",Evaluation datasets .,Computed by averaging accuracy of two different runs .,text-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.8076923076923077,90,0.7317073170731707,29,0.58,1,0,Evaluation datasets .: Computed by averaging accuracy of two different runs .
92,"In particular for this dataset , word embeddings trained on a general - domain corpus like UMBC may not accurately capture the specialized meaning of medical terms and hence , sparsity becomes an issue .",Evaluation datasets .,Computed by averaging accuracy of two different runs .,text-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.8461538461538461,91,0.7398373983739838,30,0.6,1,0,Evaluation datasets .: Computed by averaging accuracy of two different runs .
95,"Nevertheless , the use of more complex preprocessing techniques such as lemmatization and multiword grouping does not help in general .",Evaluation datasets .,Experiment 1 : Preprocessing effect,text-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.9615384615384616,94,0.7642276422764228,33,0.66,1,0,Evaluation datasets .: Experiment 1 : Preprocessing effect
96,"Even though lemmatization has proved useful in conventional linear models as an effective way to deal with sparsity , neural network architectures seem to be more capable of overcoming sparsity thanks to the generalization power of word embeddings .",Evaluation datasets .,Experiment 1 : Preprocessing effect,text-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,1.0,95,0.7723577235772358,34,0.68,1,0,Evaluation datasets .: Experiment 1 : Preprocessing effect
99,shows the results for this experiment .,Experiment 2 : Cross-preprocessing,Experiment 2 : Cross-preprocessing,text-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.1333333333333333,98,0.7967479674796748,37,0.74,1,0,Experiment 2 : Cross-preprocessing
101,In this case the same set of words is learnt but single tokens inside multiword expressions are not trained .,Experiment 2 : Cross-preprocessing,Experiment 2 : Cross-preprocessing,text-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.2666666666666666,100,0.8130081300813008,39,0.78,1,0,Experiment 2 : Cross-preprocessing
102,"Instead , these single tokens are considered in isolation only , without the added noise when considered inside the multiword expression as well .",Experiment 2 : Cross-preprocessing,Experiment 2 : Cross-preprocessing,text-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.3333333333333333,101,0.8211382113821138,40,0.8,1,0,Experiment 2 : Cross-preprocessing
103,"For instance , the word Apple has a clearly different meaning in isolation from the one inside :",Experiment 2 : Cross-preprocessing,Experiment 2 : Cross-preprocessing,text-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.4,102,0.8292682926829268,41,0.82,1,0,Experiment 2 : Cross-preprocessing
106,indicates results that are statistically significant with respect to the top result .,Experiment 2 : Cross-preprocessing,Experiment 2 : Cross-preprocessing,text-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.6,105,0.8536585365853658,44,0.88,1,0,Experiment 2 : Cross-preprocessing
107,"the multiword expression Big Apple , hence it can be seen as beneficial not to train the word",Experiment 2 : Cross-preprocessing,Experiment 2 : Cross-preprocessing,text-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.6666666666666666,106,0.8617886178861789,45,0.9,1,0,Experiment 2 : Cross-preprocessing
108,Apple when part of this multiword expression .,Experiment 2 : Cross-preprocessing,,text-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.7333333333333333,107,0.8699186991869918,46,0.92,1,0,Experiment 2 : Cross-preprocessing
110,"This could provide hints on the excellent results provided by pre-trained Word2vec embeddings trained on the Google News corpus , which learns multiwords similarly to our setting .",Experiment 2 : Cross-preprocessing,Apple when part of this multiword expression .,text-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.8666666666666667,109,0.8861788617886179,48,0.96,1,0,Experiment 2 : Cross-preprocessing: Apple when part of this multiword expression .
112,"In fact , the relatively weaker performance of lemmatization and lowercasing in this crossprocessing experiment is somehow expected as the coverage of word embeddings in vanilla - tokenized datasets is limited , e.g. , many entities which are capitalized in the datasets are not covered in the case of lowercasing , and inflected forms are missing in the case of lemmatizing .",Experiment 2 : Cross-preprocessing,Apple when part of this multiword expression .,text-classification,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,1.0,111,0.902439024390244,50,1.0,1,0,Experiment 2 : Cross-preprocessing: Apple when part of this multiword expression .
113,Conclusions,,,text-classification,3,['O'],['O'],0,0.0,112,0.9105691056910568,0,0.0,1,0,
3,abstract,,,text-classification,4,['O'],['O'],0,0.0,2,0.0090497737556561,0,0.0,1,0,
5,"Despite their success , most existing CNN models employed in NLP share the same learned ( and static ) set of filters for all input sentences .",abstract,abstract,text-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.2857142857142857,4,0.0180995475113122,2,0.2857142857142857,1,0,abstract
13,CNNs are typically applied to tasks where feature extrac-tion and a corresponding supervised task are approached jointly .,Introduction,Introduction,text-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.0869565217391304,12,0.0542986425339366,2,0.0869565217391304,1,0,Introduction
14,"As an encoder network for text , CNNs typically convolve a set of filters , of window size n , with an inputsentence embedding matrix obtained via word2vec or Glove .",Introduction,Introduction,text-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.1304347826086956,13,0.0588235294117647,3,0.1304347826086956,1,0,Introduction
15,"Different filter sizes n maybe used within the same model , exploiting meaningful semantic features from different n-gram fragments .",Introduction,Introduction,text-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.1739130434782608,14,0.0633484162895927,4,0.1739130434782608,1,0,Introduction
16,"The learned weights of CNN filters , inmost cases , are assumed to be fixed regardless of the input text .",Introduction,Introduction,text-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.217391304347826,15,0.0678733031674208,5,0.217391304347826,1,0,Introduction
17,"As a result , the rich contextual information inherent in natural language sequences may not be fully captured .",Introduction,Introduction,text-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.2608695652173913,16,0.0723981900452488,6,0.2608695652173913,1,0,Introduction
21,"One common strategy is the attention mechanism , which is typically employed on top of a CNN ( or Long Short - Term Memory ( LSTM ) ) layer to guide the extraction of semantic features .",Introduction,Introduction,text-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.4347826086956521,20,0.090497737556561,10,0.4347826086956521,1,0,Introduction
23,"However , their model needs considerably more parameters to achieve performance gains over traditional CNNs .",Introduction,Introduction,text-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.5217391304347826,22,0.0995475113122171,12,0.5217391304347826,1,0,Introduction
25,"However , these approaches suffer from the problem of high matching complexity , since a similarity matrix between pairwise words needs to be computed , and thus it is computationally inefficient or even prohibitive when applied to long sentences .",Introduction,Introduction,text-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.6086956521739131,24,0.1085972850678733,14,0.6086956521739131,1,0,Introduction
27,"In contrast to traditional CNNs , the convolution operation in our framework does not have a fixed set of filters , and thus provides the network with stronger modeling flexibility and capacity .",Introduction,Introduction,text-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.6956521739130435,26,0.1176470588235294,16,0.6956521739130435,1,0,Introduction
35,Related Work,,,text-classification,4,"['O', 'O']","['O', 'O']",0,0.0,34,0.1538461538461538,0,0.0,1,0,
48,Model,,,text-classification,4,['O'],['O'],0,0.0,47,0.2126696832579185,0,0.0,1,0,
50,"The CNN architectures in are typically utilized for extracting sentence representations , by a composition of a convolutional layer and a max - pooling operation overall resulting feature maps .",Model,Model,text-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.0224719101123595,49,0.2217194570135746,1,0.0526315789473684,1,0,Model
52,The sentence can be represented as a matrix X ?,Model,Model,text-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.0449438202247191,51,0.2307692307692307,3,0.1578947368421052,1,0,Model
53,"R d T , where each column represents a d-dimensional embedding of the corresponding word .",Model,Model,text-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.0561797752808988,52,0.2352941176470588,4,0.2105263157894736,1,0,Model
54,"In the convolutional layer , a set of filters with weights W ?",Model,Model,text-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.0674157303370786,53,0.2398190045248869,5,0.2631578947368421,1,0,Model
55,"R Khd is convolved with every window of h words within the sentence , i.e. , {x 1:h , x 2:h+1 , . . . , x T ?h+1:T } , where K is the number of output feature maps ( and filters ) .",Model,Model,text-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.0786516853932584,54,0.2443438914027149,6,0.3157894736842105,1,0,Model
56,"In this manner , feature maps p for these h-gram text fragments are generated as :",Model,Model,text-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.0898876404494382,55,0.248868778280543,7,0.3684210526315789,1,0,Model
57,"where i = 1 , 2 , ... , T ? h + 1 and denotes the convolution operator at the ith shift location .",Model,Model,text-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.1011235955056179,56,0.253393665158371,8,0.4210526315789473,1,0,Model
58,Parameter b ?,Model,Model,text-classification,4,"['O', 'O', 'O']","['O', 'O', 'O']",10,0.1123595505617977,57,0.2579185520361991,9,0.4736842105263157,1,0,Model
59,"R K is the bias term and f ( ) is a non-linear function , implemented as a rectified linear unit ( ReLU ) in our experiments .",Model,Model,text-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.1235955056179775,58,0.2624434389140271,10,0.5263157894736842,1,0,Model
60,"The output feature maps of the convolutional layer , i.e. , p ? R K(T ?h+1 ) are then passed to the pooling layer , which takes the maximum value in every row of p , forming a K-dimensional vector , z.",Model,Model,text-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.1348314606741573,59,0.2669683257918552,11,0.5789473684210527,1,0,Model
63,"Note that in basic CNN sentence encoders , filter weights are the same for different inputs , which maybe suboptimal for feature extraction , especially in the case where conditional information is available .",Model,Model,text-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.1685393258426966,62,0.2805429864253393,14,0.7368421052631579,1,0,Model
68,The general ACNN framework is shown schematically in .,Model,Model,text-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.2247191011235955,67,0.3031674208144796,19,1.0,1,0,Model
69,Convolution module,Model,,text-classification,4,"['O', 'O']","['O', 'O']",21,0.2359550561797752,68,0.3076923076923077,0,0.0,1,0,Model
71,Filter generation module,Model,Convolution module,text-classification,4,"['O', 'O', 'O']","['O', 'O', 'O']",23,0.2584269662921348,70,0.3167420814479638,0,0.0,1,0,Model: Convolution module
75,"On top of this hidden representation z , a deconvolutional layer , which performs transposed operations of convolutions , is further applied to produce a unique set of filters for X ( as illustrated in ) :",Model,Convolution module,text-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.3033707865168539,74,0.334841628959276,4,0.2,1,0,Model: Convolution module
76,(,Model,Convolution module,text-classification,4,['O'],['O'],28,0.3146067415730337,75,0.3393665158371041,5,0.25,1,0,Model: Convolution module
77,where ?,Model,Convolution module,text-classification,4,"['O', 'O']","['O', 'O']",29,0.3258426966292135,76,0.3438914027149321,6,0.3,1,0,Model: Convolution module
78,e and ?,Model,Convolution module,text-classification,4,"['O', 'O', 'O']","['O', 'O', 'O']",30,0.3370786516853932,77,0.3484162895927601,7,0.35,1,0,Model: Convolution module
79,"dare the learned parameters in each layer of the filter - generating module , respectively .",Model,Convolution module,text-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",31,0.348314606741573,78,0.3529411764705882,8,0.4,1,0,Model: Convolution module
80,"Specifically , we convolve z with a filter of size ( f s , l , k x , k y ) , where f sis the number of generated filters and the kernel size is ( k x , k y ) .",Model,Convolution module,text-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",32,0.3595505617977528,79,0.3574660633484163,9,0.45,1,0,Model: Convolution module
81,"The output will be a tensor of shape ( f s , k x , k y ) .",Model,Convolution module,text-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",33,0.3707865168539326,80,0.3619909502262443,10,0.5,1,0,Model: Convolution module
82,"Since the dimension of hidden representation z is independent of input - sentence length , this framework guarantees that the generated filters are of the same shape and size for every sentence .",Model,Convolution module,text-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",34,0.3820224719101123,81,0.3665158371040724,11,0.55,1,0,Model: Convolution module
83,"Intuitively , the encoding part of filter generation module abstracts the information from sentence X into z .",Model,Convolution module,text-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",35,0.3932584269662921,82,0.3710407239819004,12,0.6,1,0,Model: Convolution module
84,"Based on this representation , the deconvolutional up - sampling layer determines a set of fixedsize , fine - grained filters f for the specific input .",Model,Convolution module,text-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",36,0.4044943820224719,83,0.3755656108597285,13,0.65,1,0,Model: Convolution module
85,Adaptive convolution module,Model,Convolution module,text-classification,4,"['O', 'O', 'O']","['O', 'O', 'O']",37,0.4157303370786517,84,0.3800904977375565,14,0.7,1,0,Model: Convolution module
86,The adaptive convolution module takes as inputs the generated filters f and an input sentence .,Model,Convolution module,text-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",38,0.4269662921348314,85,0.3846153846153846,15,0.75,1,0,Model: Convolution module
87,This sentence and the input to the filter - generation module maybe identical ( as in ) or different ( as in .,Model,Convolution module,text-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",39,0.4382022471910112,86,0.3891402714932127,16,0.8,1,0,Model: Convolution module
89,"Notably , there are no additional parameters in the adaptive convolution module ( no bias term is employed ) .",Model,Convolution module,text-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",41,0.4606741573033708,88,0.3981900452488687,18,0.9,1,0,Model: Convolution module
90,"Our ACNN framework can be seen as a generalization of the basic CNN , which can be represented as an ACNN by setting the outputs of the filter - generation module to a constant , regardless of the contextual information from input sentence ( s ) .",Model,Convolution module,text-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",42,0.4719101123595505,89,0.4027149321266968,19,0.95,1,0,Model: Convolution module
91,"Because of the learning - to - learn nature of the proposed ACNN framework , it tends to have greater representational power than the basic CNN .",Model,Convolution module,text-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",43,0.4831460674157303,90,0.4072398190045249,20,1.0,1,0,Model: Convolution module
92,Extension to text sequence matching,Model,Convolution module,text-classification,4,"['O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O']",44,0.4943820224719101,91,0.4117647058823529,0,0.0,1,0,Model: Convolution module
96,"Given a factual question q ( associated with a list of candidate answers {a 1 , a 2 , . . . , am } and their corresponding labels y = {y 1 , y 2 , . . . , y m } ) , the goal of the model is to identify the correct answers from the set of candidates .",Model,Convolution module,text-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",48,0.5393258426966292,95,0.4298642533936652,4,0.1,1,0,Model: Convolution module
97,"For i = 1 , 2 , . . . , m , if a i correctly answers q , then y i = 1 , and otherwise y i =",Model,Convolution module,text-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",49,0.550561797752809,96,0.4343891402714932,5,0.125,1,0,Model: Convolution module
98,"0 . Therefore , the task can be cast as a classification problem where , given an unlabeled question - answer pair ( q i , a i ) , we seek to predict the judgement y i .",Model,Convolution module,text-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",50,0.5617977528089888,97,0.4389140271493212,6,0.15,1,0,Model: Convolution module
99,"Conventionally , a question q and an answer a are independently encoded by two basic CNNs to fixed - length vector representations , denoted h q and ha , respectively .",Model,Convolution module,text-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",51,0.5730337078651685,98,0.4434389140271493,7,0.175,1,0,Model: Convolution module
100,They are then directly employed to predict the judgement y .,Model,Convolution module,text-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",52,0.5842696629213483,99,0.4479638009049774,8,0.2,1,0,Model: Convolution module
101,"This strategy could be suboptimal , since no communication ( information sharing ) occurs between the questionanswer pair until the top prediction layer .",Model,Convolution module,text-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",53,0.5955056179775281,100,0.4524886877828054,9,0.225,1,0,Model: Convolution module
102,"Intuitively , while the model is inferring the representation fora question , if the meaning of the answer is The AdaQA model can be divided into three modules : filter generation , adaptive convolution , and matching modules , as depicted schematically in .",Model,Convolution module,text-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",54,0.6067415730337079,101,0.4570135746606334,10,0.25,1,0,Model: Convolution module
103,"Assume there is a question - answer pair to be matched , represented by word - embedding matrices , i.e. Q ? R Tqd and A ?",Model,Convolution module,text-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",55,0.6179775280898876,102,0.4615384615384615,11,0.275,1,0,Model: Convolution module
104,"R Tad , where dis the embedding dimension and T q and Ta are respective sentence lengths .",Model,Convolution module,text-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",56,0.6292134831460674,103,0.4660633484162896,12,0.3,1,0,Model: Convolution module
108,"where CNN and DCNN denote the basic CNN unit and deconvolution layer , respectively , as discussed in Section 2.1 .",Model,Convolution module,text-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",60,0.6741573033707865,107,0.4841628959276018,16,0.4,1,0,Model: Convolution module
109,Parameters ?,Model,Convolution module,text-classification,4,"['O', 'O']","['O', 'O']",61,0.6853932584269663,108,0.4886877828054298,17,0.425,1,0,Model: Convolution module
110,q e and ?,Model,Convolution module,text-classification,4,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",62,0.6966292134831461,109,0.4932126696832579,18,0.45,1,0,Model: Convolution module
111,q dare to be learned .,Model,Convolution module,text-classification,4,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O']",63,0.7078651685393258,110,0.497737556561086,19,0.475,1,0,Model: Convolution module
112,"The same process can be utilized to produce encodings z a and filters fa for the answer input , A , with parameters ?",Model,Convolution module,text-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",64,0.7191011235955056,111,0.502262443438914,20,0.5,1,0,Model: Convolution module
113,a e and ?,Model,Convolution module,text-classification,4,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",65,0.7303370786516854,112,0.5067873303167421,21,0.525,1,0,Model: Convolution module
114,"ad , respectively .",Model,Convolution module,text-classification,4,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",66,0.7415730337078652,113,0.5113122171945701,22,0.55,1,0,Model: Convolution module
116,"That is , the question embedding is convolved with the filters produced by the answer and vise versa (? q and ?",Model,Convolution module,text-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",68,0.7640449438202247,115,0.5203619909502263,24,0.6,1,0,Model: Convolution module
117,a are the bias terms to be learned ) .,Model,Convolution module,text-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",69,0.7752808988764045,116,0.5248868778280543,25,0.625,1,0,Model: Convolution module
118,The key idea is to abstract informa-tion from the answer ( or question ) that is pertinent to the corresponding question ( or answer ) .,Model,Convolution module,text-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",70,0.7865168539325843,117,0.5294117647058824,26,0.65,1,0,Model: Convolution module
120,We then employ the question and answer representations h q ?,Model,Convolution module,text-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",72,0.8089887640449438,119,0.5384615384615384,28,0.7,1,0,Model: Convolution module
122,Rn h as inputs to the matching module ( where n h is the dimension of question / answer embeddings ) .,Model,Convolution module,text-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",74,0.8314606741573034,121,0.5475113122171946,30,0.75,1,0,Model: Convolution module
123,"Following , the matching function is defined as :",Model,Convolution module,text-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",75,0.8426966292134831,122,0.5520361990950227,31,0.775,1,0,Model: Convolution module
124,where ?,Model,Convolution module,text-classification,4,"['O', 'O']","['O', 'O']",76,0.8539325842696629,123,0.5565610859728507,32,0.8,1,0,Model: Convolution module
125,"and denote an element - wise subtraction and element - wise product , respectively .",Model,Convolution module,text-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",77,0.8651685393258427,124,0.5610859728506787,33,0.825,1,0,Model: Convolution module
126,[h a ; h b ] indicates that ha and h bare stacked as column vectors .,Model,Convolution module,text-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",78,0.8764044943820225,125,0.5656108597285068,34,0.85,1,0,Model: Convolution module
127,The resulting matching vector t ?,Model,Convolution module,text-classification,4,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O']",79,0.8876404494382022,126,0.5701357466063348,35,0.875,1,0,Model: Convolution module
128,R 4n h is then sent through an MLP layer ( with sigmoid activation function and parameters ?,Model,Convolution module,text-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",80,0.898876404494382,127,0.5746606334841629,36,0.9,1,0,Model: Convolution module
129,"to be learned ) to model the desired conditional distribution p ( y i = 1|h q , ha ) .",Model,Convolution module,text-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",81,0.9101123595505618,128,0.579185520361991,37,0.925,1,0,Model: Convolution module
135,"However , attention is typically operated on top of the hidden units preprocessed by CNN or LSTM layers , and assigns different weights to each unit according to a context vector .",Model,Convolution module,text-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",87,0.9775280898876404,134,0.6063348416289592,2,0.5,1,0,Model: Convolution module
138,Experimental Setup,,,text-classification,4,"['O', 'O']","['O', 'O']",0,0.0,137,0.6199095022624435,0,0.0,1,0,
139,Datasets,Experimental Setup,,text-classification,4,['O'],['O'],1,0.0833333333333333,138,0.6244343891402715,1,0.0303030303030303,1,0,Experimental Setup
150,A summary of all datasets is presented in .,Experimental Setup,Datasets,text-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,1.0,149,0.6742081447963801,12,0.3636363636363636,1,0,Experimental Setup: Datasets
151,Training Details,,,text-classification,4,"['O', 'O']","['O', 'O']",0,0.0,150,0.6787330316742082,13,0.3939393939393939,1,0,
160,"As described in Section 3.3 , the vector t , output from the matching module , is fed to the prediction layer , implemented as a one - layer MLP followed by the sigmoid function .",Training Details,Training Details,text-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.6923076923076923,159,0.7194570135746606,22,0.6666666666666666,1,0,Training Details
165,Baselines,,,text-classification,4,['O'],['O'],0,0.0,164,0.7420814479638009,27,0.8181818181818182,1,0,
168,Evaluation Metrics,,,text-classification,4,"['O', 'O']","['O', 'O']",0,0.0,167,0.755656108597285,30,0.9090909090909092,1,0,
169,"For document categorization and paraphrase identification tasks , we em - , are reported by , and are reported by .",Evaluation Metrics,Evaluation Metrics,text-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.3333333333333333,168,0.7601809954751131,31,0.9393939393939394,1,0,Evaluation Metrics
170,ploy the percentage of correct predictions on the test set to evaluate and compare different models .,Evaluation Metrics,Evaluation Metrics,text-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.6666666666666666,169,0.7647058823529411,32,0.9696969696969696,1,0,Evaluation Metrics
172,Experimental Results,,,text-classification,4,"['O', 'O']","['O', 'O']",0,0.0,171,0.7737556561085973,0,0.0,1,0,
176,"As a result , with only one convolutional filter and thus very limited modeling capacity , our S - ACNN model tends to be much more expressive than the basic CNN model , due to the flexibility of applying different filters to different sentences .",Experimental Results,Document Classification,text-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.0816326530612244,175,0.7918552036199095,3,0.2142857142857142,1,0,Experimental Results: Document Classification
183,Effect of number of filters,Experimental Results,Document Classification,text-classification,4,"['O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O']",11,0.2244897959183673,182,0.8235294117647058,10,0.7142857142857143,1,0,Experimental Results: Document Classification
188,Answer Sentence Selection,Experimental Results,,text-classification,4,"['O', 'O', 'O']","['O', 'O', 'O']",16,0.3265306122448979,187,0.8461538461538461,0,0.0,1,0,Experimental Results
195,This observation indicates that the bidirectional filter gener - Model Accuracy Siamese - CNN 0.7960,Experimental Results,Answer Sentence Selection,text-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",23,0.4693877551020408,194,0.8778280542986425,7,0.3888888888888889,1,0,Experimental Results: Answer Sentence Selection
199,"The same trend is also observed on the SelQA dataset ( as shown in ) , which is a much larger dataset than Wiki QA .",Experimental Results,Answer Sentence Selection,text-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.5510204081632653,198,0.8959276018099548,11,0.6111111111111112,1,0,Experimental Results: Answer Sentence Selection
202,"However , in our AdaQA model , the communication between two sentences is inherent in the convolution operation , and thus can provide the abstracted features with more flexibility ; ( ii ) the bidirectional filter generation mechanism in our AdaQA model generates co-dependent representations for the question and candidate answer , which could enable the model to recover from initial local maxima corresponding to incorrect predictions .",Experimental Results,Answer Sentence Selection,text-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",30,0.6122448979591837,201,0.9095022624434388,14,0.7777777777777778,1,0,Experimental Results: Answer Sentence Selection
203,Paragraph Identification,Experimental Results,,text-classification,4,"['O', 'O']","['O', 'O']",31,0.6326530612244898,202,0.9140271493212668,15,0.8333333333333334,1,0,Experimental Results
207,Discussion,Experimental Results,,text-classification,4,['O'],['O'],35,0.7142857142857143,206,0.9321266968325792,0,0.0,1,0,Experimental Results
208,Reasoning ability,Experimental Results,,text-classification,4,"['O', 'O']","['O', 'O']",36,0.7346938775510204,207,0.9366515837104072,1,0.1666666666666666,1,0,Experimental Results
209,"To associate the improved answer sentence selection results with the reasoning capabilities of our AdaQA model , we further categorize the questions in the WikiQA test set into 5 types containing : ' What ' , ' Where ' , ' How ' , ' When ' or ' Who ' .",Experimental Results,Reasoning ability,text-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",37,0.7551020408163265,208,0.9411764705882352,2,0.3333333333333333,1,0,Experimental Results: Reasoning ability
214,Filter visualization,Experimental Results,,text-classification,4,"['O', 'O']","['O', 'O']",42,0.8571428571428571,213,0.9638009049773756,0,0.0,1,0,Experimental Results
216,The corresponding results are shown in ( c ) .,Experimental Results,Filter visualization,text-classification,4,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",44,0.8979591836734694,215,0.9728506787330315,2,0.2857142857142857,1,0,Experimental Results: Filter visualization
3,abstract,,,text-classification,5,['O'],['O'],0,0.0,2,0.0079365079365079,0,0.0,1,0,
4,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch .",abstract,abstract,text-classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.2,3,0.0119047619047619,1,0.2,1,0,abstract
11,"Applied CV models ( including object detection , classification , and segmentation ) are rarely trained from scratch , but instead are fine - tuned from models that have been pretrained on ImageNet , MS - COCO , and other datasets .",Introduction,Introduction,text-classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.0769230769230769,10,0.0396825396825396,2,0.0769230769230769,1,0,Introduction
12,"Text classification is a category of Natural Language Processing ( NLP ) tasks with real - world applications such as spam , fraud , and bot detection , emergency response , and commercial document classification , such as for legal discovery .",Introduction,Introduction,text-classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.1153846153846153,11,0.0436507936507936,3,0.1153846153846153,1,0,Introduction
13,1 http://nlp.fast.ai/ulmfit.,Introduction,Introduction,text-classification,5,"['O', 'O']","['O', 'O']",4,0.1538461538461538,12,0.0476190476190476,4,0.1538461538461538,1,0,Introduction
14,Equal contribution .,Introduction,,text-classification,5,"['O', 'O', 'O']","['O', 'O', 'O']",5,0.1923076923076923,13,0.0515873015873015,5,0.1923076923076923,1,0,Introduction
15,"Jeremy focused on the algorithm development and implementation , Sebastian focused on the experiments and writing .",Introduction,Equal contribution .,text-classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.2307692307692307,14,0.0555555555555555,6,0.2307692307692307,1,0,Introduction: Equal contribution .
16,"While Deep Learning models have achieved state - of - the - art on many NLP tasks , these models are trained from scratch , requiring large datasets , and days to converge .",Introduction,Equal contribution .,text-classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.2692307692307692,15,0.0595238095238095,7,0.2692307692307692,1,0,Introduction: Equal contribution .
17,Research in NLP focused mostly on transductive transfer .,Introduction,,text-classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.3076923076923077,16,0.0634920634920634,8,0.3076923076923077,1,0,Introduction
18,"For inductive transfer , fine - tuning pretrained word embeddings , a simple transfer technique that only targets a model 's first layer , has had a large impact in practice and is used inmost state - of - the - art models .",Introduction,Research in NLP focused mostly on transductive transfer .,text-classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.3461538461538461,17,0.0674603174603174,9,0.3461538461538461,1,0,Introduction: Research in NLP focused mostly on transductive transfer .
20,"In light of the benefits of pretraining , we should be able to do better than randomly initializing the remaining parameters of our models .",Introduction,Research in NLP focused mostly on transductive transfer .,text-classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.4230769230769231,19,0.0753968253968253,11,0.4230769230769231,1,0,Introduction: Research in NLP focused mostly on transductive transfer .
21,"However , inductive transfer via finetuning has been unsuccessful for NLP .",Introduction,Research in NLP focused mostly on transductive transfer .,text-classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.4615384615384615,20,0.0793650793650793,12,0.4615384615384615,1,0,Introduction: Research in NLP focused mostly on transductive transfer .
22,"first proposed finetuning a language model ( LM ) but require millions of in - domain documents to achieve good performance , which severely limits its applicability .",Introduction,Research in NLP focused mostly on transductive transfer .,text-classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.5,21,0.0833333333333333,13,0.5,1,0,Introduction: Research in NLP focused mostly on transductive transfer .
25,"Compared to CV , NLP models are typically more shallow and thus require different fine - tuning methods .",Introduction,Research in NLP focused mostly on transductive transfer .,text-classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.6153846153846154,24,0.0952380952380952,16,0.6153846153846154,1,0,Introduction: Research in NLP focused mostly on transductive transfer .
29,Contributions,Introduction,,text-classification,5,['O'],['O'],20,0.7692307692307693,28,0.1111111111111111,20,0.7692307692307693,1,0,Introduction
30,Our contributions are the following :,Introduction,Contributions,text-classification,5,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O']",21,0.8076923076923077,29,0.115079365079365,21,0.8076923076923077,1,0,Introduction: Contributions
36,Related work,,,text-classification,5,"['O', 'O']","['O', 'O']",0,0.0,35,0.1388888888888889,0,0.0,1,0,
147,Experiments,,,text-classification,5,['O'],['O'],0,0.0,146,0.5793650793650794,0,0.0,1,0,
149,Experimental setup,,,text-classification,5,"['O', 'O']","['O', 'O']",0,0.0,148,0.5873015873015873,0,0.0,1,0,
150,Datasets and tasks,Experimental setup,,text-classification,5,"['O', 'O', 'O']","['O', 'O', 'O']",1,0.1111111111111111,149,0.5912698412698413,1,0.1111111111111111,1,0,Experimental setup
155,"For topic classification , we evaluate on the large - scale AG news and DBpedia ontology datasets created by .",Experimental setup,Topic classification,text-classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.6666666666666666,154,0.6111111111111112,6,0.6666666666666666,1,0,Experimental setup: Topic classification
156,Pre-processing,Experimental setup,,text-classification,5,['O'],['O'],7,0.7777777777777778,155,0.6150793650793651,7,0.7777777777777778,1,0,Experimental setup
172,Results,,,text-classification,5,['O'],['O'],0,0.0,171,0.6785714285714286,0,0.0,1,0,
179,IMDb in particular is reflective of realworld datasets :,Results,Results,text-classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.3181818181818182,178,0.7063492063492064,7,0.4666666666666667,1,0,Results
180,"It s documents are generally a few paragraphs long - similar to emails ( e.g for legal discovery ) and online comments ( e.g for community management ) ; and sentiment analysis is similar to many commercial applications , e.g. product response tracking and support email routing .",Results,Results,text-classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.3636363636363636,179,0.7103174603174603,8,0.5333333333333333,1,0,Results
188,Analysis,Results,,text-classification,5,['O'],['O'],16,0.7272727272727273,187,0.7420634920634921,0,0.0,1,0,Results
193,Low - shot learning,Results,,text-classification,5,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",21,0.9545454545454546,192,0.7619047619047619,5,0.8333333333333334,1,0,Results
194,One of the main benefits of transfer learning is being able to train a model for,Results,Low - shot learning,text-classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,1.0,193,0.7658730158730159,6,1.0,1,0,Results: Low - shot learning
195,Pretraining,,,text-classification,5,['O'],['O'],0,0.0,194,0.7698412698412699,0,0.0,1,0,
205,Impact of pretraining,,,text-classification,5,"['O', 'O', 'O']","['O', 'O', 'O']",0,0.0,204,0.8095238095238095,0,0.0,1,0,
207,"Pretraining is most useful for small and medium - sized datasets , which are most common in commercial applications .",Impact of pretraining,Impact of pretraining,text-classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.0476190476190476,206,0.8174603174603174,2,0.0588235294117647,1,0,Impact of pretraining
209,Impact of LM quality,Impact of pretraining,,text-classification,5,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",4,0.0952380952380952,208,0.8253968253968254,4,0.1176470588235294,1,0,Impact of pretraining
213,Impact of LM fine - tuning,Impact of pretraining,,text-classification,5,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O']",8,0.1904761904761904,212,0.8412698412698413,8,0.2352941176470588,1,0,Impact of pretraining
217,Impact of classifier fine - tuning,Impact of pretraining,,text-classification,5,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O']",12,0.2857142857142857,216,0.8571428571428571,12,0.3529411764705882,1,0,Impact of pretraining
221,"We use a learning rate ? L = 0.01 for ' Discr ' , learning rates",Impact of pretraining,Impact of classifier fine - tuning,text-classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.3809523809523809,220,0.873015873015873,16,0.4705882352941176,1,0,Impact of pretraining: Impact of classifier fine - tuning
222,8,Impact of pretraining,Impact of classifier fine - tuning,text-classification,5,['O'],['O'],17,0.4047619047619047,221,0.876984126984127,17,0.5,1,0,Impact of pretraining: Impact of classifier fine - tuning
230,"Importantly , ULMFiT is the only method that shows excellent performance across the board - and is therefore the only universal method .",Impact of pretraining,We show the results in .,text-classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.5952380952380952,229,0.9087301587301588,25,0.7352941176470589,1,0,Impact of pretraining: We show the results in .
231,Classifier fine - tuning behavior,Impact of pretraining,,text-classification,5,"['O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O']",26,0.6190476190476191,230,0.9126984126984128,26,0.7647058823529411,1,0,Impact of pretraining
232,"While our results demonstrate that how we fine - tune the classifier makes a significant difference , fine - tuning for inductive transfer is currently under-explored in NLP as it mostly has been thought to be unhelpful .",Impact of pretraining,Classifier fine - tuning behavior,text-classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.6428571428571429,231,0.9166666666666666,27,0.7941176470588235,1,0,Impact of pretraining: Classifier fine - tuning behavior
235,The error then increases as the model starts to overfit and knowledge captured through pretraining is lost .,Impact of pretraining,Classifier fine - tuning behavior,text-classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",30,0.7142857142857143,234,0.9285714285714286,30,0.8823529411764706,1,0,Impact of pretraining: Classifier fine - tuning behavior
237,Impact of bidirectionality,Impact of pretraining,,text-classification,5,"['O', 'O', 'O']","['O', 'O', 'O']",32,0.7619047619047619,236,0.9365079365079364,32,0.9411764705882352,1,0,Impact of pretraining
240,Discussion and future directions,Impact of pretraining,,text-classification,5,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",35,0.8333333333333334,239,0.9484126984126984,0,0.0,1,0,Impact of pretraining
242,"Given that transfer learning and particularly fine - tuning for NLP is under - explored , many future directions are possible .",Impact of pretraining,Discussion and future directions,text-classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",37,0.8809523809523809,241,0.9563492063492064,2,0.2857142857142857,1,0,Impact of pretraining: Discussion and future directions
243,"One possible direction is to improve language model pretraining and fine - tuning and make them more scalable : for Image Net , predicting far fewer classes only incurs a small performance drop , while recent work shows that an alignment between source and target task label sets is important ) - focusing on predicting a subset of words such as the most frequent ones might retain most of the performance while speeding up training .",Impact of pretraining,Discussion and future directions,text-classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",38,0.9047619047619048,242,0.9603174603174603,3,0.4285714285714285,1,0,Impact of pretraining: Discussion and future directions
244,"Language modeling can also be augmented with additional tasks in a multi-task learning fashion or enriched with additional supervision , e.g. syntax - sensitive dependencies to create a model that is more general or better suited for certain downstream tasks , ideally in a weakly - supervised manner to retain its universal properties .",Impact of pretraining,Discussion and future directions,text-classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",39,0.9285714285714286,243,0.9642857142857144,4,0.5714285714285714,1,0,Impact of pretraining: Discussion and future directions
245,Another direction is to apply the method to novel tasks and models .,Impact of pretraining,Discussion and future directions,text-classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",40,0.9523809523809524,244,0.9682539682539684,5,0.7142857142857143,1,0,Impact of pretraining: Discussion and future directions
246,"While an extension to sequence labeling is straightforward , other tasks with more complex interactions such as entailment or question answering may require novel ways to pretrain and fine - tune .",Impact of pretraining,Discussion and future directions,text-classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",41,0.9761904761904762,245,0.9722222222222222,6,0.8571428571428571,1,0,Impact of pretraining: Discussion and future directions
247,"Finally , while we have provided a series of analyses and ablations , more studies are required to better understand what knowledge a pretrained language model captures , how this changes during fine - tuning , and what information different tasks require .",Impact of pretraining,Discussion and future directions,text-classification,5,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",42,1.0,246,0.9761904761904762,7,1.0,1,0,Impact of pretraining: Discussion and future directions
248,Conclusion,,,text-classification,5,['O'],['O'],0,0.0,247,0.98015873015873,0,0.0,1,0,
2,Universal Sentence Encoder,title,,text-classification,6,"['O', 'O', 'O']","['O', 'O', 'O']",1,0.0,1,0.0067567567567567,1,0.0,1,0,title
3,abstract,,,text-classification,6,['O'],['O'],0,0.0,2,0.0135135135135135,0,0.0,1,0,
14,Limited amounts of training data are available for many NLP tasks .,Introduction,Introduction,text-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0666666666666666,13,0.0878378378378378,1,0.0666666666666666,1,0,Introduction
16,"Given the high cost of annotating supervised training data , very large training sets are usually not available for most research or industry NLP tasks .",Introduction,Introduction,text-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.2,15,0.1013513513513513,3,0.2,1,0,Introduction
17,Many models address the problem by implicitly performing limited transfer learning through the use of pre-trained word embeddings such as those produced by word2vec or Glo Ve .,Introduction,Introduction,text-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.2666666666666666,16,0.1081081081081081,4,0.2666666666666666,1,0,Introduction
18,"However , recent work has demonstrated strong transfer task performance using pre-trained sentence level embeddings .",Introduction,Introduction,text-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.3333333333333333,17,0.1148648648648648,5,0.3333333333333333,1,0,Introduction
23,Engineering characteristics of models used for transfer learning are an important consideration .,Introduction,Introduction,text-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.6666666666666666,22,0.1486486486486486,10,0.6666666666666666,1,0,Introduction
26,"import tensorflow_hub as hub embed = hub.Module ( "" https://tfhub.dev/google/ "" "" universal- sentence - encoder / 1 "" ) embedding = embed ( [",Introduction,Introduction,text-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.8666666666666667,25,0.1689189189189189,13,0.8666666666666667,1,0,Introduction
27,""" The quick brown fox jumps over the lazy dog . "" ] )",Introduction,Introduction,text-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.9333333333333332,26,0.1756756756756756,14,0.9333333333333332,1,0,Introduction
38,Encoders,Model Toolkit,,text-classification,6,['O'],['O'],9,0.2903225806451613,37,0.25,0,0.0,1,0,Model Toolkit
43,Transformer,Model Toolkit,,text-classification,6,['O'],['O'],14,0.4516129032258064,42,0.2837837837837837,0,0.0,1,0,Model Toolkit
51,The Skip - Thought task replaces the LSTM used in the original formulation with a model based on the Transformer architecture .,Model Toolkit,Transformer,text-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.7096774193548387,50,0.3378378378378378,8,0.4705882352941176,1,0,Model Toolkit: Transformer
53,"However , this comes at the cost of compute time and memory usage scaling dramatically with sentence length .",Model Toolkit,Transformer,text-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.7741935483870968,52,0.3513513513513513,10,0.5882352941176471,1,0,Model Toolkit: Transformer
54,Deep Averaging Network ( DAN ),Model Toolkit,Transformer,text-classification,6,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O']",25,0.8064516129032258,53,0.3581081081081081,11,0.6470588235294118,1,0,Model Toolkit: Transformer
59,The primary advantage of the DAN encoder is that compute time is linear in the length of the input sequence .,Model Toolkit,Transformer,text-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",30,0.967741935483871,58,0.3918918918918919,16,0.9411764705882352,1,0,Model Toolkit: Transformer
61,Encoder Training Data,,,text-classification,6,"['O', 'O', 'O']","['O', 'O', 'O']",0,0.0,60,0.4054054054054054,0,0.0,1,0,
66,Transfer Tasks,Encoder Training Data,,text-classification,6,"['O', 'O']","['O', 'O']",5,0.25,65,0.4391891891891892,0,0.0,1,0,Encoder Training Data
80,"5 sim ( u , v ) = 1 ? arccos u v | | u || | | v|| /?",Encoder Training Data,Transfer Tasks,text-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.95,79,0.5337837837837838,14,0.9333333333333332,1,0,Encoder Training Data: Transfer Tasks
81,( 1 ),Encoder Training Data,Transfer Tasks,text-classification,6,"['O', 'O', 'O']","['O', 'O', 'O']",20,1.0,80,0.5405405405405406,15,1.0,1,0,Encoder Training Data: Transfer Tasks
82,Baselines,,,text-classification,6,['O'],['O'],0,0.0,81,0.5472972972972973,0,0.0,1,0,
92,T is the universal sentence encoder ( USE ) using Transformer .,Combined Transfer Models,USE,text-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.3636363636363636,91,0.6148648648648649,4,0.3636363636363636,1,0,Combined Transfer Models: USE
100,Experiments,,,text-classification,6,['O'],['O'],0,0.0,99,0.668918918918919,0,0.0,1,0,
103,"Otherwise , hyperparameters are tuned by crossvalidation on the task training data when available or the evaluation test data when neither training nor dev data are provided .",Experiments,Experiments,text-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.3333333333333333,102,0.6891891891891891,3,0.3333333333333333,1,0,Experiments
105,Transfer learning is critically important when training data fora target task is limited .,Experiments,Experiments,text-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.5555555555555556,104,0.7027027027027027,5,0.5555555555555556,1,0,Experiments
110,Results,,,text-classification,6,['O'],['O'],0,0.0,109,0.7364864864864865,0,0.0,1,0,
119,contrasts 's findings on bias within GloVe embeddings with the DAN variant of the universal encoder .,Results,Results,text-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.2903225806451613,118,0.7972972972972973,9,0.75,1,0,Results
122,The differences in word association patterns can be attributed to differences in the training data composition and the mixture of tasks used to train the sentence embeddings .,Results,Results,text-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.3870967741935484,121,0.8175675675675675,12,1.0,1,0,Results
123,Discussion,Results,,text-classification,6,['O'],['O'],13,0.4193548387096774,122,0.8243243243243243,0,0.0,1,0,Results
125,Using transfer learning is more critical when less training data is available .,Results,Discussion,text-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.4838709677419355,124,0.8378378378378378,2,0.4,1,0,Results: Discussion
126,"When task performance is close , the correct modeling choice should take into account engineering trade - offs regarding the memory and compute 6 Researchers and developers are strongly encouraged to independently verify whether biases in their overall model or model components impacts their use case .",Results,Discussion,text-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.5161290322580645,125,0.8445945945945946,3,0.6,1,0,Results: Discussion
129,Resource Usage,Results,,text-classification,6,"['O', 'O']","['O', 'O']",19,0.6129032258064516,128,0.8648648648648649,0,0.0,1,0,Results
132,Compute Usage,Results,,text-classification,6,"['O', 'O']","['O', 'O']",22,0.7096774193548387,131,0.8851351351351351,3,0.25,1,0,Results
137,"Since the DAN model is remarkably computational efficient , using GPUs over CPUs will often have a much larger practical impact for the transformer based encoder .",Results,Compute Usage,text-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.8709677419354839,136,0.918918918918919,8,0.6666666666666666,1,0,Results: Compute Usage
138,Memory Usage,Results,,text-classification,6,"['O', 'O']","['O', 'O']",28,0.9032258064516128,137,0.9256756756756755,9,0.75,1,0,Results
140,"We note that , for the DAN model , memory usage is dominated by the parameters used to store the model unigram and bigram embeddings .",Results,Memory Usage,text-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",30,0.967741935483871,139,0.9391891891891893,11,0.9166666666666666,1,0,Results: Memory Usage
141,"Since the transformer model only needs to store unigram embeddings , for short sequences it requires nearly half as much memory as the DAN model .",Results,Memory Usage,text-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",31,1.0,140,0.945945945945946,12,1.0,1,0,Results: Memory Usage
142,Conclusion,,,text-classification,6,['O'],['O'],0,0.0,141,0.9527027027027029,0,0.0,1,0,
3,abstract,,,text-classification,7,['O'],['O'],0,0.0,2,0.0082304526748971,0,0.0,1,0,
9,"To the best of our knowledge , this is the first work that capsule networks have been empirically investigated for text modeling 1 . * Corresponding author ( min.yang@siat.ac.cn )",abstract,abstract,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.8571428571428571,8,0.0329218106995884,6,0.8571428571428571,1,0,abstract
13,"It could be as simple as a keyword / phrase matching problem , but it could also be a nontrivial problem if compositions , hierarchies , and structures of texts are considered .",Introduction,Introduction,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.074074074074074,12,0.0493827160493827,2,0.074074074074074,1,0,Introduction
14,"For example , a news article which mentions a single phrase "" US election "" maybe categorized into the political news with high probability .",Introduction,Introduction,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.1111111111111111,13,0.0534979423868312,3,0.1111111111111111,1,0,Introduction
15,"But it could be very difficult fora computer to predict which presidential candidate is favored by its author , or whether the author 's view in the article is more liberal or more conservative .",Introduction,Introduction,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.1481481481481481,14,0.0576131687242798,4,0.1481481481481481,1,0,Introduction
16,"Earlier efforts in modeling texts have achieved limited success on text categorization using a simple bag - of - words classifier , implying understanding the meaning of the individual word or n-gram is a necessary step towards more sophisticated models .",Introduction,Introduction,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.1851851851851851,15,0.0617283950617283,5,0.1851851851851851,1,0,Introduction
17,"It is therefore not a surprise that distributed representations of words , a.k.a. word embeddings , have received great attention from NLP community addressing the question "" what "" to be modeled at the basic level .",Introduction,Introduction,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.2222222222222222,16,0.0658436213991769,6,0.2222222222222222,1,0,Introduction
18,"In order to model higher level concepts and facts in texts , an NLP researcher has to think cautiously the so - called "" what "" question : what is actually modeled beyond word meanings .",Introduction,Introduction,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.2592592592592592,17,0.0699588477366255,7,0.2592592592592592,1,0,Introduction
19,"A common approach to the question is to treat the texts as sequences and focus on their spatial patterns , whose representatives include convolutional neural networks ( CNNs ) and long shortterm memory networks ( LSTMs ) .",Introduction,Introduction,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.2962962962962963,18,0.074074074074074,8,0.2962962962962963,1,0,Introduction
20,"Another common approach is to completely ignore the order of words but focus on their compositions as a collection , whose representatives include probabilistic topic modeling and Earth Mover 's Distance based modeling .",Introduction,Introduction,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.3333333333333333,19,0.0781893004115226,9,0.3333333333333333,1,0,Introduction
21,"Those two approaches , albeit quite different from the computational perspective , actually follow a common measure to be diagnosed regarding their answers to the "" what "" question .",Introduction,Introduction,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.3703703703703703,20,0.0823045267489712,10,0.3703703703703703,1,0,Introduction
24,"For example , CNN builds convolutional feature detectors to extract local patterns from a window of vector sequences and uses max - pooling to select the most prominent ones .",Introduction,Introduction,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.4814814814814814,23,0.0946502057613168,13,0.4814814814814814,1,0,Introduction
26,"Being a spatially sensitive model , CNN pays a price for the inefficiency of replicating feature detectors on a grid .",Introduction,Introduction,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.5555555555555556,25,0.102880658436214,15,0.5555555555555556,1,0,Introduction
27,"As argued in , one has to choose between replicating detectors whose size grows exponentially with the number of dimensions , or increasing the volume of the labeled training set in a similar exponential way .",Introduction,Introduction,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.5925925925925926,26,0.1069958847736625,16,0.5925925925925926,1,0,Introduction
28,"On the other hand , methods that are spatially insensitive are perfectly efficient at the inference time regardless of any order of words or local patterns .",Introduction,Introduction,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.6296296296296297,27,0.1111111111111111,17,0.6296296296296297,1,0,Introduction
29,"However , they are unavoidably more restricted to encode rich structures presented in a sequence .",Introduction,Introduction,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.6666666666666666,28,0.1152263374485596,18,0.6666666666666666,1,0,Introduction
30,Improving the efficiency to encode spatial patterns while keeping the flexibility of their representation capability is thus a central issue .,Introduction,Introduction,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.7037037037037037,29,0.1193415637860082,19,0.7037037037037037,1,0,Introduction
31,A recent method called capsule network introduced by possesses this attractive potential to address the aforementioned issue .,Introduction,Introduction,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.7407407407407407,30,0.1234567901234567,20,0.7407407407407407,1,0,Introduction
33,A metaphor ( also as an argument ) they made is that human visual system intelligently assigns parts to wholes at the inference time without hard - coding patterns to be perspective relevant .,Introduction,Introduction,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.8148148148148148,32,0.1316872427983539,22,0.8148148148148148,1,0,Introduction
34,"As an outcome , their model could encode the intrinsic spatial relationship between apart and a whole constituting viewpoint invariant knowledge that automatically generalizes to novel viewpoints .",Introduction,Introduction,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",23,0.8518518518518519,33,0.1358024691358024,23,0.8518518518518519,1,0,Introduction
39,Our Model,,,text-classification,7,"['O', 'O']","['O', 'O']",0,0.0,38,0.1563786008230452,0,0.0,1,0,
40,"Our capsule network , depicted in , is a variant of the capsule networks proposed in .",Our Model,Our Model,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0106382978723404,39,0.1604938271604938,1,0.0555555555555555,1,0,Our Model
44,N - gram Convolutional Layer,Our Model,Our Model,text-classification,7,"['O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O']",5,0.0531914893617021,43,0.176954732510288,5,0.2777777777777778,1,0,Our Model
46,Suppose x ?,Our Model,Our Model,text-classification,7,"['O', 'O', 'O']","['O', 'O', 'O']",7,0.0744680851063829,45,0.1851851851851851,7,0.3888888888888889,1,0,Our Model
47,R LV denotes the input sentence representation where L is the length of the sentence and V is the embedding size of words .,Our Model,Our Model,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.0851063829787234,46,0.1893004115226337,8,0.4444444444444444,1,0,Our Model
49,RV be the V - dimensional word vector corresponding to the i - th word in the sentence .,Our Model,Our Model,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.1063829787234042,48,0.1975308641975308,10,0.5555555555555556,1,0,Our Model
50,Let W a ? R K 1,Our Model,Our Model,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.1170212765957446,49,0.2016460905349794,11,0.6111111111111112,1,0,Our Model
51,"V be the filter for the convolution operation , where K 1 is the N - gram size while sliding over a sentence for the purpose of detecting features at different positions .",Our Model,Our Model,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.1276595744680851,50,0.205761316872428,12,0.6666666666666666,1,0,Our Model
52,A filter W a convolves with the word - window x i:i+K 1 ? 1 at each possible position ( with stride of 1 ) to produce a column feature map ma ?,Our Model,Our Model,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.1382978723404255,51,0.2098765432098765,13,0.7222222222222222,1,0,Our Model
53,"R L?K 1 + 1 , each element ma i ?",Our Model,Our Model,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.1489361702127659,52,0.2139917695473251,14,0.7777777777777778,1,0,Our Model
54,R of the feature map is produced by,Our Model,Our Model,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.1595744680851064,53,0.2181069958847736,15,0.8333333333333334,1,0,Our Model
55,"where is element - wise multiplication , b 0 is a bias term , and f is a nonlinear activate function ( i.e. , ReLU ) .",Our Model,Our Model,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.1702127659574468,54,0.2222222222222222,16,0.8888888888888888,1,0,Our Model
57,"Hence , fora = 1 , . . . , B , totally B filters with the same N - gram size , one can generate B feature maps which can be rearranged as",Our Model,Our Model,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.1914893617021276,56,0.2304526748971193,18,1.0,1,0,Our Model
58,Primary Capsule Layer,Our Model,,text-classification,7,"['O', 'O', 'O']","['O', 'O', 'O']",19,0.202127659574468,57,0.2345679012345679,0,0.0,1,0,Our Model
61,"Rd denotes the instantiated parameters of a capsule , where dis the dimension of the capsule .",Our Model,Primary Capsule Layer,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.2340425531914893,60,0.2469135802469135,3,0.4285714285714285,1,0,Our Model: Primary Capsule Layer
63,Bd be the filter shared in different sliding windows .,Our Model,Primary Capsule Layer,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.2553191489361702,62,0.2551440329218107,5,0.7142857142857143,1,0,Our Model: Primary Capsule Layer
64,"For each matrix multiplication , we have a window sliding over each Ngram vector denoted as M i ?",Our Model,Primary Capsule Layer,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.2659574468085106,63,0.2592592592592592,6,0.8571428571428571,1,0,Our Model: Primary Capsule Layer
65,"R B , then the corresponding N - gram phrases in the form of capsule are produced with",Our Model,Primary Capsule Layer,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.2765957446808511,64,0.2633744855967078,7,1.0,1,0,Our Model: Primary Capsule Layer
67,"Probability column - list of capsules p ? R ( L?K 1 + 1 ) d , each capsule pi ?",Our Model,ConvCaps Capsule,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",28,0.2978723404255319,66,0.2716049382716049,1,0.0588235294117647,1,0,Our Model: ConvCaps Capsule
68,Rd in the column - list is computed as,Our Model,ConvCaps Capsule,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",29,0.3085106382978723,67,0.2757201646090535,2,0.1176470588235294,1,0,Our Model: ConvCaps Capsule
69,"where g is nonlinear squash function through the entire vector , b 1 is the capsule bias term .",Our Model,ConvCaps Capsule,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",30,0.3191489361702128,68,0.279835390946502,3,0.1764705882352941,1,0,Our Model: ConvCaps Capsule
71,where totally ( L ? K 1 + 1 ) C d-dimensional vectors are collected as capsules in P .,Our Model,ConvCaps Capsule,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",32,0.3404255319148936,70,0.2880658436213992,5,0.2941176470588235,1,0,Our Model: ConvCaps Capsule
72,Child - Parent Relationships,Our Model,,text-classification,7,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",33,0.351063829787234,71,0.2921810699588477,6,0.3529411764705882,1,0,Our Model
73,"As argued in , capsule network tries to address the representational limitation and exponential inefficiencies of convolutions with transformation matrices .",Our Model,Child - Parent Relationships,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",34,0.3617021276595745,72,0.2962962962962963,7,0.4117647058823529,1,0,Our Model: Child - Parent Relationships
75,"In text classification tasks , different sentences with the same category are supposed to have the similar topic but with different viewpoints .",Our Model,Child - Parent Relationships,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",36,0.3829787234042553,74,0.3045267489711934,9,0.5294117647058824,1,0,Our Model: Child - Parent Relationships
76,"In this paper , we explore two different types of transformation matrices to generate prediction vector ( vote ) j|i ?",Our Model,Child - Parent Relationships,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",37,0.3936170212765957,75,0.3086419753086419,10,0.5882352941176471,1,0,Our Model: Child - Parent Relationships
77,Rd from it s child capsule i to the parent capsule j.,Our Model,Child - Parent Relationships,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",38,0.4042553191489361,76,0.3127572016460905,11,0.6470588235294118,1,0,Our Model: Child - Parent Relationships
78,The first one shares weights W t 1 ?,Our Model,Child - Parent Relationships,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",39,0.4148936170212766,77,0.3168724279835391,12,0.7058823529411765,1,0,Our Model: Child - Parent Relationships
79,"RN dd across child capsules in the layer below , where N is the number of parent capsules in the layer above .",Our Model,Child - Parent Relationships,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",40,0.425531914893617,78,0.3209876543209876,13,0.7647058823529411,1,0,Our Model: Child - Parent Relationships
80,"Formally , each corresponding vote can be computed by :",Our Model,Child - Parent Relationships,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",41,0.4361702127659574,79,0.3251028806584362,14,0.8235294117647058,1,0,Our Model: Child - Parent Relationships
81,where u i is a child - capsule in the layer below and b j|i is the capsule bias term .,Our Model,Child - Parent Relationships,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",42,0.4468085106382978,80,0.3292181069958848,15,0.8823529411764706,1,0,Our Model: Child - Parent Relationships
82,"In the second design , we replace the shared weight matrix W t 1 j with non-shared weight matrix W t 2 i , j , where the weight matrices W t 2 ?",Our Model,Child - Parent Relationships,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",43,0.4574468085106383,81,0.3333333333333333,16,0.9411764705882352,1,0,Our Model: Child - Parent Relationships
83,R HN dd and H is the number of child capsules in the layer below .,Our Model,Child - Parent Relationships,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",44,0.4680851063829787,82,0.3374485596707819,17,1.0,1,0,Our Model: Child - Parent Relationships
88,Orphan Category,Our Model,,text-classification,7,"['O', 'O']","['O', 'O']",49,0.5212765957446809,87,0.3580246913580246,4,0.4444444444444444,1,0,Our Model
90,"Adding "" orphan "" category in the text is more effective than in image since there is no single consistent "" background "" object in images , while the stop words are consistent in texts such as predicate "" s "" , "" am "" and pronouns "" his "" , "" she "" .",Our Model,Orphan Category,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",51,0.5425531914893617,89,0.3662551440329218,6,0.6666666666666666,1,0,Our Model: Orphan Category
93,"Despite the orphan category in the last capsule layer , we also need a light - weight method between two consecutive layers to route the noise child capsules to extra dimension without any additional parameters and computation consuming .",Our Model,Leaky - Softmax,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",54,0.574468085106383,92,0.3786008230452675,9,1.0,1,0,Our Model: Leaky - Softmax
94,Coefficients Amendment,Our Model,,text-classification,7,"['O', 'O']","['O', 'O']",55,0.5851063829787234,93,0.382716049382716,0,0.0,1,0,Our Model
95,We also attempt to use the probability of existence of child capsules in the layer below to iteratively amend the connection strength as Eq.6 .,Our Model,Coefficients Amendment,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",56,0.5957446808510638,94,0.3868312757201646,1,0.0833333333333333,1,0,Our Model: Coefficients Amendment
96,"Algorithm 1 : Dynamic Routing Algorithm 1 procedure ROUTING ( j|i , j|i , r , l ) 2 Initialize the logits of coupling coefficients b j|i = 0 3 for r iterations do 4 for all capsule i in layer land capsule j in layer l + 1 :",Our Model,Coefficients Amendment,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",57,0.6063829787234043,95,0.3909465020576131,2,0.1666666666666666,1,0,Our Model: Coefficients Amendment
97,for all capsule i in layer land capsule j in,Our Model,Coefficients Amendment,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",58,0.6170212765957447,96,0.3950617283950617,3,0.25,1,0,Our Model: Coefficients Amendment
98,"Given each prediction vector j|i and its probability of existence j|i , where j|i = i , each iterative coupling coefficient of connection strength c j|i is updated by",Our Model,Coefficients Amendment,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",59,0.6276595744680851,97,0.3991769547325103,4,0.3333333333333333,1,0,Our Model: Coefficients Amendment
99,where b j|i is the logits of coupling coefficients .,Our Model,Coefficients Amendment,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",60,0.6382978723404256,98,0.4032921810699588,5,0.4166666666666667,1,0,Our Model: Coefficients Amendment
100,Each parent capsule v j in the layer above is a weighted sum overall prediction vectors j|i :,Our Model,Coefficients Amendment,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",61,0.648936170212766,99,0.4074074074074074,6,0.5,1,0,Our Model: Coefficients Amendment
101,"where a j is the probabilities of parent capsules , g is nonlinear squash function through the entire vector .",Our Model,Coefficients Amendment,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",62,0.6595744680851063,100,0.411522633744856,7,0.5833333333333334,1,0,Our Model: Coefficients Amendment
102,"Once all of the parent capsules are produced , each coupling coefficient b j|i is updated by :",Our Model,Coefficients Amendment,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",63,0.6702127659574468,101,0.4156378600823045,8,0.6666666666666666,1,0,Our Model: Coefficients Amendment
103,"For simplicity of notation , the parent capsules and their probabilities in the layer above are denoted as v , a = Routing ( )",Our Model,Coefficients Amendment,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",64,0.6808510638297872,102,0.419753086419753,9,0.75,1,0,Our Model: Coefficients Amendment
104,"where denotes all of the child capsules in the layer below , v denotes all of the parent - capsules and their probabilities a.",Our Model,Coefficients Amendment,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",65,0.6914893617021277,103,0.4238683127572016,10,0.8333333333333334,1,0,Our Model: Coefficients Amendment
105,Our dynamic routing algorithm is summarized in Algorithm,Our Model,Coefficients Amendment,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",66,0.7021276595744681,104,0.4279835390946502,11,0.9166666666666666,1,0,Our Model: Coefficients Amendment
106,1 .,Our Model,Coefficients Amendment,text-classification,7,"['O', 'O']","['O', 'O']",67,0.7127659574468085,105,0.4320987654320987,12,1.0,1,0,Our Model: Coefficients Amendment
107,Convolutional Capsule Layer,Our Model,,text-classification,7,"['O', 'O', 'O']","['O', 'O', 'O']",68,0.723404255319149,106,0.4362139917695473,0,0.0,1,0,Our Model
108,"In this layer , each capsule is connected only to a local region K 2 C spatially in the layer below .",Our Model,Convolutional Capsule Layer,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",69,0.7340425531914894,107,0.4403292181069959,1,0.1111111111111111,1,0,Our Model: Convolutional Capsule Layer
109,Those capsules in the region multiply transformation matrices to learn child - parent relationships followed by routing by agreement to produce parent capsules in the layer above .,Our Model,Convolutional Capsule Layer,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",70,0.7446808510638298,108,0.4444444444444444,2,0.2222222222222222,1,0,Our Model: Convolutional Capsule Layer
110,Suppose W c 1 ? R Ddd and W c 2 ?,Our Model,Convolutional Capsule Layer,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",71,0.7553191489361702,109,0.448559670781893,3,0.3333333333333333,1,0,Our Model: Convolutional Capsule Layer
111,R K,Our Model,,text-classification,7,"['O', 'O']","['O', 'O']",72,0.7659574468085106,110,0.4526748971193415,4,0.4444444444444444,1,0,Our Model
112,"2 CDdd denote shared and non-shared weights , respectively , where K 2 C is the number of child capsules in a local region in the layer below , Dis the number of parent capsules which the child capsules are sent to .",Our Model,R K,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",73,0.776595744680851,111,0.4567901234567901,5,0.5555555555555556,1,0,Our Model: R K
113,"When the transformation matrices are shared across the child capsules , each potential parent - capsule j|i is produced b?",Our Model,R K,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",74,0.7872340425531915,112,0.4609053497942387,6,0.6666666666666666,1,0,Our Model: R K
114,"where b j|i is the capsule bias term , u i is a child capsule in a local region K 2 C and W c 1 j is the j th matrix in tensor W c 1 .",Our Model,R K,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",75,0.7978723404255319,113,0.4650205761316872,7,0.7777777777777778,1,0,Our Model: R K
116,"When using the non-shared weights across the child capsules , we replace the transformation matrix W c 1 j in Eq. ( 10 ) with W c 2 j .",Our Model,R K,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",77,0.8191489361702128,115,0.4732510288065844,9,1.0,1,0,Our Model: R K
117,Fully Connected Capsule Layer,Our Model,,text-classification,7,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",78,0.8297872340425532,116,0.4773662551440329,0,0.0,1,0,Our Model
118,The capsules in the layer below are flattened into a list of capsules and fed into fully connected capsule layer in which capsules are multiplied by transformation matrix W d 1 ?,Our Model,Fully Connected Capsule Layer,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",79,0.8404255319148937,117,0.4814814814814814,1,0.1666666666666666,1,0,Our Model: Fully Connected Capsule Layer
120,R HEdd followed by routing - by - agreement to produce final capsule v j ?,Our Model,Fully Connected Capsule Layer,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",81,0.8617021276595744,119,0.4897119341563786,3,0.5,1,0,Our Model: Fully Connected Capsule Layer
121,Rd and its probability a j ?,Our Model,Fully Connected Capsule Layer,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O']",82,0.8723404255319149,120,0.4938271604938271,4,0.6666666666666666,1,0,Our Model: Fully Connected Capsule Layer
122,R for each category .,Our Model,Fully Connected Capsule Layer,text-classification,7,"['O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O']",83,0.8829787234042553,121,0.4979423868312757,5,0.8333333333333334,1,0,Our Model: Fully Connected Capsule Layer
123,"Here , H is the number of child capsules in the layer below , E is the number of categories plus an extra orphan category .",Our Model,Fully Connected Capsule Layer,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",84,0.8936170212765957,122,0.5020576131687243,6,1.0,1,0,Our Model: Fully Connected Capsule Layer
124,The Architectures of Capsule Network,Our Model,,text-classification,7,"['O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O']",85,0.9042553191489362,123,0.5061728395061729,0,0.0,1,0,Our Model
128,Each capsule has 16 - dimensional ( d = 16 ) instantiated parameters and their length ( norm ) can describe the probability of the existence of capsules .,Our Model,The Architectures of Capsule Network,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",89,0.946808510638298,127,0.522633744855967,4,0.4444444444444444,1,0,Our Model: The Architectures of Capsule Network
130,"The basic structure of Capsule - B is similar to Capsule - A except that we adopt three parallel networks with filter windows ( N ) of 3 , 4 , 5 in the N - gram convolutional layer ( see ) .",Our Model,The Architectures of Capsule Network,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",91,0.9680851063829788,129,0.5308641975308642,6,0.6666666666666666,1,0,Our Model: The Architectures of Capsule Network
133,3 Experimental Setup,Our Model,The Architectures of Capsule Network,text-classification,7,"['O', 'O', 'O']","['O', 'O', 'O']",94,1.0,132,0.5432098765432098,9,1.0,1,0,Our Model: The Architectures of Capsule Network
134,Experimental Datasets,,,text-classification,7,"['O', 'O']","['O', 'O']",0,0.0,133,0.5473251028806584,0,0.0,1,0,
137,The detailed statistics are presented in,Experimental Datasets,Experimental Datasets,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O']",3,1.0,136,0.5596707818930041,3,1.0,1,0,Experimental Datasets
138,Implementation Details,,,text-classification,7,"['O', 'O']","['O', 'O']",0,0.0,137,0.5637860082304527,0,0.0,1,0,
143,Baseline methods,,,text-classification,7,"['O', 'O']","['O', 'O']",0,0.0,142,0.5843621399176955,0,0.0,1,0,
145,Experimental Results,,,text-classification,7,"['O', 'O']","['O', 'O']",0,0.0,144,0.5925925925925926,0,0.0,1,0,
151,Ablation Study,,,text-classification,7,"['O', 'O']","['O', 'O']",0,0.0,150,0.6172839506172839,0,0.0,1,0,
156,Single - Label to Multi - Label Text Classification,Ablation Study,,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.0909090909090909,155,0.6378600823045267,5,0.1785714285714285,1,0,Ablation Study
158,"Multi-label text classification is , however , a more challenging practical problem .",Ablation Study,Single - Label to Multi - Label Text Classification,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.1272727272727272,157,0.6460905349794238,7,0.25,1,0,Ablation Study: Single - Label to Multi - Label Text Classification
159,"From singlelabel to multi-label ( with n category labels ) text classification , the label space is expanded from n to 2 n , thus more training is required to cover the whole label space .",Ablation Study,Single - Label to Multi - Label Text Classification,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.1454545454545454,158,0.6502057613168725,8,0.2857142857142857,1,0,Ablation Study: Single - Label to Multi - Label Text Classification
160,"For single - label texts , it is practically easy to collect and annotate the samples .",Ablation Study,Single - Label to Multi - Label Text Classification,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.1636363636363636,159,0.654320987654321,9,0.3214285714285714,1,0,Ablation Study: Single - Label to Multi - Label Text Classification
161,"However , the burden of collection and annotation fora large scale multi-label text dataset is generally extremely high .",Ablation Study,Single - Label to Multi - Label Text Classification,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.1818181818181818,160,0.6584362139917695,10,0.3571428571428571,1,0,Ablation Study: Single - Label to Multi - Label Text Classification
162,"How deep neural networks ( e.g. , CNN and LSTM ) best cope with multi-label text classification still remains a problem since obtaining large scale of multi-label dataset is a timeconsuming and expensive process .",Ablation Study,Single - Label to Multi - Label Text Classification,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.2,161,0.6625514403292181,11,0.3928571428571428,1,0,Ablation Study: Single - Label to Multi - Label Text Classification
168,"For dev and training , we only use the single - label documents in the Reuters dev and training sets .",Ablation Study,Single - Label to Multi - Label Text Classification,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.3090909090909091,167,0.6872427983539094,17,0.6071428571428571,1,0,Ablation Study: Single - Label to Multi - Label Text Classification
169,"For testing , Reuters - Multi - label only uses the multi-label documents in testing dataset , while Reuters - Full includes all documents in test set .",Ablation Study,Single - Label to Multi - Label Text Classification,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.3272727272727272,168,0.691358024691358,18,0.6428571428571429,1,0,Ablation Study: Single - Label to Multi - Label Text Classification
180,Connection Strength Visualization,Ablation Study,,text-classification,7,"['O', 'O', 'O']","['O', 'O', 'O']",29,0.5272727272727272,179,0.7366255144032922,0,0.0,1,0,Ablation Study
181,"To visualize the connection strength between capsule layers clearly , we remove the convolutional capsule layer and make the primary capsule layer followed by the fully connected capsule layer directly , where the primary capsules denote N-gram phrases in the form of capsules .",Ablation Study,Connection Strength Visualization,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",30,0.5454545454545454,180,0.7407407407407407,1,0.0833333333333333,1,0,Ablation Study: Connection Strength Visualization
187,"The stronger the connection strength , the bigger the font size .",Ablation Study,Connection Strength Visualization,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",36,0.6545454545454545,186,0.7654320987654321,7,0.5833333333333334,1,0,Ablation Study: Connection Strength Visualization
189,"The histograms are used to show the intensity of connection strengths between primary capsules and the fully connected capsules , as shown in To experimentally verify the convergence of the routing algorithm , we also plot learning curve to show the training loss overtime with different iterations of routing .",Ablation Study,Connection Strength Visualization,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",38,0.6909090909090909,188,0.7736625514403292,9,0.75,1,0,Ablation Study: Connection Strength Visualization
193,Interest Rates,Ablation Study,,text-classification,7,"['O', 'O']","['O', 'O']",42,0.7636363636363637,192,0.7901234567901234,0,0.0,1,0,Ablation Study
195,"Chancellor of the Exchequer Nigel Lawson had stated target rates for sterling against the dollar and mark , dealers said .",Ablation Study,Interest Rates,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",44,0.8,194,0.7983539094650206,2,0.1818181818181818,1,0,Ablation Study: Interest Rates
196,"They said this had come as a surprise and expected the targets , 2.90 marks and 1.60 dlrs , to be promptly tested in the foreign exchange markets .",Ablation Study,Interest Rates,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",45,0.8181818181818182,195,0.8024691358024691,3,0.2727272727272727,1,0,Ablation Study: Interest Rates
199,Operators feel the foreign exchanges are likely to test sterling on the downside and that this seems to make a fall in U.K .,Ablation Study,Interest Rates,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",48,0.8727272727272727,198,0.8148148148148148,6,0.5454545454545454,1,0,Ablation Study: Interest Rates
200,"Base lending rates even less likely in the near term , dealers said .",Ablation Study,Interest Rates,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",49,0.8909090909090909,199,0.8189300411522634,7,0.6363636363636364,1,0,Ablation Study: Interest Rates
201,"The feeling remains in the market , however , that fundamental factors have not really changed and that arise in U.K .",Ablation Study,Interest Rates,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",50,0.9090909090909092,200,0.823045267489712,8,0.7272727272727273,1,0,Ablation Study: Interest Rates
202,Interest rates is not very likely .,Ablation Study,Interest Rates,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O']",51,0.9272727272727272,201,0.8271604938271605,9,0.8181818181818182,1,0,Ablation Study: Interest Rates
205,Orphan,Ablation Study,,text-classification,7,['O'],['O'],54,0.9818181818181818,204,0.8395061728395061,0,0.0,1,0,Ablation Study
206,Mergers / Acquisitions Money / Foreign Exchange Trade Interest Rates,Ablation Study,Orphan,text-classification,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",55,1.0,205,0.8436213991769548,1,0.0,1,0,Ablation Study: Orphan
207,Related Work,,,text-classification,7,"['O', 'O']","['O', 'O']",0,0.0,206,0.8477366255144033,0,0.0,1,0,
228,Conclusion,,,text-classification,7,['O'],['O'],0,0.0,227,0.934156378600823,0,0.0,1,0,
3,abstract,,,text-classification,8,['O'],['O'],0,0.0,2,0.0074349442379182,0,0.0,1,0,
4,"Many deep learning architectures have been proposed to model the compositionality in text sequences , requiring a substantial number of parameters and expensive computations .",abstract,abstract,text-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.1428571428571428,3,0.0111524163568773,1,0.1428571428571428,1,0,abstract
5,"However , there has not been a rigorous evaluation regarding the added value of sophisticated compositional functions .",abstract,abstract,text-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.2857142857142857,4,0.0148698884758364,2,0.2857142857142857,1,0,abstract
15,"These methods range from simple operations like addition , to more sophisticated compositional functions such as Recurrent Neural Networks ( RNNs ) , Convolutional Neural Networks ( CNNs ) and Recursive Neural Networks .",Introduction,Introduction,text-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.1818181818181818,14,0.0520446096654275,4,0.1818181818181818,1,0,Introduction
16,"Models with more expressive compositional functions , e.g. , RNNs or CNNs , have demonstrated impressive results ; however , they are typically computationally expensive , due to the need to estimate hundreds of thousands , if not millions , of parameters .",Introduction,Introduction,text-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.2272727272727272,15,0.0557620817843866,5,0.2272727272727272,1,0,Introduction
17,"In contrast , models with simple compositional functions often compute a sentence or document embedding by simply adding , or averaging , over the word embedding of each sequence element obtained via , e.g. , word2vec , or Glo Ve .",Introduction,Introduction,text-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.2727272727272727,16,0.0594795539033457,6,0.2727272727272727,1,0,Introduction
18,"Generally , such a Simple Word - Embedding - based Model ( SWEM ) does not explicitly account for spatial , word - order information within a text sequence .",Introduction,Introduction,text-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.3181818181818182,17,0.0631970260223048,7,0.3181818181818182,1,0,Introduction
19,"However , they possess the desirable property of having significantly fewer parameters , enjoying much faster training , relative to RNN - or CNN - based models .",Introduction,Introduction,text-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.3636363636363636,18,0.0669144981412639,8,0.3636363636363636,1,0,Introduction
33,"Thus , according to Occam 's razor , simple models are preferred .",Introduction,Introduction,text-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,1.0,32,0.1189591078066914,22,1.0,1,0,Introduction
34,Related Work,,,text-classification,8,"['O', 'O']","['O', 'O']",0,0.0,33,0.1226765799256505,0,0.0,1,0,
49,"Consider a text sequence represented as X ( either a sentence or a document ) , composed of a sequence of words : {w 1 , w 2 , .... , w L } , where L is the number of tokens , i.e. , the sentence / document length .",Models & training,Models & training,text-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.0333333333333333,48,0.1784386617100371,1,0.2,1,0,Models & training
50,"Let {v 1 , v 2 , .... , v L } denote the respective word embeddings for each token , where v l ?",Models & training,Models & training,text-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.0666666666666666,49,0.1821561338289963,2,0.4,1,0,Models & training
51,R K .,Models & training,,text-classification,8,"['O', 'O', 'O']","['O', 'O', 'O']",3,0.1,50,0.1858736059479553,3,0.6,1,0,Models & training
54,Recurrent Sequence Encoder,Models & training,R K .,text-classification,8,"['O', 'O', 'O']","['O', 'O', 'O']",6,0.2,53,0.1970260223048327,0,0.0,1,0,Models & training: R K .
55,"A widely adopted compositional function is defined in a recurrent manner : the model successively takes word vector v tat position t , along with the hidden unit h t?1 from the last position t ?",Models & training,R K .,text-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.2333333333333333,54,0.2007434944237918,1,0.2,1,0,Models & training: R K .
56,"1 , to update the current hidden unit via",Models & training,R K .,text-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.2666666666666666,55,0.2044609665427509,2,0.4,1,0,Models & training: R K .
57,"To address the issue of learning long - term dependencies , f ( ) is often defined as Long Short - Term Memory ( LSTM ) , which employs gates to control the flow of information abstracted from a sequence .",Models & training,R K .,text-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.3,56,0.20817843866171,3,0.6,1,0,Models & training: R K .
58,We omit the details of the LSTM and refer the interested readers to the work by for further explanation .,Models & training,R K .,text-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.3333333333333333,57,0.2118959107806691,4,0.8,1,0,Models & training: R K .
59,"Intuitively , the LSTM encodes a text sequence considering its word - order information , but yields additional compositional parameters that must be learned .",Models & training,R K .,text-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.3666666666666666,58,0.2156133828996282,5,1.0,1,0,Models & training: R K .
60,Convolutional Sequence Encoder,Models & training,R K .,text-classification,8,"['O', 'O', 'O']","['O', 'O', 'O']",12,0.4,59,0.2193308550185873,0,0.0,1,0,Models & training: R K .
61,The Convolutional Neural Network ( CNN ) architecture is another strategy extensively employed as the compositional function to encode text sequences .,Models & training,R K .,text-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.4333333333333333,60,0.2230483271375464,1,0.0555555555555555,1,0,Models & training: R K .
65,"However , Deep CNN text models have also been developed , and are considered in a few of our experiments .",Models & training,R K .,text-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.5666666666666667,64,0.2379182156133829,5,0.2777777777777778,1,0,Models & training: R K .
66,Simple Word - Embedding Model,Models & training,R K .,text-classification,8,"['O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O']",18,0.6,65,0.241635687732342,6,0.3333333333333333,1,0,Models & training: R K .
67,( SWEM ),Models & training,R K .,text-classification,8,"['O', 'O', 'O']","['O', 'O', 'O']",19,0.6333333333333333,66,0.2453531598513011,7,0.3888888888888889,1,0,Models & training: R K .
69,"Among them , the simplest strategy is to compute the element - wise average over word vectors fora given sequence :",Models & training,R K .,text-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.7,68,0.2527881040892193,9,0.5,1,0,Models & training: R K .
70,"The model in can be seen as an average pooling operation , which takes the mean over each of the K dimensions for all word embeddings , resulting in a representation z with the same dimension as the embedding itself , termed here SWEM - aver .",Models & training,R K .,text-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.7333333333333333,69,0.2565055762081784,10,0.5555555555555556,1,0,Models & training: R K .
71,"Intuitively , z takes the information of every sequence element into account via the addition operation .",Models & training,R K .,text-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",23,0.7666666666666667,70,0.2602230483271375,11,0.6111111111111112,1,0,Models & training: R K .
74,This strategy is similar to the max - over - time pooling operation in convolutional neural networks :,Models & training,Max Pooling,text-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.8666666666666667,73,0.2713754646840148,14,0.7777777777777778,1,0,Models & training: Max Pooling
75,We denote this model variant as SWEM - max .,Models & training,Max Pooling,text-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.9,74,0.275092936802974,15,0.8333333333333334,1,0,Models & training: Max Pooling
76,"Here the j - th component of z is the maximum element in the set {v 1 j , . . . , v Lj } , where v 1j is , for example , the j - th component of v 1 .",Models & training,Max Pooling,text-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",28,0.9333333333333332,75,0.2788104089219331,16,0.8888888888888888,1,0,Models & training: Max Pooling
77,"With this pooling operation , those words that are unimportant or unrelated to the corresponding tasks will be ignored in the encoding process ( as the components of the embedding vectors will have small amplitude ) , unlike SWEM - aver where every word contributes equally to the representation .",Models & training,Max Pooling,text-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",29,0.9666666666666668,76,0.2825278810408922,17,0.9444444444444444,1,0,Models & training: Max Pooling
78,"Considering that SWEM - aver and SWEM - max are complementary , in the sense of accounting for different types of information from text sequences ,",Models & training,Max Pooling,text-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",30,1.0,77,0.2862453531598513,18,1.0,1,0,Models & training: Max Pooling
79,Model,,,text-classification,8,['O'],['O'],0,0.0,78,0.2899628252788104,0,0.0,1,0,
82,"This finding is consistent with , where they hypothesize that the positional information of a word in text sequences maybe beneficial to predict sentiment .",Model,Model,text-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.088235294117647,81,0.3011152416356877,3,0.4285714285714285,1,0,Model
83,"This is intuitively reasonable since , for instance , the phrase "" not really good "" and "" really not good "" convey different levels of negative sentiment , while being different only by their word orderings .",Model,Model,text-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.1176470588235294,82,0.3048327137546468,4,0.5714285714285714,1,0,Model
84,"Contrary to SWEM , CNN and LSTM models can both capture this type of information via convolutional filters or recurrent transition functions .",Model,Model,text-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.1470588235294117,83,0.3085501858736059,5,0.7142857142857143,1,0,Model
85,"However , as suggested above , such word - order patterns maybe much less useful for predicting the topic of a document .",Model,Model,text-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.1764705882352941,84,0.312267657992565,6,0.8571428571428571,1,0,Model
86,"This maybe attributed to the fact that word embeddings alone already provide sufficient topic information of a document , at least when the text sequences considered are relatively long .",Model,Model,text-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.2058823529411764,85,0.3159851301115242,7,1.0,1,0,Model
87,Parameters,Model,,text-classification,8,['O'],['O'],8,0.2352941176470588,86,0.3197026022304832,0,0.0,1,0,Model
88,Complexity Sequential,Model,,text-classification,8,"['O', 'O']","['O', 'O']",9,0.2647058823529412,87,0.3234200743494423,1,0.0384615384615384,1,0,Model
90,"For all SWEM variants , there are no additional compositional parameters to be learned .",Model,Complexity Sequential,text-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.3235294117647059,89,0.3308550185873606,3,0.1153846153846153,1,0,Model: Complexity Sequential
92,"Hierarchical Pooling Both SWEM - aver and SWEM - max do not take word - order or spatial information into consideration , which could be useful for certain NLP applications .",Model,Complexity Sequential,text-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.3823529411764705,91,0.3382899628252788,5,0.1923076923076923,1,0,Model: Complexity Sequential
94,"Let v i:i+n?1 refer to the local window consisting of n consecutive words words ,",Model,Complexity Sequential,text-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.4411764705882353,93,0.345724907063197,7,0.2692307692307692,1,0,Model: Complexity Sequential
95,"First , an average - pooling is performed on each local window , v i:i+n?1 .",Model,Complexity Sequential,text-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.4705882352941176,94,0.3494423791821561,8,0.3076923076923077,1,0,Model: Complexity Sequential
97,We call this approach SWEM - hier due to its layered pooling .,Model,Complexity Sequential,text-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.5294117647058824,96,0.3568773234200743,10,0.3846153846153846,1,0,Model: Complexity Sequential
101,Parameters & Computation,Model,,text-classification,8,"['O', 'O', 'O']","['O', 'O', 'O']",22,0.6470588235294118,100,0.3717472118959107,14,0.5384615384615384,1,0,Model
102,Comparison,Model,,text-classification,8,['O'],['O'],23,0.6764705882352942,101,0.3754646840148699,15,0.5769230769230769,1,0,Model
104,"K denotes the dimension of word embeddings , as above .",Model,Comparison,text-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.7352941176470589,103,0.3828996282527881,17,0.6538461538461539,1,0,Model: Comparison
105,"For the CNN , we use n to denote the filter width ( assumed constant for all filters , for simplicity of analysis , but in practice variable n is commonly used ) .",Model,Comparison,text-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.7647058823529411,104,0.3866171003717472,18,0.6923076923076923,1,0,Model: Comparison
106,We defined as the dimension of the final sequence representation .,Model,Comparison,text-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.7941176470588235,105,0.3903345724907063,19,0.7307692307692307,1,0,Model: Comparison
107,"Specifically , d represents the dimension of hidden units or the number of filters in LSTM or CNN , respectively .",Model,Comparison,text-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",28,0.8235294117647058,106,0.3940520446096654,20,0.7692307692307693,1,0,Model: Comparison
109,"As shown in , both the CNN and LSTM have a large number of parameters , to model the semantic compositionality of text sequences , whereas SWEM has no such parameters .",Model,Comparison,text-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",30,0.8823529411764706,108,0.4014869888475836,22,0.8461538461538461,1,0,Model: Comparison
111,SWEM tends to be more efficient than CNN and LSTM in terms of computation complexity .,Model,Comparison,text-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",32,0.9411764705882352,110,0.4089219330855018,24,0.9230769230769232,1,0,Model: Comparison
114,Experiments,,,text-classification,8,['O'],['O'],0,0.0,113,0.4200743494423792,0,0.0,1,0,
130,"Answer than CNN / LSTM , with only 61 K parameters ( one - tenth the number of LSTM parameters , or one - third the number of CNN parameters ) , while taking a fraction of the training time relative to the CNN or LSTM .",Experiments,Experiments,text-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,1.0,129,0.4795539033457249,16,1.0,1,0,Experiments
138,"This suggests that the model may only depend on a few key words , among the entire vocabulary , for predictions ( since most words do not contribute to the max - pooling operation in SWEM - max ) .",Interpreting model predictions,Interpreting model predictions,text-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.0972222222222222,137,0.5092936802973977,7,0.4117647058823529,1,0,Interpreting model predictions
140,"In this regard , the nature of max - pooling pro - cess gives rise to a more interpretable model .",Interpreting model predictions,Interpreting model predictions,text-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.125,139,0.516728624535316,9,0.5294117647058824,1,0,Interpreting model predictions
142,"Thus , we suspect that semantically similar words may have large values in some shared dimensions .",Interpreting model predictions,Interpreting model predictions,text-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.1527777777777778,141,0.5241635687732342,11,0.6470588235294118,1,0,Interpreting model predictions
143,"So motivated , after training the SWEM - max model on the Yahoo dataset , we selected five words with the largest values , among the entire vocabulary , for each word embedding dimension ( these words are selected preferentially in the corresponding dimension , by the max operation ) .",Interpreting model predictions,Interpreting model predictions,text-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.1666666666666666,142,0.5278810408921933,12,0.7058823529411765,1,0,Interpreting model predictions
145,"For example , the words in the first column of are all political terms , which could be assigned to the Politics & Government topic .",Interpreting model predictions,Interpreting model predictions,text-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.1944444444444444,144,0.5353159851301115,14,0.8235294117647058,1,0,Interpreting model predictions
146,Note that our model can even learn locally interpretable structure that is not explicitly indicated by the label information .,Interpreting model predictions,Interpreting model predictions,text-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.2083333333333333,145,0.5390334572490706,15,0.8823529411764706,1,0,Interpreting model predictions
147,"For instance , all words in the fifth column are Chemistry - related .",Interpreting model predictions,Interpreting model predictions,text-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.2222222222222222,146,0.5427509293680297,16,0.9411764705882352,1,0,Interpreting model predictions
148,"However , we do not have a chemistry label in the dataset , and regardless they should belong to the Science topic .",Interpreting model predictions,Interpreting model predictions,text-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.2361111111111111,147,0.5464684014869888,17,1.0,1,0,Interpreting model predictions
149,Text Sequence Matching,Interpreting model predictions,Interpreting model predictions,text-classification,8,"['O', 'O', 'O']","['O', 'O', 'O']",18,0.25,148,0.550185873605948,0,0.0,1,0,Interpreting model predictions
151,The corresponding performance metrics are shown in .,Interpreting model predictions,,text-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.2777777777777778,150,0.5576208178438662,2,0.0952380952380952,1,0,Interpreting model predictions
155,1 .,Interpreting model predictions,The corresponding performance metrics are shown in .,text-classification,8,"['O', 'O']","['O', 'O']",24,0.3333333333333333,154,0.5724907063197026,6,0.2857142857142857,1,0,Interpreting model predictions: The corresponding performance metrics are shown in .
156,"The strong results of the SWEM approach on these tasks may stem from the fact that when matching natural language sentences , it is sufficient inmost cases to simply model the word - level alignments between two sequences .",Interpreting model predictions,The corresponding performance metrics are shown in .,text-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.3472222222222222,155,0.5762081784386617,7,0.3333333333333333,1,0,Interpreting model predictions: The corresponding performance metrics are shown in .
157,"From this perspective , word - order information becomes much less useful for predicting relationship between sentences .",Interpreting model predictions,The corresponding performance metrics are shown in .,text-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.3611111111111111,156,0.5799256505576208,8,0.3809523809523809,1,0,Interpreting model predictions: The corresponding performance metrics are shown in .
158,"Moreover , considering the simpler model architecture of SWEM , they could be much easier to be optimized than LSTM or CNN - based models , and thus give rise to better empirical results .",Interpreting model predictions,The corresponding performance metrics are shown in .,text-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.375,157,0.5836431226765799,9,0.4285714285714285,1,0,Interpreting model predictions: The corresponding performance metrics are shown in .
160,"One possible disadvantage of SWEM is that it ignores the word - order information within a text sequence , which could be potentially captured by CNN - or LSTM - based models .",Interpreting model predictions,Importance of word - order information,text-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",29,0.4027777777777778,159,0.5910780669144982,11,0.5238095238095238,1,0,Interpreting model predictions: Importance of word - order information
162,"In this regard , one natural question would be : how important are word - order features for these tasks ?",Interpreting model predictions,Importance of word - order information,text-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",31,0.4305555555555556,161,0.5985130111524164,13,0.6190476190476191,1,0,Interpreting model predictions: Importance of word - order information
173,"Taking the first sentence as an example , several words in the review are generally positive , i.e. friendly , nice , okay , great and likes .",Interpreting model predictions,Case Study,text-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",42,0.5833333333333334,172,0.6394052044609665,2,0.1052631578947368,1,0,Interpreting model predictions: Case Study
174,"However , the most vital features for predicting the sentiment of this sentence could be the phrase / sentence ' is just okay ' , ' not great ' or ' makes me wonder why everyone likes ' , which can not be captured without considering word - order features .",Interpreting model predictions,Case Study,text-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",43,0.5972222222222222,173,0.6431226765799256,3,0.1578947368421052,1,0,Interpreting model predictions: Case Study
175,It is worth noting the hints for predictions in this case are actually ngram phrases from the input document .,Interpreting model predictions,Case Study,text-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",44,0.6111111111111112,174,0.6468401486988847,4,0.2105263157894736,1,0,Interpreting model predictions: Case Study
178,"However , according to the case study above , the most important features for sentiment prediction maybe some key n-gram phrase / words from Negative :",Interpreting model predictions,SWEM - hier for sentiment analysis,text-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",47,0.6527777777777778,177,0.6579925650557621,7,0.3684210526315789,1,0,Interpreting model predictions: SWEM - hier for sentiment analysis
182,Positive :,Interpreting model predictions,"Food is just okay , not great .",text-classification,8,"['O', 'O']","['O', 'O']",51,0.7083333333333334,181,0.6728624535315985,11,0.5789473684210527,1,0,"Interpreting model predictions: Food is just okay , not great ."
183,"The store is small , but it carries specialties that are difficult to find in Pittsburgh .",Interpreting model predictions,"Food is just okay , not great .",text-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",52,0.7222222222222222,182,0.6765799256505576,12,0.631578947368421,1,0,"Interpreting model predictions: Food is just okay , not great ."
184,I was particularly excited to find middle eastern chili sauce and chocolate covered turkish delights .,Interpreting model predictions,"Food is just okay , not great .",text-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",53,0.7361111111111112,183,0.6802973977695167,13,0.6842105263157895,1,0,"Interpreting model predictions: Food is just okay , not great ."
185,the input document .,Interpreting model predictions,"Food is just okay , not great .",text-classification,8,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",54,0.75,184,0.6840148698884758,14,0.7368421052631579,1,0,"Interpreting model predictions: Food is just okay , not great ."
187,"Inspired by this observation , we propose using another simple pooling operation termed as hierarchical ( SWEM - hier ) , as detailed in Section 3.3 .",Interpreting model predictions,"Food is just okay , not great .",text-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",56,0.7777777777777778,186,0.6914498141263941,16,0.8421052631578947,1,0,"Interpreting model predictions: Food is just okay , not great ."
191,Short Sentence Processing,Interpreting model predictions,"Food is just okay , not great .",text-classification,8,"['O', 'O', 'O']","['O', 'O', 'O']",60,0.8333333333333334,190,0.7063197026022305,0,0.0,1,0,"Interpreting model predictions: Food is just okay , not great ."
194,The corresponding results are shown in .,Interpreting model predictions,,text-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O']",63,0.875,193,0.7174721189591078,3,0.2727272727272727,1,0,Interpreting model predictions
200,"This maybe due to the fact that fora shorter text sequence , word - order features tend to be more important since the semantic information provided byword embeddings alone is relatively limited .",Interpreting model predictions,The corresponding results are shown in .,text-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",69,0.9583333333333334,199,0.7397769516728625,9,0.8181818181818182,1,0,Interpreting model predictions: The corresponding results are shown in .
201,"Moreover , we note that the results on these relatively small datasets are highly sensitive to model regularization techniques due to the overfitting issues .",Interpreting model predictions,The corresponding results are shown in .,text-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",70,0.9722222222222222,200,0.7434944237918215,10,0.9090909090909092,1,0,Interpreting model predictions: The corresponding results are shown in .
202,"In this regard , one interesting future direction maybe to develop specific regularization strategies for the SWEM framework , and thus make them work better on small sentence classification datasets .",Interpreting model predictions,The corresponding results are shown in .,text-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",71,0.9861111111111112,201,0.7472118959107806,11,1.0,1,0,Interpreting model predictions: The corresponding results are shown in .
203,Discussion,Interpreting model predictions,,text-classification,8,['O'],['O'],72,1.0,202,0.7509293680297398,0,0.0,1,0,Interpreting model predictions
204,Comparison via subspace training,,,text-classification,8,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",0,0.0,203,0.7546468401486989,0,0.0,1,0,
206,"It constrains the optimization of the trainable parameters in a subspace of low dimension d , the intrinsic dimension dint defines the minimum d that yield a good solution .",Comparison via subspace training,Comparison via subspace training,text-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.074074074074074,205,0.7620817843866171,2,0.125,1,0,Comparison via subspace training
214,SWEM seems to have an easier loss landspace than CNN for word embeddings to find the best solutions .,Comparison via subspace training,Comparison via subspace training,text-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.3703703703703703,213,0.79182156133829,10,0.625,1,0,Comparison via subspace training
215,"According to Occam 's razor , simple models are preferred , if all else are the same .",Comparison via subspace training,Comparison via subspace training,text-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.4074074074074074,214,0.7955390334572491,11,0.6875,1,0,Comparison via subspace training
220,"However , in , CNN can leverage more trainable parameters to achieve higher accuracy when dis large .",Comparison via subspace training,Comparison via subspace training,text-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.5925925925925926,219,0.8141263940520446,16,1.0,1,0,Comparison via subspace training
221,Linear classifiers,Comparison via subspace training,,text-classification,8,"['O', 'O']","['O', 'O']",17,0.6296296296296297,220,0.8178438661710037,0,0.0,1,0,Comparison via subspace training
231,It also implies that Chinese is more sensitive to local word - order features than English .,Comparison via subspace training,Extension to other languages,text-classification,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,1.0,230,0.8550185873605948,5,1.0,1,0,Comparison via subspace training: Extension to other languages
232,Conclusions,,,text-classification,8,['O'],['O'],0,0.0,231,0.8587360594795539,0,0.0,1,0,
3,abstract,,,text-classification,9,['O'],['O'],0,0.0,2,0.0079365079365079,0,0.0,1,0,
4,"In sentence classification tasks , additional contexts , such as the neighboring sentences , may improve the accuracy of the classifier .",abstract,abstract,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.1428571428571428,3,0.0119047619047619,1,0.1428571428571428,1,0,abstract
5,"However , such contexts are domain - dependent and thus can not be used for another classification task with an inappropriate domain .",abstract,abstract,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.2857142857142857,4,0.0158730158730158,2,0.2857142857142857,1,0,abstract
16,"To overcome this limitation , recent works attempted to augment different kinds of features to the sentence , such as the neighboring sentences and the topics of the sentences .",Introduction,Introduction,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.0675675675675675,15,0.0595238095238095,5,0.1219512195121951,1,0,Introduction
17,"However , these methods used domain - dependent contexts that are only effective when the domain of the task is appropriate .",Introduction,Introduction,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.081081081081081,16,0.0634920634920634,6,0.1463414634146341,1,0,Introduction
18,"For one thing , neighboring sentences may not be available in some tasks such as question type classification .",Introduction,Introduction,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.0945945945945946,17,0.0674603174603174,7,0.1707317073170731,1,0,Introduction
19,"Moreover , topics inferred using topic models may produce less useful topics when the data set is domain - specific such as movie review sentiment classification .",Introduction,Introduction,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.1081081081081081,18,0.0714285714285714,8,0.1951219512195122,1,0,Introduction
22,"First , each language has its own linguistic and cultural characteristics that may contain different signals to effectively classify a specific class .",Introduction,Introduction,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.1486486486486486,21,0.0833333333333333,11,0.2682926829268293,1,0,Introduction
25,"For example , the green class , or the numeric question type , is circled in the Arabic space as it is clearly separated from other classes , while such separation can not be observed in English .",Introduction,Introduction,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.1891891891891892,24,0.0952380952380952,14,0.3414634146341463,1,0,Introduction
26,"Meanwhile , location type questions ( in orange ) are better classified in English .",Introduction,Introduction,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.2027027027027027,25,0.0992063492063492,15,0.3658536585365853,1,0,Introduction
27,"Second , the original sentences may include languagespecific ambiguity , which maybe resolved when presented with its translations .",Introduction,Introduction,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.2162162162162162,26,0.1031746031746031,16,0.3902439024390244,1,0,Introduction
28,"Consider the example English sentence "" The movie is terribly amazing "" for the sentiment classification task .",Introduction,Introduction,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.2297297297297297,27,0.1071428571428571,17,0.4146341463414634,1,0,Introduction
29,"In this case , terribly can be used in both positive and negative sense , thus introduces ambiguity in the sentence .",Introduction,Introduction,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,0.2432432432432432,28,0.1111111111111111,18,0.4390243902439024,1,0,Introduction
30,"When translated to Korean , it becomes "" ? ? ?? ?? ? ? ? ?? ? ?? ? ? ? ? ? ? ?? ? ?? ?? ? "" which means "" The movie is greatly magnificent "" , removing the ambiguity .",Introduction,Introduction,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.2567567567567567,29,0.115079365079365,19,0.4634146341463415,1,0,Introduction
32,"Thankfully , translation services ( e.g. Google Translate )",Introduction,Introduction,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.2837837837837837,31,0.123015873015873,21,0.5121951219512195,1,0,Introduction
35,"However , despite the maturity of translation , naively concatenating their vectors to the original sentence vector may introduce more noise than signals .",Introduction,Introduction,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.3243243243243243,34,0.1349206349206349,24,0.5853658536585366,1,0,Introduction
36,The unaltered translation space on the left of shows an example where translation noises make the two classes indistinguishable .,Introduction,Introduction,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.3378378378378378,35,0.1388888888888889,25,0.6097560975609756,1,0,Introduction
38,Suppose there are two translated sentences a and b with slight errors .,Introduction,Introduction,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.3648648648648648,37,0.1468253968253968,27,0.6585365853658537,1,0,Introduction
40,1 .,Introduction,Introduction,text-classification,9,"['O', 'O']","['O', 'O']",29,0.3918918918918919,39,0.1547619047619047,29,0.7073170731707317,1,0,Introduction
41,"Revisiting the example above , to fix the vector of the English sentence "" The movie is terribly amazing "" , we use the Korean translation to move the vector towards the location where the vector "" The movie is greatly magnificent "" is .",Introduction,Introduction,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",30,0.4054054054054054,40,0.1587301587301587,30,0.7317073170731707,1,0,Introduction
44,"Fixing the vectors is done by selectively moving the vectors to a location in the same vector space that better separates the class , as shown in .",Introduction,Introduction,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",33,0.4459459459459459,43,0.1706349206349206,33,0.8048780487804879,1,0,Introduction
45,Noises from translation may cause adverse effects to the vector itself ( e.g. when a noisy vector is directly used for the task ) and relatively to other vectors ( e.g. when a noisy vector is used to fix another noisy vector ) .,Introduction,Introduction,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",34,0.4594594594594595,44,0.1746031746031746,34,0.8292682926829268,1,0,Introduction
53,Preliminaries,Introduction,,text-classification,9,['O'],['O'],42,0.5675675675675675,52,0.2063492063492063,0,0.0,1,0,Introduction
60,It is a simple variation of the original CNN for texts to be used on sentences .,Introduction,Base Model : Convolutional Neural Network .,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",49,0.6621621621621622,59,0.2341269841269841,7,0.21875,1,0,Introduction: Base Model : Convolutional Neural Network .
62,Rd be the d-dimensional word vector of the i - th word in a sentence of length n .,Introduction,Base Model : Convolutional Neural Network .,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",51,0.6891891891891891,61,0.242063492063492,9,0.28125,1,0,Introduction: Base Model : Convolutional Neural Network .
63,A convolution operation involves applying a filter matrix W ?,Introduction,Base Model : Convolutional Neural Network .,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",52,0.7027027027027027,62,0.246031746031746,10,0.3125,1,0,Introduction: Base Model : Convolutional Neural Network .
64,R hd to a window of h words and producing anew feature vector c i using the equation,Introduction,Base Model : Convolutional Neural Network .,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",53,0.7162162162162162,63,0.25,11,0.34375,1,0,Introduction: Base Model : Convolutional Neural Network .
65,bias vector and f ( . ) is a non-linear function .,Introduction,Base Model : Convolutional Neural Network .,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",54,0.7297297297297297,64,0.2539682539682539,12,0.375,1,0,Introduction: Base Model : Convolutional Neural Network .
66,"By doing this on all possible windows of words we produce a feature map c = [ c 1 , c 2 , ... ] .",Introduction,Base Model : Convolutional Neural Network .,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",55,0.7432432432432432,65,0.2579365079365079,13,0.40625,1,0,Introduction: Base Model : Convolutional Neural Network .
69,We can then use this vector as input features to train a classifier such as logistic regression .,Introduction,Base Model : Convolutional Neural Network .,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",58,0.7837837837837838,68,0.2698412698412698,16,0.5,1,0,Introduction: Base Model : Convolutional Neural Network .
71,"From hereon , we refer to these vectors as v s , v t1 , v t2 , ... , v tn , respectively .",Introduction,Base Model : Convolutional Neural Network .,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",60,0.8108108108108109,70,0.2777777777777778,18,0.5625,1,0,Introduction: Base Model : Convolutional Neural Network .
72,We refer to them collectively as V .,Introduction,Base Model : Convolutional Neural Network .,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",61,0.8243243243243243,71,0.2817460317460317,19,0.59375,1,0,Introduction: Base Model : Convolutional Neural Network .
74,A simple method in order to use the translated sentences as additional context is to naively concatenate their vectors with the vector of the original sentence .,Introduction,Base Model : Convolutional Neural Network .,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",63,0.8513513513513513,73,0.2896825396825397,21,0.65625,1,0,Introduction: Base Model : Convolutional Neural Network .
75,"That is , we create a wide vectorv = [ v s ; v t1 ; ... ; v tn ] , and use this as the input feature vector of the sentence to the classifier .",Introduction,Base Model : Convolutional Neural Network .,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",64,0.8648648648648649,74,0.2936507936507936,22,0.6875,1,0,Introduction: Base Model : Convolutional Neural Network .
77,"However , sentences translated using machine translation models usually contain incorrect translation .",Introduction,Base Model : Convolutional Neural Network .,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",66,0.8918918918918919,76,0.3015873015873015,24,0.75,1,0,Introduction: Base Model : Convolutional Neural Network .
78,"In effect , this method will have adverse effects on the overall performance of the classifier .",Introduction,Base Model : Convolutional Neural Network .,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",67,0.9054054054054054,77,0.3055555555555556,25,0.78125,1,0,Introduction: Base Model : Convolutional Neural Network .
79,This will especially be very evident if the number of additional sentences increases .,Introduction,Base Model : Convolutional Neural Network .,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",68,0.918918918918919,78,0.3095238095238095,26,0.8125,1,0,Introduction: Base Model : Convolutional Neural Network .
81,"In order to alleviate the problems above , we can use L2 regularization to automatically select useful features by weakening the appropriate weights .",Introduction,Base Model : Convolutional Neural Network .,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",70,0.945945945945946,80,0.3174603174603174,28,0.875,1,0,Introduction: Base Model : Convolutional Neural Network .
82,The main problem of this method occurs when almost all of the weights coming from the vectors of the translated sentence are weakened .,Introduction,Base Model : Convolutional Neural Network .,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",71,0.9594594594594594,81,0.3214285714285714,29,0.90625,1,0,Introduction: Base Model : Convolutional Neural Network .
83,This leads to making the additional context vectors useless and to having a similar performance when there are no additional context .,Introduction,Base Model : Convolutional Neural Network .,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",72,0.972972972972973,82,0.3253968253968254,30,0.9375,1,0,Introduction: Base Model : Convolutional Neural Network .
84,"Ultimately , this method does not make use of the full potential of the additional context .",Introduction,Base Model : Convolutional Neural Network .,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",73,0.9864864864864864,83,0.3293650793650793,31,0.96875,1,0,Introduction: Base Model : Convolutional Neural Network .
86,Model,,,text-classification,9,['O'],['O'],0,0.0,85,0.3373015873015873,0,0.0,1,0,
89,"The sentence vectors are altered using other sentence vectors as context ( e.g. v t 1 is altered using v s , v t2 , ... , v tn ) .",Model,Model,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.0576923076923076,88,0.3492063492063492,3,0.6,1,0,Model
90,This results to moving the vectors in the same vector space .,Model,Model,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.0769230769230769,89,0.3531746031746032,4,0.8,1,0,Model
91,The full architecture is shown in .,Model,Model,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.0961538461538461,90,0.3571428571428571,5,1.0,1,0,Model
94,"However , other vectors might also contain errors which may reflect to the fixing of the source sentence vector .",Model,Self Usability Module,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.1538461538461538,93,0.369047619047619,2,0.0714285714285714,1,0,Model: Self Usability Module
96,"A self usability module contains the self usability of the vector ? i ( a ) , which measures how confident sentence a is for the task at hand .",Model,Self Usability Module,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.1923076923076923,95,0.376984126984127,4,0.1428571428571428,1,0,Model: Self Usability Module
97,"For example , an ambiguous sentence ( e.g. "" The movie is terribly amazing "" ) may receive a low self usability , while a clear and definite sentence ( e.g. "" The movie is very good "" ) may receive a high self usability .",Model,Self Usability Module,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.2115384615384615,96,0.3809523809523809,5,0.1785714285714285,1,0,Model: Self Usability Module
98,"Mathematically , we calculate the self usability of the vector vi of sentence i , denoted as ? i ( v i ) , using the equation",Model,Self Usability Module,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.2307692307692307,97,0.3849206349206349,6,0.2142857142857142,1,0,Model: Self Usability Module
99,is a matrix to be learned .,Model,Self Usability Module,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.25,98,0.3888888888888889,7,0.25,1,0,Model: Self Usability Module
100,The produced value is a single real number from 0 to 1 .,Model,Self Usability Module,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",14,0.2692307692307692,99,0.3928571428571428,8,0.2857142857142857,1,0,Model: Self Usability Module
101,We pre-calculate the self usability of all sentence vectors vi ?,Model,Self Usability Module,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.2884615384615384,100,0.3968253968253968,9,0.3214285714285714,1,0,Model: Self Usability Module
102,V .,Model,,text-classification,9,"['O', 'O']","['O', 'O']",16,0.3076923076923077,101,0.4007936507936508,10,0.3571428571428571,1,0,Model
105,Module,Model,,text-classification,9,['O'],['O'],19,0.3653846153846153,104,0.4126984126984127,13,0.4642857142857143,1,0,Model
107,"There are two main differences between ? i ( a ) and ? r ( a , b ) .",Model,Module,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.4038461538461538,106,0.4206349206349206,15,0.5357142857142857,1,0,Model: Module
108,"First , ? i ( a ) is calculated before a knows about b while ? r ( a , b ) is calculated when a knows about b.",Model,Module,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",22,0.4230769230769231,107,0.4246031746031746,16,0.5714285714285714,1,0,Model: Module
109,"Second , ? r ( a , b ) can below even though ? i ( a ) is not .",Model,Module,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",23,0.4423076923076923,108,0.4285714285714285,17,0.6071428571428571,1,0,Model: Module
110,This means that a is notable to help in fixing the wrong information in b .,Model,Module,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.4615384615384615,109,0.4325396825396825,18,0.6428571428571429,1,0,Model: Module
113,One major challenge in using the attention mechanism in our problem is that the sentence vectors do not belong to the same vector space .,Model,Module,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.5192307692307693,112,0.4444444444444444,21,0.75,1,0,Model: Module
114,"Moreover , one characteristic of our problem is that the sentence vectors can be both a source and a context vector ( e.g. v scan be both sand ti in Equation 1 ) .",Model,Module,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",28,0.5384615384615384,113,0.4484126984126984,22,0.7857142857142857,1,0,Model: Module
115,"Because of these , we can not directly use the additive attention module .",Model,Module,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",29,0.5576923076923077,114,0.4523809523809524,23,0.8214285714285714,1,0,Model: Module
116,We extend the module such that ( 1 ) each sentence vector v k has its own projection matrix X k ?,Model,Module,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",30,0.5769230769230769,115,0.4563492063492063,24,0.8571428571428571,1,0,Model: Module
117,"R dd , and ( 2 ) each projection matrix X k can be used as projection matrix of both the source ( e.g. when sentence k is the current source ) and the context vectors .",Model,Module,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",31,0.5961538461538461,116,0.4603174603174603,25,0.8928571428571429,1,0,Model: Module
119,i ( v k ) to reflect the self usability of a sentence .,Model,Module,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",33,0.6346153846153846,118,0.4682539682539682,27,0.9642857142857144,1,0,Model: Module
120,"Ultimately , the relative usability denoted as ? r ( v i , v j ) is calculated using the equations below , where is the multiplication of a vector and a scalar through broadcasting .",Model,Module,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",34,0.6538461538461539,119,0.4722222222222222,28,1.0,1,0,Model: Module
121,Vector Fixing Module,Model,,text-classification,9,"['O', 'O', 'O']","['O', 'O', 'O']",35,0.6730769230769231,120,0.4761904761904761,0,0.0,1,0,Model
124,The weighted gate vector is used to determine to what extent should a dimension of the source sentence vector be altered .,Model,Vector Fixing Module,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",38,0.7307692307692307,123,0.4880952380952381,3,0.1764705882352941,1,0,Model: Vector Fixing Module
125,The common way to apply the attention weights to the context vectors and create an integrated context vector c i is to directly do weighted sum of all the context vectors .,Model,Vector Fixing Module,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",39,0.75,124,0.492063492063492,4,0.2352941176470588,1,0,Model: Vector Fixing Module
126,"However , this is not possible because the context vectors are not on the same space .",Model,Vector Fixing Module,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",40,0.7692307692307693,125,0.496031746031746,5,0.2941176470588235,1,0,Model: Vector Fixing Module
127,"Thus , we use a projection matrix U k ?",Model,Vector Fixing Module,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",41,0.7884615384615384,126,0.5,6,0.3529411764705882,1,0,Model: Vector Fixing Module
128,R dd to linearly project the sentence vector v k to transform the sentence vectors into a common vector space .,Model,Vector Fixing Module,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",42,0.8076923076923077,127,0.503968253968254,7,0.4117647058823529,1,0,Model: Vector Fixing Module
129,The integrated context vector c i is then calculated as,Model,Vector Fixing Module,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",43,0.8269230769230769,128,0.5079365079365079,8,0.4705882352941176,1,0,Model: Vector Fixing Module
130,"Finally , we construct a weighted gate vector wk and use it to fix the source sentence vectors using the equations below , where V k ?",Model,Vector Fixing Module,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",44,0.8461538461538461,129,0.5119047619047619,9,0.5294117647058824,1,0,Model: Vector Fixing Module
131,R 2dd is a trainable parameter and ?,Model,Vector Fixing Module,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",45,0.8653846153846154,130,0.5158730158730159,10,0.5882352941176471,1,0,Model: Vector Fixing Module
132,is the element - wise multiplication procedure .,Model,Vector Fixing Module,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",46,0.8846153846153846,131,0.5198412698412699,11,0.6470588235294118,1,0,Model: Vector Fixing Module
133,The weighted gate vector is a vector of real numbers between 0 and 1 to modify the intensity of per-dimension values of the sentence vector .,Model,Vector Fixing Module,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",47,0.903846153846154,132,0.5238095238095238,12,0.7058823529411765,1,0,Model: Vector Fixing Module
134,This causes the vector to move in the same vector space towards the correct direction .,Model,Vector Fixing Module,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",48,0.9230769230769232,133,0.5277777777777778,13,0.7647058823529411,1,0,Model: Vector Fixing Module
135,"An alternative approach to do vector correction is using a residual - style correction , where instead of multiplying agate vector , a residual vector is added to the original vector .",Model,Vector Fixing Module,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",49,0.9423076923076924,134,0.5317460317460317,14,0.8235294117647058,1,0,Model: Vector Fixing Module
136,"However , this approach makes the correction not interpretable ; it is hard to explain what does adding a value to a specific dimension mean .",Model,Vector Fixing Module,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",50,0.9615384615384616,135,0.5357142857142857,15,0.8823529411764706,1,0,Model: Vector Fixing Module
137,One major advantage of MCFA is that the corrections in the vectors are interpretable ; the weights in the gate vector correspond to the importance of the per-dimension features of the vector .,Model,Vector Fixing Module,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",51,0.9807692307692308,136,0.5396825396825397,16,0.9411764705882352,1,0,Model: Vector Fixing Module
138,"The altered vector ? v s , ... , v tn are then concatenated and fed directly as an input vector to the logistic regression classifier for training .",Model,Vector Fixing Module,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",52,1.0,137,0.5436507936507936,17,1.0,1,0,Model: Vector Fixing Module
139,Experiments,,,text-classification,9,['O'],['O'],0,0.0,138,0.5476190476190477,0,0.0,1,0,
140,Experimental Setting,,,text-classification,9,"['O', 'O']","['O', 'O']",0,0.0,139,0.5515873015873016,0,0.0,1,0,
143,( b ) SUBJ : Subjectivity data where the task is to classify whether the sentence is subjective or objective .,Experimental Setting,Experimental Setting,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.1153846153846153,142,0.5634920634920635,3,0.1153846153846153,1,0,Experimental Setting
148,"For the additional contexts , we use ten other languages , selected based on their diversity and their performance on prior experiments : Arabic , Finnish , French , Italian , Korean , Mongolian , Norwegian , Polish , Russian , and Ukranian .",Experimental Setting,Experimental Setting,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.3076923076923077,147,0.5833333333333334,8,0.3076923076923077,1,0,Experimental Setting
156,"We also use an l 2 constraint of 3 , following for accurate comparisons .",Experimental Setting,Experimental Setting,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.6153846153846154,155,0.6150793650793651,16,0.6153846153846154,1,0,Experimental Setting
163,TopCNN uses two types of topics : word- specific topic and sentence - specific topic ; and ( D ) CNN+ B1 and CNN +,Experimental Setting,Experimental Setting,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",23,0.8846153846153846,162,0.6428571428571429,23,0.8846153846153846,1,0,Experimental Setting
164,B2 are the two baselines presented in this paper .,Experimental Setting,Experimental Setting,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.9230769230769232,163,0.6468253968253969,24,0.9230769230769232,1,0,Experimental Setting
165,We do not show results from RNN models because they were shown to be less effective in sentence classification in our prior experiments .,Experimental Setting,Experimental Setting,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.9615384615384616,164,0.6507936507936508,25,0.9615384615384616,1,0,Experimental Setting
166,"For models with additional context , we further use an ensemble classification model using a commonly used method by averaging the class probability scores generated by the multiple variants ( in our model 's case , N = 1 and N = 10 models ) , following .",Experimental Setting,Experimental Setting,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,1.0,165,0.6547619047619048,26,1.0,1,0,Experimental Setting
167,Results and Discussion,,,text-classification,9,"['O', 'O', 'O']","['O', 'O', 'O']",0,0.0,166,0.6587301587301587,0,0.0,1,0,
175,"C refers to the additional context , N refers to the number of translations .",Results and Discussion,Results and Discussion,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",8,0.3333333333333333,174,0.6904761904761905,8,0.3333333333333333,1,0,Results and Discussion
176,"In TopCNN , word refers to using word - specific topic while sentence refers to using sentence - specific topic .",Results and Discussion,Results and Discussion,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",9,0.375,175,0.6944444444444444,9,0.375,1,0,Results and Discussion
179,The winning result is underlined .,Results and Discussion,Results and Discussion,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O']",12,0.5,178,0.7063492063492064,12,0.5,1,0,Results and Discussion
180,"The number inside the parenthesis indicates the increase from the base model , CNN . on most data sets .",Results and Discussion,Results and Discussion,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.5416666666666666,179,0.7103174603174603,13,0.5416666666666666,1,0,Results and Discussion
187,"This is resolved by CNN + B2 by applying L2 regularization , however the increase in performance is marginal .",Results and Discussion,Results and Discussion,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",20,0.8333333333333334,186,0.7380952380952381,20,0.8333333333333334,1,0,Results and Discussion
192,Model Interpretation,,,text-classification,9,"['O', 'O']","['O', 'O']",0,0.0,191,0.7579365079365079,0,0.0,1,0,
194,"In the first example , it is hard to classify whether the translated sentence is positive or negative , thus it is given a low self usability score .",Model Interpretation,Model Interpretation,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.048780487804878,193,0.7658730158730159,2,0.048780487804878,1,0,Model Interpretation
195,"In the second example , although the sentence contains mistranslations , these are minimal and may actually help the classifier by telling it that thirst for violence is not a attention ( negative sentence ) the mothman prophecies , which is mostly a bore , seems to exist only for its climactic setpiece . negative phrase .",Model Interpretation,Model Interpretation,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.073170731707317,194,0.7698412698412699,3,0.073170731707317,1,0,Model Interpretation
198,"The larger the attention weight is , the more the context is used to fix the Korean sentence .",Model Interpretation,Model Interpretation,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.1463414634146341,197,0.7817460317460317,6,0.1463414634146341,1,0,Model Interpretation
199,In the Original sentence : skip this turd and pick your nose instead because you 're sure to get more out of the latter experience .,Model Interpretation,Model Interpretation,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.1707317073170731,198,0.7857142857142857,7,0.1707317073170731,1,0,Model Interpretation
203,Self,Model Interpretation,,text-classification,9,['O'],['O'],11,0.2682926829268293,202,0.8015873015873016,11,0.2682926829268293,1,0,Model Interpretation
204,Usability : 0.3958 ( a ) Low self usability example Original sentence : michael moore 's latest documentary about america 's thirst for violence is his best film yet . . .,Model Interpretation,Self,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.2926829268292683,203,0.8055555555555556,12,0.2926829268292683,1,0,Model Interpretation: Self
208,Self,Model Interpretation,,text-classification,9,['O'],['O'],16,0.3902439024390244,207,0.8214285714285714,16,0.3902439024390244,1,0,Model Interpretation
212,NN ( altered ),Model Interpretation,Self,text-classification,9,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",20,0.4878048780487805,211,0.8373015873015873,20,0.4878048780487805,1,0,Model Interpretation: Self
213,"after scenes of nonsense , you 'll be wistful for the testosteronecharged wizardry of jerry bruckheimer productions , especially because half past dead is like the rock on walmart budget . :",Model Interpretation,Self,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.5121951219512195,212,0.8412698412698413,21,0.5121951219512195,1,0,Model Interpretation: Self
216,"first example , the Korean sentence contains translation errors ; especially , the words bore and climactic setpiece were not translated and were only spelled using the Korean alphabet .",Model Interpretation,Self,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",24,0.5853658536585366,215,0.8531746031746031,24,0.5853658536585366,1,0,Model Interpretation: Self
217,"In this example , the English attention weight is larger than the Korean attention weight .",Model Interpretation,Self,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",25,0.6097560975609756,216,0.8571428571428571,25,0.6097560975609756,1,0,Model Interpretation: Self
219,"However , this phrase is not necessary to classify the sentence correctly , and may induce possible vagueness because of the word trouble .",Model Interpretation,Self,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",27,0.6585365853658537,218,0.8650793650793651,27,0.6585365853658537,1,0,Model Interpretation: Self
220,"Thus , the Korean attention weight is larger .",Model Interpretation,Self,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",28,0.6829268292682927,219,0.8690476190476191,28,0.6829268292682927,1,0,Model Interpretation: Self
222,"In the first example , the unaltered sentence vectors are mostly in the middle of the vector space , making it hard to draw a boundary between the two examples .",Model Interpretation,Self,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",30,0.7317073170731707,221,0.876984126984127,30,0.7317073170731707,1,0,Model Interpretation: Self
223,"After the fixing , the boundary is much clearer .",Model Interpretation,Self,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",31,0.7560975609756098,222,0.8809523809523809,31,0.7560975609756098,1,0,Model Interpretation: Self
225,"Even without fixing the unaltered English sentence vectors , it is easy to distinguish both classes .",Model Interpretation,Self,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",33,0.8048780487804879,224,0.8888888888888888,33,0.8048780487804879,1,0,Model Interpretation: Self
229,"In the first example , the unaltered vector focuses on the meaning of "" wasted yours "" in the sentence , which puts it near sentences regarding wasted time or money .",Model Interpretation,Self,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",37,0.902439024390244,228,0.9047619047619048,37,0.902439024390244,1,0,Model Interpretation: Self
231,"In the second example , all three sentences have highly descriptive tones , however , the nearest neighbor on the altered space is hyperbolically negative , comparing the movie to a description unrelated to the movie itself .",Model Interpretation,Self,text-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",39,0.951219512195122,230,0.9126984126984128,39,0.951219512195122,1,0,Model Interpretation: Self
232,NN ( Unaltered ),Model Interpretation,Self,text-classification,9,"['O', 'O', 'O', 'O']","['O', 'O', 'O', 'O']",40,0.975609756097561,231,0.9166666666666666,40,0.975609756097561,1,0,Model Interpretation: Self
234,Related Work,,,text-classification,9,"['O', 'O']","['O', 'O']",0,0.0,233,0.9246031746031746,0,0.0,1,0,
248,Conclusion,,,text-classification,9,['O'],['O'],0,0.0,247,0.98015873015873,0,0.0,1,0,
