idx,text,main_heading,heading,topic,paper_idx,offset1,pro1,offset2,pro2,offset3,pro3,mask,labels
1,title,,,machine-translation,0,0,0.0,0,0.0,0,0.0,1,0
2,Learning Phrase Representations using RNN Encoder - Decoder for Statistical Machine Translation,title,title,machine-translation,0,1,0.0,1,0.0045662100456621,1,0.0,1,1
3,abstract,,,machine-translation,0,0,0.0,2,0.0091324200913242,0,0.0,1,0
4,"In this paper , we propose a novel neural network model called RNN Encoder - Decoder that consists of two recurrent neural networks ( RNN ) .",abstract,abstract,machine-translation,0,1,0.2,3,0.0136986301369863,1,0.2,1,0
5,"One RNN encodes a sequence of symbols into a fixedlength vector representation , and the other decodes the representation into another sequence of symbols .",abstract,abstract,machine-translation,0,2,0.4,4,0.0182648401826484,2,0.4,1,0
6,The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence .,abstract,abstract,machine-translation,0,3,0.6,5,0.0228310502283105,3,0.6,1,0
7,The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder - Decoder as an additional feature in the existing log - linear model .,abstract,abstract,machine-translation,0,4,0.8,6,0.0273972602739726,4,0.8,1,0
8,"Qualitatively , we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases .",abstract,abstract,machine-translation,0,5,1.0,7,0.0319634703196347,5,1.0,1,0
9,Introduction,,,machine-translation,0,0,0.0,8,0.0365296803652968,0,0.0,1,0
10,"Deep neural networks have shown great success in various applications such as objection recognition ( see , e.g. , ) and speech recognition ( see , e.g. , ) .",Introduction,Introduction,machine-translation,0,1,0.0092592592592592,9,0.0410958904109589,1,0.0263157894736842,1,0
11,"Furthermore , many recent works showed that neural networks can be successfully used in a number of tasks in natural language processing ( NLP ) .",Introduction,Introduction,machine-translation,0,2,0.0185185185185185,10,0.045662100456621,2,0.0526315789473684,1,0
12,"These include , but are not limited to , language modeling , paraphrase detection and word embedding extraction .",Introduction,Introduction,machine-translation,0,3,0.0277777777777777,11,0.0502283105022831,3,0.0789473684210526,1,0
13,"In the field of statistical machine translation ( SMT ) , deep neural networks have begun to show promising results .",Introduction,Introduction,machine-translation,0,4,0.037037037037037,12,0.0547945205479452,4,0.1052631578947368,1,0
14,summarizes a successful usage of feedforward neural networks in the framework of phrase - based SMT system .,Introduction,Introduction,machine-translation,0,5,0.0462962962962962,13,0.0593607305936073,5,0.131578947368421,1,0
15,"Along this line of research on using neural networks for SMT , this paper focuses on a novel neural network architecture that can be used as apart of the conventional phrase - based SMT system .",Introduction,Introduction,machine-translation,0,6,0.0555555555555555,14,0.0639269406392694,6,0.1578947368421052,1,1
16,"The proposed neural network architecture , which we will refer to as an RNN Encoder - Decoder , consists of two recurrent neural networks ( RNN ) that act as an encoder and a decoder pair .",Introduction,Introduction,machine-translation,0,7,0.0648148148148148,15,0.0684931506849315,7,0.1842105263157894,1,1
17,"The encoder maps a variable - length source sequence to a fixed - length vector , and the decoder maps the vector representation back to a variable - length target sequence .",Introduction,Introduction,machine-translation,0,8,0.074074074074074,16,0.0730593607305936,8,0.2105263157894736,1,1
18,The two networks are trained jointly to maximize the conditional probability of the target sequence given a source sequence .,Introduction,Introduction,machine-translation,0,9,0.0833333333333333,17,0.0776255707762557,9,0.2368421052631578,1,1
19,"Additionally , we propose to use a rather sophisticated hidden unit in order to improve both the memory capacity and the ease of training .",Introduction,Introduction,machine-translation,0,10,0.0925925925925925,18,0.0821917808219178,10,0.2631578947368421,1,1
20,The proposed RNN Encoder - Decoder with a novel hidden unit is empirically evaluated on the task of translating from English to French .,Introduction,Introduction,machine-translation,0,11,0.1018518518518518,19,0.0867579908675799,11,0.2894736842105263,1,0
21,We train the model to learn the translation probability of an English phrase to a corresponding French phrase .,Introduction,Introduction,machine-translation,0,12,0.1111111111111111,20,0.091324200913242,12,0.3157894736842105,1,0
22,The model is then used as apart of a standard phrase - based SMT system by scoring each phrase pair in the phrase table .,Introduction,Introduction,machine-translation,0,13,0.1203703703703703,21,0.0958904109589041,13,0.3421052631578947,1,0
23,The empirical evaluation reveals that this approach of scoring phrase pairs with an RNN Encoder - Decoder improves the translation performance .,Introduction,Introduction,machine-translation,0,14,0.1296296296296296,22,0.1004566210045662,14,0.3684210526315789,1,0
24,We qualitatively analyze the trained RNN Encoder - Decoder by comparing its phrase scores with those given by the existing translation model .,Introduction,Introduction,machine-translation,0,15,0.1388888888888889,23,0.1050228310502283,15,0.3947368421052631,1,0
25,"The qualitative analysis shows that the RNN Encoder - Decoder is better at capturing the linguistic regularities in the phrase table , indirectly explaining the quantitative improvements in the overall translation performance .",Introduction,Introduction,machine-translation,0,16,0.1481481481481481,24,0.1095890410958904,16,0.4210526315789473,1,0
26,The further analysis of the model reveals that the RNN Encoder - Decoder learns a continuous space representation of a phrase that preserves both the semantic and syntactic structure of the phrase .,Introduction,Introduction,machine-translation,0,17,0.1574074074074074,25,0.1141552511415525,17,0.4473684210526316,1,0
27,"A recurrent neural network ( RNN ) is a neural network that consists of a hidden state hand an optional output y which operates on a variablelength sequence x = ( x 1 , . . . , x T ) .",Introduction,Introduction,machine-translation,0,18,0.1666666666666666,26,0.1187214611872146,18,0.4736842105263157,1,0
28,"At each time step t , the hidden state ht of the RNN is updated by",Introduction,Introduction,machine-translation,0,19,0.1759259259259259,27,0.1232876712328767,19,0.5,1,0
29,where f is a non-linear activation function .,Introduction,Introduction,machine-translation,0,20,0.1851851851851851,28,0.1278538812785388,20,0.5263157894736842,1,0
30,f maybe as simple as an elementwise logistic sigmoid function and as complex as along short - term memory ( LSTM ) unit ) .,Introduction,Introduction,machine-translation,0,21,0.1944444444444444,29,0.1324200913242009,21,0.5526315789473685,1,0
31,An RNN can learn a probability distribution over a sequence by being trained to predict the next symbol in a sequence .,Introduction,Introduction,machine-translation,0,22,0.2037037037037037,30,0.136986301369863,22,0.5789473684210527,1,0
32,"In that case , the output at each timestep t is the conditional distribution p ( x t | x t?1 , . . . , x 1 ) .",Introduction,Introduction,machine-translation,0,23,0.2129629629629629,31,0.1415525114155251,23,0.6052631578947368,1,0
33,"For example , a multinomial distribution ( 1 - of - K coding ) can be output using a softmax activation function",Introduction,Introduction,machine-translation,0,24,0.2222222222222222,32,0.1461187214611872,24,0.631578947368421,1,0
34,"for all possible symbols j = 1 , . . . , K , where w j are the rows of a weight matrix W. By combining these probabilities , we can compute the probability of the sequence x using",Introduction,Introduction,machine-translation,0,25,0.2314814814814814,33,0.1506849315068493,25,0.6578947368421053,1,0
35,"From this learned distribution , it is straightforward to sample anew sequence by iteratively sampling a symbol at each time step .",Introduction,Introduction,machine-translation,0,26,0.2407407407407407,34,0.1552511415525114,26,0.6842105263157895,1,0
36,RNN Encoder - Decoder,Introduction,Introduction,machine-translation,0,27,0.25,35,0.1598173515981735,27,0.7105263157894737,1,0
37,The RNN Encoder - Decoder used in the experiment had 1000 hidden units with the proposed gates at the encoder and at the decoder .,Introduction,Introduction,machine-translation,0,28,0.2592592592592592,36,0.1643835616438356,28,0.7368421052631579,1,0
38,"The input matrix between each input symbol x t and the hidden unit is approximated with two lower - rank matrices , and the output matrix is approximated similarly .",Introduction,Introduction,machine-translation,0,29,0.2685185185185185,37,0.1689497716894977,29,0.7631578947368421,1,0
39,"We used rank - 100 matrices , equivalent to learning an embedding of dimension 100 for each word .",Introduction,Introduction,machine-translation,0,30,0.2777777777777778,38,0.1735159817351598,30,0.7894736842105263,1,0
40,The activation function used forh in Eq. ( 8 ) is a hyperbolic tangent function .,Introduction,Introduction,machine-translation,0,31,0.287037037037037,39,0.1780821917808219,31,0.8157894736842105,1,0
41,"The computation from the hidden state in the decoder to the output is implemented as a deep neural network ( Pascanu et al. , 2014 ) with a single intermediate layer having 500 maxout units each pooling 2 inputs .",Introduction,Introduction,machine-translation,0,32,0.2962962962962963,40,0.182648401826484,32,0.8421052631578947,1,0
42,"All the weight parameters in the RNN Encoder - Decoder were initialized by sampling from an isotropic zero-mean ( white ) Gaussian distribution with its standard deviation fixed to 0.01 , except for the recurrent weight parameters .",Introduction,Introduction,machine-translation,0,33,0.3055555555555556,41,0.1872146118721461,33,0.868421052631579,1,0
43,"For the recurrent weight matrices , we first sampled from a white Gaussian distribution and used its left singular vectors matrix , following .",Introduction,Introduction,machine-translation,0,34,0.3148148148148148,42,0.1917808219178082,34,0.8947368421052632,1,0
44,"We used Adadelta and stochastic gradient descent to train the RNN Encoder - Decoder with hyperparameters = 10 ?6 and ? = 0.95 ( Zeiler , 2012 ) .",Introduction,Introduction,machine-translation,0,35,0.324074074074074,43,0.1963470319634703,35,0.9210526315789472,1,0
45,"At each update , we used 64 randomly selected phrase pairs from a phrase table ( which was created from 348 M words ) .",Introduction,Introduction,machine-translation,0,36,0.3333333333333333,44,0.2009132420091324,36,0.9473684210526316,1,0
46,The model was trained for approximately three days .,Introduction,Introduction,machine-translation,0,37,0.3425925925925926,45,0.2054794520547945,37,0.9736842105263158,1,0
47,Details of the architecture used in the experiments are explained in more depth in the supplementary material .,Introduction,Introduction,machine-translation,0,38,0.3518518518518518,46,0.2100456621004566,38,1.0,1,0
48,Hidden Unit that Adaptively Remembers and Forgets,Introduction,,machine-translation,0,39,0.3611111111111111,47,0.2146118721461187,0,0.0,1,0
49,"In addition to a novel model architecture , we also propose anew type of hidden unit ( f in Eq .",Introduction,Hidden Unit that Adaptively Remembers and Forgets,machine-translation,0,40,0.3703703703703703,48,0.2191780821917808,1,0.0454545454545454,1,0
50,( 1 ) ) that has been motivated by the LSTM unit but is much simpler to compute and implement .,Introduction,Hidden Unit that Adaptively Remembers and Forgets,machine-translation,0,41,0.3796296296296296,49,0.2237442922374429,2,0.0909090909090909,1,0
51,1 shows the graphical depiction of the proposed hidden unit .,Introduction,Hidden Unit that Adaptively Remembers and Forgets,machine-translation,0,42,0.3888888888888889,50,0.228310502283105,3,0.1363636363636363,1,0
52,Let us describe how the activation of the j - th hidden unit is computed .,Introduction,Hidden Unit that Adaptively Remembers and Forgets,machine-translation,0,43,0.3981481481481481,51,0.2328767123287671,4,0.1818181818181818,1,0
53,"First , the reset gate r j is computed by",Introduction,Hidden Unit that Adaptively Remembers and Forgets,machine-translation,0,44,0.4074074074074074,52,0.2374429223744292,5,0.2272727272727272,1,0
54,where ?,Introduction,Hidden Unit that Adaptively Remembers and Forgets,machine-translation,0,45,0.4166666666666667,53,0.2420091324200913,6,0.2727272727272727,1,0
55,"is the logistic sigmoid function , and [. ] j denotes the j - th element of a vector .",Introduction,Hidden Unit that Adaptively Remembers and Forgets,machine-translation,0,46,0.4259259259259259,54,0.2465753424657534,7,0.3181818181818182,1,0
56,"x and h t?1 are the input and the previous hidden state , respectively .",Introduction,Hidden Unit that Adaptively Remembers and Forgets,machine-translation,0,47,0.4351851851851852,55,0.2511415525114155,8,0.3636363636363636,1,0
57,W rand Ur are weight matrices which are learned .,Introduction,Hidden Unit that Adaptively Remembers and Forgets,machine-translation,0,48,0.4444444444444444,56,0.2557077625570776,9,0.4090909090909091,1,0
58,"Similarly , the update gate z j is computed by",Introduction,Hidden Unit that Adaptively Remembers and Forgets,machine-translation,0,49,0.4537037037037037,57,0.2602739726027397,10,0.4545454545454545,1,0
59,The actual activation of the proposed unit h j is then computed by,Introduction,Hidden Unit that Adaptively Remembers and Forgets,machine-translation,0,50,0.4629629629629629,58,0.2648401826484018,11,0.5,1,0
60,wher ?,Introduction,Hidden Unit that Adaptively Remembers and Forgets,machine-translation,0,51,0.4722222222222222,59,0.2694063926940639,12,0.5454545454545454,1,0
61,h,Introduction,Hidden Unit that Adaptively Remembers and Forgets,machine-translation,0,52,0.4814814814814814,60,0.273972602739726,13,0.5909090909090909,1,0
62,"In this formulation , when the reset gate is close to 0 , the hidden state is forced to ignore the previous hidden state and reset with the current input only .",Introduction,Hidden Unit that Adaptively Remembers and Forgets,machine-translation,0,53,0.4907407407407407,61,0.2785388127853881,14,0.6363636363636364,1,0
63,"This effectively allows the hidden state to drop any information that is found to be irrelevant later in the future , thus , allowing a more compact representation .",Introduction,Hidden Unit that Adaptively Remembers and Forgets,machine-translation,0,54,0.5,62,0.2831050228310502,15,0.6818181818181818,1,0
64,"On the other hand , the update gate controls how much information from the previous hidden state will carryover to the current hidden state .",Introduction,Hidden Unit that Adaptively Remembers and Forgets,machine-translation,0,55,0.5092592592592593,63,0.2876712328767123,16,0.7272727272727273,1,0
65,This acts similarly to the memory cell in the LSTM network and helps the RNN to remember longterm information .,Introduction,Hidden Unit that Adaptively Remembers and Forgets,machine-translation,0,56,0.5185185185185185,64,0.2922374429223744,17,0.7727272727272727,1,0
66,"Furthermore , this maybe considered an adaptive variant of a leaky - integration unit .",Introduction,Hidden Unit that Adaptively Remembers and Forgets,machine-translation,0,57,0.5277777777777778,65,0.2968036529680365,18,0.8181818181818182,1,0
67,"As each hidden unit has separate reset and update gates , each hidden unit will learn to capture dependencies over different time scales .",Introduction,Hidden Unit that Adaptively Remembers and Forgets,machine-translation,0,58,0.5370370370370371,66,0.3013698630136986,19,0.8636363636363636,1,0
68,"Those units that learn to capture short - term dependencies will tend to have reset gates that are frequently active , but those that capture longer - term dependencies will have update gates that are mostly active .",Introduction,Hidden Unit that Adaptively Remembers and Forgets,machine-translation,0,59,0.5462962962962963,67,0.3059360730593607,20,0.9090909090909092,1,0
69,"In our preliminary experiments , we found that it is crucial to use this new unit with gating units .",Introduction,Hidden Unit that Adaptively Remembers and Forgets,machine-translation,0,60,0.5555555555555556,68,0.3105022831050228,21,0.9545454545454546,1,0
70,We were notable to get meaningful result with an oft - used tanh unit without any gating .,Introduction,Hidden Unit that Adaptively Remembers and Forgets,machine-translation,0,61,0.5648148148148148,69,0.3150684931506849,22,1.0,1,0
71,Statistical Machine Translation,Introduction,,machine-translation,0,62,0.5740740740740741,70,0.319634703196347,0,0.0,1,0
72,"Ina commonly used statistical machine translation system ( SMT ) , the goal of the system ( decoder , specifically ) is to find a translation f given a source sentence e , which maximizes",Introduction,Statistical Machine Translation,machine-translation,0,63,0.5833333333333334,71,0.3242009132420091,1,0.0217391304347826,1,0
73,"where the first term at the right hand side is called translation model and the latter language model ( see , e.g. , ) .",Introduction,Statistical Machine Translation,machine-translation,0,64,0.5925925925925926,72,0.3287671232876712,2,0.0434782608695652,1,0
74,"In practice , however , most SMT systems model log p ( f | e ) as a loglinear model with additional features and corre - sponding weights : where f n and w n are the n - th feature and weight , respectively .",Introduction,Statistical Machine Translation,machine-translation,0,65,0.6018518518518519,73,0.3333333333333333,3,0.0652173913043478,1,0
75,Z ( e ) is a normalization constant that does not depend on the weights .,Introduction,Statistical Machine Translation,machine-translation,0,66,0.6111111111111112,74,0.3378995433789954,4,0.0869565217391304,1,0
76,The weights are often optimized to maximize the BLEU score on a development set .,Introduction,Statistical Machine Translation,machine-translation,0,67,0.6203703703703703,75,0.3424657534246575,5,0.108695652173913,1,0
77,"In the phrase - based SMT framework introduced in and , the translation model log p ( e | f ) is factorized into the translation probabilities of matching phrases in the source and target sentences .",Introduction,Statistical Machine Translation,machine-translation,0,68,0.6296296296296297,76,0.3470319634703196,6,0.1304347826086956,1,0
78,2,Introduction,Statistical Machine Translation,machine-translation,0,69,0.6388888888888888,77,0.3515981735159817,7,0.1521739130434782,1,0
79,These probabilities are once again considered additional features in the log - linear model ( see Eq. ) and are weighted accordingly to maximize the BLEU score .,Introduction,Statistical Machine Translation,machine-translation,0,70,0.6481481481481481,78,0.3561643835616438,8,0.1739130434782608,1,0
80,"Since the neural net language model was proposed in , neural networks have been used widely in SMT systems .",Introduction,Statistical Machine Translation,machine-translation,0,71,0.6574074074074074,79,0.3607305936073059,9,0.1956521739130435,1,0
81,"In many cases , neural networks have been used to rescore translation hypotheses ( n- best lists ) ( see , e.g. , ) .",Introduction,Statistical Machine Translation,machine-translation,0,72,0.6666666666666666,80,0.365296803652968,10,0.217391304347826,1,0
82,"Recently , however , there has been interest in training neural networks to score the translated sentence ( or phrase pairs ) using a representation of the source sentence as an additional input .",Introduction,Statistical Machine Translation,machine-translation,0,73,0.6759259259259259,81,0.3698630136986301,11,0.2391304347826087,1,0
83,"See , e.g. , , and .",Introduction,Statistical Machine Translation,machine-translation,0,74,0.6851851851851852,82,0.3744292237442922,12,0.2608695652173913,1,0
84,Scoring Phrase Pairs with RNN Encoder - Decoder,Introduction,,machine-translation,0,75,0.6944444444444444,83,0.3789954337899543,13,0.2826086956521739,1,0
85,Here we propose to train the RNN Encoder - Decoder ( see Sec. 2.2 ) on a table of phrase pairs and use its scores as additional features in the loglinear model in Eq. ( 9 ) when tuning the SMT decoder .,Introduction,Scoring Phrase Pairs with RNN Encoder - Decoder,machine-translation,0,76,0.7037037037037037,84,0.3835616438356164,14,0.3043478260869565,1,0
86,"When we train the RNN Encoder - Decoder , we ignore the ( normalized ) frequencies of each phrase pair in the original corpora .",Introduction,Scoring Phrase Pairs with RNN Encoder - Decoder,machine-translation,0,77,0.7129629629629629,85,0.3881278538812785,15,0.3260869565217391,1,0
87,This measure was taken in order ( 1 ) to reduce the computational expense of randomly selecting phrase pairs from a large phrase table according to the normalized frequencies and ( 2 ) to ensure that the RNN Encoder - Decoder does not simply learn to rank the phrase pairs according to their numbers of occurrences .,Introduction,Scoring Phrase Pairs with RNN Encoder - Decoder,machine-translation,0,78,0.7222222222222222,86,0.3926940639269406,16,0.3478260869565217,1,0
88,One underlying reason for this choice was that the existing translation probability in the phrase table already reflects the frequencies of the phrase pairs in the original corpus .,Introduction,Scoring Phrase Pairs with RNN Encoder - Decoder,machine-translation,0,79,0.7314814814814815,87,0.3972602739726027,17,0.3695652173913043,1,0
89,"With a fixed capacity of the RNN Encoder - Decoder , we try to ensure that most of the capacity of the model is focused toward learning linguistic regularities , i.e. , distinguishing between plausible and implausible translations , or learning the "" manifold "" ( region of probability concentration ) of plausible translations .",Introduction,Scoring Phrase Pairs with RNN Encoder - Decoder,machine-translation,0,80,0.7407407407407407,88,0.4018264840182648,18,0.391304347826087,1,0
90,"Once the RNN Encoder - Decoder is trained , we add anew score for each phrase pair to the existing phrase table .",Introduction,Scoring Phrase Pairs with RNN Encoder - Decoder,machine-translation,0,81,0.75,89,0.4063926940639269,19,0.4130434782608695,1,0
91,This allows the new scores to enter into the existing tuning algorithm with minimal additional overhead in computation .,Introduction,Scoring Phrase Pairs with RNN Encoder - Decoder,machine-translation,0,82,0.7592592592592593,90,0.410958904109589,20,0.4347826086956521,1,0
92,"As Schwenk pointed out in , it is possible to completely replace the existing phrase table with the proposed RNN Encoder - Decoder .",Introduction,Scoring Phrase Pairs with RNN Encoder - Decoder,machine-translation,0,83,0.7685185185185185,91,0.4155251141552511,21,0.4565217391304347,1,0
93,"In that case , fora given source phrase , the RNN Encoder - Decoder will need to generate a list of ( good ) target phrases .",Introduction,Scoring Phrase Pairs with RNN Encoder - Decoder,machine-translation,0,84,0.7777777777777778,92,0.4200913242009132,22,0.4782608695652174,1,0
94,"This requires , however , an expensive sampling procedure to be performed repeatedly .",Introduction,Scoring Phrase Pairs with RNN Encoder - Decoder,machine-translation,0,85,0.7870370370370371,93,0.4246575342465753,23,0.5,1,0
95,"In this paper , thus , we only consider rescoring the phrase pairs in the phrase table .",Introduction,Scoring Phrase Pairs with RNN Encoder - Decoder,machine-translation,0,86,0.7962962962962963,94,0.4292237442922374,24,0.5217391304347826,1,0
96,Related Approaches : Neural Networks in Machine Translation,Introduction,,machine-translation,0,87,0.8055555555555556,95,0.4337899543378995,25,0.5434782608695652,1,0
97,"Before presenting the empirical results , we discuss a number of recent works that have proposed to use neural networks in the context of SMT .",Introduction,Related Approaches : Neural Networks in Machine Translation,machine-translation,0,88,0.8148148148148148,96,0.4383561643835616,26,0.5652173913043478,1,0
98,Schwenk in proposed a similar approach of scoring phrase pairs .,Introduction,Related Approaches : Neural Networks in Machine Translation,machine-translation,0,89,0.8240740740740741,97,0.4429223744292237,27,0.5869565217391305,1,0
99,"Instead of the RNN - based neural network , he used a feedforward neural network that has fixed - size inputs ( 7 words in his case , with zero - padding for shorter phrases ) and fixed - size outputs ( 7 words in the target language ) .",Introduction,Related Approaches : Neural Networks in Machine Translation,machine-translation,0,90,0.8333333333333334,98,0.4474885844748858,28,0.6086956521739131,1,0
100,"When it is used specifically for scoring phrases for the SMT system , the maximum phrase length is often chosen to be small .",Introduction,Related Approaches : Neural Networks in Machine Translation,machine-translation,0,91,0.8425925925925926,99,0.4520547945205479,29,0.6304347826086957,1,0
101,"However , as the length of phrases increases or as we apply neural networks to other variable - length sequence data , it is important that the neural network can handle variable - length input and output .",Introduction,Related Approaches : Neural Networks in Machine Translation,machine-translation,0,92,0.8518518518518519,100,0.45662100456621,30,0.6521739130434783,1,0
102,The proposed RNN Encoder - Decoder is well - suited for these applications .,Introduction,Related Approaches : Neural Networks in Machine Translation,machine-translation,0,93,0.8611111111111112,101,0.4611872146118721,31,0.6739130434782609,1,0
103,"Similar to , Devlin et al. proposed to use a feedforward neural network to model a translation model , however , by predicting one word in a target phrase at a time .",Introduction,Related Approaches : Neural Networks in Machine Translation,machine-translation,0,94,0.8703703703703703,102,0.4657534246575342,32,0.6956521739130435,1,0
104,"They reported an impressive improvement , but their approach still requires the maximum length of the input phrase ( or context words ) to be fixed a priori .",Introduction,Related Approaches : Neural Networks in Machine Translation,machine-translation,0,95,0.8796296296296297,103,0.4703196347031963,33,0.717391304347826,1,0
105,"Although it is not exactly a neural network they train , the authors of proposed to learn a bilingual embedding of words / phrases .",Introduction,Related Approaches : Neural Networks in Machine Translation,machine-translation,0,96,0.8888888888888888,104,0.4748858447488584,34,0.7391304347826086,1,0
106,They use the learned embedding to compute the distance between a pair of phrases which is used as an additional score of the phrase pair in an SMT system .,Introduction,Related Approaches : Neural Networks in Machine Translation,machine-translation,0,97,0.8981481481481481,105,0.4794520547945205,35,0.7608695652173914,1,0
107,"In , a feedforward neural network was trained to learn a mapping from a bag - of - words representation of an input phrase to an output phrase .",Introduction,Related Approaches : Neural Networks in Machine Translation,machine-translation,0,98,0.9074074074074074,106,0.4840182648401826,36,0.782608695652174,1,0
108,"This is closely related to both the proposed RNN Encoder - Decoder and the model proposed in , except that their input representation of a phrase is a bag - of - words .",Introduction,Related Approaches : Neural Networks in Machine Translation,machine-translation,0,99,0.9166666666666666,107,0.4885844748858447,37,0.8043478260869565,1,0
109,A similar approach of using bag - of - words representations was proposed in as well .,Introduction,Related Approaches : Neural Networks in Machine Translation,machine-translation,0,100,0.925925925925926,108,0.4931506849315068,38,0.8260869565217391,1,0
110,"Earlier , a similar encoder - decoder model using two recursive neural networks was proposed in ) , but their model was restricted to a monolingual setting , i.e. the model reconstructs an input sentence .",Introduction,Related Approaches : Neural Networks in Machine Translation,machine-translation,0,101,0.9351851851851852,109,0.4977168949771689,39,0.8478260869565217,1,0
111,"More recently , another encoder - decoder model using an RNN was proposed in , where the decoder is conditioned on a representation of either a source sentence or a source context .",Introduction,Related Approaches : Neural Networks in Machine Translation,machine-translation,0,102,0.9444444444444444,110,0.502283105022831,40,0.8695652173913043,1,0
112,One important difference between the proposed RNN Encoder - Decoder and the approaches in and is that the order of the words in source and target phrases is taken into account .,Introduction,Related Approaches : Neural Networks in Machine Translation,machine-translation,0,103,0.9537037037037036,111,0.5068493150684932,41,0.8913043478260869,1,0
113,"The RNN Encoder - Decoder naturally distinguishes between sequences that have the same words but in a different order , whereas the aforementioned approaches effectively ignore order information .",Introduction,Related Approaches : Neural Networks in Machine Translation,machine-translation,0,104,0.9629629629629628,112,0.5114155251141552,42,0.9130434782608696,1,0
114,The closest approach related to the proposed RNN Encoder - Decoder is the Recurrent Continuous Translation Model ( Model 2 ) proposed in .,Introduction,Related Approaches : Neural Networks in Machine Translation,machine-translation,0,105,0.9722222222222222,113,0.5159817351598174,43,0.9347826086956522,1,0
115,"In their paper , they proposed a similar model that consists of an encoder and decoder .",Introduction,Related Approaches : Neural Networks in Machine Translation,machine-translation,0,106,0.9814814814814816,114,0.5205479452054794,44,0.9565217391304348,1,0
116,The difference with our model is that they used a convolutional n-gram model ( CGM ) for the encoder and the hybrid of an inverse CGM and a recurrent neural network for the decoder .,Introduction,Related Approaches : Neural Networks in Machine Translation,machine-translation,0,107,0.9907407407407408,115,0.5251141552511416,45,0.9782608695652174,1,0
117,"They , however , evaluated their model on rescoring the n-best list proposed by the conventional SMT system and computing the perplexity of the gold standard translations .",Introduction,Related Approaches : Neural Networks in Machine Translation,machine-translation,0,108,1.0,116,0.5296803652968036,46,1.0,1,0
118,Experiments,,,machine-translation,0,0,0.0,117,0.5342465753424658,0,0.0,1,0
119,We evaluate our approach on the English / French translation task of the WMT ' 14 workshop .,Experiments,Experiments,machine-translation,0,1,0.0,118,0.5388127853881278,1,0.0,1,0
120,Data and Baseline System,,,machine-translation,0,0,0.0,119,0.54337899543379,0,0.0,1,0
121,Large amounts of resources are available to build an English / French SMT system in the framework of the WMT ' 14 translation task .,Data and Baseline System,Data and Baseline System,machine-translation,0,1,0.0625,120,0.547945205479452,1,0.0625,1,0
122,"The bilingual corpora include Europarl ( 61M words ) , news commentary ( 5.5 M ) , UN ( 421 M ) , and two crawled corpora of 90 M and 780M words respectively .",Data and Baseline System,Data and Baseline System,machine-translation,0,2,0.125,121,0.5525114155251142,2,0.125,1,0
123,The last two corpora are quite noisy .,Data and Baseline System,Data and Baseline System,machine-translation,0,3,0.1875,122,0.5570776255707762,3,0.1875,1,0
124,"To train the French language model , about 712M words of crawled newspaper material is available in addition to the target side of the bitexts .",Data and Baseline System,Data and Baseline System,machine-translation,0,4,0.25,123,0.5616438356164384,4,0.25,1,0
125,All the word counts refer to French words after tokenization .,Data and Baseline System,Data and Baseline System,machine-translation,0,5,0.3125,124,0.5662100456621004,5,0.3125,1,0
126,"It is commonly acknowledged that training statistical models on the concatenation of all this data does not necessarily lead to optimal performance , and results in extremely large models which are difficult to handle .",Data and Baseline System,Data and Baseline System,machine-translation,0,6,0.375,125,0.5707762557077626,6,0.375,1,0
127,"Instead , one should focus on the most relevant subset of the data fora given task .",Data and Baseline System,Data and Baseline System,machine-translation,0,7,0.4375,126,0.5753424657534246,7,0.4375,1,0
128,"We have done so by applying the data selection method proposed in , and its extension to bitexts .",Data and Baseline System,Data and Baseline System,machine-translation,0,8,0.5,127,0.5799086757990868,8,0.5,1,0
129,By these means we selected a subset of 418 M words out of more than 2G words for language modeling and a subset of 348 M out of 850 M words for training the RNN Encoder - Decoder .,Data and Baseline System,Data and Baseline System,machine-translation,0,9,0.5625,128,0.5844748858447488,9,0.5625,1,0
130,"We used the test set newstest2012 and 2013 for data selection and weight tuning with MERT , and newstest2014 as our test set .",Data and Baseline System,Data and Baseline System,machine-translation,0,10,0.625,129,0.589041095890411,10,0.625,1,0
131,Each set has more than 70 thousand words and a single reference translation .,Data and Baseline System,Data and Baseline System,machine-translation,0,11,0.6875,130,0.593607305936073,11,0.6875,1,0
132,"For training the neural networks , including the proposed RNN Encoder - Decoder , we limited the source and target vocabulary to the most frequent 15,000 words for both English and French .",Data and Baseline System,Data and Baseline System,machine-translation,0,12,0.75,131,0.5981735159817352,12,0.75,1,0
133,This covers approximately 93 % of the dataset .,Data and Baseline System,Data and Baseline System,machine-translation,0,13,0.8125,132,0.6027397260273972,13,0.8125,1,0
134,All the out - of - vocabulary words were mapped to a special token ( [ UNK ] ) .,Data and Baseline System,Data and Baseline System,machine-translation,0,14,0.875,133,0.6073059360730594,14,0.875,1,0
135,The baseline phrase - based SMT system was built using Moses with default settings .,Data and Baseline System,Data and Baseline System,machine-translation,0,15,0.9375,134,0.6118721461187214,15,0.9375,1,1
136,"This system achieves a BLEU score of 30.64 and 33.3 on the development and test sets , respectively ( see Table 1 ) .",Data and Baseline System,Data and Baseline System,machine-translation,0,16,1.0,135,0.6164383561643836,16,1.0,1,0
137,Neural Language Model,,,machine-translation,0,0,0.0,136,0.6210045662100456,0,0.0,1,0
138,"In order to assess the effectiveness of scoring phrase pairs with the proposed RNN Encoder - Decoder , we also tried a more traditional approach of using a neural network for learning a target language model ( CSLM ) .",Neural Language Model,Neural Language Model,machine-translation,0,1,0.0144927536231884,137,0.6255707762557078,1,0.0769230769230769,1,0
139,"Especially , the comparison between the SMT system using CSLM and that using the proposed approach of phrase scoring by RNN Encoder - Decoder will clarify whether the contributions from multiple neural networks in different parts of the SMT sys - tem add up or are redundant .",Neural Language Model,Neural Language Model,machine-translation,0,2,0.0289855072463768,138,0.6301369863013698,2,0.1538461538461538,1,0
140,We trained the CSLM model on 7 - grams from the target corpus .,Neural Language Model,Neural Language Model,machine-translation,0,3,0.0434782608695652,139,0.634703196347032,3,0.2307692307692307,1,0
141,"Each input word was projected into the embedding space R 512 , and they were concatenated to form a 3072 dimensional vector .",Neural Language Model,Neural Language Model,machine-translation,0,4,0.0579710144927536,140,0.639269406392694,4,0.3076923076923077,1,0
142,The concatenated vector was fed through two rectified layers ( of size 1536 and 1024 ) .,Neural Language Model,Neural Language Model,machine-translation,0,5,0.072463768115942,141,0.6438356164383562,5,0.3846153846153846,1,0
143,The output layer was a simple softmax layer ( see Eq. ) .,Neural Language Model,Neural Language Model,machine-translation,0,6,0.0869565217391304,142,0.6484018264840182,6,0.4615384615384615,1,0
144,"All the weight parameters were initialized uniformly between ? 0.01 and 0.01 , and the model was trained until the validation perplexity did not improve for 10 epochs .",Neural Language Model,Neural Language Model,machine-translation,0,7,0.1014492753623188,143,0.6529680365296804,7,0.5384615384615384,1,0
145,"After training , the language model achieved a perplexity of 45.80 .",Neural Language Model,Neural Language Model,machine-translation,0,8,0.1159420289855072,144,0.6575342465753424,8,0.6153846153846154,1,0
146,The validation set was a random selection of 0.1 % of the corpus .,Neural Language Model,Neural Language Model,machine-translation,0,9,0.1304347826086956,145,0.6621004566210046,9,0.6923076923076923,1,0
147,"The model was used to score partial translations during the decoding process , which generally leads to higher gains in BLEU score than n-best list rescoring .",Neural Language Model,Neural Language Model,machine-translation,0,10,0.144927536231884,146,0.6666666666666666,10,0.7692307692307693,1,0
148,To address the computational complexity of using a CSLM in the decoder a buffer was used to aggregate n-grams during the stacksearch performed by the decoder .,Neural Language Model,Neural Language Model,machine-translation,0,11,0.1594202898550724,147,0.6712328767123288,11,0.8461538461538461,1,0
149,"Only when the buffer is full , or a stack is about to be pruned , the n-grams are scored by the CSLM .",Neural Language Model,Neural Language Model,machine-translation,0,12,0.1739130434782608,148,0.6757990867579908,12,0.9230769230769232,1,0
150,This allows us to perform fast matrixmatrix multiplication on GPU using Theano .,Neural Language Model,Neural Language Model,machine-translation,0,13,0.1884057971014492,149,0.680365296803653,13,1.0,1,0
151,Quantitative Analysis,Neural Language Model,,machine-translation,0,14,0.2028985507246377,150,0.684931506849315,0,0.0,1,0
152,We tried the following combinations : :,Neural Language Model,Quantitative Analysis,machine-translation,0,15,0.217391304347826,151,0.6894977168949772,1,0.05,1,0
153,The top scoring target phrases fora small set of source phrases according to the translation model ( direct translation probability ) and by the RNN Encoder - Decoder .,Neural Language Model,Quantitative Analysis,machine-translation,0,16,0.2318840579710145,152,0.6940639269406392,2,0.1,1,0
154,Source phrases were randomly selected from phrases with 4 or more words .,Neural Language Model,Quantitative Analysis,machine-translation,0,17,0.2463768115942029,153,0.6986301369863014,3,0.15,1,0
155,?,Neural Language Model,Quantitative Analysis,machine-translation,0,18,0.2608695652173913,154,0.7031963470319634,4,0.2,1,0
156,denotes an incomplete ( partial ) character .,Neural Language Model,Quantitative Analysis,machine-translation,0,19,0.2753623188405797,155,0.7077625570776256,5,0.25,1,0
157,r is a Cyrillic letter ghe .,Neural Language Model,Quantitative Analysis,machine-translation,0,20,0.2898550724637681,156,0.7123287671232876,6,0.3,1,0
158,The results are presented in .,Neural Language Model,Quantitative Analysis,machine-translation,0,21,0.3043478260869565,157,0.7168949771689498,7,0.35,1,0
159,"As expected , adding features computed by neural networks consistently improves the performance over the baseline performance .",Neural Language Model,Quantitative Analysis,machine-translation,0,22,0.3188405797101449,158,0.7214611872146118,8,0.4,1,1
160,The best performance was achieved when we used both CSLM and the phrase scores from the RNN Encoder - Decoder .,Neural Language Model,Quantitative Analysis,machine-translation,0,23,0.3333333333333333,159,0.726027397260274,9,0.45,1,1
161,This suggests that the contributions of the CSLM and the RNN Encoder - Decoder are not too correlated and that one can expect better results by improving each method independently .,Neural Language Model,Quantitative Analysis,machine-translation,0,24,0.3478260869565217,160,0.730593607305936,10,0.5,1,0
162,"Furthermore , we tried penalizing the number of words that are unknown to the neural networks ( i.e. words which are not in the shortlist ) .",Neural Language Model,Quantitative Analysis,machine-translation,0,25,0.3623188405797101,161,0.7351598173515982,11,0.55,1,0
163,We do so by simply adding the number of unknown words as an additional feature the loglinear model in Eq. ( 9 ) .,Neural Language Model,Quantitative Analysis,machine-translation,0,26,0.3768115942028985,162,0.7397260273972602,12,0.6,1,0
164,3,Neural Language Model,Quantitative Analysis,machine-translation,0,27,0.391304347826087,163,0.7442922374429224,13,0.65,1,0
165,"However , in this case we 3 To understand the effect of the penalty , consider the set of all words in the 15,000 large shortlist , SL .",Neural Language Model,Quantitative Analysis,machine-translation,0,28,0.4057971014492754,164,0.7488584474885844,14,0.7,1,0
166,All words x i / ?,Neural Language Model,Quantitative Analysis,machine-translation,0,29,0.4202898550724637,165,0.7534246575342466,15,0.75,1,0
167,SL are replaced by a special token [ UNK ] before being scored by the neural networks .,Neural Language Model,Quantitative Analysis,machine-translation,0,30,0.4347826086956521,166,0.7579908675799086,16,0.8,1,0
168,"Hence , the conditional probability of any xi t / ?",Neural Language Model,Quantitative Analysis,machine-translation,0,31,0.4492753623188406,167,0.7625570776255708,17,0.85,1,0
169,SL is actually given by the model as,Neural Language Model,Quantitative Analysis,machine-translation,0,32,0.463768115942029,168,0.7671232876712328,18,0.9,1,0
170,"where x <t is a shorthand notation for xt ? 1 , . . . , x 1 .",Neural Language Model,Quantitative Analysis,machine-translation,0,33,0.4782608695652174,169,0.771689497716895,19,0.95,1,0
171,"were notable to achieve better performance on the test set , but only on the development set .",Neural Language Model,Quantitative Analysis,machine-translation,0,34,0.4927536231884058,170,0.776255707762557,20,1.0,1,0
172,Qualitative Analysis,Neural Language Model,,machine-translation,0,35,0.5072463768115942,171,0.7808219178082192,0,0.0,1,0
173,"In order to understand where the performance improvement comes from , we analyze the phrase pair scores computed by the RNN Encoder - Decoder against the corresponding p ( f | e ) from the translation model .",Neural Language Model,Qualitative Analysis,machine-translation,0,36,0.5217391304347826,172,0.7853881278538812,1,0.05,1,0
174,"Since the existing translation model relies solely on the statistics of the phrase pairs in the corpus , we expect its scores to be better estimated for the frequent phrases but badly estimated for rare phrases .",Neural Language Model,Qualitative Analysis,machine-translation,0,37,0.5362318840579711,173,0.7899543378995434,2,0.1,1,0
175,"Also , as we mentioned earlier in Sec. 3.1 , we further expect the RNN Encoder - Decoder which was trained without any frequency information to score the phrase pairs based rather on the linguistic regularities than on the statistics of their occurrences in the corpus .",Neural Language Model,Qualitative Analysis,machine-translation,0,38,0.5507246376811594,174,0.7945205479452054,3,0.15,1,0
176,We focus on those pairs whose source phrase is long ( more than 3 words per source phrase ) and,Neural Language Model,Qualitative Analysis,machine-translation,0,39,0.5652173913043478,175,0.7990867579908676,4,0.2,1,0
177,"As a result , the probability of words not in the shortlist is always overestimated .",Neural Language Model,Qualitative Analysis,machine-translation,0,40,0.5797101449275363,176,0.8036529680365296,5,0.25,1,0
178,"It is possible to address this issue by backing off to an existing model that contain non-shortlisted words ( see ) In this paper , however , we opt for introducing a word penalty instead , which counteracts the word probability overestimation .",Neural Language Model,Qualitative Analysis,machine-translation,0,41,0.5942028985507246,177,0.8082191780821918,6,0.3,1,0
179,frequent .,Neural Language Model,Qualitative Analysis,machine-translation,0,42,0.6086956521739131,178,0.8127853881278538,7,0.35,1,0
180,"For each such source phrase , we look at the target phrases that have been scored high either by the translation probability p ( f | e ) or by the RNN Encoder - Decoder .",Neural Language Model,Qualitative Analysis,machine-translation,0,43,0.6231884057971014,179,0.817351598173516,8,0.4,1,0
181,"Similarly , we perform the same procedure with those pairs whose source phrase is long but rare in the corpus .",Neural Language Model,Qualitative Analysis,machine-translation,0,44,0.6376811594202898,180,0.821917808219178,9,0.45,1,0
182,lists the top - 3 target phrases per source phrase favored either by the translation model or by the RNN Encoder - Decoder .,Neural Language Model,Qualitative Analysis,machine-translation,0,45,0.6521739130434783,181,0.8264840182648402,10,0.5,1,0
183,The source phrases were randomly chosen among long ones having more than 4 or 5 words .,Neural Language Model,Qualitative Analysis,machine-translation,0,46,0.6666666666666666,182,0.8310502283105022,11,0.55,1,0
184,"In most cases , the choices of the target phrases by the RNN Encoder - Decoder are closer to actual or literal translations .",Neural Language Model,Qualitative Analysis,machine-translation,0,47,0.6811594202898551,183,0.8356164383561644,12,0.6,1,0
185,We can observe that the RNN Encoder - Decoder prefers shorter phrases in general .,Neural Language Model,Qualitative Analysis,machine-translation,0,48,0.6956521739130435,184,0.8401826484018264,13,0.65,1,0
186,"Interestingly , many phrase pairs were scored similarly by both the translation model and the RNN Encoder - Decoder , but there were as many other phrase pairs that were scored radically different ( see ) .",Neural Language Model,Qualitative Analysis,machine-translation,0,49,0.7101449275362319,185,0.8447488584474886,14,0.7,1,0
187,"This could arise from the proposed approach of training the RNN Encoder - Decoder on a set of unique phrase pairs , discouraging the RNN Encoder - Decoder from learning simply the frequencies of the phrase pairs from the corpus , as explained earlier . , we show for each of the source phrases in , the generated samples from the RNN Encoder - Decoder .",Neural Language Model,Qualitative Analysis,machine-translation,0,50,0.7246376811594203,186,0.8493150684931506,15,0.75,1,0
188,"For each source phrase , we generated 50 samples and show the top - five phrases accordingly to their scores .",Neural Language Model,Qualitative Analysis,machine-translation,0,51,0.7391304347826086,187,0.8538812785388128,16,0.8,1,0
189,We can see that the RNN Encoder - Decoder is able to propose well - formed target phrases without looking at the actual phrase table .,Neural Language Model,Qualitative Analysis,machine-translation,0,52,0.7536231884057971,188,0.8584474885844748,17,0.85,1,0
190,"Importantly , the generated phrases do not overlap completely with the target phrases from the phrase table .",Neural Language Model,Qualitative Analysis,machine-translation,0,53,0.7681159420289855,189,0.863013698630137,18,0.9,1,0
191,This encourages us to further investigate the possibility of replacing the whole or apart of the phrase table with the proposed RNN Encoder - Decoder in the future .,Neural Language Model,Qualitative Analysis,machine-translation,0,54,0.782608695652174,190,0.867579908675799,19,0.95,1,0
192,"Furthermore , in",Neural Language Model,Qualitative Analysis,machine-translation,0,55,0.7971014492753623,191,0.8721461187214612,20,1.0,1,0
193,Word and Phrase Representations,Neural Language Model,,machine-translation,0,56,0.8115942028985508,192,0.8767123287671232,0,0.0,1,0
194,"Since the proposed RNN Encoder - Decoder is not specifically designed only for the task of machine translation , here we briefly look at the properties of the trained model .",Neural Language Model,Word and Phrase Representations,machine-translation,0,57,0.8260869565217391,193,0.8812785388127854,1,0.0769230769230769,1,0
195,"It has been known for sometime that continuous space language models using neural networks are able to learn semantically meaningful embeddings ( See , e.g. , ) .",Neural Language Model,Word and Phrase Representations,machine-translation,0,58,0.8405797101449275,194,0.8858447488584474,2,0.1538461538461538,1,0
196,"Since the proposed RNN Encoder - Decoder also projects to and maps back from a sequence of words into a continuous space vector , we expect to see a similar property with the proposed model as well .",Neural Language Model,Word and Phrase Representations,machine-translation,0,59,0.855072463768116,195,0.8904109589041096,3,0.2307692307692307,1,0
197,The left plot in shows the 2 - D embedding of the words using the word embedding matrix learned by the RNN Encoder - Decoder .,Neural Language Model,Word and Phrase Representations,machine-translation,0,60,0.8695652173913043,196,0.8949771689497716,4,0.3076923076923077,1,0
198,The projection was done by the recently proposed Barnes - Hut - SNE .,Neural Language Model,Word and Phrase Representations,machine-translation,0,61,0.8840579710144928,197,0.8995433789954338,5,0.3846153846153846,1,0
199,We can clearly see that semantically similar words are clustered with each other ( see the zoomed - in plots in .,Neural Language Model,Word and Phrase Representations,machine-translation,0,62,0.8985507246376812,198,0.9041095890410958,6,0.4615384615384615,1,0
200,The proposed RNN Encoder - Decoder naturally generates a continuous - space representation of a phrase .,Neural Language Model,Word and Phrase Representations,machine-translation,0,63,0.9130434782608696,199,0.908675799086758,7,0.5384615384615384,1,0
201,The representation ( c in ) in this case is a 1000 - dimensional vector .,Neural Language Model,Word and Phrase Representations,machine-translation,0,64,0.927536231884058,200,0.91324200913242,8,0.6153846153846154,1,0
202,"Similarly to the word representations , we visualize the representations of the phrases that consists of four or more words using the Barnes - Hut - SNE in .",Neural Language Model,Word and Phrase Representations,machine-translation,0,65,0.9420289855072465,201,0.9178082191780822,9,0.6923076923076923,1,0
203,"From the visualization , it is clear that the RNN Encoder - Decoder captures both semantic and syntactic structures of the phrases .",Neural Language Model,Word and Phrase Representations,machine-translation,0,66,0.9565217391304348,202,0.9223744292237442,10,0.7692307692307693,1,0
204,"For instance , in the bottom - left plot , most of the phrases are about the duration of time , while those phrases that are syntactically similar are clustered together .",Neural Language Model,Word and Phrase Representations,machine-translation,0,67,0.9710144927536232,203,0.9269406392694064,11,0.8461538461538461,1,0
205,The bottom - right plot shows the cluster of phrases that are semantically similar ( countries or regions ) .,Neural Language Model,Word and Phrase Representations,machine-translation,0,68,0.9855072463768116,204,0.9315068493150684,12,0.9230769230769232,1,0
206,"On the other hand , the top - right plot shows the phrases that are syntactically similar .",Neural Language Model,Word and Phrase Representations,machine-translation,0,69,1.0,205,0.9360730593607306,13,1.0,1,0
207,Conclusion,,,machine-translation,0,0,0.0,206,0.9406392694063926,0,0.0,1,0
208,"In this paper , we proposed anew neural network architecture , called an RNN Encoder - Decoder that is able to learn the mapping from a sequence of an arbitrary length to another sequence , possibly from a different set , of an arbitrary length .",Conclusion,Conclusion,machine-translation,0,1,0.0833333333333333,207,0.9452054794520548,1,0.0833333333333333,0,0
209,The proposed RNN Encoder - Decoder is able to either score a pair of sequences ( in terms of a conditional probability ) or generate a target sequence given a source sequence .,Conclusion,Conclusion,machine-translation,0,2,0.1666666666666666,208,0.9497716894977168,2,0.1666666666666666,0,0
210,"Along with the new architecture , we proposed a novel hidden unit that includes a reset gate and an update gate that adaptively control how much each hidden unit remembers or forgets while reading / generating a sequence .",Conclusion,Conclusion,machine-translation,0,3,0.25,209,0.954337899543379,3,0.25,0,0
211,"We evaluated the proposed model with the task of statistical machine translation , where we used the RNN Encoder - Decoder to score each phrase pair in the phrase table .",Conclusion,Conclusion,machine-translation,0,4,0.3333333333333333,210,0.958904109589041,4,0.3333333333333333,0,0
212,"Qualitatively , we were able to show that the new model is able to capture linguistic regularities in the phrase pairs well and also that the RNN Encoder - Decoder is able to propose well - formed target phrases .",Conclusion,Conclusion,machine-translation,0,5,0.4166666666666667,211,0.9634703196347032,5,0.4166666666666667,0,0
213,The scores by the RNN Encoder - Decoder were found to improve the overall translation performance in terms of BLEU scores .,Conclusion,Conclusion,machine-translation,0,6,0.5,212,0.9680365296803652,6,0.5,0,0
214,"Also , we found that the contribution by the RNN Encoder - Decoder is rather orthogonal to the existing approach of using neural networks in the SMT system , so that we can improve further the performance by using , for instance , the RNN Encoder - Decoder and the neural net language model together .",Conclusion,Conclusion,machine-translation,0,7,0.5833333333333334,213,0.9726027397260274,7,0.5833333333333334,0,0
215,Our qualitative analysis of the trained model shows that it indeed captures the linguistic regularities in multiple levels i.e. at the word level as well as phrase level .,Conclusion,Conclusion,machine-translation,0,8,0.6666666666666666,214,0.9771689497716894,8,0.6666666666666666,0,0
216,This suggests that there maybe more natural language related applications that may benefit from the proposed RNN Encoder - Decoder .,Conclusion,Conclusion,machine-translation,0,9,0.75,215,0.9817351598173516,9,0.75,0,0
217,The proposed architecture has large potential for further improvement and analysis .,Conclusion,Conclusion,machine-translation,0,10,0.8333333333333334,216,0.9863013698630136,10,0.8333333333333334,0,0
218,"One approach that was not investigated here is to replace the whole , or apart of the phrase table by letting the RNN Encoder - Decoder propose target phrases .",Conclusion,Conclusion,machine-translation,0,11,0.9166666666666666,217,0.9908675799086758,11,0.9166666666666666,0,0
219,"Also , noting that the proposed model is not limited to being used with written language , it will bean important future research to apply the proposed architecture to other applications such as speech transcription .",Conclusion,Conclusion,machine-translation,0,12,1.0,218,0.9954337899543378,12,1.0,0,0
1,title,,,machine-translation,1,0,0.0,0,0.0,0,0.0,1,0
2,Neural Machine Translation in Linear Time,title,,machine-translation,1,1,0.0,1,0.0049751243781094,1,0.0,1,1
3,abstract,,,machine-translation,1,0,0.0,2,0.0099502487562189,0,0.0,1,0
4,We present a novel neural network for processing sequences .,abstract,abstract,machine-translation,1,1,0.1111111111111111,3,0.0149253731343283,1,0.1111111111111111,1,0
5,"The ByteNet is a one-dimensional convolutional neural network that is composed of two parts , one to encode the source sequence and the other to decode the target sequence .",abstract,abstract,machine-translation,1,2,0.2222222222222222,4,0.0199004975124378,2,0.2222222222222222,1,0
6,The two network parts are connected by stacking the decoder on top of the encoder and preserving the temporal resolution of the sequences .,abstract,abstract,machine-translation,1,3,0.3333333333333333,5,0.0248756218905472,3,0.3333333333333333,1,0
7,"To address the differing lengths of the source and the target , we introduce an efficient mechanism by which the decoder is dynamically unfolded over the representation of the encoder .",abstract,abstract,machine-translation,1,4,0.4444444444444444,6,0.0298507462686567,4,0.4444444444444444,1,0
8,The ByteNet uses dilation in the convolutional layers to increase its receptive field .,abstract,abstract,machine-translation,1,5,0.5555555555555556,7,0.0348258706467661,5,0.5555555555555556,1,0
9,The resulting network has two core properties : it runs in time that is linear in the length of the sequences and it sidesteps the need for excessive memorization .,abstract,abstract,machine-translation,1,6,0.6666666666666666,8,0.0398009950248756,6,0.6666666666666666,1,0
10,The ByteNet decoder attains state - of - the - art performance on character - level language modelling and outperforms the previous best results obtained with recurrent networks .,abstract,abstract,machine-translation,1,7,0.7777777777777778,9,0.044776119402985,7,0.7777777777777778,1,1
11,"The ByteNet also achieves state - of - the - art performance on character - to - character machine translation on the English - to - German WMT translation task , surpassing comparable neural translation models that are based on recurrent networks with attentional pooling and run in quadratic time .",abstract,abstract,machine-translation,1,8,0.8888888888888888,10,0.0497512437810945,8,0.8888888888888888,1,1
12,We find that the latent alignment structure contained in the representations reflects the expected alignment between the tokens .,abstract,abstract,machine-translation,1,9,1.0,11,0.0547263681592039,9,1.0,1,0
13,Introduction,,,machine-translation,1,0,0.0,12,0.0597014925373134,0,0.0,1,0
14,"In neural language modelling , a neural network estimates a distribution over sequences of words or characters that belong to a given language .",Introduction,Introduction,machine-translation,1,1,0.027027027027027,13,0.0646766169154228,1,0.027027027027027,1,1
15,"In neural machine translation , the network estimates a distribution over sequences in the target language conditioned on a given sequence in the source language .",Introduction,Introduction,machine-translation,1,2,0.054054054054054,14,0.0696517412935323,2,0.054054054054054,1,1
16,The network can bethought of as composed of two parts : a source network ( the encoder ) that encodes the source sequence into a representation and a target network ( the decoder ) that uses the representation of the source encoder to generate the target sequence .,Introduction,Introduction,machine-translation,1,3,0.081081081081081,15,0.0746268656716417,3,0.081081081081081,1,0
17,"Recurrent neural networks ( RNN ) are powerful sequence models and are widely used in language modelling ) , yet they have a potential drawback .",Introduction,Introduction,machine-translation,1,4,0.1081081081081081,16,0.0796019900497512,4,0.1081081081081081,1,0
18,RNNs have an inherently serial structure that prevents them from being run in parallel along the sequence length during training and evaluation .,Introduction,Introduction,machine-translation,1,5,0.1351351351351351,17,0.0845771144278607,5,0.1351351351351351,1,0
19,Forward and backward signals in a RNN also need to traverse the full distance of the serial path to reach from one token in the sequence to another .,Introduction,Introduction,machine-translation,1,6,0.1621621621621621,18,0.0895522388059701,6,0.1621621621621621,1,0
20,"The larger the distance , the harder it is to learn the dependencies between the tokens .",Introduction,Introduction,machine-translation,1,7,0.1891891891891892,19,0.0945273631840796,7,0.1891891891891892,1,0
21,"A number of neural architectures have been proposed for modelling translation , such as encoder - decoder networks , networks with attentional pooling and twodimensional networks .",Introduction,Introduction,machine-translation,1,8,0.2162162162162162,20,0.099502487562189,8,0.2162162162162162,1,0
22,"Despite the generally good performance , the proposed models ar Xiv : 1610.10099v2 [ cs. CL ] 15 Mar 2017 EOS EOS EOS | s | | t | | t| .",Introduction,Introduction,machine-translation,1,9,0.2432432432432432,21,0.1044776119402985,9,0.2432432432432432,1,0
23,Dynamic unfolding in the ByteNet architecture .,Introduction,Introduction,machine-translation,1,10,0.2702702702702703,22,0.1094527363184079,10,0.2702702702702703,1,0
24,"At each step the decoder is conditioned on the source representation produced by the encoder for that step , or simply on no representation for steps beyond the extended length | t | .",Introduction,Introduction,machine-translation,1,11,0.2972972972972973,23,0.1144278606965174,11,0.2972972972972973,1,0
25,The decoding ends when the target network produces an end - of - sequence ( EOS ) symbol .,Introduction,Introduction,machine-translation,1,12,0.3243243243243243,24,0.1194029850746268,12,0.3243243243243243,1,0
26,"either have running time that is super - linear in the length of the source and target sequences , or they process the source sequence into a constant size representation , burdening the model with a memorization step .",Introduction,Introduction,machine-translation,1,13,0.3513513513513513,25,0.1243781094527363,13,0.3513513513513513,1,0
27,Both of these drawbacks grow more severe as the length of the sequences increases .,Introduction,Introduction,machine-translation,1,14,0.3783783783783784,26,0.1293532338308457,14,0.3783783783783784,1,0
28,We present a family of encoder - decoder neural networks that are characterized by two architectural mechanisms aimed to address the drawbacks of the conventional approaches mentioned above .,Introduction,Introduction,machine-translation,1,15,0.4054054054054054,27,0.1343283582089552,15,0.4054054054054054,1,0
29,The first mechanism involves the stacking of the decoder on top of the representation of the encoder in a manner that preserves the temporal resolution of the sequences ; this is in contrast with architectures that encode the source into a fixed - size representation .,Introduction,Introduction,machine-translation,1,16,0.4324324324324324,28,0.1393034825870646,16,0.4324324324324324,1,0
30,The second mechanism is the dynamic unfolding mechanism that allows the network to process in a simple and efficient way source and target sequences of different lengths ( Sect. 3.2 ) .,Introduction,Introduction,machine-translation,1,17,0.4594594594594595,29,0.1442786069651741,17,0.4594594594594595,1,0
31,The ByteNet is the instance within this family of models that uses one - dimensional convolutional neural networks ( CNN ) of fixed depth for both the encoder and the decoder ) .,Introduction,Introduction,machine-translation,1,18,0.4864864864864865,30,0.1492537313432835,18,0.4864864864864865,1,1
32,The two CNNs use increasing factors of dilation to rapidly grow the receptive fields ; a similar technique is also used in .,Introduction,Introduction,machine-translation,1,19,0.5135135135135135,31,0.154228855721393,19,0.5135135135135135,1,1
33,The convolutions in the decoder CNN are masked to prevent the network from seeing future tokens in the target sequence .,Introduction,Introduction,machine-translation,1,20,0.5405405405405406,32,0.1592039800995024,20,0.5405405405405406,1,1
34,The network has beneficial computational and learning properties .,Introduction,Introduction,machine-translation,1,21,0.5675675675675675,33,0.1641791044776119,21,0.5675675675675675,1,1
35,"From a computational perspective , the network has a running time that is linear in the length of the source and target sequences ( up to a constant c ?",Introduction,Introduction,machine-translation,1,22,0.5945945945945946,34,0.1691542288557214,22,0.5945945945945946,1,1
36,log d where dis the size of the desired dependency field ) .,Introduction,Introduction,machine-translation,1,23,0.6216216216216216,35,0.1741293532338308,23,0.6216216216216216,1,0
37,The computation in the encoder during training and decoding and in the decoder during training can also be run efficiently in parallel along the sequences ( Sect. 2 ) .,Introduction,Introduction,machine-translation,1,24,0.6486486486486487,36,0.1791044776119403,24,0.6486486486486487,1,0
38,"From a learning perspective , the representation of the source sequence in the ByteNet is resolution preserving ; the representation sidesteps the need for memorization and allows for maximal bandwidth between encoder and decoder .",Introduction,Introduction,machine-translation,1,25,0.6756756756756757,37,0.1840796019900497,25,0.6756756756756757,1,1
39,"In addition , the distance traversed by forward and backward signals between any input and output tokens corresponds to the fixed depth of the networks and is largely independent of the dis - tance between the tokens .",Introduction,Introduction,machine-translation,1,26,0.7027027027027027,38,0.1890547263681592,26,0.7027027027027027,1,0
40,Dependencies overlarge distances are connected by short paths and can be learnt more easily .,Introduction,Introduction,machine-translation,1,27,0.7297297297297297,39,0.1940298507462686,27,0.7297297297297297,1,0
41,We apply the ByteNet model to strings of characters for character - level language modelling and character - tocharacter machine translation .,Introduction,Introduction,machine-translation,1,28,0.7567567567567568,40,0.1990049751243781,28,0.7567567567567568,1,0
42,We evaluate the decoder network on the Hutter Prize Wikipedia task where it achieves the state - of - the - art performance of 1.31 bits / character .,Introduction,Introduction,machine-translation,1,29,0.7837837837837838,41,0.2039800995024875,29,0.7837837837837838,1,0
43,"We further evaluate the encoderdecoder network on character - to - character machine translation on the English - to - German WMT benchmark where it achieves a state - of - the - art BLEU score of 22.85 ( 0.380 bits / character ) and 25.53 ( 0.389 bits / character ) on the 2014 and 2015 test sets , respectively .",Introduction,Introduction,machine-translation,1,30,0.8108108108108109,42,0.208955223880597,30,0.8108108108108109,1,0
44,"On the character - level machine translation task , ByteNet betters a comparable version of GNMT that is a state - of - the - art system .",Introduction,Introduction,machine-translation,1,31,0.8378378378378378,43,0.2139303482587064,31,0.8378378378378378,1,0
45,"These results show that deep CNNs are simple , scalable and effective architectures for challenging linguistic processing tasks .",Introduction,Introduction,machine-translation,1,32,0.8648648648648649,44,0.2189054726368159,32,0.8648648648648649,1,0
46,The paper is organized as follows .,Introduction,Introduction,machine-translation,1,33,0.8918918918918919,45,0.2238805970149253,33,0.8918918918918919,1,0
47,Section 2 lays out the background and some desiderata for neural architectures underlying translation models .,Introduction,Introduction,machine-translation,1,34,0.918918918918919,46,0.2288557213930348,34,0.918918918918919,1,0
48,Section 3 defines the proposed family of architectures and the specific convolutional instance ( ByteNet ) used in the experiments .,Introduction,Introduction,machine-translation,1,35,0.945945945945946,47,0.2338308457711442,35,0.945945945945946,1,0
49,Section 4 analyses ByteNet as well as existing neural translation models based on the desiderata set out in Section 2 .,Introduction,Introduction,machine-translation,1,36,0.972972972972973,48,0.2388059701492537,36,0.972972972972973,1,0
50,Section 5 reports the experiments on language modelling and Section 6 reports the experiments on character - to - character machine translation .,Introduction,Introduction,machine-translation,1,37,1.0,49,0.2437810945273631,37,1.0,1,0
51,Neural Translation Model,,,machine-translation,1,0,0.0,50,0.2487562189054726,0,0.0,1,0
52,"Given a string s from a source language , a neural translation model estimates a distribution p ( t |s ) over strings t of a target language .",Neural Translation Model,Neural Translation Model,machine-translation,1,1,0.0166666666666666,51,0.2537313432835821,1,0.1,1,0
53,The distribution indicates the probability of a string t being a translation of s .,Neural Translation Model,Neural Translation Model,machine-translation,1,2,0.0333333333333333,52,0.2587064676616915,2,0.2,1,0
54,"A product of conditionals over the tokens in the target t = t 0 , ... , t N leads to a tractable formulation of the distribution :",Neural Translation Model,Neural Translation Model,machine-translation,1,3,0.05,53,0.263681592039801,3,0.3,1,0
55,Each conditional factor expresses complex and long - range dependencies among the source and target tokens .,Neural Translation Model,Neural Translation Model,machine-translation,1,4,0.0666666666666666,54,0.2686567164179104,4,0.4,1,0
56,"The strings are usually sentences of the respective languages ; the tokens are words or , as in the our case , characters .",Neural Translation Model,Neural Translation Model,machine-translation,1,5,0.0833333333333333,55,0.2736318407960199,5,0.5,1,0
57,The network that models p ( t | s ) is composed of two parts : a source network ( the encoder ) that processes the source string into a representation and a target network ( the decoder ) that uses the source representation to generate the target string .,Neural Translation Model,Neural Translation Model,machine-translation,1,6,0.1,56,0.2786069651741293,6,0.6,1,0
58,The decoder functions as a language model for the target language .,Neural Translation Model,Neural Translation Model,machine-translation,1,7,0.1166666666666666,57,0.2835820895522388,7,0.7,1,0
59,A neural translation model has some basic properties .,Neural Translation Model,Neural Translation Model,machine-translation,1,8,0.1333333333333333,58,0.2885572139303483,8,0.8,1,0
60,The decoder is autoregressive in the target tokens and the model is sensitive to the ordering of the tokens in the source and target strings .,Neural Translation Model,Neural Translation Model,machine-translation,1,9,0.15,59,0.2935323383084577,9,0.9,1,0
61,It is also useful for the model to be able to assign a non-zero probability to any string in the target language and retain an open vocabulary .,Neural Translation Model,Neural Translation Model,machine-translation,1,10,0.1666666666666666,60,0.2985074626865671,10,1.0,1,0
62,Desiderata,Neural Translation Model,,machine-translation,1,11,0.1833333333333333,61,0.3034825870646766,0,0.0,1,0
63,"Beyond these basic properties the definition of a neural translation model does not determine a unique neural architecture , so we aim at identifying some desiderata .",Neural Translation Model,Desiderata,machine-translation,1,12,0.2,62,0.308457711442786,1,0.0526315789473684,1,0
64,"First , the running time of the network should be linear in the length of the source and target strings .",Neural Translation Model,Desiderata,machine-translation,1,13,0.2166666666666666,63,0.3134328358208955,2,0.1052631578947368,1,0
65,"This ensures that the model is scalable to longer strings , which is the case when using characters as tokens .",Neural Translation Model,Desiderata,machine-translation,1,14,0.2333333333333333,64,0.3184079601990049,3,0.1578947368421052,1,0
66,The use of operations that run in parallel along the sequence length can also be beneficial for reducing computation time .,Neural Translation Model,Desiderata,machine-translation,1,15,0.25,65,0.3233830845771144,4,0.2105263157894736,1,0
67,"Second , the size of the source representation should be linear in the length of the source string , i.e. it should be resolution preserving , and not have constant size .",Neural Translation Model,Desiderata,machine-translation,1,16,0.2666666666666666,66,0.3283582089552239,5,0.2631578947368421,1,0
68,This is to avoid burdening the model with an additional memorization step before translation .,Neural Translation Model,Desiderata,machine-translation,1,17,0.2833333333333333,67,0.3333333333333333,6,0.3157894736842105,1,0
69,"In more general terms , the size of a representation should be proportional to the amount of information it represents or predicts .",Neural Translation Model,Desiderata,machine-translation,1,18,0.3,68,0.3383084577114428,7,0.3684210526315789,1,0
70,"Third , the path traversed by forward and backward signals in the network ( between input and ouput tokens ) should be short .",Neural Translation Model,Desiderata,machine-translation,1,19,0.3166666666666666,69,0.3432835820895522,8,0.4210526315789473,1,0
71,Shorter paths whose length is largely decoupled from the sequence distance between the two tokens have the potential to better propagate the signals and to let the network learn long - range dependencies more easily .,Neural Translation Model,Desiderata,machine-translation,1,20,0.3333333333333333,70,0.3482587064676617,9,0.4736842105263157,1,0
72,Byte Net,Neural Translation Model,,machine-translation,1,21,0.35,71,0.3532338308457711,10,0.5263157894736842,1,0
73,We aim at building neural language and translation models that capture the desiderata set out in Sect. 2.1 .,Neural Translation Model,Byte Net,machine-translation,1,22,0.3666666666666666,72,0.3582089552238806,11,0.5789473684210527,1,0
74,The proposed ByteNet architecture is composed of a decoder that is stacked on an encoder ( Sect. 3.1 ) and generates variable - length outputs via dynamic unfolding ( Sect. 3.2 ) .,Neural Translation Model,Byte Net,machine-translation,1,23,0.3833333333333333,73,0.36318407960199,12,0.631578947368421,1,0
75,The decoder is a language model that is formed of one - dimensional convolutional layers that are masked ( Sect. 3.4 ) and use dilation ( Sect. 3.5 ) .,Neural Translation Model,Byte Net,machine-translation,1,24,0.4,74,0.3681592039800995,13,0.6842105263157895,1,0
76,The encoder processes the source string into a representation and is formed of one - dimensional convolutional layers that use dilation but are not masked .,Neural Translation Model,Byte Net,machine-translation,1,25,0.4166666666666667,75,0.373134328358209,14,0.7368421052631579,1,0
77,depicts the two networks and their combination .,Neural Translation Model,Byte Net,machine-translation,1,26,0.4333333333333333,76,0.3781094527363184,15,0.7894736842105263,1,0
78,Encoder - Decoder Stacking,Neural Translation Model,,machine-translation,1,27,0.45,77,0.3830845771144278,16,0.8421052631578947,1,0
79,A notable feature of the proposed family of architectures is the way the encoder and the decoder are connected .,Neural Translation Model,Encoder - Decoder Stacking,machine-translation,1,28,0.4666666666666667,78,0.3880597014925373,17,0.8947368421052632,1,0
80,"To maximize the representational bandwidth between the encoder and the decoder , we place the decoder on top of the representation computed by the encoder .",Neural Translation Model,Encoder - Decoder Stacking,machine-translation,1,29,0.4833333333333333,79,0.3930348258706467,18,0.9473684210526316,1,0
81,This is in contrast to models that compress the source representation into a fixed - size vector or that pool over the source representation with a mechanism such as attentional pooling .,Neural Translation Model,Encoder - Decoder Stacking,machine-translation,1,30,0.5,80,0.3980099502487562,19,1.0,1,0
82,Dynamic Unfolding,Neural Translation Model,,machine-translation,1,31,0.5166666666666667,81,0.4029850746268656,0,0.0,1,0
83,An encoder and a decoder network that process sequences of different lengths can not be directly connected due to the different sizes of the computed representations .,Neural Translation Model,Dynamic Unfolding,machine-translation,1,32,0.5333333333333333,82,0.4079601990049751,1,0.1,1,0
84,"We circumvent this issue via a mechanism which we call dynamic unfolding , which works as follows .",Neural Translation Model,Dynamic Unfolding,machine-translation,1,33,0.55,83,0.4129353233830846,2,0.2,1,0
85,"Given source and target sequences sand t with respective lengths | s | and | t| , one first chooses a sufficiently tight upper bound | t| on the target length | t | as a linear function of the source length | s | : |",Neural Translation Model,Dynamic Unfolding,machine-translation,1,34,0.5666666666666667,84,0.417910447761194,3,0.3,1,0
86,"The tight upper bound | t| is chosen in such away that , on the one hand , it is greater than the actual length | t | in almost all cases and , on the other hand , it does not increase excessively the amount of computation that is required .",Neural Translation Model,Dynamic Unfolding,machine-translation,1,35,0.5833333333333334,85,0.4228855721393035,4,0.4,1,0
87,"Once a linear relationship is chosen , one designs the source encoder so that , given a source sequence of length | s | , the encoder outputs a representation of the established lengt ?",Neural Translation Model,Dynamic Unfolding,machine-translation,1,36,0.6,86,0.4278606965174129,5,0.5,1,0
88,| t| .,Neural Translation Model,Dynamic Unfolding,machine-translation,1,37,0.6166666666666667,87,0.4328358208955223,6,0.6,1,0
89,"In our case , we let a = 1.20 and b = 0 when translating from English into German , as German sentences tend to be somewhat longer than their English counterparts .",Neural Translation Model,Dynamic Unfolding,machine-translation,1,38,0.6333333333333333,88,0.4378109452736318,7,0.7,1,0
90,"In this manner the representation produced by the encoder can be efficiently computed , while maintaining high bandwidth and being resolution - preserving .",Neural Translation Model,Dynamic Unfolding,machine-translation,1,39,0.65,89,0.4427860696517413,8,0.8,1,0
91,"Once the encoder representation is computed , we let the decoder unfold stepby - step over the encoder representation until the decoder itself outputs an end - of - sequence symbol ; the unfolding process may freely proceed beyond the estimated length | t| of the encoder representation .",Neural Translation Model,Dynamic Unfolding,machine-translation,1,40,0.6666666666666666,90,0.4477611940298507,9,0.9,1,0
92,gives an example of dynamic unfolding .,Neural Translation Model,Dynamic Unfolding,machine-translation,1,41,0.6833333333333333,91,0.4527363184079602,10,1.0,1,0
93,Input Embedding Tensor,Neural Translation Model,,machine-translation,1,42,0.7,92,0.4577114427860697,0,0.0,1,0
94,"Given the target sequence t = t 0 , ... , tn the ByteNet decoder embeds each of the first n tokens t 0 , ... , t n?1 via a look - up table ( the n tokens t 1 , ... , tn serve as targets for the predictions ) .",Neural Translation Model,Input Embedding Tensor,machine-translation,1,43,0.7166666666666667,93,0.4626865671641791,1,0.125,1,0
95,The resulting embeddings are concatenated into a tensor of size n 2 d where dis the number of inner channels in the network .,Neural Translation Model,Input Embedding Tensor,machine-translation,1,44,0.7333333333333333,94,0.4676616915422885,2,0.25,1,0
96,Masked One-dimensional,Neural Translation Model,,machine-translation,1,45,0.75,95,0.472636815920398,3,0.375,1,0
97,Convolutions,Neural Translation Model,,machine-translation,1,46,0.7666666666666667,96,0.4776119402985074,4,0.5,1,0
98,The decoder applies masked one - dimensional convolutions ( van den to the input embedding tensor that have a masked kernel of size k.,Neural Translation Model,Convolutions,machine-translation,1,47,0.7833333333333333,97,0.4825870646766169,5,0.625,1,0
99,The masking ensures that information from future tokens does not affect the prediction of the current token .,Neural Translation Model,Convolutions,machine-translation,1,48,0.8,98,0.4875621890547263,6,0.75,1,0
100,The operation can be implemented either by zeroing out some of the weights of a wider kernel of size 2 k ?,Neural Translation Model,Convolutions,machine-translation,1,49,0.8166666666666667,99,0.4925373134328358,7,0.875,1,0
101,1 or by padding the input map .,Neural Translation Model,Convolutions,machine-translation,1,50,0.8333333333333334,100,0.4975124378109453,8,1.0,1,0
102,Dilation,Neural Translation Model,,machine-translation,1,51,0.85,101,0.5024875621890548,0,0.0,1,0
103,The masked convolutions use dilation to increase the receptive field of the target network .,Neural Translation Model,Dilation,machine-translation,1,52,0.8666666666666667,102,0.5074626865671642,1,0.25,1,0
104,"Dilation makes the receptive field grow exponentially in terms of the depth of the networks , as opposed to linearly .",Neural Translation Model,Dilation,machine-translation,1,53,0.8833333333333333,103,0.5124378109452736,2,0.5,1,0
105,We use a dilation scheme whereby the dilation rates are doubled every layer up to a maximum rater ( for our experiments r = 16 ) .,Neural Translation Model,Dilation,machine-translation,1,54,0.9,104,0.5174129353233831,3,0.75,1,0
106,The scheme is repeated multiple times in the network always starting from a dilation rate of 1 ( van den .,Neural Translation Model,Dilation,machine-translation,1,55,0.9166666666666666,105,0.5223880597014925,4,1.0,1,0
107,Residual Blocks,Neural Translation Model,,machine-translation,1,56,0.9333333333333332,106,0.527363184079602,0,0.0,1,0
108,"Each layer is wrapped in a residual block that contains additional convolutional layers with filters of size 1 1 . We adopt two variants of the residual blocks : one with ReLUs , which is used in the machine translation experiments , and one with Multiplicative Units , which is used in the language modelling experiments .",Neural Translation Model,Residual Blocks,machine-translation,1,57,0.95,107,0.5323383084577115,1,0.25,1,0
109,diagrams the two variants of the blocks .,Neural Translation Model,Residual Blocks,machine-translation,1,58,0.9666666666666668,108,0.5373134328358209,2,0.5,1,0
110,"In both cases , we use layer normalization before the activation function , as it is well suited to sequence processing where computing the activation statistics over the following future tokens ( as would be done by batch normalization ) must be avoided .",Neural Translation Model,Residual Blocks,machine-translation,1,59,0.9833333333333332,109,0.5422885572139303,3,0.75,1,0
111,"After a series of residual blocks of increased dilation , the network applies one more convolution and ReLU followed by a convolution and a final softmax layer .",Neural Translation Model,Residual Blocks,machine-translation,1,60,1.0,110,0.5472636815920398,4,1.0,1,0
112,Model Comparison,,,machine-translation,1,0,0.0,111,0.5522388059701493,0,0.0,1,0
113,In this section we analyze the properties of various previously introduced neural translation models as well as the ByteNet family of models .,Model Comparison,Model Comparison,machine-translation,1,1,0.03125,112,0.5572139303482587,1,0.5,1,0
114,"For the sake of a more complete analysis , we include two recurrent ByteNet variants ( which we do not evaluate in the experiments ) .",Model Comparison,Model Comparison,machine-translation,1,2,0.0625,113,0.5621890547263682,2,1.0,1,0
115,Recurrent ByteNets,Model Comparison,,machine-translation,1,3,0.09375,114,0.5671641791044776,0,0.0,1,0
116,The ByteNet is composed of two stacked encoder and decoder networks where the decoder network dynamically adapts to the output length .,Model Comparison,Recurrent ByteNets,machine-translation,1,4,0.125,115,0.572139303482587,1,0.125,1,0
117,This way of combining the networks is not tied to the networks being strictly convolutional .,Model Comparison,Recurrent ByteNets,machine-translation,1,5,0.15625,116,0.5771144278606966,2,0.25,1,0
118,We may consider two variants of the ByteNet that use recurrent networks for one or both of the networks ( see ) .,Model Comparison,Recurrent ByteNets,machine-translation,1,6,0.1875,117,0.582089552238806,3,0.375,1,0
119,The first variant replaces the convolutional decoder with a recurrent one that is similarly stacked and dynamically unfolded .,Model Comparison,Recurrent ByteNets,machine-translation,1,7,0.21875,118,0.5870646766169154,4,0.5,1,0
120,"The second variant also replaces the convolutional encoder with a recurrent encoder , e.g. a bidirectional RNN .",Model Comparison,Recurrent ByteNets,machine-translation,1,8,0.25,119,0.5920398009950248,5,0.625,1,0
121,The target RNN is then placed on top of the source RNN .,Model Comparison,Recurrent ByteNets,machine-translation,1,9,0.28125,120,0.5970149253731343,6,0.75,1,0
122,"Considering the latter Recurrent ByteNet , we can see that the RNN Enc - Dec network ) is a Recurrent ByteNet where all connections between source and target - except for the first one that connects s 0 and t 0 - have been severed .",Model Comparison,Recurrent ByteNets,machine-translation,1,10,0.3125,121,0.6019900497512438,7,0.875,1,0
123,"The Recurrent ByteNet is a generalization of the RNN Enc - Dec and , modulo the type of weight - sharing scheme , so is the convolutional ByteNet .",Model Comparison,Recurrent ByteNets,machine-translation,1,11,0.34375,122,0.6069651741293532,8,1.0,1,0
124,Comparison of Properties,Model Comparison,,machine-translation,1,12,0.375,123,0.6119402985074627,0,0.0,1,0
125,In our comparison we consider the following neural translation models : the Recurrent Continuous Translation Model ( RCTM ) 1 and 2 ; the RNN Enc - Dec ; the RNN Enc - Dec Att with the attentional pooling mechanism of which there area few variations ; the Grid LSTM translation model ) that uses a multi-dimensional architecture ; the Extended Neural GPU model ) that has a convolutional RNN architecture ; the ByteNet and the two Recurrent ByteNet variants .,Model Comparison,Comparison of Properties,machine-translation,1,13,0.40625,124,0.6169154228855721,1,0.0526315789473684,1,0
126,Our comparison criteria reflect the desiderata set out in Sect. 2.1 .,Model Comparison,Comparison of Properties,machine-translation,1,14,0.4375,125,0.6218905472636815,2,0.1052631578947368,1,0
127,We separate the first ( computation time ) desider - atum into three columns .,Model Comparison,Comparison of Properties,machine-translation,1,15,0.46875,126,0.6268656716417911,3,0.1578947368421052,1,0
128,The first column indicates the time complexity of the network as a function of the length of the sequences and is denoted by Time .,Model Comparison,Comparison of Properties,machine-translation,1,16,0.5,127,0.6318407960199005,4,0.2105263157894736,1,0
129,"The other two columns Net Sand Net T indicate , respectively , whether the source and the target network use a convolutional structure ( CNN ) or a recurrent one ( RNN ) ; a CNN structure has the advantage that it can be run in parallel along the length of the sequence .",Model Comparison,Comparison of Properties,machine-translation,1,17,0.53125,128,0.6368159203980099,5,0.2631578947368421,1,0
130,"The second ( resolution preservation ) desideratum corresponds to the RP column , which indicates whether the source representation in the network is resolution preserving .",Model Comparison,Comparison of Properties,machine-translation,1,18,0.5625,129,0.6417910447761194,6,0.3157894736842105,1,0
131,"Finally , the third desideratum ( short forward and backward flow paths ) is reflected by two columns .",Model Comparison,Comparison of Properties,machine-translation,1,19,0.59375,130,0.6467661691542289,7,0.3684210526315789,1,0
132,The Path S column corresponds to the length in layer steps of the shortest path between a source token and any output target token .,Model Comparison,Comparison of Properties,machine-translation,1,20,0.625,131,0.6517412935323383,8,0.4210526315789473,1,0
133,"Similarly , the Path T column corresponds to the length of the shortest path between an input target token and any output target token .",Model Comparison,Comparison of Properties,machine-translation,1,21,0.65625,132,0.6567164179104478,9,0.4736842105263157,1,0
134,Shorter paths lead to better forward and backward signal propagation .,Model Comparison,Comparison of Properties,machine-translation,1,22,0.6875,133,0.6616915422885572,10,0.5263157894736842,1,0
135,summarizes the properties of the models .,Model Comparison,Comparison of Properties,machine-translation,1,23,0.71875,134,0.6666666666666666,11,0.5789473684210527,1,0
136,"The ByteNet , the Recurrent ByteNets and the RNN Enc - Dec are the only networks that have linear running time ( up to the constant c ) .",Model Comparison,Comparison of Properties,machine-translation,1,24,0.75,135,0.6716417910447762,12,0.631578947368421,1,0
137,"The RNN Enc - Dec , however , does not preserve the source sequence resolution , a feature that aggravates learning for long sequences such as those that appear in character - to - character machine translation .",Model Comparison,Comparison of Properties,machine-translation,1,25,0.78125,136,0.6766169154228856,13,0.6842105263157895,1,0
138,"The RCTM 2 , the RNN Enc - Dec Att , the Grid LSTM and the Extended Neural GPU do preserve the resolution , but at a cost of a quadratic running time .",Model Comparison,Comparison of Properties,machine-translation,1,26,0.8125,137,0.681592039800995,14,0.7368421052631579,1,0
139,The ByteNet stands out also for its Path properties .,Model Comparison,Comparison of Properties,machine-translation,1,27,0.84375,138,0.6865671641791045,15,0.7894736842105263,1,0
140,The dilated structure of the convolutions connects any two source or target tokens in the sequences byway of a small number of network layers corresponding to the depth of the source or target networks .,Model Comparison,Comparison of Properties,machine-translation,1,28,0.875,139,0.6915422885572139,16,0.8421052631578947,1,0
141,"For character sequences where learning long - range dependencies is important , paths that are sublinear in the distance are advantageous .",Model Comparison,Comparison of Properties,machine-translation,1,29,0.90625,140,0.6965174129353234,17,0.8947368421052632,1,0
142,Phrase Based MT phrases phrases 20.7 24.0 RNN Enc - Dec words words 11.3 Reverse RNN Enc - Dec words words 14.0 RNN Enc - Dec,Model Comparison,Comparison of Properties,machine-translation,1,30,0.9375,141,0.7014925373134329,18,0.9473684210526316,1,0
143,Att words words 20.6 RNN Enc - Dec Att words words 20.9 GNMT ( RNN Enc - Dec Att ) word - pieces word - pieces 24.61,Model Comparison,Comparison of Properties,machine-translation,1,31,0.96875,142,0.7064676616915423,19,1.0,1,0
144,RNN,Model Comparison,,machine-translation,1,32,1.0,143,0.7114427860696517,0,0.0,1,0
145,Model Test,,,machine-translation,1,0,0.0,144,0.7164179104477612,0,0.0,1,0
146,Stacked LSTM,Model Test,,machine-translation,1,1,0.0208333333333333,145,0.7213930348258707,1,0.1666666666666666,1,0
147,1.67 GF - LSTM 1.58 Grid- LSTM 1.47 Layer - normalized LSTM 1.46 MI- LSTM 1.44 Recurrent Memory Array Structures,Model Test,Stacked LSTM,machine-translation,1,2,0.0416666666666666,146,0.7263681592039801,2,0.3333333333333333,1,0
148,1.40 HM- LSTM 1.40 Layer,Model Test,Stacked LSTM,machine-translation,1,3,0.0625,147,0.7313432835820896,3,0.5,1,0
149,Norm HyperLSTM 1.38 Large Layer,Model Test,Stacked LSTM,machine-translation,1,4,0.0833333333333333,148,0.736318407960199,4,0.6666666666666666,1,0
150,Norm HyperLSTM 1.34 Recurrent Highway Networks 1.32 Byte,Model Test,Stacked LSTM,machine-translation,1,5,0.1041666666666666,149,0.7412935323383084,5,0.8333333333333334,1,0
151,Net Decoder 1.31 . Negative log- likelihood results in bits / byte on the Hutter Prize Wikipedia benchmark .,Model Test,Stacked LSTM,machine-translation,1,6,0.125,150,0.746268656716418,6,1.0,1,0
152,Character Prediction,Model Test,,machine-translation,1,7,0.1458333333333333,151,0.7512437810945274,0,0.0,1,0
153,We first evaluate the ByteNet Decoder separately on a character - level language modelling benchmark .,Model Test,Character Prediction,machine-translation,1,8,0.1666666666666666,152,0.7562189054726368,1,0.024390243902439,1,0
154,"We use the Hutter Prize version of the Wikipedia dataset and follow the standard split where the first 90 million bytes are used for training , the next 5 million bytes are used for validation and the last 5 million bytes are used for testing .",Model Test,Character Prediction,machine-translation,1,9,0.1875,153,0.7611940298507462,2,0.048780487804878,1,0
155,The total number of characters in the vocabulary is 205 .,Model Test,Character Prediction,machine-translation,1,10,0.2083333333333333,154,0.7661691542288557,3,0.073170731707317,1,0
156,"The ByteNet Decoder that we use for the result has 30 residual blocks split into six sets of five blocks each ; for the five blocks in each set the dilation rates are , respectively , 1 , 2 , 4 , 8 and 16 .",Model Test,Character Prediction,machine-translation,1,11,0.2291666666666666,155,0.7711442786069652,4,0.0975609756097561,1,1
157,The masked kernel has size 3 .,Model Test,Character Prediction,machine-translation,1,12,0.25,156,0.7761194029850746,5,0.1219512195121951,1,1
158,This gives a receptive field of 315 characters .,Model Test,Character Prediction,machine-translation,1,13,0.2708333333333333,157,0.7810945273631841,6,0.1463414634146341,1,0
159,The number of hidden units dis 512 .,Model Test,Character Prediction,machine-translation,1,14,0.2916666666666667,158,0.7860696517412935,7,0.1707317073170731,1,1
160,For this task we use residual multiplicative blocks .,Model Test,Character Prediction,machine-translation,1,15,0.3125,159,0.7910447761194029,8,0.1951219512195122,1,0
161,For the optimization we use Adam with a learning rate of 0.0003 and a weight decay term of 0.0001 .,Model Test,Character Prediction,machine-translation,1,16,0.3333333333333333,160,0.7960199004975125,9,0.2195121951219512,1,1
162,We apply dropout to the last ReLU layer before the softmax dropping units with a probability of 0.1 .,Model Test,Character Prediction,machine-translation,1,17,0.3541666666666667,161,0.8009950248756219,10,0.2439024390243902,1,1
163,We do not reduce the learning rate during training .,Model Test,Character Prediction,machine-translation,1,18,0.375,162,0.8059701492537313,11,0.2682926829268293,1,0
164,"At each step we sample a batch of sequences of 500 characters each , use the first 100 characters as the minimum context and predict the latter 400 characters .",Model Test,Character Prediction,machine-translation,1,19,0.3958333333333333,163,0.8109452736318408,12,0.2926829268292683,1,1
165,lists recent results of various neural sequence models on the Wikipedia dataset .,Model Test,Character Prediction,machine-translation,1,20,0.4166666666666667,164,0.8159203980099502,13,0.3170731707317073,1,0
166,All the results except for the ByteNet result are obtained using some variant of the LSTM recurrent neural network .,Model Test,Character Prediction,machine-translation,1,21,0.4375,165,0.8208955223880597,14,0.3414634146341463,1,0
167,The ByteNet decoder achieves 1.31 bits / character on the test set .,Model Test,Character Prediction,machine-translation,1,22,0.4583333333333333,166,0.8258706467661692,15,0.3658536585365853,1,1
168,Character - Level Machine Translation,Model Test,,machine-translation,1,23,0.4791666666666667,167,0.8308457711442786,16,0.3902439024390244,1,0
169,We evaluate the full ByteNet on the WMT English to German translation task .,Model Test,Character - Level Machine Translation,machine-translation,1,24,0.5,168,0.835820895522388,17,0.4146341463414634,1,0
170,We use NewsTest 2013 for validation and NewsTest 2014 and 2015 for testing .,Model Test,Character - Level Machine Translation,machine-translation,1,25,0.5208333333333334,169,0.8407960199004975,18,0.4390243902439024,1,0
171,The English and German strings are encoded as sequences of characters ; no explicit segmentation into words or morphemes is applied to the strings .,Model Test,Character - Level Machine Translation,machine-translation,1,26,0.5416666666666666,170,0.845771144278607,19,0.4634146341463415,1,0
172,The outputs of the network are strings of characters in the target language .,Model Test,Character - Level Machine Translation,machine-translation,1,27,0.5625,171,0.8507462686567164,20,0.4878048780487805,1,0
173,We keep 323 characters in the German vocabulary and 296 in the English vocabulary .,Model Test,Character - Level Machine Translation,machine-translation,1,28,0.5833333333333334,172,0.8557213930348259,21,0.5121951219512195,1,0
174,The ByteNet used in the experiments has 30 residual blocks in the encoder and 30 residual blocks in the decoder .,Model Test,Character - Level Machine Translation,machine-translation,1,29,0.6041666666666666,173,0.8606965174129353,22,0.5365853658536586,1,1
175,"As in the ByteNet Decoder , the residual blocks are arranged in sets of five with corresponding dilation rates of 1 , 2 , 4 , 8 and 16 .",Model Test,Character - Level Machine Translation,machine-translation,1,30,0.625,174,0.8656716417910447,23,0.5609756097560976,1,1
176,For this task we use the residual blocks with ReLUs ( .,Model Test,Character - Level Machine Translation,machine-translation,1,31,0.6458333333333334,175,0.8706467661691543,24,0.5853658536585366,1,1
177,The number of hidden units dis 800 .,Model Test,Character - Level Machine Translation,machine-translation,1,32,0.6666666666666666,176,0.8756218905472637,25,0.6097560975609756,1,1
178,"The size of the kernel in the source network is 3 , whereas the size of the masked kernel in the target network is 3 .",Model Test,Character - Level Machine Translation,machine-translation,1,33,0.6875,177,0.8805970149253731,26,0.6341463414634146,1,1
179,For the optimization we use Adam with a learning rate of 0.0003 .,Model Test,Character - Level Machine Translation,machine-translation,1,34,0.7083333333333334,178,0.8855721393034826,27,0.6585365853658537,1,1
180,Each sentence is padded with special characters to the nearest greater multiple of 50 ; 20 % of further padding is ap - plied to each source sentence as apart of dynamic unfolding ( eq. 2 ) .,Model Test,Character - Level Machine Translation,machine-translation,1,35,0.7291666666666666,179,0.8905472636815921,28,0.6829268292682927,1,1
181,Each pair of sentences is mapped to a bucket based on the pair of padded lengths for efficient batching during training .,Model Test,Character - Level Machine Translation,machine-translation,1,36,0.75,180,0.8955223880597015,29,0.7073170731707317,1,1
182,We use vanilla beam search according to the total likelihood of the generated candidate and accept only candidates which end in a end -of - sentence token .,Model Test,Character - Level Machine Translation,machine-translation,1,37,0.7708333333333334,181,0.900497512437811,30,0.7317073170731707,1,1
183,We use abeam of size 12 .,Model Test,Character - Level Machine Translation,machine-translation,1,38,0.7916666666666666,182,0.9054726368159204,31,0.7560975609756098,1,1
184,"We do not use length normalization , nor do we keep score of which parts of the source sentence have been translated .",Model Test,Character - Level Machine Translation,machine-translation,1,39,0.8125,183,0.9104477611940298,32,0.7804878048780488,1,0
185,and contain the results of the experiments .,Model Test,Character - Level Machine Translation,machine-translation,1,40,0.8333333333333334,184,0.9154228855721394,33,0.8048780487804879,1,0
186,"On NewsTest 2014 the ByteNet achieves the highest performance in character - level and subword - level neural machine translation , and compared to the word - level systems it is second only to the version of GNMT that uses word - pieces .",Model Test,Character - Level Machine Translation,machine-translation,1,41,0.8541666666666666,185,0.9203980099502488,34,0.8292682926829268,1,1
187,"On NewsTest 2015 , to our knowledge , ByteNet achieves the best published results to date .",Model Test,Character - Level Machine Translation,machine-translation,1,42,0.875,186,0.9253731343283582,35,0.8536585365853658,1,1
188,contains some of the unaltered generated translations from the ByteNet that highlight reordering and other phenomena such as transliteration .,Model Test,Character - Level Machine Translation,machine-translation,1,43,0.8958333333333334,187,0.9303482587064676,36,0.8780487804878049,1,0
189,The character - level aspect of the model makes post -processing unnecessary in principle .,Model Test,Character - Level Machine Translation,machine-translation,1,44,0.9166666666666666,188,0.9353233830845772,37,0.902439024390244,1,0
190,We further visualize the sensitivity of the ByteNet 's predictions to specific source and target inputs using gradient - based visualization .,Model Test,Character - Level Machine Translation,machine-translation,1,45,0.9375,189,0.9402985074626866,38,0.926829268292683,1,0
191,represents a heatmap of the magnitude of the gradients of the generated outputs with respect to the source and target inputs .,Model Test,Character - Level Machine Translation,machine-translation,1,46,0.9583333333333334,190,0.945273631840796,39,0.951219512195122,1,0
192,"For visual clarity , we sum the gradients for all the characters that makeup each word and normalize the values along each column .",Model Test,Character - Level Machine Translation,machine-translation,1,47,0.9791666666666666,191,0.9502487562189056,40,0.975609756097561,1,0
193,"In contrast with the attentional pooling mechanism , this general technique allows us to inspect not just dependencies of the outputs on the source inputs , but also dependencies of the outputs on previous target inputs , or on any other neural network layers .",Model Test,Character - Level Machine Translation,machine-translation,1,48,1.0,192,0.9552238805970148,41,1.0,1,0
194,Conclusion,,,machine-translation,1,0,0.0,193,0.9601990049751244,0,0.0,1,0
195,"We have introduced the ByteNet , a neural translation model that has linear running time , decouples translation from memorization and has short signal propagation paths for tokens in sequences .",Conclusion,Conclusion,machine-translation,1,1,0.1428571428571428,194,0.965174129353234,1,0.1428571428571428,0,0
196,We have shown that the ByteNet decoder is a state - of - the - art character - level language model based on a convolutional neural network that outperforms recurrent neural language models .,Conclusion,Conclusion,machine-translation,1,2,0.2857142857142857,195,0.9701492537313432,2,0.2857142857142857,0,0
197,"We have also shown that the ByteNet generalizes the RNN Enc - Dec architecture and achieves state - of - the - art results for character - to - character machine translation and excellent results in general , while maintaining linear running time complexity .",Conclusion,Conclusion,machine-translation,1,3,0.4285714285714285,196,0.9751243781094528,3,0.4285714285714285,0,0
198,We have revealed the latent structure learnt by the ByteNet and found it to mirror the expected alignment between the tokens in the sentences ..,Conclusion,Conclusion,machine-translation,1,4,0.5714285714285714,197,0.9800995024875622,4,0.5714285714285714,0,0
199,Magnitude of gradients of the predicted outputs with respect to the source and target inputs .,Conclusion,Conclusion,machine-translation,1,5,0.7142857142857143,198,0.9850746268656716,5,0.7142857142857143,0,0
200,The gradients are summed for all the characters in a given word .,Conclusion,Conclusion,machine-translation,1,6,0.8571428571428571,199,0.9900497512437813,6,0.8571428571428571,0,0
201,"In the bottom heatmap the magnitudes are nonzero on the diagonal , since the prediction of a target character depends highly on the preceding target character in the same word .",Conclusion,Conclusion,machine-translation,1,7,1.0,200,0.9950248756218906,7,1.0,0,0
1,title,,,machine-translation,2,0,0.0,0,0.0,0,0.0,1,0
2,Attention Is All You Need,title,,machine-translation,2,1,0.0,1,0.0044444444444444,1,0.0,1,0
3,abstract,,,machine-translation,2,0,0.0,2,0.0088888888888888,0,0.0,1,0
4,The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder .,abstract,abstract,machine-translation,2,1,0.0625,3,0.0133333333333333,1,0.0625,1,0
5,The best performing models also connect the encoder and decoder through an attention mechanism .,abstract,abstract,machine-translation,2,2,0.125,4,0.0177777777777777,2,0.125,1,0
6,"We propose anew simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .",abstract,abstract,machine-translation,2,3,0.1875,5,0.0222222222222222,3,0.1875,1,1
7,Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train .,abstract,abstract,machine-translation,2,4,0.25,6,0.0266666666666666,4,0.25,1,0
8,"Our model achieves 28.4 BLEU on the WMT 2014 Englishto - German translation task , improving over the existing best results , including ensembles , by over 2 BLEU .",abstract,abstract,machine-translation,2,5,0.3125,7,0.0311111111111111,5,0.3125,1,0
9,"On the WMT 2014 English - to - French translation task , our model establishes anew single - model state - of - the - art BLEU score of 41.8 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .",abstract,abstract,machine-translation,2,6,0.375,8,0.0355555555555555,6,0.375,1,0
10,We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data .,abstract,abstract,machine-translation,2,7,0.4375,9,0.04,7,0.4375,1,0
11,* Equal contribution .,abstract,abstract,machine-translation,2,8,0.5,10,0.0444444444444444,8,0.5,1,0
12,Listing order is random .,abstract,abstract,machine-translation,2,9,0.5625,11,0.0488888888888888,9,0.5625,1,0
13,Jakob proposed replacing RNNs with self - attention and started the effort to evaluate this idea .,abstract,abstract,machine-translation,2,10,0.625,12,0.0533333333333333,10,0.625,1,0
14,"Ashish , with Illia , designed and implemented the first Transformer models and has been crucially involved in every aspect of this work .",abstract,abstract,machine-translation,2,11,0.6875,13,0.0577777777777777,11,0.6875,1,0
15,"Noam proposed scaled dot-product attention , multi-head attention and the parameter - free position representation and became the other person involved in nearly every detail .",abstract,abstract,machine-translation,2,12,0.75,14,0.0622222222222222,12,0.75,1,0
16,"Niki designed , implemented , tuned and evaluated countless model variants in our original codebase and tensor2tensor .",abstract,abstract,machine-translation,2,13,0.8125,15,0.0666666666666666,13,0.8125,1,0
17,"Llion also experimented with novel model variants , was responsible for our initial codebase , and efficient inference and visualizations .",abstract,abstract,machine-translation,2,14,0.875,16,0.0711111111111111,14,0.875,1,0
18,"Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor , replacing our earlier codebase , greatly improving results and massively accelerating our research .",abstract,abstract,machine-translation,2,15,0.9375,17,0.0755555555555555,15,0.9375,1,0
19,Work performed while at Google Brain .,abstract,abstract,machine-translation,2,16,1.0,18,0.08,16,1.0,1,0
20,Introduction,,,machine-translation,2,0,0.0,19,0.0844444444444444,0,0.0,1,0
21,"Recurrent neural networks , long short - term memory and gated recurrent neural networks in particular , have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation .",Introduction,Introduction,machine-translation,2,1,0.0909090909090909,20,0.0888888888888888,1,0.0909090909090909,1,0
22,Numerous efforts have since continued to push the boundaries of recurrent language models and encoder - decoder architectures .,Introduction,Introduction,machine-translation,2,2,0.1818181818181818,21,0.0933333333333333,2,0.1818181818181818,1,1
23,Recurrent models typically factor computation along the symbol positions of the input and output sequences .,Introduction,Introduction,machine-translation,2,3,0.2727272727272727,22,0.0977777777777777,3,0.2727272727272727,1,0
24,"Aligning the positions to steps in computation time , they generate a sequence of hidden states ht , as a function of the previous hidden state h t?1 and the input for position t.",Introduction,Introduction,machine-translation,2,4,0.3636363636363636,23,0.1022222222222222,4,0.3636363636363636,1,0
25,"This inherently sequential nature precludes parallelization within training examples , which becomes critical at longer sequence lengths , as memory constraints limit batching across examples .",Introduction,Introduction,machine-translation,2,5,0.4545454545454545,24,0.1066666666666666,5,0.4545454545454545,1,0
26,"Recent work has achieved significant improvements in computational efficiency through factorization tricks and conditional computation , while also improving model performance in case of the latter .",Introduction,Introduction,machine-translation,2,6,0.5454545454545454,25,0.1111111111111111,6,0.5454545454545454,1,0
27,"The fundamental constraint of sequential computation , however , remains .",Introduction,Introduction,machine-translation,2,7,0.6363636363636364,26,0.1155555555555555,7,0.6363636363636364,1,0
28,"Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences .",Introduction,Introduction,machine-translation,2,8,0.7272727272727273,27,0.12,8,0.7272727272727273,1,0
29,"In all but a few cases , however , such attention mechanisms are used in conjunction with a recurrent network .",Introduction,Introduction,machine-translation,2,9,0.8181818181818182,28,0.1244444444444444,9,0.8181818181818182,1,0
30,"In this work we propose the Transformer , a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output .",Introduction,Introduction,machine-translation,2,10,0.9090909090909092,29,0.1288888888888889,10,0.9090909090909092,1,1
31,The Transformer allows for significantly more parallelization and can reach anew state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs .,Introduction,Introduction,machine-translation,2,11,1.0,30,0.1333333333333333,11,1.0,1,0
32,Background,,,machine-translation,2,0,0.0,31,0.1377777777777777,0,0.0,1,0
33,"The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU , ByteNet and ConvS2S , all of which use convolutional neural networks as basic building block , computing hidden representations in parallel for all input and output positions .",Background,Background,machine-translation,2,1,0.1111111111111111,32,0.1422222222222222,1,0.1111111111111111,0,0
34,"In these models , the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions , linearly for ConvS2S and logarithmically for ByteNet .",Background,Background,machine-translation,2,2,0.2222222222222222,33,0.1466666666666666,2,0.2222222222222222,0,0
35,This makes it more difficult to learn dependencies between distant positions .,Background,Background,machine-translation,2,3,0.3333333333333333,34,0.1511111111111111,3,0.3333333333333333,0,0
36,"In the Transformer this is reduced to a constant number of operations , albeit at the cost of reduced effective resolution due to averaging attention - weighted positions , an effect we counteract with Multi - Head Attention as described in section 3.2 .",Background,Background,machine-translation,2,4,0.4444444444444444,35,0.1555555555555555,4,0.4444444444444444,0,0
37,"Self - attention , sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence .",Background,Background,machine-translation,2,5,0.5555555555555556,36,0.16,5,0.5555555555555556,0,0
38,"Self - attention has been used successfully in a variety of tasks including reading comprehension , abstractive summarization , textual entailment and learning task - independent sentence representations .",Background,Background,machine-translation,2,6,0.6666666666666666,37,0.1644444444444444,6,0.6666666666666666,0,0
39,End - to - end memory networks are based on a recurrent attention mechanism instead of sequencealigned recurrence and have been shown to perform well on simple - language question answering and language modeling tasks .,Background,Background,machine-translation,2,7,0.7777777777777778,38,0.1688888888888889,7,0.7777777777777778,0,0
40,"To the best of our knowledge , however , the Transformer is the first transduction model relying entirely on self - attention to compute representations of its input and output without using sequencealigned RNNs or convolution .",Background,Background,machine-translation,2,8,0.8888888888888888,39,0.1733333333333333,8,0.8888888888888888,0,0
41,"In the following sections , we will describe the Transformer , motivate self - attention and discuss its advantages over models such as and .",Background,Background,machine-translation,2,9,1.0,40,0.1777777777777777,9,1.0,0,0
42,Model Architecture,,,machine-translation,2,0,0.0,41,0.1822222222222222,0,0.0,1,0
43,Most competitive neural sequence transduction models have an encoder - decoder structure .,Model Architecture,Model Architecture,machine-translation,2,1,0.0091743119266055,42,0.1866666666666666,1,0.2,1,0
44,"Here , the encoder maps an input sequence of symbol representations ( x 1 , ... , x n ) to a sequence of continuous representations z = ( z 1 , ... , z n ) .",Model Architecture,Model Architecture,machine-translation,2,2,0.018348623853211,43,0.1911111111111111,2,0.4,1,0
45,"Given z , the decoder then generates an output sequence ( y 1 , ... , y m ) of symbols one element at a time .",Model Architecture,Model Architecture,machine-translation,2,3,0.0275229357798165,44,0.1955555555555555,3,0.6,1,0
46,"At each step the model is auto-regressive , consuming the previously generated symbols as additional input when generating the next .",Model Architecture,Model Architecture,machine-translation,2,4,0.036697247706422,45,0.2,4,0.8,1,0
47,"The Transformer follows this overall architecture using stacked self - attention and point - wise , fully connected layers for both the encoder and decoder , shown in the left and right halves of",Model Architecture,Model Architecture,machine-translation,2,5,0.0458715596330275,46,0.2044444444444444,5,1.0,1,0
48,Encoder and Decoder Stacks,Model Architecture,,machine-translation,2,6,0.055045871559633,47,0.2088888888888889,0,0.0,1,0
49,Encoder :,Model Architecture,Encoder and Decoder Stacks,machine-translation,2,7,0.0642201834862385,48,0.2133333333333333,1,0.0769230769230769,1,0
50,The encoder is composed of a stack of N = 6 identical layers .,Model Architecture,Encoder and Decoder Stacks,machine-translation,2,8,0.073394495412844,49,0.2177777777777777,2,0.1538461538461538,1,1
51,Each layer has two sub-layers .,Model Architecture,Encoder and Decoder Stacks,machine-translation,2,9,0.0825688073394495,50,0.2222222222222222,3,0.2307692307692307,1,1
52,"The first is a multi-head self - attention mechanism , and the second is a simple , positionwise fully connected feed - forward network .",Model Architecture,Encoder and Decoder Stacks,machine-translation,2,10,0.091743119266055,51,0.2266666666666666,4,0.3076923076923077,1,1
53,"We employ a residual connection around each of the two sub-layers , followed by layer normalization .",Model Architecture,Encoder and Decoder Stacks,machine-translation,2,11,0.1009174311926605,52,0.2311111111111111,5,0.3846153846153846,1,1
54,"That is , the output of each sub - layer is LayerNorm ( x + Sublayer ( x ) ) , where Sublayer ( x ) is the function implemented by the sub - layer itself .",Model Architecture,Encoder and Decoder Stacks,machine-translation,2,12,0.110091743119266,53,0.2355555555555555,6,0.4615384615384615,1,0
55,"To facilitate these residual connections , all sub- layers in the model , as well as the embedding layers , produce outputs of dimension d model = 512 .",Model Architecture,Encoder and Decoder Stacks,machine-translation,2,13,0.1192660550458715,54,0.24,7,0.5384615384615384,1,0
56,Decoder :,Model Architecture,Encoder and Decoder Stacks,machine-translation,2,14,0.128440366972477,55,0.2444444444444444,8,0.6153846153846154,1,0
57,The decoder is also composed of a stack of N = 6 identical layers .,Model Architecture,Encoder and Decoder Stacks,machine-translation,2,15,0.1376146788990825,56,0.2488888888888888,9,0.6923076923076923,1,1
58,"In addition to the two sub-layers in each encoder layer , the decoder inserts a third sub - layer , which performs multi-head attention over the output of the encoder stack .",Model Architecture,Encoder and Decoder Stacks,machine-translation,2,16,0.146788990825688,57,0.2533333333333333,10,0.7692307692307693,1,1
59,"Similar to the encoder , we employ residual connections around each of the sub-layers , followed by layer normalization .",Model Architecture,Encoder and Decoder Stacks,machine-translation,2,17,0.1559633027522936,58,0.2577777777777778,11,0.8461538461538461,1,1
60,We also modify the self - attention sub - layer in the decoder stack to prevent positions from attending to subsequent positions .,Model Architecture,Encoder and Decoder Stacks,machine-translation,2,18,0.1651376146788991,59,0.2622222222222222,12,0.9230769230769232,1,1
61,"This masking , combined with fact that the output embeddings are offset by one position , ensures that the predictions for position i can depend only on the known outputs at positions less than i.",Model Architecture,Encoder and Decoder Stacks,machine-translation,2,19,0.1743119266055046,60,0.2666666666666666,13,1.0,1,0
62,Attention,Model Architecture,,machine-translation,2,20,0.1834862385321101,61,0.2711111111111111,0,0.0,1,0
63,"An attention function can be described as mapping a query and a set of key - value pairs to an output , where the query , keys , values , and output are all vectors .",Model Architecture,Attention,machine-translation,2,21,0.1926605504587156,62,0.2755555555555555,1,0.3333333333333333,1,1
64,"The output is computed as a weighted sum of the values , where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key .",Model Architecture,Attention,machine-translation,2,22,0.2018348623853211,63,0.28,2,0.6666666666666666,1,1
65,Scaled Dot - Product,Model Architecture,,machine-translation,2,23,0.2110091743119266,64,0.2844444444444444,3,1.0,1,0
66,Attention,Model Architecture,,machine-translation,2,24,0.2201834862385321,65,0.2888888888888888,0,0.0,1,0
67,"We call our particular attention "" Scaled Dot -Product Attention "" ) .",Model Architecture,Attention,machine-translation,2,25,0.2293577981651376,66,0.2933333333333333,1,0.0416666666666666,1,0
68,"The input consists of queries and keys of dimension d k , and values of dimension d v .",Model Architecture,Attention,machine-translation,2,26,0.2385321100917431,67,0.2977777777777777,2,0.0833333333333333,1,0
69,"We compute the dot products of the query with all keys , divide each by ? d k , and apply a softmax function to obtain the weights on the values .",Model Architecture,Attention,machine-translation,2,27,0.2477064220183486,68,0.3022222222222222,3,0.125,1,0
70,"In practice , we compute the attention function on a set of queries simultaneously , packed together into a matrix Q .",Model Architecture,Attention,machine-translation,2,28,0.2568807339449541,69,0.3066666666666666,4,0.1666666666666666,1,0
71,The keys and values are also packed together into matrices K and V .,Model Architecture,Attention,machine-translation,2,29,0.2660550458715596,70,0.3111111111111111,5,0.2083333333333333,1,0
72,We compute the matrix of outputs as :,Model Architecture,Attention,machine-translation,2,30,0.2752293577981651,71,0.3155555555555555,6,0.25,1,0
73,"The two most commonly used attention functions are additive attention , and dot-product ( multiplicative ) attention .",Model Architecture,Attention,machine-translation,2,31,0.2844036697247706,72,0.32,7,0.2916666666666667,1,0
74,"Dot-product attention is identical to our algorithm , except for the scaling factor of 1",Model Architecture,Attention,machine-translation,2,32,0.2935779816513761,73,0.3244444444444444,8,0.3333333333333333,1,0
75,Additive attention computes the compatibility function using a feed - forward network with a single hidden layer .,Model Architecture,Attention,machine-translation,2,33,0.3027522935779816,74,0.3288888888888889,9,0.375,1,0
76,"While the two are similar in theoretical complexity , dot-product attention is much faster and more space - efficient in practice , since it can be implemented using highly optimized matrix multiplication code .",Model Architecture,Attention,machine-translation,2,34,0.3119266055045872,75,0.3333333333333333,10,0.4166666666666667,1,0
77,"While for small values of d k the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of d k.",Model Architecture,Attention,machine-translation,2,35,0.3211009174311927,76,0.3377777777777778,11,0.4583333333333333,1,0
78,"We suspect that for large values of d k , the dot products grow large in magnitude , pushing the softmax function into regions where it has extremely small gradients",Model Architecture,Attention,machine-translation,2,36,0.3302752293577982,77,0.3422222222222222,12,0.5,1,0
79,4 .,Model Architecture,Attention,machine-translation,2,37,0.3394495412844037,78,0.3466666666666667,13,0.5416666666666666,1,0
80,"To counteract this effect , we scale the dot products by 1",Model Architecture,Attention,machine-translation,2,38,0.3486238532110092,79,0.3511111111111111,14,0.5833333333333334,1,0
81,Multi - Head Attention,Model Architecture,,machine-translation,2,39,0.3577981651376147,80,0.3555555555555555,15,0.625,1,0
82,"Instead of performing a single attention function with d model - dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to d k , d k and d v dimensions , respectively .",Model Architecture,Multi - Head Attention,machine-translation,2,40,0.3669724770642202,81,0.36,16,0.6666666666666666,1,0
83,"On each of these projected versions of queries , keys and values we then perform the attention function in parallel , yielding d v - dimensional output values .",Model Architecture,Multi - Head Attention,machine-translation,2,41,0.3761467889908257,82,0.3644444444444444,17,0.7083333333333334,1,0
84,"These are concatenated and once again projected , resulting in the final values , as depicted in .",Model Architecture,Multi - Head Attention,machine-translation,2,42,0.3853211009174312,83,0.3688888888888889,18,0.75,1,0
85,Multi - head attention allows the model to jointly attend to information from different representation subspaces at different positions .,Model Architecture,Multi - Head Attention,machine-translation,2,43,0.3944954128440367,84,0.3733333333333333,19,0.7916666666666666,1,0
86,"With a single attention head , averaging inhibits this .",Model Architecture,Multi - Head Attention,machine-translation,2,44,0.4036697247706422,85,0.3777777777777777,20,0.8333333333333334,1,0
87,Where the projections are parameter matrices,Model Architecture,Multi - Head Attention,machine-translation,2,45,0.4128440366972477,86,0.3822222222222222,21,0.875,1,0
88,"In this work we employ h = 8 parallel attention layers , or heads .",Model Architecture,Multi - Head Attention,machine-translation,2,46,0.4220183486238532,87,0.3866666666666666,22,0.9166666666666666,1,0
89,For each of these we use,Model Architecture,Multi - Head Attention,machine-translation,2,47,0.4311926605504587,88,0.3911111111111111,23,0.9583333333333334,1,0
90,"Due to the reduced dimension of each head , the total computational cost is similar to that of single - head attention with full dimensionality .",Model Architecture,Multi - Head Attention,machine-translation,2,48,0.4403669724770642,89,0.3955555555555555,24,1.0,1,0
91,Applications of Attention in our Model,Model Architecture,,machine-translation,2,49,0.4495412844036697,90,0.4,0,0.0,1,0
92,The Transformer uses multi-head attention in three different ways :,Model Architecture,Applications of Attention in our Model,machine-translation,2,50,0.4587155963302752,91,0.4044444444444444,1,0.0588235294117647,1,0
93,"In "" encoder - decoder attention "" layers , the queries come from the previous decoder layer , and the memory keys and values come from the output of the encoder .",Model Architecture,Applications of Attention in our Model,machine-translation,2,51,0.4678899082568807,92,0.4088888888888889,2,0.1176470588235294,1,0
94,This allows every position in the decoder to attend overall positions in the input sequence .,Model Architecture,Applications of Attention in our Model,machine-translation,2,52,0.4770642201834862,93,0.4133333333333333,3,0.1764705882352941,1,0
95,This mimics the typical encoder - decoder attention mechanisms in sequence - to - sequence models such as . The encoder contains self - attention layers .,Model Architecture,Applications of Attention in our Model,machine-translation,2,53,0.4862385321100917,94,0.4177777777777778,4,0.2352941176470588,1,0
96,"Ina self - attention layer all of the keys , values and queries come from the same place , in this case , the output of the previous layer in the encoder .",Model Architecture,Applications of Attention in our Model,machine-translation,2,54,0.4954128440366973,95,0.4222222222222222,5,0.2941176470588235,1,0
97,Each position in the encoder can attend to all positions in the previous layer of the encoder .,Model Architecture,Applications of Attention in our Model,machine-translation,2,55,0.5045871559633027,96,0.4266666666666667,6,0.3529411764705882,1,0
98,"Similarly , self - attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position .",Model Architecture,Applications of Attention in our Model,machine-translation,2,56,0.5137614678899083,97,0.4311111111111111,7,0.4117647058823529,1,0
99,We need to prevent leftward information flow in the decoder to preserve the auto - regressive property .,Model Architecture,Applications of Attention in our Model,machine-translation,2,57,0.5229357798165137,98,0.4355555555555555,8,0.4705882352941176,1,0
100,We implement this inside of scaled dot-product attention by masking out ( setting to ?? ) all values in the input of the softmax which correspond to illegal connections .,Model Architecture,Applications of Attention in our Model,machine-translation,2,58,0.5321100917431193,99,0.44,9,0.5294117647058824,1,0
101,See.,Model Architecture,,machine-translation,2,59,0.5412844036697247,100,0.4444444444444444,10,0.5882352941176471,1,0
102,Position - wise Feed - Forward Networks,Model Architecture,See.,machine-translation,2,60,0.5504587155963303,101,0.4488888888888889,11,0.6470588235294118,1,0
103,"In addition to attention sub - layers , each of the layers in our encoder and decoder contains a fully connected feed - forward network , which is applied to each position separately and identically .",Model Architecture,See.,machine-translation,2,61,0.5596330275229358,102,0.4533333333333333,12,0.7058823529411765,1,1
104,This consists of two linear transformations with a ReLU activation in between .,Model Architecture,See.,machine-translation,2,62,0.5688073394495413,103,0.4577777777777778,13,0.7647058823529411,1,1
105,"While the linear transformations are the same across different positions , they use different parameters from layer to layer .",Model Architecture,See.,machine-translation,2,63,0.5779816513761468,104,0.4622222222222222,14,0.8235294117647058,1,0
106,Another way of describing this is as two convolutions with kernel size,Model Architecture,See.,machine-translation,2,64,0.5871559633027523,105,0.4666666666666667,15,0.8823529411764706,1,0
107,1 .,Model Architecture,See.,machine-translation,2,65,0.5963302752293578,106,0.4711111111111111,16,0.9411764705882352,1,0
108,"The dimensionality of input and output is d model = 512 , and the inner-layer has dimensionality d ff = 2048 .",Model Architecture,See.,machine-translation,2,66,0.6055045871559633,107,0.4755555555555555,17,1.0,1,0
109,Embeddings and Softmax,Model Architecture,,machine-translation,2,67,0.6146788990825688,108,0.48,0,0.0,1,0
110,"Similarly to other sequence transduction models , we use learned embeddings to convert the input tokens and output tokens to vectors of dimension d model .",Model Architecture,Embeddings and Softmax,machine-translation,2,68,0.6238532110091743,109,0.4844444444444444,1,0.25,1,1
111,We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next - token probabilities .,Model Architecture,Embeddings and Softmax,machine-translation,2,69,0.6330275229357798,110,0.4888888888888889,2,0.5,1,1
112,"In our model , we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation , similar to .",Model Architecture,Embeddings and Softmax,machine-translation,2,70,0.6422018348623854,111,0.4933333333333333,3,0.75,1,0
113,"In the embedding layers , we multiply those weights by ? d model .",Model Architecture,Embeddings and Softmax,machine-translation,2,71,0.6513761467889908,112,0.4977777777777777,4,1.0,1,0
114,Positional Encoding,Model Architecture,,machine-translation,2,72,0.6605504587155964,113,0.5022222222222222,0,0.0,1,0
115,"Since our model contains no recurrence and no convolution , in order for the model to make use of the order of the sequence , we must inject some information about the relative or absolute position of the : Maximum path lengths , per-layer complexity and minimum number of sequential operations for different layer types .",Model Architecture,Positional Encoding,machine-translation,2,73,0.6697247706422018,114,0.5066666666666667,1,0.5,1,1
116,"n is the sequence length , dis the representation dimension , k is the kernel size of convolutions and r the size of the neighborhood in restricted self - attention .",Model Architecture,Positional Encoding,machine-translation,2,74,0.6788990825688074,115,0.5111111111111111,2,1.0,1,1
117,Layer Type Complexity per Layer Sequential Maximum Path Length Operations,Model Architecture,Positional Encoding,machine-translation,2,75,0.6880733944954128,116,0.5155555555555555,0,0.0,1,1
118,tokens in the sequence .,Model Architecture,Positional Encoding,machine-translation,2,76,0.6972477064220184,117,0.52,1,0.0294117647058823,1,1
119,"To this end , we add "" positional encodings "" to the input embeddings at the bottoms of the encoder and decoder stacks .",Model Architecture,Positional Encoding,machine-translation,2,77,0.7064220183486238,118,0.5244444444444445,2,0.0588235294117647,1,1
120,"The positional encodings have the same dimension d model as the embeddings , so that the two can be summed .",Model Architecture,Positional Encoding,machine-translation,2,78,0.7155963302752294,119,0.5288888888888889,3,0.088235294117647,1,0
121,"There are many choices of positional encodings , learned and fixed .",Model Architecture,Positional Encoding,machine-translation,2,79,0.7247706422018348,120,0.5333333333333333,4,0.1176470588235294,1,0
122,"In this work , we use sine and cosine functions of different frequencies : where pos is the position and i is the dimension .",Model Architecture,Positional Encoding,machine-translation,2,80,0.7339449541284404,121,0.5377777777777778,5,0.1470588235294117,1,0
123,"That is , each dimension of the positional encoding corresponds to a sinusoid .",Model Architecture,Positional Encoding,machine-translation,2,81,0.7431192660550459,122,0.5422222222222223,6,0.1764705882352941,1,0
124,The wavelengths form a geometric progression from 2 ? to 10000 2 ?.,Model Architecture,Positional Encoding,machine-translation,2,82,0.7522935779816514,123,0.5466666666666666,7,0.2058823529411764,1,0
125,"We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions , since for any fixed offset k , PE pos+k can be represented as a linear function of PE pos .",Model Architecture,Positional Encoding,machine-translation,2,83,0.7614678899082569,124,0.5511111111111111,8,0.2352941176470588,1,0
126,"We also experimented with using learned positional embeddings instead , and found that the two versions produced nearly identical results ( see row ( E ) ) .",Model Architecture,Positional Encoding,machine-translation,2,84,0.7706422018348624,125,0.5555555555555556,9,0.2647058823529412,1,0
127,We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training .,Model Architecture,Positional Encoding,machine-translation,2,85,0.7798165137614679,126,0.56,10,0.2941176470588235,1,0
128,Why Self - Attention,Model Architecture,,machine-translation,2,86,0.7889908256880734,127,0.5644444444444444,11,0.3235294117647059,1,0
129,"In this section we compare various aspects of self - attention layers to the recurrent and convolutional layers commonly used for mapping one variable - length sequence of symbol representations ( x 1 , ... , x n ) to another sequence of equal length ( z 1 , ... , z n ) , with x i , z i ?",Model Architecture,Why Self - Attention,machine-translation,2,87,0.7981651376146789,128,0.5688888888888889,12,0.3529411764705882,1,0
130,"Rd , such as a hidden layer in atypical sequence transduction encoder or decoder .",Model Architecture,Why Self - Attention,machine-translation,2,88,0.8073394495412844,129,0.5733333333333334,13,0.3823529411764705,1,0
131,Motivating our use of self - attention we consider three desiderata .,Model Architecture,Why Self - Attention,machine-translation,2,89,0.8165137614678899,130,0.5777777777777777,14,0.4117647058823529,1,0
132,One is the total computational complexity per layer .,Model Architecture,Why Self - Attention,machine-translation,2,90,0.8256880733944955,131,0.5822222222222222,15,0.4411764705882353,1,0
133,"Another is the amount of computation that can be parallelized , as measured by the minimum number of sequential operations required .",Model Architecture,Why Self - Attention,machine-translation,2,91,0.8348623853211009,132,0.5866666666666667,16,0.4705882352941176,1,0
134,The third is the path length between long - range dependencies in the network .,Model Architecture,Why Self - Attention,machine-translation,2,92,0.8440366972477065,133,0.5911111111111111,17,0.5,1,0
135,Learning long - range dependencies is a key challenge in many sequence transduction tasks .,Model Architecture,Why Self - Attention,machine-translation,2,93,0.8532110091743119,134,0.5955555555555555,18,0.5294117647058824,1,0
136,One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network .,Model Architecture,Why Self - Attention,machine-translation,2,94,0.8623853211009175,135,0.6,19,0.5588235294117647,1,0
137,"The shorter these paths between any combination of positions in the input and output sequences , the easier it is to learn long - range dependencies .",Model Architecture,Why Self - Attention,machine-translation,2,95,0.8715596330275229,136,0.6044444444444445,20,0.5882352941176471,1,0
138,Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types .,Model Architecture,Why Self - Attention,machine-translation,2,96,0.8807339449541285,137,0.6088888888888889,21,0.6176470588235294,1,0
139,"As noted in , a self - attention layer connects all positions with a constant number of sequentially executed operations , whereas a recurrent layer requires O ( n ) sequential operations .",Model Architecture,Why Self - Attention,machine-translation,2,97,0.8899082568807339,138,0.6133333333333333,22,0.6470588235294118,1,0
140,"In terms of computational complexity , self - attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d , which is most often the case with sentence representations used by state - of - the - art models in machine translations , such as word - piece and byte - pair representations .",Model Architecture,Why Self - Attention,machine-translation,2,98,0.8990825688073395,139,0.6177777777777778,23,0.6764705882352942,1,0
141,"To improve computational performance for tasks involving very long sequences , self - attention could be restricted to considering only a neighborhood of sizer in the input sequence centered around the respective output position .",Model Architecture,Why Self - Attention,machine-translation,2,99,0.908256880733945,140,0.6222222222222222,24,0.7058823529411765,1,0
142,This would increase the maximum path length to O ( n / r ) .,Model Architecture,Why Self - Attention,machine-translation,2,100,0.9174311926605504,141,0.6266666666666667,25,0.7352941176470589,1,0
143,We plan to investigate this approach further in future work .,Model Architecture,Why Self - Attention,machine-translation,2,101,0.926605504587156,142,0.6311111111111111,26,0.7647058823529411,1,0
144,A single convolutional layer with kernel width k < n does not connect all pairs of input and output positions .,Model Architecture,Why Self - Attention,machine-translation,2,102,0.9357798165137616,143,0.6355555555555555,27,0.7941176470588235,1,0
145,"Doing so requires a stack of O ( n / k ) convolutional layers in the case of contiguous kernels , or O ( log k ( n ) ) in the case of dilated convolutions , increasing the length of the longest paths between any two positions in the network .",Model Architecture,Why Self - Attention,machine-translation,2,103,0.944954128440367,144,0.64,28,0.8235294117647058,1,0
146,"Convolutional layers are generally more expensive than recurrent layers , by a factor of k.",Model Architecture,Why Self - Attention,machine-translation,2,104,0.9541284403669724,145,0.6444444444444445,29,0.8529411764705882,1,0
147,"Separable convolutions , however , decrease the complexity considerably , to O ( k n d + n d 2 ) .",Model Architecture,Why Self - Attention,machine-translation,2,105,0.963302752293578,146,0.6488888888888888,30,0.8823529411764706,1,0
148,"Even with k = n , however , the complexity of a separable convolution is equal to the combination of a self - attention layer and a point - wise feed - forward layer , the approach we take in our model .",Model Architecture,Why Self - Attention,machine-translation,2,106,0.9724770642201837,147,0.6533333333333333,31,0.9117647058823528,1,0
149,"As side benefit , self - attention could yield more interpretable models .",Model Architecture,Why Self - Attention,machine-translation,2,107,0.981651376146789,148,0.6577777777777778,32,0.9411764705882352,1,0
150,We inspect attention distributions from our models and present and discuss examples in the appendix .,Model Architecture,Why Self - Attention,machine-translation,2,108,0.9908256880733946,149,0.6622222222222223,33,0.9705882352941176,1,0
151,"Not only do individual attention heads clearly learn to perform different tasks , many appear to exhibit behavior related to the syntactic and semantic structure of the sentences .",Model Architecture,Why Self - Attention,machine-translation,2,109,1.0,150,0.6666666666666666,34,1.0,1,0
152,Training,,,machine-translation,2,0,0.0,151,0.6711111111111111,0,0.0,1,0
153,This section describes the training regime for our models .,Training,Training,machine-translation,2,1,0.0,152,0.6755555555555556,1,0.0,1,0
154,Training Data and Batching,,,machine-translation,2,0,0.0,153,0.68,0,0.0,1,0
155,We trained on the standard WMT 2014 English - German dataset consisting of about 4.5 million sentence pairs .,Training Data and Batching,Training Data and Batching,machine-translation,2,1,0.04,154,0.6844444444444444,1,0.2,1,0
156,"Sentences were encoded using byte - pair encoding , which has a shared sourcetarget vocabulary of about 37000 tokens .",Training Data and Batching,Training Data and Batching,machine-translation,2,2,0.08,155,0.6888888888888889,2,0.4,1,0
157,"For English - French , we used the significantly larger WMT 2014 English - French dataset consisting of 36M sentences and split tokens into a 32000 word - piece vocabulary .",Training Data and Batching,Training Data and Batching,machine-translation,2,3,0.12,156,0.6933333333333334,3,0.6,1,0
158,Sentence pairs were batched together by approximate sequence length .,Training Data and Batching,Training Data and Batching,machine-translation,2,4,0.16,157,0.6977777777777778,4,0.8,1,0
159,Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens .,Training Data and Batching,Training Data and Batching,machine-translation,2,5,0.2,158,0.7022222222222222,5,1.0,1,0
160,Hardware and Schedule,Training Data and Batching,,machine-translation,2,6,0.24,159,0.7066666666666667,0,0.0,1,0
161,We trained our models on one machine with 8 NVIDIA P100 GPUs .,Training Data and Batching,Hardware and Schedule,machine-translation,2,7,0.28,160,0.7111111111111111,1,0.2,1,1
162,"For our base models using the hyperparameters described throughout the paper , each training step took about 0.4 seconds .",Training Data and Batching,Hardware and Schedule,machine-translation,2,8,0.32,161,0.7155555555555555,2,0.4,1,0
163,"We trained the base models fora total of 100,000 steps or 12 hours .",Training Data and Batching,Hardware and Schedule,machine-translation,2,9,0.36,162,0.72,3,0.6,1,1
164,"For our big models , ( described on the bottom line of table 3 ) , step time was 1.0 seconds .",Training Data and Batching,Hardware and Schedule,machine-translation,2,10,0.4,163,0.7244444444444444,4,0.8,1,0
165,"The big models were trained for 300,000 steps ( 3.5 days ) .",Training Data and Batching,Hardware and Schedule,machine-translation,2,11,0.44,164,0.7288888888888889,5,1.0,1,1
166,Optimizer,Training Data and Batching,,machine-translation,2,12,0.48,165,0.7333333333333333,0,0.0,1,0
167,"We used the Adam optimizer with ? 1 = 0.9 , ? 2 = 0.98 and = 10 ?9 .",Training Data and Batching,Optimizer,machine-translation,2,13,0.52,166,0.7377777777777778,1,0.25,1,1
168,"We varied the learning rate over the course of training , according to the formula :",Training Data and Batching,Optimizer,machine-translation,2,14,0.56,167,0.7422222222222222,2,0.5,1,0
169,"This corresponds to increasing the learning rate linearly for the first warmup_steps training steps , and decreasing it thereafter proportionally to the inverse square root of the step number .",Training Data and Batching,Optimizer,machine-translation,2,15,0.6,168,0.7466666666666667,3,0.75,1,0
170,We used warmup_steps = 4000 .,Training Data and Batching,Optimizer,machine-translation,2,16,0.64,169,0.7511111111111111,4,1.0,1,1
171,Regularization,Training Data and Batching,,machine-translation,2,17,0.68,170,0.7555555555555555,0,0.0,1,0
172,We employ three types of regularization during training :,Training Data and Batching,Regularization,machine-translation,2,18,0.72,171,0.76,1,0.125,1,0
173,Residual Dropout,Training Data and Batching,,machine-translation,2,19,0.76,172,0.7644444444444445,2,0.25,1,1
174,"We apply dropout to the output of each sub - layer , before it is added to the sub - layer input and normalized .",Training Data and Batching,Residual Dropout,machine-translation,2,20,0.8,173,0.7688888888888888,3,0.375,1,1
175,"In addition , we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks .",Training Data and Batching,Residual Dropout,machine-translation,2,21,0.84,174,0.7733333333333333,4,0.5,1,1
176,"For the base model , we use a rate of P drop = 0.1 .",Training Data and Batching,Residual Dropout,machine-translation,2,22,0.88,175,0.7777777777777778,5,0.625,1,0
177,Label Smoothing,Training Data and Batching,,machine-translation,2,23,0.92,176,0.7822222222222223,6,0.75,1,1
178,"During training , we employed label smoothing of value ls = 0.1 .",Training Data and Batching,Label Smoothing,machine-translation,2,24,0.96,177,0.7866666666666666,7,0.875,1,1
179,"This hurts perplexity , as the model learns to be more unsure , but improves accuracy and BLEU score .",Training Data and Batching,Label Smoothing,machine-translation,2,25,1.0,178,0.7911111111111111,8,1.0,1,0
180,Results,,,machine-translation,2,0,0.0,179,0.7955555555555556,0,0.0,1,0
181,Machine Translation,Results,,machine-translation,2,1,0.0714285714285714,180,0.8,0,0.0,1,0
182,"On the WMT 2014 English - to - German translation task , the big transformer model ( Transformer ( big ) in ) outperforms the best previously reported models ( including ensembles ) by more than 2.0 BLEU , establishing anew state - of - the - art BLEU score of 28.4 .",Results,Machine Translation,machine-translation,2,2,0.1428571428571428,181,0.8044444444444444,1,0.0769230769230769,1,1
183,The configuration of this model is listed in the bottom line of .,Results,Machine Translation,machine-translation,2,3,0.2142857142857142,182,0.8088888888888889,2,0.1538461538461538,1,0
184,Training took 3.5 days on 8 P100 GPUs .,Results,Machine Translation,machine-translation,2,4,0.2857142857142857,183,0.8133333333333334,3,0.2307692307692307,1,0
185,"Even our base model surpasses all previously published models and ensembles , at a fraction of the training cost of any of the competitive models .",Results,Machine Translation,machine-translation,2,5,0.3571428571428571,184,0.8177777777777778,4,0.3076923076923077,1,0
186,"On the WMT 2014 English - to - French translation task , our big model achieves a BLEU score of 41.0 , outperforming all of the previously published single models , at less than 1 / 4 the training cost of the previous state - of - the - art model .",Results,Machine Translation,machine-translation,2,6,0.4285714285714285,185,0.8222222222222222,5,0.3846153846153846,1,1
187,"The Transformer ( big ) model trained for English - to - French used dropout rate P drop = 0.1 , instead of 0.3 .",Results,Machine Translation,machine-translation,2,7,0.5,186,0.8266666666666667,6,0.4615384615384615,1,0
188,"For the base models , we used a single model obtained by averaging the last 5 checkpoints , which were written at 10 - minute intervals .",Results,Machine Translation,machine-translation,2,8,0.5714285714285714,187,0.8311111111111111,7,0.5384615384615384,1,0
189,"For the big models , we averaged the last 20 checkpoints .",Results,Machine Translation,machine-translation,2,9,0.6428571428571429,188,0.8355555555555556,8,0.6153846153846154,1,0
190,We used beam search with abeam size of 4 and length penalty ? = 0.6 .,Results,Machine Translation,machine-translation,2,10,0.7142857142857143,189,0.84,9,0.6923076923076923,1,0
191,These hyperparameters were chosen after experimentation on the development set .,Results,Machine Translation,machine-translation,2,11,0.7857142857142857,190,0.8444444444444444,10,0.7692307692307693,1,0
192,"We set the maximum output length during inference to input length + 50 , but terminate early when possible .",Results,Machine Translation,machine-translation,2,12,0.8571428571428571,191,0.8488888888888889,11,0.8461538461538461,1,0
193,summarizes our results and compares our translation quality and training costs to other model architectures from the literature .,Results,Machine Translation,machine-translation,2,13,0.9285714285714286,192,0.8533333333333334,12,0.9230769230769232,1,0
194,"We estimate the number of floating point operations used to train a model by multiplying the training time , the number of GPUs used , and an estimate of the sustained single - precision floating - point capacity of each GPU 5 .",Results,Machine Translation,machine-translation,2,14,1.0,193,0.8577777777777778,13,1.0,1,0
195,Model Variations,,,machine-translation,2,0,0.0,194,0.8622222222222222,0,0.0,1,0
196,"To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English - to - German translation on the development set , newstest2013 .",Model Variations,Model Variations,machine-translation,2,1,0.0476190476190476,195,0.8666666666666667,1,0.1111111111111111,1,0
197,"We used beam search as described in the previous section , but no checkpoint averaging .",Model Variations,Model Variations,machine-translation,2,2,0.0952380952380952,196,0.8711111111111111,2,0.2222222222222222,1,0
198,We present these results in .,Model Variations,Model Variations,machine-translation,2,3,0.1428571428571428,197,0.8755555555555555,3,0.3333333333333333,1,0
199,"In rows ( A ) , we vary the number of attention heads and the attention key and value dimensions , keeping the amount of computation constant , as described in Section 3.2.2 .",Model Variations,Model Variations,machine-translation,2,4,0.1904761904761904,198,0.88,4,0.4444444444444444,1,0
200,"While single - head attention is 0.9 BLEU worse than the best setting , quality also drops off with too many heads .",Model Variations,Model Variations,machine-translation,2,5,0.238095238095238,199,0.8844444444444445,5,0.5555555555555556,1,0
201,"In rows ( B ) , we observe that reducing the attention key size d k hurts model quality .",Model Variations,Model Variations,machine-translation,2,6,0.2857142857142857,200,0.8888888888888888,6,0.6666666666666666,1,0
202,This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product maybe beneficial .,Model Variations,Model Variations,machine-translation,2,7,0.3333333333333333,201,0.8933333333333333,7,0.7777777777777778,1,0
203,"We further observe in rows ( C ) and ( D ) that , as expected , bigger models are better , and dropout is very helpful in avoiding over-fitting .",Model Variations,Model Variations,machine-translation,2,8,0.3809523809523809,202,0.8977777777777778,8,0.8888888888888888,1,0
204,"In row ( E ) we replace our sinusoidal positional encoding with learned positional embeddings , and observe nearly identical results to the base model .",Model Variations,Model Variations,machine-translation,2,9,0.4285714285714285,203,0.9022222222222224,9,1.0,1,0
205,English Constituency Parsing,Model Variations,,machine-translation,2,10,0.4761904761904761,204,0.9066666666666666,0,0.0,1,0
206,To evaluate if the Transformer can generalize to other tasks we performed experiments on English constituency parsing .,Model Variations,English Constituency Parsing,machine-translation,2,11,0.5238095238095238,205,0.9111111111111112,1,0.0909090909090909,1,0
207,This task presents specific challenges : the output is subject to strong structural constraints and is significantly longer than the input .,Model Variations,English Constituency Parsing,machine-translation,2,12,0.5714285714285714,206,0.9155555555555556,2,0.1818181818181818,1,0
208,"Furthermore , RNN sequence - to - sequence models have not been able to attain state - of - the - art results in small - data regimes .",Model Variations,English Constituency Parsing,machine-translation,2,13,0.6190476190476191,207,0.92,3,0.2727272727272727,1,0
209,"We trained a 4 - layer transformer with d model = 1024 on the Wall Street Journal ( WSJ ) portion of the Penn Treebank , about 40 K training sentences .",Model Variations,English Constituency Parsing,machine-translation,2,14,0.6666666666666666,208,0.9244444444444444,4,0.3636363636363636,1,0
210,"We also trained it in a semi-supervised setting , using the larger high - confidence and BerkleyParser corpora from with approximately 17M sentences .",Model Variations,English Constituency Parsing,machine-translation,2,15,0.7142857142857143,209,0.9288888888888888,5,0.4545454545454545,1,0
211,We used a vocabulary of 16 K tokens for the WSJ only setting and a vocabulary of 32 K tokens for the semi-supervised setting .,Model Variations,English Constituency Parsing,machine-translation,2,16,0.7619047619047619,210,0.9333333333333332,6,0.5454545454545454,1,0
212,"We performed only a small number of experiments to select the dropout , both attention and residual ( section 5.4 ) , learning rates and beam size on the Section 22 development set , all other parameters remained unchanged from the English - to - German base translation model .",Model Variations,English Constituency Parsing,machine-translation,2,17,0.8095238095238095,211,0.9377777777777778,7,0.6363636363636364,1,0
213,"During inference , we increased the maximum output length to input length + 300 .",Model Variations,English Constituency Parsing,machine-translation,2,18,0.8571428571428571,212,0.9422222222222222,8,0.7272727272727273,1,0
214,We used abeam size of 21 and ? = 0.3 for both WSJ only and the semi-supervised setting .,Model Variations,English Constituency Parsing,machine-translation,2,19,0.9047619047619048,213,0.9466666666666668,9,0.8181818181818182,1,0
215,"Our results in show that despite the lack of task - specific tuning our model performs surprisingly well , yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar .",Model Variations,English Constituency Parsing,machine-translation,2,20,0.9523809523809524,214,0.9511111111111112,10,0.9090909090909092,1,1
216,"In contrast to RNN sequence - to - sequence models , the Transformer outperforms the Berkeley - Parser even when training only on the WSJ training set of 40K sentences .",Model Variations,English Constituency Parsing,machine-translation,2,21,1.0,215,0.9555555555555556,11,1.0,1,0
217,Conclusion,,,machine-translation,2,0,0.0,216,0.96,0,0.0,1,0
218,"In this work , we presented the Transformer , the first sequence transduction model based entirely on attention , replacing the recurrent layers most commonly used in encoder - decoder architectures with multi-headed self - attention .",Conclusion,Conclusion,machine-translation,2,1,0.125,217,0.9644444444444444,1,0.125,0,0
219,"For translation tasks , the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers .",Conclusion,Conclusion,machine-translation,2,2,0.25,218,0.9688888888888888,2,0.25,0,0
220,"On both WMT 2014 English - to - German and WMT 2014 English - to - French translation tasks , we achieve anew state of the art .",Conclusion,Conclusion,machine-translation,2,3,0.375,219,0.9733333333333334,3,0.375,0,0
221,In the former task our best model outperforms even all previously reported ensembles .,Conclusion,Conclusion,machine-translation,2,4,0.5,220,0.9777777777777776,4,0.5,0,0
222,We are excited about the future of attention - based models and plan to apply them to other tasks .,Conclusion,Conclusion,machine-translation,2,5,0.625,221,0.9822222222222222,5,0.625,0,0
223,"We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local , restricted attention mechanisms to efficiently handle large inputs and outputs such as images , audio and video .",Conclusion,Conclusion,machine-translation,2,6,0.75,222,0.9866666666666668,6,0.75,0,0
224,Making generation less sequential is another research goals of ours .,Conclusion,Conclusion,machine-translation,2,7,0.875,223,0.9911111111111112,7,0.875,0,0
225,The code we used to train and evaluate our models is available at https://github.com/ tensorflow/tensor2tensor.,Conclusion,Conclusion,machine-translation,2,8,1.0,224,0.9955555555555556,8,1.0,0,0
1,title,,,machine-translation,3,0,0.0,0,0.0,0,0.0,1,0
2,Deep Recurrent Models with Fast - Forward Connections for Neural Machine Translation,title,title,machine-translation,3,1,0.0,1,0.0031948881789137,1,0.0,1,1
3,abstract,,,machine-translation,3,0,0.0,2,0.0063897763578274,0,0.0,1,0
4,Neural machine translation ( NMT ) aims at solving machine translation ( MT ) problems using neural networks and has exhibited promising results in recent years .,abstract,abstract,machine-translation,3,1,0.1111111111111111,3,0.0095846645367412,1,0.1111111111111111,1,1
5,"However , most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system .",abstract,abstract,machine-translation,3,2,0.2222222222222222,4,0.0127795527156549,2,0.2222222222222222,1,0
6,"In this work , we introduce anew type of linear connections , named fastforward connections , based on deep Long Short - Term Memory ( LSTM ) networks , and an interleaved bi-directional architecture for stacking the LSTM layers .",abstract,abstract,machine-translation,3,3,0.3333333333333333,5,0.0159744408945686,3,0.3333333333333333,1,0
7,Fast - forward connections play an essential role in propagating the gradients and building a deep topology of depth 16 .,abstract,abstract,machine-translation,3,4,0.4444444444444444,6,0.0191693290734824,4,0.4444444444444444,1,0
8,"On the WMT ' 14 Englishto - French task , we achieve BLEU = 37.7 with a single attention model , which outperforms the corresponding single shallow model by 6.2 BLEU points .",abstract,abstract,machine-translation,3,5,0.5555555555555556,7,0.0223642172523961,5,0.5555555555555556,1,0
9,This is the first time that a single NMT model achieves state - of - the - art performance and outperforms the best conventional model by 0.7 BLEU points .,abstract,abstract,machine-translation,3,6,0.6666666666666666,8,0.0255591054313099,6,0.6666666666666666,1,0
10,We can still achieve BLEU = 36.3 even without using an attention mechanism .,abstract,abstract,machine-translation,3,7,0.7777777777777778,9,0.0287539936102236,7,0.7777777777777778,1,0
11,"After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 .",abstract,abstract,machine-translation,3,8,0.8888888888888888,10,0.0319488817891373,8,0.8888888888888888,1,0
12,Our models are also validated on the more difficult WMT ' 14 English - to - German task .,abstract,abstract,machine-translation,3,9,1.0,11,0.0351437699680511,9,1.0,1,0
13,Introduction,,,machine-translation,3,0,0.0,12,0.0383386581469648,0,0.0,1,0
14,Neural machine translation ( NMT ) has attracted a lot of interest in solving the machine translation ( MT ) problem in recent years .,Introduction,Introduction,machine-translation,3,1,0.0067114093959731,13,0.0415335463258785,1,0.0344827586206896,1,0
15,"Unlike conventional statistical machine translation ( SMT ) systems which consist of multiple separately tuned components , NMT models encode the source sequence into continuous representation space and generate the target sequence in an end - to - end fashon .",Introduction,Introduction,machine-translation,3,2,0.0134228187919463,14,0.0447284345047923,2,0.0689655172413793,1,0
16,"Moreover , NMT models can also be easily adapted to other tasks such as dialog systems , question answering systems and image caption generation .",Introduction,Introduction,machine-translation,3,3,0.0201342281879194,15,0.047923322683706,3,0.1034482758620689,1,0
17,"In general , there are two types of NMT topologies : the encoder - decoder network and the attention network .",Introduction,Introduction,machine-translation,3,4,0.0268456375838926,16,0.0511182108626198,4,0.1379310344827586,1,0
18,The encoder - decoder network represents the source sequence with a fixed dimensional vector and the target sequence is generated from this vector word byword .,Introduction,Introduction,machine-translation,3,5,0.0335570469798657,17,0.0543130990415335,5,0.1724137931034483,1,0
19,The attention network uses the representations from all time steps of the input sequence to build a detailed relationship between the target words and the input words .,Introduction,Introduction,machine-translation,3,6,0.0402684563758389,18,0.0575079872204472,6,0.2068965517241379,1,0
20,Recent results show that the systems based on these models can achieve similar performance to conventional SMT systems .,Introduction,Introduction,machine-translation,3,7,0.046979865771812,19,0.060702875399361,7,0.2413793103448276,1,0
21,"However , a single neural model of either of the above types has not been competitive with the best conventional system when evaluated on the WMT ' 14 English - to - French task .",Introduction,Introduction,machine-translation,3,8,0.0536912751677852,20,0.0638977635782747,8,0.2758620689655172,1,0
22,The best BLEU score from a single model with six layers is only 31.5 while the conventional method of achieves 37.0 .,Introduction,Introduction,machine-translation,3,9,0.0604026845637583,21,0.0670926517571885,9,0.3103448275862069,1,0
23,We focus on improving the single model perfor - mance by increasing the model depth .,Introduction,Introduction,machine-translation,3,10,0.0671140939597315,22,0.0702875399361022,10,0.3448275862068966,1,0
24,Deep topology has been proven to outperform the shallow architecture in computer vision .,Introduction,Introduction,machine-translation,3,11,0.0738255033557047,23,0.0734824281150159,11,0.3793103448275862,1,0
25,In the past two years the top positions of the ImageNet contest have always been occupied by systems with tensor even hundreds of layers .,Introduction,Introduction,machine-translation,3,12,0.0805369127516778,24,0.0766773162939297,12,0.4137931034482758,1,0
26,"But in NMT , the biggest depth used successfully is only six .",Introduction,Introduction,machine-translation,3,13,0.087248322147651,25,0.0798722044728434,13,0.4482758620689655,1,0
27,We attribute this problem to the properties of the Long Short - Term Memory ( LSTM ) which is widely used in NMT .,Introduction,Introduction,machine-translation,3,14,0.0939597315436241,26,0.0830670926517571,14,0.4827586206896552,1,0
28,"In the LSTM , there are more non-linear activations than in convolution layers .",Introduction,Introduction,machine-translation,3,15,0.1006711409395973,27,0.0862619808306709,15,0.5172413793103449,1,0
29,"These activations significantly decrease the magnitude of the gradient in the deep topology , especially when the gradient propagates in recurrent form .",Introduction,Introduction,machine-translation,3,16,0.1073825503355704,28,0.0894568690095846,16,0.5517241379310345,1,0
30,"There are also many efforts to increase the depth of the LSTM such as the work by , where the shortcuts do not avoid the nonlinear and recurrent computation .",Introduction,Introduction,machine-translation,3,17,0.1140939597315436,29,0.0926517571884984,17,0.5862068965517241,1,0
31,"In this work , we introduce anew type of linear connections for multi - layer recurrent networks .",Introduction,Introduction,machine-translation,3,18,0.1208053691275167,30,0.0958466453674121,18,0.6206896551724138,1,1
32,"These connections , which are called fast - forward connections , play an essential role in building a deep topology with depth of 16 .",Introduction,Introduction,machine-translation,3,19,0.1275167785234899,31,0.0990415335463258,19,0.6551724137931034,1,1
33,"In addition , we introduce an interleaved bi-directional architecture to stack LSTM layers in the encoder .",Introduction,Introduction,machine-translation,3,20,0.1342281879194631,32,0.1022364217252396,20,0.6896551724137931,1,1
34,This topology can be used for both the encoder - decoder network and the attention network .,Introduction,Introduction,machine-translation,3,21,0.1409395973154362,33,0.1054313099041533,21,0.7241379310344828,1,0
35,"On the WMT ' 14 Englishto - French task , this is the deepest NMT topology that has ever been investigated .",Introduction,Introduction,machine-translation,3,22,0.1476510067114094,34,0.108626198083067,22,0.7586206896551724,1,0
36,"With our deep attention model , the BLEU score can be improved to 37.7 outperforming the shallow model which has six layers by 6.2 BLEU points .",Introduction,Introduction,machine-translation,3,23,0.1543624161073825,35,0.1118210862619808,23,0.7931034482758621,1,0
37,This is also the first time on this task that a single NMT model achieves state - of - the - art performance and outperforms the best conventional SMT system with an improvement of 0.7 .,Introduction,Introduction,machine-translation,3,24,0.1610738255033557,36,0.1150159744408945,24,0.8275862068965517,1,0
38,"Even without using the attention mechanism , we can still achieve 36.3 with a single model .",Introduction,Introduction,machine-translation,3,25,0.1677852348993288,37,0.1182108626198083,25,0.8620689655172413,1,0
39,"After model ensembling and unknown word processing , the BLEU score can be further improved to 40.4 .",Introduction,Introduction,machine-translation,3,26,0.174496644295302,38,0.121405750798722,26,0.896551724137931,1,0
40,"When evaluated on the subset of the test corpus without unknown words , our model achieves 41.4 .",Introduction,Introduction,machine-translation,3,27,0.1812080536912751,39,0.1246006389776357,27,0.9310344827586208,1,0
41,"As a reference , previous work showed that oracle rescoring of the 1000 - best sequences generated by the SMT model can achieve the BLEU score of about 45 .",Introduction,Introduction,machine-translation,3,28,0.1879194630872483,40,0.1277955271565495,28,0.9655172413793104,1,0
42,Our models are also validated on the more difficult WMT ' 14 English - to - German task .,Introduction,Introduction,machine-translation,3,29,0.1946308724832214,41,0.1309904153354632,29,1.0,1,0
43,Neural Machine Translation,Introduction,,machine-translation,3,30,0.2013422818791946,42,0.134185303514377,0,0.0,1,0
44,"Neural machine translation aims at generating the target word sequence y = {y 1 , . . . , y n } given the source word sequence x = {x 1 , . . . , x m } with neural models .",Introduction,Neural Machine Translation,machine-translation,3,31,0.2080536912751678,43,0.1373801916932907,1,0.032258064516129,1,0
45,"In this task , the likelihood p ( y | x , ? ) of the target sequence will be maximized with parameter ?",Introduction,Neural Machine Translation,machine-translation,3,32,0.2147651006711409,44,0.1405750798722044,2,0.064516129032258,1,0
46,to learn :,Introduction,Neural Machine Translation,machine-translation,3,33,0.2214765100671141,45,0.1437699680511182,3,0.0967741935483871,1,0
47,where y 0:j?1 is the sub sequence from y 0 toy j?1 . y 0 and y m + 1 denote the start mark and end mark of target sequence respectively .,Introduction,Neural Machine Translation,machine-translation,3,34,0.2281879194630872,46,0.1469648562300319,4,0.1290322580645161,1,0
48,"The process can be explicitly split into an encoding part , a decoding part and the interface between these two parts .",Introduction,Neural Machine Translation,machine-translation,3,35,0.2348993288590604,47,0.1501597444089457,5,0.1612903225806451,1,0
49,"In the encoding part , the source sequence is processed and transformed into a group of vectors e = {e 1 , , em } for each time step .",Introduction,Neural Machine Translation,machine-translation,3,36,0.2416107382550335,48,0.1533546325878594,6,0.1935483870967742,1,0
50,Further operations will be used at the interface part to extract the final representation c of the source sequence from e.,Introduction,Neural Machine Translation,machine-translation,3,37,0.2483221476510067,49,0.1565495207667731,7,0.2258064516129032,1,0
51,"At the decoding step , the target sequence is generated from the representation c.",Introduction,Neural Machine Translation,machine-translation,3,38,0.2550335570469799,50,0.1597444089456869,8,0.2580645161290322,1,0
52,"Recently , there have been two types of NMT models which are different in the interface part .",Introduction,Neural Machine Translation,machine-translation,3,39,0.261744966442953,51,0.1629392971246006,9,0.2903225806451613,1,0
53,"In the encoder - decoder model , a single vector extracted from e is used as the representation .",Introduction,Neural Machine Translation,machine-translation,3,40,0.2684563758389262,52,0.1661341853035143,10,0.3225806451612903,1,0
54,"In the attention model , c is dynamically obtained according to the relationship between the target sequence and the source sequence .",Introduction,Neural Machine Translation,machine-translation,3,41,0.2751677852348993,53,0.1693290734824281,11,0.3548387096774194,1,0
55,"The recurrent neural network ( RNN ) , or its specific form the LSTM , is generally used as the basic unit of the encoding and decoding part .",Introduction,Neural Machine Translation,machine-translation,3,42,0.2818791946308724,54,0.1725239616613418,12,0.3870967741935484,1,0
56,"However , the topology of most of the existing models is shallow .",Introduction,Neural Machine Translation,machine-translation,3,43,0.2885906040268456,55,0.1757188498402556,13,0.4193548387096774,1,0
57,"In the attention network , the encoding part and the decoding part have only one LSTM layer respectively .",Introduction,Neural Machine Translation,machine-translation,3,44,0.2953020134228188,56,0.1789137380191693,14,0.4516129032258064,1,0
58,"In the encoder - decoder network , researchers have used at most six LSTM layers .",Introduction,Neural Machine Translation,machine-translation,3,45,0.3020134228187919,57,0.182108626198083,15,0.4838709677419355,1,0
59,"Because machine translation is a difficult problem , we believe more complex encoding and decoding architecture is needed for modeling the relationship between the source sequence and the target sequence .",Introduction,Neural Machine Translation,machine-translation,3,46,0.3087248322147651,58,0.1853035143769968,16,0.5161290322580645,1,0
60,"In this work , we focus on enhancing the complexity of the encoding / decoding architecture by increasing the model depth .",Introduction,Neural Machine Translation,machine-translation,3,47,0.3154362416107382,59,0.1884984025559105,17,0.5483870967741935,1,0
61,Deep neural models have been studied in a wide range of problems .,Introduction,Neural Machine Translation,machine-translation,3,48,0.3221476510067114,60,0.1916932907348242,18,0.5806451612903226,1,0
62,"In computer vision , models with more than ten convolution layers outperform shallow ones on a series of image tasks in recent years .",Introduction,Neural Machine Translation,machine-translation,3,49,0.3288590604026846,61,0.194888178913738,19,0.6129032258064516,1,0
63,Different kinds of shortcut connections are proposed to decrease the length of the gradient propagation path .,Introduction,Neural Machine Translation,machine-translation,3,50,0.3355704697986577,62,0.1980830670926517,20,0.6451612903225806,1,0
64,"Training networks based on LSTM layers , which are widely used in language problems , is a much more challenging task .",Introduction,Neural Machine Translation,machine-translation,3,51,0.3422818791946309,63,0.2012779552715655,21,0.6774193548387096,1,0
65,"Because of the existence of many more nonlinear activations and the recurrent computation , gradient values are not stable and are generally smaller .",Introduction,Neural Machine Translation,machine-translation,3,52,0.348993288590604,64,0.2044728434504792,22,0.7096774193548387,1,0
66,"Following the same spirit for convolutional networks , a lot of effort has also been spent on training deep LSTM networks .",Introduction,Neural Machine Translation,machine-translation,3,53,0.3557046979865771,65,0.2076677316293929,23,0.7419354838709677,1,0
67,"introduced depth - gated shortcuts , connecting LSTM cells at adjacent layers , to provide a fast way to propagate the gradients .",Introduction,Neural Machine Translation,machine-translation,3,54,0.3624161073825503,66,0.2108626198083067,24,0.7741935483870968,1,0
68,They validated the modification of these shortcuts on an MT task and a language modeling task .,Introduction,Neural Machine Translation,machine-translation,3,55,0.3691275167785235,67,0.2140575079872204,25,0.8064516129032258,1,0
69,"However , the best score was obtained using models with three layers .",Introduction,Neural Machine Translation,machine-translation,3,56,0.3758389261744966,68,0.2172523961661341,26,0.8387096774193549,1,0
70,"Similarly , proposed a two dimensional structure for the LSTM .",Introduction,Neural Machine Translation,machine-translation,3,57,0.3825503355704698,69,0.2204472843450479,27,0.8709677419354839,1,0
71,Their structure decreases the number of nonlinear activations and path length .,Introduction,Neural Machine Translation,machine-translation,3,58,0.3892617449664429,70,0.2236421725239616,28,0.9032258064516128,1,0
72,"However , the gradient propagation still relies on the recurrent computation .",Introduction,Neural Machine Translation,machine-translation,3,59,0.3959731543624161,71,0.2268370607028754,29,0.935483870967742,1,0
73,"The investigations were also made on question - answering to encode the questions , whereat most two LSTM layers were stacked .",Introduction,Neural Machine Translation,machine-translation,3,60,0.4026845637583892,72,0.2300319488817891,30,0.967741935483871,1,0
74,"Based on the above considerations , we propose new connections to facilitate gradient propagation in the following section .",Introduction,Neural Machine Translation,machine-translation,3,61,0.4093959731543624,73,0.2332268370607028,31,1.0,1,0
75,Deep Topology,Introduction,,machine-translation,3,62,0.4161073825503356,74,0.2364217252396166,0,0.0,1,0
76,We build the deep LSTM network with the new proposed linear connections .,Introduction,Deep Topology,machine-translation,3,63,0.4228187919463087,75,0.2396166134185303,1,0.25,1,0
77,The shortest paths through the proposed connections do not include any nonlinear transformations and do not rely on any recurrent computation .,Introduction,Deep Topology,machine-translation,3,64,0.4295302013422818,76,0.242811501597444,2,0.5,1,0
78,We call these connections fastforward connections .,Introduction,Deep Topology,machine-translation,3,65,0.436241610738255,77,0.2460063897763578,3,0.75,1,0
79,"Within the deep topology , we also introduce an interleaved bi-directional architecture to stack the LSTM layers .",Introduction,Deep Topology,machine-translation,3,66,0.4429530201342282,78,0.2492012779552715,4,1.0,1,0
80,Network,Introduction,,machine-translation,3,67,0.4496644295302013,79,0.2523961661341853,0,0.0,1,0
81,Our entire deep neural network is shown in .,Introduction,Network,machine-translation,3,68,0.4563758389261745,80,0.255591054313099,1,0.0121951219512195,1,0
82,"This topology can be divided into three parts : the encoder part ( P -E ) on the left , the decoder part ( P - D ) on the right and the interface between these two parts ( P - I ) which extracts the representation of the source sequence .",Introduction,Network,machine-translation,3,69,0.4630872483221476,81,0.2587859424920127,2,0.024390243902439,1,0
83,"We have two instantiations of this topology : Deep - ED and Deep - Att , which correspond to the extension of the encoder - decoder network and the attention network respectively .",Introduction,Network,machine-translation,3,70,0.4697986577181208,82,0.2619808306709265,3,0.0365853658536585,1,0
84,Our main innovation is the novel scheme for connecting adjacent recurrent layers .,Introduction,Network,machine-translation,3,71,0.4765100671140939,83,0.2651757188498402,4,0.048780487804878,1,0
85,We will start with the basic RNN model for the sake of clarity .,Introduction,Network,machine-translation,3,72,0.4832214765100671,84,0.268370607028754,5,0.0609756097560975,1,0
86,Recurrent layer :,Introduction,Network,machine-translation,3,73,0.4899328859060403,85,0.2715654952076677,6,0.073170731707317,1,0
87,"When an input sequence {x 1 , . . . , x m } is given to a recurrent layer , the output ht at each time step t can be computed as ( see )",Introduction,Network,machine-translation,3,74,0.4966442953020134,86,0.2747603833865815,7,0.0853658536585365,1,0
88,where the bias parameter is not included for simplicity .,Introduction,Network,machine-translation,3,75,0.5033557046979866,87,0.2779552715654952,8,0.0975609756097561,1,0
89,We use a red circle and a blue empty square to denote an input and a hidden state .,Introduction,Network,machine-translation,3,76,0.5100671140939598,88,0.2811501597444089,9,0.1097560975609756,1,0
90,"A blue square with a "" - "" denotes the previous hidden state .",Introduction,Network,machine-translation,3,77,0.5167785234899329,89,0.2843450479233226,10,0.1219512195121951,1,0
91,A dotted line means that the hidden state is used recurrently .,Introduction,Network,machine-translation,3,78,0.5234899328859061,90,0.2875399361022364,11,0.1341463414634146,1,0
92,This computation can be equivalently split into two consecutive steps :,Introduction,Network,machine-translation,3,79,0.5302013422818792,91,0.2907348242811501,12,0.1463414634146341,1,0
93,"Feed-Forward computation : ft = W f x t . Left part in ) . "" f "" block .",Introduction,Network,machine-translation,3,80,0.5369127516778524,92,0.2939297124600639,13,0.1585365853658536,1,0
94,"Recurrent computation : RNN ( f t , h t?1 ) .",Introduction,Network,machine-translation,3,81,0.5436241610738255,93,0.2971246006389776,14,0.1707317073170731,1,0
95,Right part and the sum operation ( + ) followed by activation in .,Introduction,Network,machine-translation,3,82,0.5503355704697986,94,0.3003194888178914,15,0.1829268292682926,1,0
96,""" r "" block .",Introduction,Network,machine-translation,3,83,0.5570469798657718,95,0.3035143769968051,16,0.1951219512195122,1,0
97,"For a deep topology with stacked recurrent layers , the input of each block "" f "" at recurrent layer k ( denoted by f k ) is usually the output of block "" r "" at its previous recurrent layer k ?",Introduction,Network,machine-translation,3,84,0.5637583892617449,96,0.3067092651757188,17,0.2073170731707317,1,0
98,1 ( denoted by h k?1 ) .,Introduction,Network,machine-translation,3,85,0.5704697986577181,97,0.3099041533546325,18,0.2195121951219512,1,0
99,"In our work , we add fast - forward connections ( F - F connections ) which connect two feed - forward computation blocks "" f "" of adjacent recurrent layers .",Introduction,Network,machine-translation,3,86,0.5771812080536913,98,0.3130990415335463,19,0.2317073170731707,1,0
100,"It means that each block "" f "" at recurrent layer k takes both the outputs of block "" f "" and block "" r "" at its previous layer as input ( ) .",Introduction,Network,machine-translation,3,87,0.5838926174496645,99,0.31629392971246,20,0.2439024390243902,1,0
101,F - F connections are denoted by dashed red lines in and .,Introduction,Network,machine-translation,3,88,0.5906040268456376,100,0.3194888178913738,21,0.2560975609756097,1,0
102,The path of F - F connections contains neither nonlinear activations nor recurrent computation .,Introduction,Network,machine-translation,3,89,0.5973154362416108,101,0.3226837060702875,22,0.2682926829268293,1,0
103,"It provides a fast path for information to propagate , so we call this path fast - forward connections .",Introduction,Network,machine-translation,3,90,0.6040268456375839,102,0.3258785942492013,23,0.2804878048780488,1,0
104,"Additionally , in order to learn more temporal dependencies , the sequences can be processed in different directions at each pair of adjacent recurrent layers .",Introduction,Network,machine-translation,3,91,0.610738255033557,103,0.329073482428115,24,0.2926829268292683,1,0
105,This is quantitatively expressed in Eq. 3 :,Introduction,Network,machine-translation,3,92,0.6174496644295302,104,0.3322683706070287,25,0.3048780487804878,1,0
106,The opposite directions are marked by the direction term ( ? 1 ) k .,Introduction,Network,machine-translation,3,93,0.6241610738255033,105,0.3354632587859425,26,0.3170731707317073,1,0
107,"At the first recurrent layer , the block "" f "" takes x t as the input .",Introduction,Network,machine-translation,3,94,0.6308724832214765,106,0.3386581469648562,27,0.3292682926829268,1,0
108,"[ , ] denotes the concatenation of vectors .",Introduction,Network,machine-translation,3,95,0.6375838926174496,107,0.3418530351437699,28,0.3414634146341463,1,0
109,This is shown in .,Introduction,Network,machine-translation,3,96,0.6442953020134228,108,0.3450479233226837,29,0.3536585365853658,1,0
110,The two changes are summarized here :,Introduction,Network,machine-translation,3,97,0.6510067114093959,109,0.3482428115015974,30,0.3658536585365853,1,0
111,We add a connection between f kt and f k ?1 t .,Introduction,Network,machine-translation,3,98,0.6577181208053692,110,0.3514376996805112,31,0.3780487804878049,1,0
112,"Without f k ?1 t , our model will be reduced to the traditional stacked model .",Introduction,Network,machine-translation,3,99,0.6644295302013423,111,0.3546325878594249,32,0.3902439024390244,1,0
113,We alternate the RNN direction at different layers k with the direction term ( ? 1 ) k .,Introduction,Network,machine-translation,3,100,0.6711409395973155,112,0.3578274760383386,33,0.4024390243902439,1,0
114,"If we fix the direction term to ? 1 , all layers work in the forward direction .",Introduction,Network,machine-translation,3,101,0.6778523489932886,113,0.3610223642172524,34,0.4146341463414634,1,0
115,LSTM layer :,Introduction,Network,machine-translation,3,102,0.6845637583892618,114,0.3642172523961661,35,0.4268292682926829,1,0
116,"In our experiments , instead of an RNN , a specific type of recurrent layer called LSTM ) is used .",Introduction,Network,machine-translation,3,103,0.6912751677852349,115,0.3674121405750798,36,0.4390243902439024,1,0
117,The LSTM is structurally more complex than the basic RNN in Eq .,Introduction,Network,machine-translation,3,104,0.697986577181208,116,0.3706070287539936,37,0.4512195121951219,1,0
118,"2 . We define the computation of the LSTM as a function which maps the input f and its state - output pair ( h , s ) at the previous time step to the current stateoutput pair .",Introduction,Network,machine-translation,3,105,0.7046979865771812,117,0.3738019169329074,38,0.4634146341463415,1,0
119,"The exact computations for ( h t , st ) = LSTM ( f t , h t?1 , s t?1 ) are the following :",Introduction,Network,machine-translation,3,106,0.7114093959731543,118,0.3769968051118211,39,0.4756097560975609,1,0
120,"is the concatenation of four vectors of equal size , means element - wise multiplication , ?",Introduction,Network,machine-translation,3,107,0.7181208053691275,119,0.3801916932907348,40,0.4878048780487805,1,0
121,"i is the input activation function , ?",Introduction,Network,machine-translation,3,108,0.7248322147651006,120,0.3833865814696485,41,0.5,1,0
122,"o is the output activation function , ?",Introduction,Network,machine-translation,3,109,0.7315436241610739,121,0.3865814696485623,42,0.5121951219512195,1,0
123,"g is the activation function for gates , and W r , ? ? , ? ? , and ? ?",Introduction,Network,machine-translation,3,110,0.738255033557047,122,0.389776357827476,43,0.524390243902439,1,0
124,are the parameters of the LSTM .,Introduction,Network,machine-translation,3,111,0.7449664429530202,123,0.3929712460063898,44,0.5365853658536586,1,0
125,It is slightly different from the standard notation in that we do not have a matrix to multiply with the input fin our notation .,Introduction,Network,machine-translation,3,112,0.7516778523489933,124,0.3961661341853035,45,0.5487804878048781,1,0
126,"With this notation , we can write down the computations for our deep bi-directional LSTM model with F - F connections :",Introduction,Network,machine-translation,3,113,0.7583892617449665,125,0.3993610223642173,46,0.5609756097560976,1,0
127,where x t is the input to the deep bi-directional LSTM model .,Introduction,Network,machine-translation,3,114,0.7651006711409396,126,0.402555910543131,47,0.573170731707317,1,0
128,"For the encoder , x t is the embedding of the t th word in the source sentence .",Introduction,Network,machine-translation,3,115,0.7718120805369127,127,0.4057507987220447,48,0.5853658536585366,1,0
129,For the decoder x t is the concatenation of the embedding of the t th word in the target sentence and the encoder representation for step t.,Introduction,Network,machine-translation,3,116,0.7785234899328859,128,0.4089456869009584,49,0.5975609756097561,1,0
130,"In our final model two additional operations are used with Eq. 5 , which is shown in Eq .",Introduction,Network,machine-translation,3,117,0.785234899328859,129,0.4121405750798722,50,0.6097560975609756,1,0
131,"6 . Half ( f ) denotes the first half of the elements off , and Dr ( h ) is the dropout operation which randomly sets an element of h to zero with a certain probability .",Introduction,Network,machine-translation,3,118,0.7919463087248322,130,0.4153354632587859,51,0.6219512195121951,1,0
132,The use of Half ( ) is to reduce the parameter size and does not affect the performance .,Introduction,Network,machine-translation,3,119,0.7986577181208053,131,0.4185303514376997,52,0.6341463414634146,1,0
133,"We observed noticeable performance degradation when using only the first third of the elements of "" f "" .",Introduction,Network,machine-translation,3,120,0.8053691275167785,132,0.4217252396166134,53,0.6463414634146342,1,0
134,"With the F - F connections , we build a fast channel to propagate the gradients in the deep topology .",Introduction,Network,machine-translation,3,121,0.8120805369127517,133,0.4249201277955272,54,0.6585365853658537,1,0
135,F - F connections can accelerate the model convergence and while improving the performance .,Introduction,Network,machine-translation,3,122,0.8187919463087249,134,0.4281150159744409,55,0.6707317073170732,1,0
136,A similar idea was also used in .,Introduction,Network,machine-translation,3,123,0.825503355704698,135,0.4313099041533546,56,0.6829268292682927,1,0
137,Encoder : The LSTM layers are stacked following Eq. 5 .,Introduction,Network,machine-translation,3,124,0.8322147651006712,136,0.4345047923322683,57,0.6951219512195121,1,0
138,We call this type of encoder interleaved bidirectional encoder .,Introduction,Network,machine-translation,3,125,0.8389261744966443,137,0.4376996805111821,58,0.7073170731707317,1,0
139,"In addition , there are two similar columns ( a 1 and a 2 ) in the encoder part .",Introduction,Network,machine-translation,3,126,0.8456375838926175,138,0.4408945686900958,59,0.7195121951219512,1,0
140,Each column consists of n e stacked LSTM layers .,Introduction,Network,machine-translation,3,127,0.8523489932885906,139,0.4440894568690096,60,0.7317073170731707,1,0
141,There is no connection between the two columns .,Introduction,Network,machine-translation,3,128,0.8590604026845637,140,0.4472843450479233,61,0.7439024390243902,1,0
142,The first layers of the two columns process the word representations of the source sequence in different directions .,Introduction,Network,machine-translation,3,129,0.8657718120805369,141,0.4504792332268371,62,0.7560975609756098,1,0
143,"At the last LSTM layers , there are two groups of vectors representing the source sequence .",Introduction,Network,machine-translation,3,130,0.87248322147651,142,0.4536741214057508,63,0.7682926829268293,1,0
144,The group size is the same as the length of the input sequence .,Introduction,Network,machine-translation,3,131,0.8791946308724832,143,0.4568690095846645,64,0.7804878048780488,1,0
145,Interface : Prior encoder - decoder models and attention models are different in their method of extracting the representations of the source sequences .,Introduction,Network,machine-translation,3,132,0.8859060402684564,144,0.4600638977635782,65,0.7926829268292683,1,0
146,"In our work , as a consequence of the introduced F - F connections , we have 4 output vectors ( h For Deep - Att , we do not need the above two operations .",Introduction,Network,machine-translation,3,133,0.8926174496644296,145,0.463258785942492,66,0.8048780487804879,1,0
147,"We only concatenate the 4 output vectors at each time step to obtain e t , and a soft attention mechanism is used to calculate the final representation ct from e t .",Introduction,Network,machine-translation,3,134,0.8993288590604027,146,0.4664536741214057,67,0.8170731707317073,1,0
148,e t is summarized as :,Introduction,Network,machine-translation,3,135,0.906040268456376,147,0.4696485623003195,68,0.8292682926829268,1,0
149,Note that the vector dimensionality off is four times larger than that of h ( see Eq. 4 ) .,Introduction,Network,machine-translation,3,136,0.912751677852349,148,0.4728434504792332,69,0.8414634146341463,1,0
150,ct is summarized as :,Introduction,Network,machine-translation,3,137,0.9194630872483222,149,0.476038338658147,70,0.8536585365853658,1,0
151,?,Introduction,Network,machine-translation,3,138,0.9261744966442952,150,0.4792332268370607,71,0.8658536585365854,1,0
152,"t,t is the normalized attention weight computed by :",Introduction,Network,machine-translation,3,139,0.9328859060402684,151,0.4824281150159744,72,0.8780487804878049,1,0
153,"the concatenated vector e t to a vector with 1 / 4 dimension size , denoted by the ( fully connected ) block "" fc "" in .",Introduction,Network,machine-translation,3,140,0.9395973154362416,152,0.4856230031948881,73,0.8902439024390244,1,0
154,Decoder :,Introduction,Network,machine-translation,3,141,0.9463087248322148,153,0.4888178913738019,74,0.902439024390244,1,0
155,The decoder follows Eq. 5 and Eq. 6 with fixed direction term ? 1 .,Introduction,Network,machine-translation,3,142,0.953020134228188,154,0.4920127795527156,75,0.9146341463414634,1,0
156,"At the first layer , we use the following x t :",Introduction,Network,machine-translation,3,143,0.959731543624161,155,0.4952076677316294,76,0.926829268292683,1,0
157,y t?1 is the target word embedding at the previous time step and y 0 is zero .,Introduction,Network,machine-translation,3,144,0.9664429530201344,156,0.4984025559105431,77,0.9390243902439024,1,0
158,There is a single column of n d stacked LSTM layers .,Introduction,Network,machine-translation,3,145,0.9731543624161074,157,0.5015974440894568,78,0.951219512195122,1,0
159,We also use the F - F connections like those in the encoder and all layers are in the forward direction .,Introduction,Network,machine-translation,3,146,0.9798657718120806,158,0.5047923322683706,79,0.9634146341463414,1,0
160,"Note that at the last LSTM layer , we only use ht to make the prediction with a softmax layer .",Introduction,Network,machine-translation,3,147,0.9865771812080536,159,0.5079872204472844,80,0.975609756097561,1,0
161,"Although the network is deep , the training technique is straightforward .",Introduction,Network,machine-translation,3,148,0.9932885906040269,160,0.5111821086261981,81,0.9878048780487804,1,0
162,We will describe this in the next part .,Introduction,Network,machine-translation,3,149,1.0,161,0.5143769968051118,82,1.0,1,0
163,Training technique,,,machine-translation,3,0,0.0,162,0.5175718849840255,0,0.0,1,0
164,We take the parallel data as the only input without using any monolingual data for either word representation pre-training or language modeling .,Training technique,Training technique,machine-translation,3,1,0.05,163,0.5207667731629393,1,0.0833333333333333,1,0
165,"Because of the deep bi-directional structure , we do not need to reverse the sequence order as .",Training technique,Training technique,machine-translation,3,2,0.1,164,0.5239616613418531,2,0.1666666666666666,1,0
166,"The deep topology brings difficulties for the model training , especially when first order methods such as stochastic gradient descent ( SGD ) are used .",Training technique,Training technique,machine-translation,3,3,0.15,165,0.5271565495207667,3,0.25,1,0
167,The parameters should be properly initialized and the converging process can be slow .,Training technique,Training technique,machine-translation,3,4,0.2,166,0.5303514376996805,4,0.3333333333333333,1,0
168,"We tried several optimization techniques such as AdaDelta ( Zeiler , 2012 ) , RMSProp ( Tieleman and and .",Training technique,Training technique,machine-translation,3,5,0.25,167,0.5335463258785943,5,0.4166666666666667,1,0
169,We found that all of them were able to speedup the process a lot compared to simple SGD while no significant performance difference was observed among them .,Training technique,Training technique,machine-translation,3,6,0.3,168,0.536741214057508,6,0.5,1,0
170,"In this work , we chose Adam for model training and do not present a detailed comparison with other optimization methods .",Training technique,Training technique,machine-translation,3,7,0.35,169,0.5399361022364217,7,0.5833333333333334,1,0
171,Dropout is also used to avoid over-fitting .,Training technique,Training technique,machine-translation,3,8,0.4,170,0.5431309904153354,8,0.6666666666666666,1,0
172,It is utilized on the LSTM nodes h kt ( See Eq. 5 ) with a ratio of pd for both the encoder and decoder .,Training technique,Training technique,machine-translation,3,9,0.45,171,0.5463258785942492,9,0.75,1,0
173,"During the whole model training process , we keep all hyper parameters fixed without any intermediate interruption .",Training technique,Training technique,machine-translation,3,10,0.5,172,0.549520766773163,10,0.8333333333333334,1,0
174,The hyper parameters are selected according to the performance on the development set .,Training technique,Training technique,machine-translation,3,11,0.55,173,0.5527156549520766,11,0.9166666666666666,1,0
175,"For such a deep and large network , it is not easy to determine the tuning strategy and this will be considered in future work .",Training technique,Training technique,machine-translation,3,12,0.6,174,0.5559105431309904,12,1.0,1,0
176,Generation,Training technique,,machine-translation,3,13,0.65,175,0.5591054313099042,0,0.0,1,0
177,We use the common left - to - right beam - search method for sequence generation .,Training technique,Generation,machine-translation,3,14,0.7,176,0.5623003194888179,1,0.1428571428571428,1,0
178,"At each time step t , the wordy t can be predicted by :",Training technique,Generation,machine-translation,3,15,0.75,177,0.5654952076677316,2,0.2857142857142857,1,0
179,where ?,Training technique,Generation,machine-translation,3,16,0.8,178,0.5686900958466453,3,0.4285714285714285,1,0
180,t is the predicted target word .?,Training technique,Generation,machine-translation,3,17,0.85,179,0.5718849840255591,4,0.5714285714285714,1,0
181,0:t ? 1 is the generated sequence from time step 0 tot ?,Training technique,Generation,machine-translation,3,18,0.9,180,0.5750798722044729,5,0.7142857142857143,1,0
182,"1 . We keep n b best candidates according to Eq. 11 at each time step , until the end of sentence mark is generated .",Training technique,Generation,machine-translation,3,19,0.95,181,0.5782747603833865,6,0.8571428571428571,1,0
183,"The hypotheses are ranked by the total likelihood of the generated sequence , although normalized likelihood is used in some works .",Training technique,Generation,machine-translation,3,20,1.0,182,0.5814696485623003,7,1.0,1,0
184,Experiments,,,machine-translation,3,0,0.0,183,0.5846645367412141,0,0.0,1,0
185,We evaluate our method mainly on the widely used WMT ' 14 English - to - French translation task .,Experiments,Experiments,machine-translation,3,1,0.0625,184,0.5878594249201278,1,0.3333333333333333,1,0
186,"In order to validate our model on more difficult language pairs , we also provide results on the WMT ' 14 English - to - German translation task .",Experiments,Experiments,machine-translation,3,2,0.125,185,0.5910543130990416,2,0.6666666666666666,1,0
187,Our models are implemented in the PADDLE ( PArallel Distributed Deep LEarning ) platform .,Experiments,Experiments,machine-translation,3,3,0.1875,186,0.5942492012779552,3,1.0,1,0
188,Data sets,Experiments,,machine-translation,3,4,0.25,187,0.597444089456869,0,0.0,1,0
189,"For both tasks , we use the full WMT ' 14 parallel corpus as our training data .",Experiments,Data sets,machine-translation,3,5,0.3125,188,0.6006389776357828,1,0.0833333333333333,1,0
190,The detailed data sets are listed below :,Experiments,Data sets,machine-translation,3,6,0.375,189,0.6038338658146964,2,0.1666666666666666,1,0
191,"English - to - French : Europarl v7 , Common Crawl , UN , News Commentary , Gigaword",Experiments,Data sets,machine-translation,3,7,0.4375,190,0.6070287539936102,3,0.25,1,0
192,"English - to - German : Europarl v7 , Common Crawl , News Commentary",Experiments,Data sets,machine-translation,3,8,0.5,191,0.610223642172524,4,0.3333333333333333,1,0
193,"In total , the English - to - French corpus includes 36 million sentence pairs , and the English - to - German corpus includes 4.5 million sentence pairs .",Experiments,Data sets,machine-translation,3,9,0.5625,192,0.6134185303514377,5,0.4166666666666667,1,0
194,"The news - test - 2012 and news - test - 2013 are concatenated as our development set , and the news - test - 2014 is the test set .",Experiments,Data sets,machine-translation,3,10,0.625,193,0.6166134185303515,6,0.5,1,0
195,Our data partition is consistent with previous works on NMT to ensure fair comparison .,Experiments,Data sets,machine-translation,3,11,0.6875,194,0.6198083067092651,7,0.5833333333333334,1,0
196,"For the source language , we select the most frequent 200K words as the input vocabulary .",Experiments,Data sets,machine-translation,3,12,0.75,195,0.6230031948881789,8,0.6666666666666666,1,0
197,For the target language we select the most frequent 80 K French words and the most frequent 160K German words as the output vocabulary .,Experiments,Data sets,machine-translation,3,13,0.8125,196,0.6261980830670927,9,0.75,1,0
198,"The full vocabulary of the German corpus is larger , so we select more German words to build the target vocabulary .",Experiments,Data sets,machine-translation,3,14,0.875,197,0.6293929712460063,10,0.8333333333333334,1,0
199,Out - of - vocabulary words are replaced with the unknown symbol unk .,Experiments,Data sets,machine-translation,3,15,0.9375,198,0.6325878594249201,11,0.9166666666666666,1,0
200,"For complete comparison to previous work on the Englishto - French task , we also show the results with a smaller vocabulary of 30K input words and 30 K output words on the sub train set with selected 12M parallel sequences .",Experiments,Data sets,machine-translation,3,16,1.0,199,0.6357827476038339,12,1.0,1,0
201,Model settings,,,machine-translation,3,0,0.0,200,0.6389776357827476,0,0.0,1,0
202,"We have two models as described above , named Deep - ED and Deep - Att.",Model settings,Model settings,machine-translation,3,1,0.032258064516129,201,0.6421725239616614,1,0.1,1,0
203,Both models have exactly the same configuration and layer size except the interface part P - I.,Model settings,Model settings,machine-translation,3,2,0.064516129032258,202,0.645367412140575,2,0.2,1,0
204,We use 256 dimensional word embeddings for both the source and target languages .,Model settings,Model settings,machine-translation,3,3,0.0967741935483871,203,0.6485623003194888,3,0.3,1,1
205,"All LSTM layers , including the 2n e layers in the encoder and then d layers in the decoder , have 512 memory cells .",Model settings,Model settings,machine-translation,3,4,0.1290322580645161,204,0.6517571884984026,4,0.4,1,1
206,The output layer size is the same as the size of the target vocabulary .,Model settings,Model settings,machine-translation,3,5,0.1612903225806451,205,0.6549520766773163,5,0.5,1,0
207,The dimension of ct is 5120 and 1280 for Deep - ED and Deep - Att respectively .,Model settings,Model settings,machine-translation,3,6,0.1935483870967742,206,0.65814696485623,6,0.6,1,0
208,"For each LSTM layer , the activation functions for gates , inputs and outputs are sigmoid , tanh , and tanh respectively .",Model settings,Model settings,machine-translation,3,7,0.2258064516129032,207,0.6613418530351438,7,0.7,1,1
209,Our network is narrow on word embeddings and LSTM layers .,Model settings,Model settings,machine-translation,3,8,0.2580645161290322,208,0.6645367412140575,8,0.8,1,0
210,"Note that in previous work , 1000 dimensional word embeddings and 1000 dimensional LSTM layers are used .",Model settings,Model settings,machine-translation,3,9,0.2903225806451613,209,0.6677316293929713,9,0.9,1,0
211,We also tried larger scale models but did not obtain further improvements .,Model settings,Model settings,machine-translation,3,10,0.3225806451612903,210,0.670926517571885,10,1.0,1,0
212,Optimization,Model settings,,machine-translation,3,11,0.3548387096774194,211,0.6741214057507987,0,0.0,1,0
213,"Note that each LSTM layer includes two parts as described in Eq. 3 , feed - forward computation and recurrent computation .",Model settings,Optimization,machine-translation,3,12,0.3870967741935484,212,0.6773162939297125,1,0.05,1,0
214,"Since there are non-linear activations in the recurrent computation , a larger learning rate l r = 5 10 ? 4 is used , while for the feed - forward computation a smaller learning rate l f = 4 10 ? 5 is used .",Model settings,Optimization,machine-translation,3,13,0.4193548387096774,213,0.6805111821086262,2,0.1,1,1
215,Word embeddings and the softmax layer also use this learning rate l f .,Model settings,Optimization,machine-translation,3,14,0.4516129032258064,214,0.6837060702875399,3,0.15,1,0
216,We refer all the parameters not used for recurrent computation as non-recurrent part of the model .,Model settings,Optimization,machine-translation,3,15,0.4838709677419355,215,0.6869009584664537,4,0.2,1,0
217,"Because of the large model size , we use strong L 2 regularization to constrain the parameter matrix v in the following way :",Model settings,Optimization,machine-translation,3,16,0.5161290322580645,216,0.6900958466453674,5,0.25,1,0
218,"Here r is the regularization strength , l is the corresponding learning rate , g stands for the gradients of v.",Model settings,Optimization,machine-translation,3,17,0.5483870967741935,217,0.6932907348242812,6,0.3,1,0
219,The two embedding layers are not regularized .,Model settings,Optimization,machine-translation,3,18,0.5806451612903226,218,0.6964856230031949,7,0.35,1,0
220,All the other layers have the same r = 2 .,Model settings,Optimization,machine-translation,3,19,0.6129032258064516,219,0.6996805111821086,8,0.4,1,0
221,The parameters of the recurrent computation part are initialized to zero .,Model settings,Optimization,machine-translation,3,20,0.6451612903225806,220,0.7028753993610224,9,0.45,1,0
222,All non-recurrent parts are randomly initialized with zero mean and standard deviation of 0.07 .,Model settings,Optimization,machine-translation,3,21,0.6774193548387096,221,0.7060702875399361,10,0.5,1,0
223,A detailed guide for setting hyperparameters can be found in .,Model settings,Optimization,machine-translation,3,22,0.7096774193548387,222,0.7092651757188498,11,0.55,1,0
224,The dropout ratio pd is 0.1 .,Model settings,Optimization,machine-translation,3,23,0.7419354838709677,223,0.7124600638977636,12,0.6,1,1
225,"In each batch , there are 500 ? 800 sequences in our work .",Model settings,Optimization,machine-translation,3,24,0.7741935483870968,224,0.7156549520766773,13,0.65,1,0
226,The exact number depends on the sequence lengths and model size .,Model settings,Optimization,machine-translation,3,25,0.8064516129032258,225,0.7188498402555911,14,0.7,1,0
227,We also find that larger batch size results in better convergence although the improvement is not large .,Model settings,Optimization,machine-translation,3,26,0.8387096774193549,226,0.7220447284345048,15,0.75,1,0
228,"However , the largest batch size is constrained by the GPU memory .",Model settings,Optimization,machine-translation,3,27,0.8709677419354839,227,0.7252396166134185,16,0.8,1,0
229,We use 4 ? 8 GPU machines ( each has 4 K40 GPU cards ) running for 10 days to train the full model with parallelization at the data batch level .,Model settings,Optimization,machine-translation,3,28,0.9032258064516128,228,0.7284345047923323,17,0.85,1,1
230,It takes nearly 1.5 days for each pass .,Model settings,Optimization,machine-translation,3,29,0.935483870967742,229,0.731629392971246,18,0.9,1,0
231,One thing we want to emphasize here is that our deep model is not sensitive to these settings .,Model settings,Optimization,machine-translation,3,30,0.967741935483871,230,0.7348242811501597,19,0.95,1,0
232,Small variation does not affect the final performance .,Model settings,Optimization,machine-translation,3,31,1.0,231,0.7380191693290735,20,1.0,1,0
233,Results,,,machine-translation,3,0,0.0,232,0.7412140575079872,0,0.0,1,0
234,We evaluate the same way as previous NMT works .,Results,Results,machine-translation,3,1,0.3333333333333333,233,0.744408945686901,1,0.3333333333333333,1,0
235,All reported BLEU scores are computed with the multi-bleu. perl 1 script which is also used in the above works .,Results,Results,machine-translation,3,2,0.6666666666666666,234,0.7476038338658147,2,0.6666666666666666,1,0
236,The results are for tokenized and case sensitive evaluation .,Results,Results,machine-translation,3,3,1.0,235,0.7507987220447284,3,1.0,1,0
237,Single models,,,machine-translation,3,0,0.0,236,0.7539936102236422,0,0.0,1,0
238,English - to - French : First we list our single model results on the English - to - French task in Tab .,Single models,Single models,machine-translation,3,1,0.0714285714285714,237,0.7571884984025559,1,0.0714285714285714,1,0
239,1 . In the first block we show the results with the full corpus .,Single models,Single models,machine-translation,3,2,0.1428571428571428,238,0.7603833865814696,2,0.1428571428571428,1,0
240,The previous best single NMT encoderdecoder model ( Enc - Dec ) with six layers achieves BLEU = 31.5 .,Single models,Single models,machine-translation,3,3,0.2142857142857142,239,0.7635782747603834,3,0.2142857142857142,1,0
241,"From Deep - ED , we obtain the BLEU score of 36.3 , which outperforms Enc - Dec model by 4.8 BLEU points .",Single models,Single models,machine-translation,3,4,0.2857142857142857,240,0.7667731629392971,4,0.2857142857142857,1,1
242,"This result is even better than the ensemble result of eight Enc - Dec models , which is 35.6 .",Single models,Single models,machine-translation,3,5,0.3571428571428571,241,0.7699680511182109,5,0.3571428571428571,1,0
243,"This shows that , in addition to the convolutional layers for computer vision , deep topologies can also work for LSTM layers .",Single models,Single models,machine-translation,3,6,0.4285714285714285,242,0.7731629392971247,6,0.4285714285714285,1,0
244,"For Deep - Att , the performance is further improved to 37.7 .",Single models,Single models,machine-translation,3,7,0.5,243,0.7763578274760383,7,0.5,1,1
245,We also list the previous state - of - the - art performance from a conventional SMT system with the BLEU of 37.0 .,Single models,Single models,machine-translation,3,8,0.5714285714285714,244,0.7795527156549521,8,0.5714285714285714,1,0
246,This is the first time that a single NMT model trained in an end - to - end form beats the best conventional system on this task .,Single models,Single models,machine-translation,3,9,0.6428571428571429,245,0.7827476038338658,9,0.6428571428571429,1,0
247,We also show the results on the smaller data set with 12M sentence pairs and 30 K vocabulary in the second block .,Single models,Single models,machine-translation,3,10,0.7142857142857143,246,0.7859424920127795,10,0.7142857142857143,1,0
248,"The two attention models , RNNsearch and RNNsearch - LV , achieve BLEU scores of 28.5 and 32.7 respectively .",Single models,Single models,machine-translation,3,11,0.7857142857142857,247,0.7891373801916933,11,0.7857142857142857,1,0
249,Note that RNNsearch - LV uses a large output vocabulary of 500K words based on the standard attention model RNNsearch .,Single models,Single models,machine-translation,3,12,0.8571428571428571,248,0.792332268370607,12,0.8571428571428571,1,0
250,We obtain BLEU = 35.9 which outperforms its corresponding shallow model RNNsearch by 7.4 BLEU points .,Single models,Single models,machine-translation,3,13,0.9285714285714286,249,0.7955271565495208,13,0.9285714285714286,1,0
251,The SMT result from is also listed and falls behind our model by 2.6 BLEU points .,Single models,Single models,machine-translation,3,14,1.0,250,0.7987220447284346,14,1.0,1,0
252,Methods,,,machine-translation,3,0,0.0,251,0.8019169329073482,0,0.0,1,0
253,Data Voc BLEU RNNsearch 4.5M 50K 16.5 RNNsearch-LV 4.5M 500K 16.9 SMT 4.5 M Full 20.7 Deep - Att ( Ours ) 4.5M 160K 20.6 : English - to - German task : BLEU scores of single neural models .,Methods,Methods,machine-translation,3,1,0.0208333333333333,252,0.805111821086262,1,0.5,1,0
254,We also list the conventional SMT system for comparison .,Methods,Methods,machine-translation,3,2,0.0416666666666666,253,0.8083067092651757,2,1.0,1,0
255,Post processing,Methods,,machine-translation,3,3,0.0625,254,0.8115015974440895,0,0.0,1,0
256,Two post processing techniques are used to improve the performance further on the English - to - French task .,Methods,Post processing,machine-translation,3,4,0.0833333333333333,255,0.8146964856230032,1,0.0625,1,0
257,"First , three Deep - Att models are built for ensemble results .",Methods,Post processing,machine-translation,3,5,0.1041666666666666,256,0.8178913738019169,2,0.125,1,0
258,"They are initialized with different random parameters ; in addition , the training corpus for these models is shuffled with different random seeds .",Methods,Post processing,machine-translation,3,6,0.125,257,0.8210862619808307,3,0.1875,1,0
259,We sum over the predicted probabilities of the target words and normalize the final distribution to generate the next word .,Methods,Post processing,machine-translation,3,7,0.1458333333333333,258,0.8242811501597445,4,0.25,1,0
260,It is shown in Tab .,Methods,Post processing,machine-translation,3,8,0.1666666666666666,259,0.8274760383386581,5,0.3125,1,0
261,8 that the model ensemble can improve the performance further to 38.9 .,Methods,Post processing,machine-translation,3,9,0.1875,260,0.8306709265175719,6,0.375,1,0
262,"In and there are eight models for the best scores , but we only use three models and we do not obtain further gain from more models . : BLEU scores of different models .",Methods,Post processing,machine-translation,3,10,0.2083333333333333,261,0.8338658146964856,7,0.4375,1,0
263,The first two blocks are our results of two single models and models with post processing .,Methods,Post processing,machine-translation,3,11,0.2291666666666666,262,0.8370607028753994,8,0.5,1,0
264,In the last block we list two baselines of the best conventional SMT system and NMT system .,Methods,Post processing,machine-translation,3,12,0.25,263,0.8402555910543131,9,0.5625,1,0
265,"Second , we recover the unknown words in the generated sequences with the Positional Unknown ( Pos Unk ) model introduced in .",Methods,Post processing,machine-translation,3,13,0.2708333333333333,264,0.8434504792332268,10,0.625,1,0
266,The full parallel corpus is used to obtain the word mappings .,Methods,Post processing,machine-translation,3,14,0.2916666666666667,265,0.8466453674121406,11,0.6875,1,0
267,"We find this method provides an additional 1.5 BLEU points , which is consistent with the conclusion in .",Methods,Post processing,machine-translation,3,15,0.3125,266,0.8498402555910544,12,0.75,1,0
268,We obtain the new BLEU score of 39.2 with a single Deep - Att model .,Methods,Post processing,machine-translation,3,16,0.3333333333333333,267,0.853035143769968,13,0.8125,1,0
269,"For the ensemble models of Deep - Att , the BLEU score rises to 40.4 .",Methods,Post processing,machine-translation,3,17,0.3541666666666667,268,0.8562300319488818,14,0.875,1,0
270,"In the last two lines , we list the conventional SMT model and the previous best neural models based system Enc - Dec for comparison .",Methods,Post processing,machine-translation,3,18,0.375,269,0.8594249201277955,15,0.9375,1,0
271,We find our best score outperforms the previous best score by nearly 3 points .,Methods,Post processing,machine-translation,3,19,0.3958333333333333,270,0.8626198083067093,16,1.0,1,0
272,Analysis,Methods,,machine-translation,3,20,0.4166666666666667,271,0.865814696485623,0,0.0,1,0
273,Length,Methods,,machine-translation,3,21,0.4375,272,0.8690095846645367,0,0.0,1,0
274,"On the English - to - French task , we analyze the effect of the source sentence length on our models as shown in .",Methods,Length,machine-translation,3,22,0.4583333333333333,273,0.8722044728434505,1,0.2,1,0
275,"Here we show five curves : our Deep - Att single model , our Deep - Att ensemble model , our Deep - ED model , a previously proposed Enc - Dec model with four layers and an SMT model .",Methods,Length,machine-translation,3,23,0.4791666666666667,274,0.8753993610223643,2,0.4,1,0
276,We find our Deep - Att model works better than the previous two models ( Enc - Dec and SMT ) on nearly all sentence lengths .,Methods,Length,machine-translation,3,24,0.5,275,0.8785942492012779,3,0.6,1,0
277,"It is also shown that for very long sequences with length over 70 words , the performance of our Deep - Att does not degrade , when compared to another NMT model Enc - Dec.",Methods,Length,machine-translation,3,25,0.5208333333333334,276,0.8817891373801917,4,0.8,1,0
278,"Our Deep - ED also has much better performance than the shallow Enc - Dec model on nearly all lengths , although for long sequences it degrades and starts to fall behind Deep - Att .",Methods,Length,machine-translation,3,26,0.5416666666666666,277,0.8849840255591054,5,1.0,1,0
279,Unknown words,Methods,,machine-translation,3,27,0.5625,278,0.8881789137380192,0,0.0,1,0
280,Next we look into the detail of the effect of unknown words on the English - to - French task .,Methods,Unknown words,machine-translation,3,28,0.5833333333333334,279,0.8913738019169329,1,0.1,1,0
281,We select the subset without unknown words on target sentences from the original test set .,Methods,Unknown words,machine-translation,3,29,0.6041666666666666,280,0.8945686900958466,2,0.2,1,0
282,There are 1705 such sentences ( 56.8 % ) .,Methods,Unknown words,machine-translation,3,30,0.625,281,0.8977635782747604,3,0.3,1,0
283,We compute the BLEU scores on this subset and the results are shown in Tab .,Methods,Unknown words,machine-translation,3,31,0.6458333333333334,282,0.9009584664536742,4,0.4,1,0
284,9 . We also list the results from SMT model the score 37.7 on the full test set .,Methods,Unknown words,machine-translation,3,32,0.6666666666666666,283,0.9041533546325878,5,0.5,1,0
285,"On this subset , the SMT model achieves 37.5 , which is similar to its score 37.0 on the full test set .",Methods,Unknown words,machine-translation,3,33,0.6875,284,0.9073482428115016,6,0.6,1,0
286,This suggests that the difficulty on this subset is not much different from that on the full set .,Methods,Unknown words,machine-translation,3,34,0.7083333333333334,285,0.9105431309904152,7,0.7,1,0
287,We therefore attribute the larger gap for Deep - att to the existence of unknown words .,Methods,Unknown words,machine-translation,3,35,0.7291666666666666,286,0.9137380191693292,8,0.8,1,0
288,We also compute the BLEU score on the subset of the ensemble model and obtain 41.4 .,Methods,Unknown words,machine-translation,3,36,0.75,287,0.9169329073482428,9,0.9,1,0
289,"As a reference related to human performance , in , it has been tested that the BLEU score of oracle re-scoring the LIUM 1000 - best results is 45 .",Methods,Unknown words,machine-translation,3,37,0.7708333333333334,288,0.9201277955271564,10,1.0,1,0
290,Over-fitting,Methods,,machine-translation,3,38,0.7916666666666666,289,0.9233226837060704,0,0.0,1,0
291,"Deep models have more parameters , and thus have a stronger ability to fit the large data set .",Methods,Over-fitting,machine-translation,3,39,0.8125,290,0.926517571884984,1,0.1,1,0
292,"However , our experimental results suggest that deep models are less prone to the problem of over-fitting .",Methods,Over-fitting,machine-translation,3,40,0.8333333333333334,291,0.9297124600638976,2,0.2,1,0
293,"In , we show three results from models with a different depth on the English - to - French task .",Methods,Over-fitting,machine-translation,3,41,0.8541666666666666,292,0.9329073482428116,3,0.3,1,0
294,"These three models are evaluated by token error rate , which is defined as the ratio of incorrectly predicted words in the whole target sequence with correct historical input .",Methods,Over-fitting,machine-translation,3,42,0.875,293,0.9361022364217252,4,0.4,1,0
295,The curve with square marks corresponds to Deep - Att with n e = 9 and n d = 7 .,Methods,Over-fitting,machine-translation,3,43,0.8958333333333334,294,0.939297124600639,5,0.5,1,0
296,The curve with circle marks corresponds ton e = 5 and n d = 3 .,Methods,Over-fitting,machine-translation,3,44,0.9166666666666666,295,0.9424920127795527,6,0.6,1,0
297,The curve with triangle marks corresponds ton e = 1 and n d = 1 .,Methods,Over-fitting,machine-translation,3,45,0.9375,296,0.9456869009584664,7,0.7,1,0
298,We find that the deep model has better performance on the test set when the token error rate is the same as that of the shallow models on the training set .,Methods,Over-fitting,machine-translation,3,46,0.9583333333333334,297,0.9488817891373802,8,0.8,1,0
299,"This shows that , with decreased token error rate , the deep model is more advantageous in avoiding the over - fitting phenomenon .",Methods,Over-fitting,machine-translation,3,47,0.9791666666666666,298,0.952076677316294,9,0.9,1,0
300,"We only plot the early training stage curves because , during the late training stage , the curves are not smooth .",Methods,Over-fitting,machine-translation,3,48,1.0,299,0.9552715654952076,10,1.0,1,0
301,Conclusion,,,machine-translation,3,0,0.0,300,0.9584664536741214,0,0.0,1,0
302,"With the introduction of fast - forward connections to the deep LSTM network , we build a fast path with neither non-linear transformations nor recurrent computation to propagate the gradients from the top to the deep bottom .",Conclusion,Conclusion,machine-translation,3,1,0.0833333333333333,301,0.9616613418530352,1,0.0833333333333333,0,0
303,"On this path , gradients decay much slower compared to the standard deep network .",Conclusion,Conclusion,machine-translation,3,2,0.1666666666666666,302,0.9648562300319488,2,0.1666666666666666,0,0
304,This enables us to build the deep topology of NMT models .,Conclusion,Conclusion,machine-translation,3,3,0.25,303,0.9680511182108626,3,0.25,0,0
305,We trained NMT models with depth of 16 including 25 LSTM layers and evaluated them mainly on the WMT ' 14 English - to - French translation task .,Conclusion,Conclusion,machine-translation,3,4,0.3333333333333333,304,0.9712460063897764,4,0.3333333333333333,0,0
306,This is the deepest topology that has been investigated in the NMT area on this task .,Conclusion,Conclusion,machine-translation,3,5,0.4166666666666667,305,0.97444089456869,5,0.4166666666666667,0,0
307,"We showed that our Deep - Att exhibits 6.2 BLEU points improvement over the previous best single model , achieving a 37.7 BLEU score .",Conclusion,Conclusion,machine-translation,3,6,0.5,306,0.977635782747604,6,0.5,0,0
308,This single end - toend NMT model outperforms the best conventional SMT system and achieves a state - of - the - art performance .,Conclusion,Conclusion,machine-translation,3,7,0.5833333333333334,307,0.9808306709265175,7,0.5833333333333334,0,0
309,"After utilizing unknown word processing and model ensemble of three models , we obtained a BLEU score of 40.4 , an improvement of 2.9 BLEU points over the previous best result .",Conclusion,Conclusion,machine-translation,3,8,0.6666666666666666,308,0.9840255591054312,8,0.6666666666666666,0,0
310,"When evaluated on the subset of the test corpus without unknown words , our model achieves 41.4 .",Conclusion,Conclusion,machine-translation,3,9,0.75,309,0.987220447284345,9,0.75,0,0
311,Our model is also validated on the more difficult English - to - German task .,Conclusion,Conclusion,machine-translation,3,10,0.8333333333333334,310,0.9904153354632588,10,0.8333333333333334,0,0
312,Our model is also efficient in sequence generation .,Conclusion,Conclusion,machine-translation,3,11,0.9166666666666666,311,0.9936102236421726,11,0.9166666666666666,0,0
313,"The best results from both a single model and model ensemble are obtained with abeam size of 3 , much smaller than previous NMT systems where beam size is about 12",Conclusion,Conclusion,machine-translation,3,12,1.0,312,0.9968051118210862,12,1.0,0,0
1,title,,,machine-translation,4,0,0.0,0,0.0,0,0.0,1,0
2,Unsupervised Neural Machine Translation with Weight Sharing,title,,machine-translation,4,1,0.0,1,0.00418410041841,1,0.0,1,1
3,abstract,,,machine-translation,4,0,0.0,2,0.00836820083682,0,0.0,1,0
4,Unsupervised neural machine translation ( NMT ) is a recently proposed approach for machine translation which aims to train the model without using any labeled data .,abstract,abstract,machine-translation,4,1,0.2,3,0.0125523012552301,1,0.2,1,1
5,"The models proposed for unsupervised NMT often use only one shared encoder to map the pairs of sentences from different languages to a shared - latent space , which is weak in keeping the unique and internal characteristics of each language , such as the style , terminology , and sentence structure .",abstract,abstract,machine-translation,4,2,0.4,4,0.0167364016736401,2,0.4,1,1
6,"To address this issue , we introduce an extension by utilizing two independent encoders but sharing some partial weights which are responsible for extracting high - level representations of the input sentences .",abstract,abstract,machine-translation,4,3,0.6,5,0.0209205020920502,3,0.6,1,0
7,"Besides , two different generative adversarial networks ( GANs ) , namely the local GAN and global GAN , are proposed to enhance the cross - language translation .",abstract,abstract,machine-translation,4,4,0.8,6,0.0251046025104602,4,0.8,1,0
8,"With this new approach , we achieve significant improvements on English - German , English - French and Chinese - to - English translation tasks .",abstract,abstract,machine-translation,4,5,1.0,7,0.0292887029288702,5,1.0,1,0
9,Introduction,,,machine-translation,4,0,0.0,8,0.0334728033472803,0,0.0,1,0
10,"Neural machine translation , directly applying a single neural network to transform the source sentence into the target sentence , has now reached impressive performance .",Introduction,Introduction,machine-translation,4,1,0.0285714285714285,9,0.0376569037656903,1,0.0285714285714285,1,0
11,The NMT typically consists of two sub neural networks .,Introduction,Introduction,machine-translation,4,2,0.0571428571428571,10,0.0418410041841004,2,0.0571428571428571,1,0
12,"The encoder network reads and encodes the source sentence into a 1 Feng Wang is the corresponding author of this paper context vector , and the decoder network generates the target sentence iteratively based on the context vector .",Introduction,Introduction,machine-translation,4,3,0.0857142857142857,11,0.0460251046025104,3,0.0857142857142857,1,0
13,NMT can be studied in supervised and unsupervised learning settings .,Introduction,Introduction,machine-translation,4,4,0.1142857142857142,12,0.0502092050209205,4,0.1142857142857142,1,0
14,"In the supervised setting , bilingual corpora is available for training the NMT model .",Introduction,Introduction,machine-translation,4,5,0.1428571428571428,13,0.0543933054393305,5,0.1428571428571428,1,0
15,"In the unsupervised setting , we only have two independent monolingual corpora with one for each language and there is no bilingual training example to provide alignment information for the two languages .",Introduction,Introduction,machine-translation,4,6,0.1714285714285714,14,0.0585774058577405,6,0.1714285714285714,1,0
16,"Due to lack of alignment information , the unsupervised NMT is considered more challenging .",Introduction,Introduction,machine-translation,4,7,0.2,15,0.0627615062761506,7,0.2,1,0
17,"However , this task is very promising , since the monolingual corpora is usually easy to be collected .",Introduction,Introduction,machine-translation,4,8,0.2285714285714285,16,0.0669456066945606,8,0.2285714285714285,1,0
18,"Motivated by recent success in unsupervised cross - lingual embeddings , the models proposed for unsupervised NMT often assume that a pair of sentences from two different languages can be mapped to a same latent representation in a shared - latent space .",Introduction,Introduction,machine-translation,4,9,0.2571428571428571,17,0.0711297071129707,9,0.2571428571428571,1,0
19,"Following this assumption , use a single encoder and a single decoder for both the source and target languages .",Introduction,Introduction,machine-translation,4,10,0.2857142857142857,18,0.0753138075313807,10,0.2857142857142857,1,0
20,"The encoder and decoder , acting as a standard auto - encoder ( AE ) , are trained to reconstruct the inputs .",Introduction,Introduction,machine-translation,4,11,0.3142857142857143,19,0.0794979079497908,11,0.3142857142857143,1,0
21,And utilize a shared encoder but two independent decoders .,Introduction,Introduction,machine-translation,4,12,0.3428571428571428,20,0.0836820083682008,12,0.3428571428571428,1,0
22,"With some good performance , they share a glaring defect , i.e. , only one encoder is shared by the source and target languages .",Introduction,Introduction,machine-translation,4,13,0.3714285714285714,21,0.0878661087866108,13,0.3714285714285714,1,0
23,"Although the shared encoder is vital for mapping sentences from different languages into the shared - latent space , it is weak in keeping the uniqueness and internal characteristics of each language , such as the style , terminology and sentence structure .",Introduction,Introduction,machine-translation,4,14,0.4,22,0.0920502092050209,14,0.4,1,0
24,"Since each language has its own characteristics , the source and target languages should be encoded and learned independently .",Introduction,Introduction,machine-translation,4,15,0.4285714285714285,23,0.0962343096234309,15,0.4285714285714285,1,0
25,"Therefore , we conjecture that the shared encoder maybe a factor limit - ing the potential translation performance .",Introduction,Introduction,machine-translation,4,16,0.4571428571428571,24,0.100418410041841,16,0.4571428571428571,1,0
26,"In order to address this issue , we extend the encoder - shared model , i.e. , the model with one shared encoder , by leveraging two independent encoders with each for one language .",Introduction,Introduction,machine-translation,4,17,0.4857142857142857,25,0.104602510460251,17,0.4857142857142857,1,1
27,"Similarly , two independent decoders are utilized .",Introduction,Introduction,machine-translation,4,18,0.5142857142857142,26,0.1087866108786611,18,0.5142857142857142,1,1
28,"For each language , the encoder and its corresponding decoder perform an AE , where the encoder generates the latent representations from the perturbed input sentences and the decoder reconstructs the sentences from the latent representations .",Introduction,Introduction,machine-translation,4,19,0.5428571428571428,27,0.1129707112970711,19,0.5428571428571428,1,1
29,"To map the latent representations from different languages to a shared - latent space , we propose the weightsharing constraint to the two AEs .",Introduction,Introduction,machine-translation,4,20,0.5714285714285714,28,0.1171548117154811,20,0.5714285714285714,1,1
30,"Specifically , we share the weights of the last few layers of two encoders that are responsible for extracting highlevel representations of input sentences .",Introduction,Introduction,machine-translation,4,21,0.6,29,0.1213389121338912,21,0.6,1,0
31,"Similarly , we share the weights of the first few layers of two decoders .",Introduction,Introduction,machine-translation,4,22,0.6285714285714286,30,0.1255230125523012,22,0.6285714285714286,1,0
32,"To enforce the shared - latent space , the word embeddings are used as a reinforced encoding component in our encoders .",Introduction,Introduction,machine-translation,4,23,0.6571428571428571,31,0.1297071129707113,23,0.6571428571428571,1,0
33,"For cross - language translation , we utilize the backtranslation following .",Introduction,Introduction,machine-translation,4,24,0.6857142857142857,32,0.1338912133891213,24,0.6857142857142857,1,1
34,"Additionally , two different generative adversarial networks ( GAN ) , namely the local and global GAN , are proposed to further improve the cross - language translation .",Introduction,Introduction,machine-translation,4,25,0.7142857142857143,33,0.1380753138075313,25,0.7142857142857143,1,1
35,"We utilize the local GAN to constrain the source and target latent representations to have the same distribution , whereby the encoder tries to fool a local discriminator which is simultaneously trained to distinguish the language of a given latent representation .",Introduction,Introduction,machine-translation,4,26,0.7428571428571429,34,0.1422594142259414,26,0.7428571428571429,1,1
36,"We apply the global GAN to finetune the corresponding generator , i.e. , the composition of the encoder and decoder of the other language , where a global discriminator is leveraged to guide the training of the generator by assessing how far the generated sentence is from the true data distribution 1 .",Introduction,Introduction,machine-translation,4,27,0.7714285714285715,35,0.1464435146443514,27,0.7714285714285715,1,1
37,"In summary , we mainly make the following contributions :",Introduction,Introduction,machine-translation,4,28,0.8,36,0.1506276150627615,28,0.8,1,0
38,"We propose the weight - sharing constraint to unsupervised NMT , enabling the model to utilize an independent encoder for each language .",Introduction,Introduction,machine-translation,4,29,0.8285714285714286,37,0.1548117154811715,29,0.8285714285714286,1,0
39,"To enforce the shared - latent space , we also propose the embedding - reinforced encoders and two different GANs for our model .",Introduction,Introduction,machine-translation,4,30,0.8571428571428571,38,0.1589958158995816,30,0.8571428571428571,1,0
40,We conduct extensive experiments on The code that we utilized to train and evaluate our models can be found at https://github.com/ZhenYangIACAS/unsupervised-NMT,Introduction,Introduction,machine-translation,4,31,0.8857142857142857,39,0.1631799163179916,31,0.8857142857142857,1,0
41,"English - German , English - French and Chinese - to - English translation tasks .",Introduction,Introduction,machine-translation,4,32,0.9142857142857144,40,0.1673640167364016,32,0.9142857142857144,1,0
42,Experimental results show that the proposed approach consistently achieves great success .,Introduction,Introduction,machine-translation,4,33,0.9428571428571428,41,0.1715481171548117,33,0.9428571428571428,1,0
43,"Last but not least , we introduce the directional self - attention to model temporal order information for the proposed model .",Introduction,Introduction,machine-translation,4,34,0.9714285714285714,42,0.1757322175732217,34,0.9714285714285714,1,0
44,Experimental results reveal that it deserves more efforts for researchers to investigate the temporal order information within self - attention layers of NMT .,Introduction,Introduction,machine-translation,4,35,1.0,43,0.1799163179916318,35,1.0,1,0
45,Related Work,,,machine-translation,4,0,0.0,44,0.1841004184100418,0,0.0,1,0
46,Several approaches have been proposed to train NMT models without direct parallel corpora .,Related Work,Related Work,machine-translation,4,1,0.0909090909090909,45,0.1882845188284518,1,0.0833333333333333,0,0
47,The scenario that has been widely investigated is one where two languages have little parallel data between them but are well connected by one pivot language .,Related Work,Related Work,machine-translation,4,2,0.1818181818181818,46,0.1924686192468619,2,0.1666666666666666,0,0
48,The most typical approach in this scenario is to independently translate from the source language to the pivot language and from the pivot language to the target language ( Saha et al .,Related Work,Related Work,machine-translation,4,3,0.2727272727272727,47,0.1966527196652719,3,0.25,0,0
49,The two works mentioned above both use a single shared encoder to guarantee the shared latent space .,Related Work,Related Work,machine-translation,4,4,0.3636363636363636,48,0.200836820083682,4,0.3333333333333333,0,0
50,"However , a concomitant defect is that the shared encoder is weak in keeping the uniqueness of each language .",Related Work,Related Work,machine-translation,4,5,0.4545454545454545,49,0.205020920502092,5,0.4166666666666667,0,0
51,"Our work also belongs to this more ambitious scenario , and to the best of our knowledge , we are one among the first endeavors to investigate how to train an NMT model with monolingual corpora only .",Related Work,Related Work,machine-translation,4,6,0.5454545454545454,50,0.209205020920502,6,0.5,0,0
52,is the translation in reversed direction .,Related Work,Related Work,machine-translation,4,7,0.6363636363636364,51,0.2133891213389121,7,0.5833333333333334,0,0
53,D l is utilized to assess whether the hidden representation of the encoder is from the source or target language .,Related Work,Related Work,machine-translation,4,8,0.7272727272727273,52,0.2175732217573222,8,0.6666666666666666,0,0
54,D g 1 and D g 2 are used to evaluate whether the translated sentences are realistic for each language respectively .,Related Work,Related Work,machine-translation,4,9,0.8181818181818182,53,0.2217573221757322,9,0.75,0,0
55,Z represents the shared - latent space .,Related Work,Related Work,machine-translation,4,10,0.9090909090909092,54,0.2259414225941422,10,0.8333333333333334,0,0
56,3,Related Work,Related Work,machine-translation,4,11,1.0,55,0.2301255230125523,11,0.9166666666666666,0,0
57,The Approach,,,machine-translation,4,0,0.0,56,0.2343096234309623,12,1.0,1,0
58,Model Architecture,,,machine-translation,4,0,0.0,57,0.2384937238493724,0,0.0,1,0
59,"The model architecture , as illustrated in figure 1 , is based on the AE and GAN .",Model Architecture,Model Architecture,machine-translation,4,1,0.0256410256410256,58,0.2426778242677824,1,0.0769230769230769,1,0
60,"It consists of seven sub networks : including two encoders Enc sand Enc t , two decoders Dec sand Dec t , the local discriminator D l , and the global discriminators D g 1 and D g 2 .",Model Architecture,Model Architecture,machine-translation,4,2,0.0512820512820512,59,0.2468619246861924,2,0.1538461538461538,1,0
61,"For the encoder and decoder , we follow the newly emerged Transformer .",Model Architecture,Model Architecture,machine-translation,4,3,0.0769230769230769,60,0.2510460251046025,3,0.2307692307692307,1,0
62,"Specifically , the encoder is composed of a stack of four identical layers",Model Architecture,Model Architecture,machine-translation,4,4,0.1025641025641025,61,0.2552301255230125,4,0.3076923076923077,1,0
63,2 .,Model Architecture,Model Architecture,machine-translation,4,5,0.1282051282051282,62,0.2594142259414226,5,0.3846153846153846,1,0
64,Each layer consists of a multi-head self - attention and a simple position - wise fully connected feed - forward network .,Model Architecture,Model Architecture,machine-translation,4,6,0.1538461538461538,63,0.2635983263598326,6,0.4615384615384615,1,0
65,The decoder is also composed of four identical layers .,Model Architecture,Model Architecture,machine-translation,4,7,0.1794871794871795,64,0.2677824267782426,7,0.5384615384615384,1,0
66,"In addition to the two sub-layers in each encoder layer , the decoder inserts a third sublayer , which performs multi-head attention over the output of the encoder stack .",Model Architecture,Model Architecture,machine-translation,4,8,0.2051282051282051,65,0.2719665271966527,8,0.6153846153846154,1,0
67,"For more details about the multi-head self - attention layer , we refer the reader to .",Model Architecture,Model Architecture,machine-translation,4,9,0.2307692307692307,66,0.2761506276150627,9,0.6923076923076923,1,0
68,We implement the local discriminator as a multi -layer perceptron and implement the global discriminator based on the convolutional neural network ( CNN ) .,Model Architecture,Model Architecture,machine-translation,4,10,0.2564102564102564,67,0.2803347280334728,10,0.7692307692307693,1,0
69,Several ways exist to interpret the roles of the sub networks are summarised in table,Model Architecture,Model Architecture,machine-translation,4,11,0.282051282051282,68,0.2845188284518828,11,0.8461538461538461,1,0
70,1 .,Model Architecture,Model Architecture,machine-translation,4,12,0.3076923076923077,69,0.2887029288702928,12,0.9230769230769232,1,0
71,"The proposed system has several striking components , which are critical either for the system to be trained in an 2 The layer number is selected according to our preliminary experiment , which is presented in appendix A. unsupervised manner or for improving the translation performance .",Model Architecture,Model Architecture,machine-translation,4,13,0.3333333333333333,70,0.2928870292887029,13,1.0,1,0
72,Networks,Model Architecture,,machine-translation,4,14,0.358974358974359,71,0.2970711297071129,0,0.0,1,0
73,Roles : Interpretation of the roles for the subnetworks in the proposed system .,Model Architecture,Networks,machine-translation,4,15,0.3846153846153846,72,0.301255230125523,1,0.04,1,0
74,Directional self - attention,Model Architecture,Networks,machine-translation,4,16,0.4102564102564102,73,0.3054393305439331,2,0.08,1,0
75,"Compared to recurrent neural network , a disadvantage of the simple self - attention mechanism is that the temporal order information is lost .",Model Architecture,Networks,machine-translation,4,17,0.4358974358974359,74,0.309623430962343,3,0.12,1,0
76,"Although the Transformer applies the positional encoding to the sequence before processed by the self - attention , how to model temporal order information within an attention is still an open question .",Model Architecture,Networks,machine-translation,4,18,0.4615384615384615,75,0.3138075313807531,4,0.16,1,0
77,"Following , we build the encoders in our model on the directional self - attention which utilizes the positional masks to encode temporal order information into attention output .",Model Architecture,Networks,machine-translation,4,19,0.4871794871794871,76,0.3179916317991632,5,0.2,1,0
78,"More concretely , two positional masks , namely the forward mask M f and backward mask Mb , are calculated as :",Model Architecture,Networks,machine-translation,4,20,0.5128205128205128,77,0.3221757322175732,6,0.24,1,0
79,"With the forward mask M f , the later token only makes attention connections to the early tokens in the sequence , and vice versa with the backward mask .",Model Architecture,Networks,machine-translation,4,21,0.5384615384615384,78,0.3263598326359833,7,0.28,1,0
80,"Similar to , we utilize a self - attention network to process the input sequence in forward direction .",Model Architecture,Networks,machine-translation,4,22,0.5641025641025641,79,0.3305439330543933,8,0.32,1,0
81,"The output of this layer is taken by an upper self - attention network as input , processed in the reverse direction .",Model Architecture,Networks,machine-translation,4,23,0.5897435897435898,80,0.3347280334728033,9,0.36,1,0
82,Weight sharing,Model Architecture,,machine-translation,4,24,0.6153846153846154,81,0.3389121338912134,10,0.4,1,0
83,"Based on the shared - latent space assumption , we apply the weight sharing constraint to relate the two AEs .",Model Architecture,Weight sharing,machine-translation,4,25,0.6410256410256411,82,0.3430962343096234,11,0.44,1,0
84,"Specifically , we share the weights of the last few layers of the Enc sand Enc t , which are responsible for extracting high - level representations of the input sentences .",Model Architecture,Weight sharing,machine-translation,4,26,0.6666666666666666,83,0.3472803347280335,12,0.48,1,0
85,"Similarly , we also share the first few layers of the Dec sand Dec t , which are expected to decode high - level representations that are vital for reconstructing the input sentences .",Model Architecture,Weight sharing,machine-translation,4,27,0.6923076923076923,84,0.3514644351464435,13,0.52,1,0
86,"Compared to which use the fully shared encoder , we only share partial weights for the encoders and decoders .",Model Architecture,Weight sharing,machine-translation,4,28,0.717948717948718,85,0.3556485355648535,14,0.56,1,0
87,"In the proposed model , the independent weights of the two encoders are expected to learn and encode the hidden features about the internal characteristics of each language , such as the terminology , style , and sentence structure .",Model Architecture,Weight sharing,machine-translation,4,29,0.7435897435897436,86,0.3598326359832636,15,0.6,1,0
88,The shared weights are utilized to map the hidden features extracted by the independent weights to the shared - latent space .,Model Architecture,Weight sharing,machine-translation,4,30,0.7692307692307693,87,0.3640167364016736,16,0.64,1,0
89,Embedding reinforced encoder,Model Architecture,Weight sharing,machine-translation,4,31,0.7948717948717948,88,0.3682008368200837,17,0.68,1,0
90,We use pretrained cross - lingual embeddings in the encoders that are kept fixed during training .,Model Architecture,Weight sharing,machine-translation,4,32,0.8205128205128205,89,0.3723849372384937,18,0.72,1,0
91,And the fixed embeddings are used as a reinforced encoding component in our encoder .,Model Architecture,Weight sharing,machine-translation,4,33,0.8461538461538461,90,0.3765690376569037,19,0.76,1,0
92,"Formally , given the input sequence embedding vectors E = {e 1 , . . . , e t } and the initial output sequence of the encoder stack H = {h 1 , . . . , ht } , we compute H r as :",Model Architecture,Weight sharing,machine-translation,4,34,0.8717948717948718,91,0.3807531380753138,20,0.8,1,0
93,"where H r is the final output sequence of the encoder which will be attended by the decoder ( In Transformer , H is the final output of the encoder ) , g is agate unit and computed as :",Model Architecture,Weight sharing,machine-translation,4,35,0.8974358974358975,92,0.3849372384937238,21,0.84,1,0
94,"where W 1 , W 2 and bare trainable parameters and they are shared by the two encoders .",Model Architecture,Weight sharing,machine-translation,4,36,0.9230769230769232,93,0.3891213389121339,22,0.88,1,0
95,The motivation behind is twofold .,Model Architecture,Weight sharing,machine-translation,4,37,0.9487179487179488,94,0.3933054393305439,23,0.92,1,0
96,"Firstly , taking the fixed cross - lingual embedding as the other encoding component is helpful to reinforce the sharedlatent space .",Model Architecture,Weight sharing,machine-translation,4,38,0.9743589743589745,95,0.3974895397489539,24,0.96,1,0
97,"Additionally , from the point of multichannel encoders , providing encoding components with different levels of composition enables the decoder to take pieces of source sentence at varying composition levels suiting its own linguistic structure .",Model Architecture,Weight sharing,machine-translation,4,39,1.0,96,0.401673640167364,25,1.0,1,0
98,Unsupervised Training,,,machine-translation,4,0,0.0,97,0.405857740585774,0,0.0,1,0
99,"Based on the architecture proposed above , we train the NMT model with the monolingual corpora only using the following four strategies :",Unsupervised Training,Unsupervised Training,machine-translation,4,1,0.0232558139534883,98,0.4100418410041841,1,0.0232558139534883,1,0
100,Denoising auto - encoding,Unsupervised Training,Unsupervised Training,machine-translation,4,2,0.0465116279069767,99,0.4142259414225941,2,0.0465116279069767,1,0
101,"Firstly , we train the two AEs to reconstruct their inputs respectively .",Unsupervised Training,Unsupervised Training,machine-translation,4,3,0.0697674418604651,100,0.4184100418410041,3,0.0697674418604651,1,0
102,"In this form , each encoder should learn to compose the embeddings of its corresponding language and each decoder is expected to learn to decompose this representation into its corresponding language .",Unsupervised Training,Unsupervised Training,machine-translation,4,4,0.0930232558139534,101,0.4225941422594142,4,0.0930232558139534,1,0
103,"Nevertheless , without any constraint , the AE quickly learns to merely copy every word one by one , without capturing any internal structure of the language involved .",Unsupervised Training,Unsupervised Training,machine-translation,4,5,0.1162790697674418,102,0.4267782426778242,5,0.1162790697674418,1,0
104,"To address this problem , we utilize the same strategy of denoising AE and add some noise to the input sentences .",Unsupervised Training,Unsupervised Training,machine-translation,4,6,0.1395348837209302,103,0.4309623430962343,6,0.1395348837209302,1,0
105,"To this end , we shuffle the input sentences randomly .",Unsupervised Training,Unsupervised Training,machine-translation,4,7,0.1627906976744186,104,0.4351464435146444,7,0.1627906976744186,1,0
106,"Specifically , we apply a random permutation ?",Unsupervised Training,Unsupervised Training,machine-translation,4,8,0.1860465116279069,105,0.4393305439330544,8,0.1860465116279069,1,0
107,"to the input sentence , verifying the condition :",Unsupervised Training,Unsupervised Training,machine-translation,4,9,0.2093023255813953,106,0.4435146443514644,9,0.2093023255813953,1,0
108,"where n is the length of the input sentence , steps is the global steps the model has been updated , k and s are the tunable parameters which can beset by users beforehand .",Unsupervised Training,Unsupervised Training,machine-translation,4,10,0.2325581395348837,107,0.4476987447698745,10,0.2325581395348837,1,0
109,"This way , the system needs to learn some useful structure of the involved languages to be able to recover the correct word order .",Unsupervised Training,Unsupervised Training,machine-translation,4,11,0.2558139534883721,108,0.4518828451882845,11,0.2558139534883721,1,0
110,"In practice , we set k = 2 and s = 100000 .",Unsupervised Training,Unsupervised Training,machine-translation,4,12,0.2790697674418604,109,0.4560669456066946,12,0.2790697674418604,1,0
111,Back - translation,Unsupervised Training,Unsupervised Training,machine-translation,4,13,0.3023255813953488,110,0.4602510460251046,13,0.3023255813953488,1,0
112,"In spite of denoising autoencoding , the training procedure still involves a single language at each time , without considering our final goal of mapping an input sentence from the source / target language to the target / source language .",Unsupervised Training,Unsupervised Training,machine-translation,4,14,0.3255813953488372,111,0.4644351464435146,14,0.3255813953488372,1,0
113,"For the cross language training , we utilize the back - translation approach for our unsupervised training procedure .",Unsupervised Training,Unsupervised Training,machine-translation,4,15,0.3488372093023256,112,0.4686192468619247,15,0.3488372093023256,1,0
114,Back - translation has shown its great effectiveness on improving NMT model with monolingual data and has been widely investigated by .,Unsupervised Training,Unsupervised Training,machine-translation,4,16,0.3720930232558139,113,0.4728033472803347,16,0.3720930232558139,1,0
115,"In our approach , given an input sentence in a given language , we apply the corresponding encoder and the decoder of the other language to translate it to the other language 3 .",Unsupervised Training,Unsupervised Training,machine-translation,4,17,0.3953488372093023,114,0.4769874476987448,17,0.3953488372093023,1,0
116,"By combining the translation with its original sentence , we get a pseudo - parallel corpus which is utilized to train the model to reconstruct the original sentence from its translation .",Unsupervised Training,Unsupervised Training,machine-translation,4,18,0.4186046511627907,115,0.4811715481171548,18,0.4186046511627907,1,0
117,Local GAN,Unsupervised Training,,machine-translation,4,19,0.4418604651162791,116,0.4853556485355648,19,0.4418604651162791,1,0
118,"Although the weight sharing constraint is vital for the shared - latent space assumption , it alone does not guarantee that the corresponding sentences in two languages will have the same or similar latent code .",Unsupervised Training,Local GAN,machine-translation,4,20,0.4651162790697674,117,0.4895397489539749,20,0.4651162790697674,1,0
119,"To further enforce the shared - latent space , we train a discriminative neural network , referred to as the local discriminator , to classify between the encoding of source sentences and the encoding of target sentences .",Unsupervised Training,Local GAN,machine-translation,4,21,0.4883720930232558,118,0.4937238493723849,21,0.4883720930232558,1,0
120,"The local discriminator , implemented as a multilayer perceptron with two hidden layers of size 256 , takes the output of the encoder , i.e. , H r calculated as equation 3 , as input , and produces a binary prediction about the language of the input sentence .",Unsupervised Training,Local GAN,machine-translation,4,22,0.5116279069767442,119,0.497907949790795,22,0.5116279069767442,1,0
121,The local discriminator is trained to predict the language by minimizing the following crossentropy loss :,Unsupervised Training,Local GAN,machine-translation,4,23,0.5348837209302325,120,0.502092050209205,23,0.5348837209302325,1,0
122,where ?,Unsupervised Training,Local GAN,machine-translation,4,24,0.5581395348837209,121,0.5062761506276151,24,0.5581395348837209,1,0
123,"D l represents the parameters of the local discriminator and f ? {s , t}.",Unsupervised Training,Local GAN,machine-translation,4,25,0.5813953488372093,122,0.5104602510460251,25,0.5813953488372093,1,0
124,The encoders are trained to fool the local discriminator :,Unsupervised Training,Local GAN,machine-translation,4,26,0.6046511627906976,123,0.5146443514644351,26,0.6046511627906976,1,0
125,where ?,Unsupervised Training,Local GAN,machine-translation,4,27,0.627906976744186,124,0.5188284518828452,27,0.627906976744186,1,0
126,Encs and ?,Unsupervised Training,Local GAN,machine-translation,4,28,0.6511627906976745,125,0.5230125523012552,28,0.6511627906976745,1,0
127,Enct are the parameters of the two encoders .,Unsupervised Training,Local GAN,machine-translation,4,29,0.6744186046511628,126,0.5271966527196653,29,0.6744186046511628,1,0
128,Global GAN,Unsupervised Training,,machine-translation,4,30,0.6976744186046512,127,0.5313807531380753,30,0.6976744186046512,1,0
129,"We apply the global GANs to fine tune the whole model so that the model is able to generate sentences undistinguishable from the true data , i.e. , sentences in the training corpus .",Unsupervised Training,Global GAN,machine-translation,4,31,0.7209302325581395,128,0.5355648535564853,31,0.7209302325581395,1,0
130,"Different from the local GANs which updates the parameters of the encoders locally , the global GANs are utilized to update the whole parameters of the proposed model , including the parameters of encoders and decoders .",Unsupervised Training,Global GAN,machine-translation,4,32,0.7441860465116279,129,0.5397489539748954,32,0.7441860465116279,1,0
131,The proposed model has two global GANs : GAN g 1 and GAN g 2 .,Unsupervised Training,Global GAN,machine-translation,4,33,0.7674418604651163,130,0.5439330543933054,33,0.7674418604651163,1,0
132,"In GAN g 1 , the Enc t and Dec s act as the generator , which generates the sentencex t 4 from x t .",Unsupervised Training,Global GAN,machine-translation,4,34,0.7906976744186046,131,0.5481171548117155,34,0.7906976744186046,1,0
133,"The D g 1 , implemented based on CNN , assesses whether the generated sentencex t is the true target - language sentence or the generated sentence .",Unsupervised Training,Global GAN,machine-translation,4,35,0.813953488372093,132,0.5523012552301255,35,0.813953488372093,1,0
134,"The global discriminator aims to distinguish among the true sentences and generated sentences , and it is trained to minimize its classification error rate .",Unsupervised Training,Global GAN,machine-translation,4,36,0.8372093023255814,133,0.5564853556485355,36,0.8372093023255814,1,0
135,"During training , the D g 1 feeds back its assessment to finetune the encoder Enc t and decoder Dec s .",Unsupervised Training,Global GAN,machine-translation,4,37,0.8604651162790697,134,0.5606694560669456,37,0.8604651162790697,1,0
136,"Since the machine translation is a sequence generation problem , following , we leverage policy gradient reinforcement training to back - propagate the assessment .",Unsupervised Training,Global GAN,machine-translation,4,38,0.8837209302325582,135,0.5648535564853556,38,0.8837209302325582,1,0
137,We apply a similar processing to GAN g2 ( The details about the architecture of the global discriminator and the training procedure of the global GANs can be seen in appendix B and C ) .,Unsupervised Training,Global GAN,machine-translation,4,39,0.9069767441860463,136,0.5690376569037657,39,0.9069767441860463,1,0
138,There are two stages in the proposed unsupervised training .,Unsupervised Training,Global GAN,machine-translation,4,40,0.9302325581395348,137,0.5732217573221757,40,0.9302325581395348,1,0
139,"In the first stage , we train the proposed model with denoising auto - encoding , backtranslation and the local GANs , until no improvement is achieved on the development set .",Unsupervised Training,Global GAN,machine-translation,4,41,0.9534883720930232,138,0.5774058577405857,41,0.9534883720930232,1,0
140,"Specifically , we perform one batch of denoising autoencoding for the source and target languages , one batch of back - translation for the two languages , and another batch of local GAN for the two languages .",Unsupervised Training,Global GAN,machine-translation,4,42,0.9767441860465116,139,0.5815899581589958,42,0.9767441860465116,1,0
141,"In the second stage , we fine tune the proposed model with the global GANs .",Unsupervised Training,Global GAN,machine-translation,4,43,1.0,140,0.5857740585774058,43,1.0,1,0
142,Experiments and Results,,,machine-translation,4,0,0.0,141,0.5899581589958159,0,0.0,1,0
143,"We evaluate the proposed approach on English - German , English - French and Chinese - to - English translation tasks",Experiments and Results,Experiments and Results,machine-translation,4,1,0.0285714285714285,142,0.5941422594142259,1,0.3333333333333333,1,0
144,5 .,Experiments and Results,Experiments and Results,machine-translation,4,2,0.0571428571428571,143,0.5983263598326359,2,0.6666666666666666,1,0
145,"We firstly describe the datasets , pre-processing and model hyper - parameters we used , then we introduce the baseline systems , and finally we present our experimental results .",Experiments and Results,Experiments and Results,machine-translation,4,3,0.0857142857142857,144,0.602510460251046,3,1.0,1,0
146,Data Sets and Preprocessing,Experiments and Results,,machine-translation,4,4,0.1142857142857142,145,0.606694560669456,0,0.0,1,0
147,"In English - German and English - French translation , we make our experiments comparable with previous work by using the datasets from the WMT 2014 and WMT 2016 shared tasks respectively .",Experiments and Results,Data Sets and Preprocessing,machine-translation,4,5,0.1428571428571428,146,0.6108786610878661,1,0.032258064516129,1,0
148,"For Chinese - to - English translation , we use the datasets from LDC , which has been widely utilized by previous works .",Experiments and Results,Data Sets and Preprocessing,machine-translation,4,6,0.1714285714285714,147,0.6150627615062761,2,0.064516129032258,1,0
149,"WMT14 English - French Similar to , we use the full training set of 36M sentence pairs and we lower - case them and remove sentences longer than 50 words , resulting in a parallel corpus of about 30M pairs of sentences .",Experiments and Results,Data Sets and Preprocessing,machine-translation,4,7,0.2,148,0.6192468619246861,3,0.0967741935483871,1,0
150,"To guarantee no exact correspondence between the source and target monolingual sets , we build monolingual corpora by selecting English sentences from 15M random pairs , and selecting the French sentences from the complementary set .",Experiments and Results,Data Sets and Preprocessing,machine-translation,4,8,0.2285714285714285,149,0.6234309623430963,4,0.1290322580645161,1,0
151,"Sentences are encoded with byte - pair encoding , which has an English vocabulary of about 32000 tokens , and French vocabulary of about 33000 tokens .",Experiments and Results,Data Sets and Preprocessing,machine-translation,4,9,0.2571428571428571,150,0.6276150627615062,5,0.1612903225806451,1,0
152,We report results on newstest2014 .,Experiments and Results,Data Sets and Preprocessing,machine-translation,4,10,0.2857142857142857,151,0.6317991631799164,6,0.1935483870967742,1,0
153,WMT16 English - German,Experiments and Results,Data Sets and Preprocessing,machine-translation,4,11,0.3142857142857143,152,0.6359832635983264,7,0.2258064516129032,1,0
154,"We follow the same procedure mentioned above to create monolingual training corpora for English - German translation , and we get two monolingual training data of 1.8 M sentences each .",Experiments and Results,Data Sets and Preprocessing,machine-translation,4,12,0.3428571428571428,153,0.6401673640167364,8,0.2580645161290322,1,0
155,The two languages share a vocabulary of about 32000 tokens .,Experiments and Results,Data Sets and Preprocessing,machine-translation,4,13,0.3714285714285714,154,0.6443514644351465,9,0.2903225806451613,1,0
156,We report results on newstest2016 .,Experiments and Results,Data Sets and Preprocessing,machine-translation,4,14,0.4,155,0.6485355648535565,10,0.3225806451612903,1,0
157,"LDC Chinese - English For Chinese - to - English translation , our training data consists of 1.6 M sentence pairs randomly extracted from LDC corpora",Experiments and Results,Data Sets and Preprocessing,machine-translation,4,15,0.4285714285714285,156,0.6527196652719666,11,0.3548387096774194,1,0
158,6 .,Experiments and Results,Data Sets and Preprocessing,machine-translation,4,16,0.4571428571428571,157,0.6569037656903766,12,0.3870967741935484,1,0
159,"Since the data set is not big enough , we just build the monolingual data set by randomly shuffling the Chinese and English sentences respectively .",Experiments and Results,Data Sets and Preprocessing,machine-translation,4,17,0.4857142857142857,158,0.6610878661087866,13,0.4193548387096774,1,0
160,"In spite of the fact that some correspondence between examples in these two monolingual sets may exist , we never utilize this alignment information in our training procedure ( see Section 3.2 ) .",Experiments and Results,Data Sets and Preprocessing,machine-translation,4,18,0.5142857142857142,159,0.6652719665271967,14,0.4516129032258064,1,0
161,Both the Chinese and English sentences are encoded with byte - pair encoding .,Experiments and Results,Data Sets and Preprocessing,machine-translation,4,19,0.5428571428571428,160,0.6694560669456067,15,0.4838709677419355,1,0
162,"We get an English vocabulary of about 34000 tokens , and Chinese vocabulary of about 38000 tokens .",Experiments and Results,Data Sets and Preprocessing,machine-translation,4,20,0.5714285714285714,161,0.6736401673640168,16,0.5161290322580645,1,0
163,The results are reported on N IST 02 .,Experiments and Results,Data Sets and Preprocessing,machine-translation,4,21,0.6,162,0.6778242677824268,17,0.5483870967741935,1,0
164,"Since the proposed system relies on the pretrained cross - lingual embeddings , we utilize the monolingual corpora described above to train the embeddings for each language independently by using word2 vec .",Experiments and Results,Data Sets and Preprocessing,machine-translation,4,22,0.6285714285714286,163,0.6820083682008368,18,0.5806451612903226,1,0
165,"We then apply the public implementation 7 of the method proposed by to map these 6 LDC2002L27 , LDC2002T01 , LDC2002E18 , LDC2003E07 , LDC2004T08 , LDC2004E12 , LDC2005T10 7 https://github.com/artetxem/vecmap",Experiments and Results,Data Sets and Preprocessing,machine-translation,4,23,0.6571428571428571,164,0.6861924686192469,19,0.6129032258064516,1,0
166,embeddings to a shared - latent space 8 .,Experiments and Results,Data Sets and Preprocessing,machine-translation,4,24,0.6857142857142857,165,0.6903765690376569,20,0.6451612903225806,1,0
167,Model Hyper - parameters and Evaluation,Experiments and Results,,machine-translation,4,25,0.7142857142857143,166,0.694560669456067,21,0.6774193548387096,1,0
168,"Following the base model in , we set the dimension of word embedding as 512 , dropout rate as 0.1 and the head number as 8 .",Experiments and Results,Model Hyper - parameters and Evaluation,machine-translation,4,26,0.7428571428571429,167,0.698744769874477,22,0.7096774193548387,1,1
169,We use beam search with abeam size of 4 and length penalty ? = 0.6 .,Experiments and Results,Model Hyper - parameters and Evaluation,machine-translation,4,27,0.7714285714285715,168,0.702928870292887,23,0.7419354838709677,1,1
170,The model is implemented in TensorFlow and trained on up to four K80 GPUs synchronously in a multi - GPU setup on a single machine .,Experiments and Results,Model Hyper - parameters and Evaluation,machine-translation,4,28,0.8,169,0.7071129707112971,24,0.7741935483870968,1,1
171,"For model selection , we stop training when the model achieves no improvement for the tenth evaluation on the development set , which is comprised of 3000 source and target sentences extracted randomly from the monolingual training corpora .",Experiments and Results,Model Hyper - parameters and Evaluation,machine-translation,4,29,0.8285714285714286,170,0.7112970711297071,25,0.8064516129032258,1,0
172,"Following ( Lample et al. , 2017 ) , we translate the source sentences to the target language , and then translate the resulting sentences back to the source language .",Experiments and Results,Model Hyper - parameters and Evaluation,machine-translation,4,30,0.8571428571428571,171,0.7154811715481172,26,0.8387096774193549,1,0
173,The quality of the model is then evaluated by computing the BLEU score over the original inputs and their reconstructions via this two - step translation process .,Experiments and Results,Model Hyper - parameters and Evaluation,machine-translation,4,31,0.8857142857142857,172,0.7196652719665272,27,0.8709677419354839,1,0
174,"The performance is finally averaged over two directions , i.e. , from source to target and from target to source .",Experiments and Results,Model Hyper - parameters and Evaluation,machine-translation,4,32,0.9142857142857144,173,0.7238493723849372,28,0.9032258064516128,1,0
175,BLEU is utilized as the evaluation metric .,Experiments and Results,Model Hyper - parameters and Evaluation,machine-translation,4,33,0.9428571428571428,174,0.7280334728033473,29,0.935483870967742,1,0
176,"For Chinese - to - English , we apply the script mteval - v11 b. pl to evaluate the translation performance .",Experiments and Results,Model Hyper - parameters and Evaluation,machine-translation,4,34,0.9714285714285714,175,0.7322175732217573,30,0.967741935483871,1,0
177,"For English - German and English - French , we evaluate the translation performance with the script multi-belu.pl 9 .",Experiments and Results,Model Hyper - parameters and Evaluation,machine-translation,4,35,1.0,176,0.7364016736401674,31,1.0,1,0
178,Baseline Systems,,,machine-translation,4,0,0.0,177,0.7405857740585774,0,0.0,1,0
179,Word - by - word translation ( WBW ) The first baseline we consider is a system that performs word - by - word translations using the inferred bilingual dictionary .,Baseline Systems,Baseline Systems,machine-translation,4,1,0.1111111111111111,178,0.7447698744769874,1,0.0833333333333333,1,1
180,"Specifically , it translates a sentence word - by - word , replacing each word with its nearest neighbor in the other language .",Baseline Systems,Baseline Systems,machine-translation,4,2,0.2222222222222222,179,0.7489539748953975,2,0.1666666666666666,1,0
181,Lample et al .,Baseline Systems,Baseline Systems,machine-translation,4,3,0.3333333333333333,180,0.7531380753138075,3,0.25,1,1
182,The second baseline is a previous work that uses the same training and testing sets with this paper .,Baseline Systems,Baseline Systems,machine-translation,4,4,0.4444444444444444,181,0.7573221757322176,4,0.3333333333333333,1,0
183,"Their model belongs to the standard attention - based encoder - decoder framework , which implements the encoder using a bidirectional long short term memory network ( LSTM ) and implements the decoder using a sim - 8 The configuration we used to run these open - source toolkits can be found in appendix D 9 https://github.com/mosessmt/mosesdecoder/blob/617e8c8/scripts/generic/multibleu.perl;mteval-v11b.pl",Baseline Systems,Baseline Systems,machine-translation,4,5,0.5555555555555556,182,0.7615062761506276,5,0.4166666666666667,1,0
184,en - de de - en en - fr fr- en zh - en are copied directly from their paper .,Baseline Systems,Baseline Systems,machine-translation,4,6,0.6666666666666666,183,0.7656903765690377,6,0.5,1,0
185,"We do not present the results of ( Artetxe et al. , 2017 b ) since we use different training sets .",Baseline Systems,Baseline Systems,machine-translation,4,7,0.7777777777777778,184,0.7698744769874477,7,0.5833333333333334,1,0
186,ple forward LSTM .,Baseline Systems,Baseline Systems,machine-translation,4,8,0.8888888888888888,185,0.7740585774058577,8,0.6666666666666666,1,0
187,They apply one single encoder and decoder for the source and target languages .,Baseline Systems,Baseline Systems,machine-translation,4,9,1.0,186,0.7782426778242678,9,0.75,1,0
188,Supervised training,,,machine-translation,4,0,0.0,187,0.7824267782426778,10,0.8333333333333334,1,1
189,"We finally consider exactly the same model as ours , but trained using the standard cross - entropy loss on the original parallel sentences .",Supervised training,Supervised training,machine-translation,4,1,0.5,188,0.7866108786610879,11,0.9166666666666666,1,0
190,This model can be viewed as an upper bound for the proposed unsupervised model .,Supervised training,Supervised training,machine-translation,4,2,1.0,189,0.7907949790794979,12,1.0,1,0
191,Results and Analysis,,,machine-translation,4,0,0.0,190,0.7949790794979079,0,0.0,1,0
192,Number of weight - sharing layers,Results and Analysis,Results and Analysis,machine-translation,4,1,0.0454545454545454,191,0.799163179916318,1,0.0454545454545454,1,0
193,We firstly investigate how the number of weightsharing layers affects the translation performance .,Results and Analysis,Results and Analysis,machine-translation,4,2,0.0909090909090909,192,0.803347280334728,2,0.0909090909090909,1,0
194,"In this experiment , we vary the number of weightsharing layers in the AEs from 0 to 4 .",Results and Analysis,Results and Analysis,machine-translation,4,3,0.1363636363636363,193,0.8075313807531381,3,0.1363636363636363,1,0
195,"Sharing one layer in AEs means sharing one layer for the encoders and in the meanwhile , sharing one layer for the decoders .",Results and Analysis,Results and Analysis,machine-translation,4,4,0.1818181818181818,194,0.8117154811715481,4,0.1818181818181818,1,0
196,"The BLEU scores of English - to - German , English - to - French and Chinese - to - English translation tasks are reported in figure",Results and Analysis,Results and Analysis,machine-translation,4,5,0.2272727272727272,195,0.8158995815899581,5,0.2272727272727272,1,0
197,2 . Each curve corresponds to a different translation task and the x - axis denotes the number of weight - sharing layers for the AEs .,Results and Analysis,Results and Analysis,machine-translation,4,6,0.2727272727272727,196,0.8200836820083682,6,0.2727272727272727,1,0
198,We find that the number of weight - sharing layers shows much effect on the translation performance .,Results and Analysis,Results and Analysis,machine-translation,4,7,0.3181818181818182,197,0.8242677824267782,7,0.3181818181818182,1,0
199,And the best translation performance is achieved when only one layer is shared in our system .,Results and Analysis,Results and Analysis,machine-translation,4,8,0.3636363636363636,198,0.8284518828451883,8,0.3636363636363636,1,1
200,"When all of the four layers are shared , i.e. , only one shared encoder is utilized , we get poor translation performance in all of the three translation tasks .",Results and Analysis,Results and Analysis,machine-translation,4,9,0.4090909090909091,199,0.8326359832635983,9,0.4090909090909091,1,1
201,This verifies our conjecture that the shared encoder is detrimental to the performance of unsupervised NMT especially for the translation tasks on distant language pairs .,Results and Analysis,Results and Analysis,machine-translation,4,10,0.4545454545454545,200,0.8368200836820083,10,0.4545454545454545,1,0
202,"More concretely , for the related language pair translation , i.e. , English - to - French , the encoder - shared model achieves - 0.53 BLEU points decline than the best model where only one layer is shared .",Results and Analysis,Results and Analysis,machine-translation,4,11,0.5,201,0.8410041841004184,11,0.5,1,0
203,"For the more distant language pair English - to - German , the encoder - shared model achieves more significant decline , i.e. , - 0.85 BLEU points decline .",Results and Analysis,Results and Analysis,machine-translation,4,12,0.5454545454545454,202,0.8451882845188284,12,0.5454545454545454,1,0
204,"And for the most distant language pair Chinese - to - English , the decline is as large as - 1.66 BLEU points .",Results and Analysis,Results and Analysis,machine-translation,4,13,0.5909090909090909,203,0.8493723849372385,13,0.5909090909090909,1,0
205,"We explain this as that the more distant the language pair is , the more different characteristics they have .",Results and Analysis,Results and Analysis,machine-translation,4,14,0.6363636363636364,204,0.8535564853556485,14,0.6363636363636364,1,0
206,And the shared encoder is weak in keeping the unique characteristic of each language .,Results and Analysis,Results and Analysis,machine-translation,4,15,0.6818181818181818,205,0.8577405857740585,15,0.6818181818181818,1,0
207,"Additionally , we also notice that using two completely independent encoders , i.e. , setting the number of weight - sharing layers as 0 , results in poor translation performance too .",Results and Analysis,Results and Analysis,machine-translation,4,16,0.7272727272727273,206,0.8619246861924686,16,0.7272727272727273,1,0
208,This confirms our intuition that the shared layers are vital to map the source and target latent representations to a shared - latent space .,Results and Analysis,Results and Analysis,machine-translation,4,17,0.7727272727272727,207,0.8661087866108786,17,0.7727272727272727,1,0
209,"In the rest of our experiments , we set the number of weightsharing layer as 1 . model only trained with monolingual data effectively learns to use the context information and the internal structure of each language .",Results and Analysis,Results and Analysis,machine-translation,4,18,0.8181818181818182,208,0.8702928870292888,18,0.8181818181818182,1,0
210,"Compared to the work of ( Lample et al. , 2017 ) , our model also achieves up to + 1.92 BLEU points improvement on English - to - French translation task .",Results and Analysis,Results and Analysis,machine-translation,4,19,0.8636363636363636,209,0.8744769874476988,19,0.8636363636363636,1,0
211,We believe that the unsupervised NMT is very promising .,Results and Analysis,Results and Analysis,machine-translation,4,20,0.9090909090909092,210,0.8786610878661087,20,0.9090909090909092,1,0
212,"However , there is still a large room for improvement compared to the supervised upper bound .",Results and Analysis,Results and Analysis,machine-translation,4,21,0.9545454545454546,211,0.8828451882845189,21,0.9545454545454546,1,0
213,The gap between the supervised and unsupervised model is as large as 12.3 - 25.5 BLEU points depending on the language pair and translation direction .,Results and Analysis,Results and Analysis,machine-translation,4,22,1.0,212,0.8870292887029289,22,1.0,1,0
214,Translation results,,,machine-translation,4,0,0.0,213,0.891213389121339,0,0.0,1,0
215,Ablation study,,,machine-translation,4,0,0.0,214,0.895397489539749,0,0.0,1,0
216,"To understand the importance of different components of the proposed system , we perform an ablation study by training multiple versions of our model with some missing components : the local GANs , the global GANs , the directional self - attention , the weight - sharing , the embeddingreinforced encoders , etc .",Ablation study,Ablation study,machine-translation,4,1,0.0909090909090909,215,0.899581589958159,1,0.0909090909090909,1,0
217,Results are reported in table 3 .,Ablation study,Ablation study,machine-translation,4,2,0.1818181818181818,216,0.9037656903765692,2,0.1818181818181818,1,0
218,"We do not test the the importance of the auto - encoding , back - translation and the pre-trained embeddings because they have been widely tested in .",Ablation study,Ablation study,machine-translation,4,3,0.2727272727272727,217,0.9079497907949792,3,0.2727272727272727,1,0
219,shows that the best performance is obtained with the simultaneous use of all the tested elements .,Ablation study,Ablation study,machine-translation,4,4,0.3636363636363636,218,0.9121338912133892,4,0.3636363636363636,1,0
220,"The most critical component is the weight - sharing constraint , which is vital to map sentences of different languages to the shared - latent space .",Ablation study,Ablation study,machine-translation,4,5,0.4545454545454545,219,0.9163179916317992,5,0.4545454545454545,1,1
221,The embedding - reinforced encoder also brings some improvement on all of the translation tasks .,Ablation study,Ablation study,machine-translation,4,6,0.5454545454545454,220,0.9205020920502092,6,0.5454545454545454,1,1
222,"When we remove the directional self - attention , we getup to - 0.3 BLEU points decline .",Ablation study,Ablation study,machine-translation,4,7,0.6363636363636364,221,0.9246861924686192,7,0.6363636363636364,1,1
223,This indicates that it deserves more efforts to investigate the temporal order information in self - attention mechanism .,Ablation study,Ablation study,machine-translation,4,8,0.7272727272727273,222,0.9288702928870292,8,0.7272727272727273,1,0
224,The GANs also significantly improve the translation performance of our system .,Ablation study,Ablation study,machine-translation,4,9,0.8181818181818182,223,0.9330543933054394,9,0.8181818181818182,1,1
225,"Specifically , the global GANs achieve improvement up to + 0.78 BLEU points on English - to - French translation and the local GANs also obtain improvement up to + 0.57 BLEU points on English - to - French translation .",Ablation study,Ablation study,machine-translation,4,10,0.9090909090909092,224,0.9372384937238494,10,0.9090909090909092,1,0
226,This reveals that the proposed model benefits a lot from the crossdomain loss defined by GANs .,Ablation study,Ablation study,machine-translation,4,11,1.0,225,0.9414225941422594,11,1.0,1,0
227,Conclusion and Future work,,,machine-translation,4,0,0.0,226,0.9456066945606696,0,0.0,1,0
228,The models proposed recently for unsupervised NMT use a single encoder to map sentences from different languages to a shared - latent space .,Conclusion and Future work,Conclusion and Future work,machine-translation,4,1,0.0833333333333333,227,0.9497907949790796,1,0.0833333333333333,0,0
229,We conjecture that the shared encoder is problematic for keeping the unique and inherent characteristic of each language .,Conclusion and Future work,Conclusion and Future work,machine-translation,4,2,0.1666666666666666,228,0.9539748953974896,2,0.1666666666666666,0,0
230,"In this paper , we propose the weight - sharing constraint in unsupervised NMT to address this issue .",Conclusion and Future work,Conclusion and Future work,machine-translation,4,3,0.25,229,0.9581589958158996,3,0.25,0,0
231,"To enhance the cross - language translation performance , we also propose the embedding - reinforced encoders , local GAN and global GAN into the proposed system .",Conclusion and Future work,Conclusion and Future work,machine-translation,4,4,0.3333333333333333,230,0.9623430962343096,4,0.3333333333333333,0,0
232,"Additionally , the directional self - attention is introduced to model the temporal order information for our system .",Conclusion and Future work,Conclusion and Future work,machine-translation,4,5,0.4166666666666667,231,0.9665271966527196,5,0.4166666666666667,0,0
233,"We test the proposed model on English - German , English - French and Chinese - to - English translation tasks .",Conclusion and Future work,Conclusion and Future work,machine-translation,4,6,0.5,232,0.9707112970711296,6,0.5,0,0
234,The experimental results reveal that our approach achieves significant improvement and verify our conjecture that the shared encoder is really a bottleneck for improving the unsupervised NMT .,Conclusion and Future work,Conclusion and Future work,machine-translation,4,7,0.5833333333333334,233,0.9748953974895398,7,0.5833333333333334,0,0
235,The ablation study shows that each component of our system achieves some improvement for the final translation performance .,Conclusion and Future work,Conclusion and Future work,machine-translation,4,8,0.6666666666666666,234,0.9790794979079498,8,0.6666666666666666,0,0
236,Unsupervised NMT opens exciting opportunities for the future research .,Conclusion and Future work,Conclusion and Future work,machine-translation,4,9,0.75,235,0.9832635983263598,9,0.75,0,0
237,"However , there is still a large room for improvement compared to the supervised NMT .",Conclusion and Future work,Conclusion and Future work,machine-translation,4,10,0.8333333333333334,236,0.98744769874477,10,0.8333333333333334,0,0
238,"In the future , we would like to investigate how to utilize the monolingual data more effectively , such as incorporating the language model and syntactic information into unsupervised NMT .",Conclusion and Future work,Conclusion and Future work,machine-translation,4,11,0.9166666666666666,237,0.99163179916318,11,0.9166666666666666,0,0
239,"Besides , we decide to make more efforts to explore how to reinforce the temporal order information for the proposed model .",Conclusion and Future work,Conclusion and Future work,machine-translation,4,12,1.0,238,0.99581589958159,12,1.0,0,0
1,title,,,machine-translation,5,0,0.0,0,0.0,0,0.0,1,0
2,Tilde 's Machine Translation Systems for WMT 2018,title,title,machine-translation,5,1,0.0,1,0.0069930069930069,1,0.0,1,1
3,abstract,,,machine-translation,5,0,0.0,2,0.0139860139860139,0,0.0,1,0
4,The paper describes the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation .,abstract,abstract,machine-translation,5,1,0.25,3,0.0209790209790209,1,0.25,1,1
5,"We describe the data filtering and pre-processing workflows , the NMT system training architectures , and automatic evaluation results .",abstract,abstract,machine-translation,5,2,0.5,4,0.0279720279720279,2,0.5,1,0
6,"For the WMT 2018 shared task , we submitted seven systems ( both constrained and unconstrained ) for English - Estonian and Estonian - English translation directions .",abstract,abstract,machine-translation,5,3,0.75,5,0.0349650349650349,3,0.75,1,0
7,The submitted systems were trained using Transformer models .,abstract,abstract,machine-translation,5,4,1.0,6,0.0419580419580419,4,1.0,1,0
8,Introduction,,,machine-translation,5,0,0.0,7,0.0489510489510489,0,0.0,1,0
9,Neural machine translation ( NMT ) is a rapidly changing research area .,Introduction,Introduction,machine-translation,5,1,0.0666666666666666,8,0.0559440559440559,1,0.0666666666666666,1,1
10,"Since 2016 when NMT systems first showed to achieve significantly better results than statistical machine translation ( SMT ) systems , the dominant neural network ( NN ) architectures for NMT have changed on a yearly ( and even more frequent ) basis .",Introduction,Introduction,machine-translation,5,2,0.1333333333333333,9,0.0629370629370629,2,0.1333333333333333,1,0
11,The state - of - the - art in 2016 were shallow attention - based recurrent neural networks ( RNN ) with gated recurrent units ( GRU ) in recurrent layers .,Introduction,Introduction,machine-translation,5,3,0.2,10,0.0699300699300699,3,0.2,1,0
12,"In 2017 , multiplicative long short - term memory ( MLSTM ) units and deep GRU models were introduced in NMT .",Introduction,Introduction,machine-translation,5,4,0.2666666666666666,11,0.0769230769230769,4,0.2666666666666666,1,0
13,"The same year , selfattentional ( Transformer ) models were introduced .",Introduction,Introduction,machine-translation,5,5,0.3333333333333333,12,0.0839160839160839,5,0.3333333333333333,1,0
14,"Consequently , in 2018 , most of the top scoring systems in the shared task on news translation of the Third Conference on Machine Translation ( WMT ) were trained using Transformer models",Introduction,Introduction,machine-translation,5,6,0.4,13,0.0909090909090909,6,0.4,1,0
15,1 .,Introduction,Introduction,machine-translation,5,7,0.4666666666666667,14,0.0979020979020979,7,0.4666666666666667,1,0
16,"However , it is already evident that the state - of - the - art architectures will 1 All 14 of the best automatically scored systems according to the information provided by participants in the official submission portal http://matrix.statmt.org were indicated as being based on Transformer models .",Introduction,Introduction,machine-translation,5,8,0.5333333333333333,15,0.1048951048951049,8,0.5333333333333333,1,0
17,be pushed even further in 2018 .,Introduction,Introduction,machine-translation,5,9,0.6,16,0.1118881118881118,9,0.6,1,0
18,"For instance , have recently proposed RNMT + models that combine deep LSTM - based models with multi-head attention and showed that the models outperform Transformer models .",Introduction,Introduction,machine-translation,5,10,0.6666666666666666,17,0.1188811188811188,10,0.6666666666666666,1,0
19,"In WMT 2017 , Tilde participated with MLSTM - based NMT systems .",Introduction,Introduction,machine-translation,5,11,0.7333333333333333,18,0.1258741258741259,11,0.7333333333333333,1,0
20,"In this paper , we compare the MLSTMbased models with Transformer models for English - Estonian and Estonian - English and we show that the state - of - the - art of WMT 2017 is well behind the new models .",Introduction,Introduction,machine-translation,5,12,0.8,19,0.1328671328671328,12,0.8,1,0
21,"Therefore , for WMT 2018 , Tilde submitted NMT systems that were trained using Transformer models .",Introduction,Introduction,machine-translation,5,13,0.8666666666666667,20,0.1398601398601398,13,0.8666666666666667,1,0
22,The paper is further structured as follows :,Introduction,Introduction,machine-translation,5,14,0.9333333333333332,21,0.1468531468531468,14,0.9333333333333332,1,0
23,"Section 2 provides an overview of systems submitted for the WMT 2018 shared task on news translation , Section 3 describes the data used to train the NMT systems and the data pre-processing workflows , Section 4 describes all NMT systems trained and experiments on handling of named entities and combination of systems , Section 5 provides automatic evaluation results , and Section 6 concludes the paper .",Introduction,Introduction,machine-translation,5,15,1.0,22,0.1538461538461538,15,1.0,1,0
24,System Overview,,,machine-translation,5,0,0.0,23,0.1608391608391608,0,0.0,1,0
25,"For the WMT 2018 shared task on news translation , Tilde submitted both constrained and unconstrained NMT systems ( 7 in total ) .",System Overview,System Overview,machine-translation,5,1,0.0136986301369863,24,0.1678321678321678,1,0.1111111111111111,1,1
26,The following is a list of the five MT systems submitted :,System Overview,System Overview,machine-translation,5,2,0.0273972602739726,25,0.1748251748251748,2,0.2222222222222222,1,1
27,Constrained English - Estonian and Estonian - English NMT systems ( tilde - c-nmt ) that were deployed as ensembles of averaged factored data ( see Section 3 ) Transformer models .,System Overview,System Overview,machine-translation,5,3,0.0410958904109589,26,0.1818181818181818,3,0.3333333333333333,1,1
28,The models were trained using parallel data and back - translated data in a 1 - to - 1 proportion .,System Overview,System Overview,machine-translation,5,4,0.0547945205479452,27,0.1888111888111888,4,0.4444444444444444,1,1
29,Unconstrained English - Estonian and Estonian - English NMT systems ( tilde - nc - nmt ) that were deployed as averaged Transformer models .,System Overview,System Overview,machine-translation,5,5,0.0684931506849315,28,0.1958041958041958,5,0.5555555555555556,1,1
30,"These models were also trained using back - translated data similarly to the constrained systems , however , the data , taking into account their relatively large size , were not factored .",System Overview,System Overview,machine-translation,5,6,0.0821917808219178,29,0.2027972027972027,6,0.6666666666666666,1,1
31,A constrained Estonian - English NMT system ( tilde - c - nmt - comb ) that is a system combination of six factored data NMT systems .,System Overview,System Overview,machine-translation,5,7,0.0958904109589041,30,0.2097902097902098,7,0.7777777777777778,1,1
32,Constrained English - Estonian and Estonian - English NMT systems ( tilde - c - nmt - 2 bt ) averaged from multiple best NMT models .,System Overview,System Overview,machine-translation,5,8,0.1095890410958904,31,0.2167832167832167,8,0.8888888888888888,1,1
33,The models were trained using two sets of back - translated data in a 1 - to - 1 proportion to the clean parallel data - one set was backtranslated using a system trained on parallelonly data and the other set -using an NMT system trained on parallel data and the first set of back - translated data .,System Overview,System Overview,machine-translation,5,9,0.1232876712328767,32,0.2237762237762237,9,1.0,1,1
34,Data,System Overview,,machine-translation,5,10,0.136986301369863,33,0.2307692307692307,0,0.0,1,0
35,"Data preparation was done using one of two distinct workflows - we used the full workflow for tilde - c - nmt , tilde - nc - nmt and tilde - c - nmt - comb submissions .",System Overview,Data,machine-translation,5,11,0.1506849315068493,34,0.2377622377622377,1,0.5,1,0
36,For the tilde - c - nmt - 2 bt submission we used the light data preparation workflow .,System Overview,Data,machine-translation,5,12,0.1643835616438356,35,0.2447552447552447,2,1.0,1,0
37,Full Workflow,System Overview,,machine-translation,5,13,0.1780821917808219,36,0.2517482517482518,0,0.0,1,0
38,"First , we trained constrained system baseline models using the filtered datasets .",System Overview,Full Workflow,machine-translation,5,14,0.1917808219178082,37,0.2587412587412587,1,0.0909090909090909,1,0
39,"For baseline models , we used the MLSTM and transf configurations ( see ) .",System Overview,Full Workflow,machine-translation,5,15,0.2054794520547945,38,0.2657342657342657,2,0.1818181818181818,1,0
40,"Then , we used the best - performing models ( based on translation quality on the vali -dation set ) , which were the Transformer models ( see , and back - translated monolingual data .",System Overview,Full Workflow,machine-translation,5,16,0.2191780821917808,39,0.2727272727272727,3,0.2727272727272727,1,0
41,"As mentioned before , for the unconstrained systems , we back - translated the monolingual data using pre-existing MLSTM - based NMT systems .",System Overview,Full Workflow,machine-translation,5,17,0.2328767123287671,40,0.2797202797202797,4,0.3636363636363636,1,0
42,"Then , using the final training data ( parallel and the two synthetic corpora ) , we trained final Transformer models .",System Overview,Full Workflow,machine-translation,5,18,0.2465753424657534,41,0.2867132867132867,5,0.4545454545454545,1,0
43,"For the constrained scenario , we trained multiple models ( three for each translation direction ) by experimenting with multiple model configurations .",System Overview,Full Workflow,machine-translation,5,19,0.2602739726027397,42,0.2937062937062937,6,0.5454545454545454,1,0
44,"For the unconstrained scenario , we trained one model in each of the directions .",System Overview,Full Workflow,machine-translation,5,20,0.273972602739726,43,0.3006993006993007,7,0.6363636363636364,1,0
45,"In order to acquire the translations for the submissions , we performed model averaging and ensembling as follows :",System Overview,Full Workflow,machine-translation,5,21,0.2876712328767123,44,0.3076923076923077,8,0.7272727272727273,1,0
46,"For the tilde - c - nmt ( constrained NMT ) systems , we performed model averaging of the best four models ( according to perplexity ) of the three different run NMT systems and deployed the averaged models in an ensemble .",System Overview,Full Workflow,machine-translation,5,22,0.3013698630136986,45,0.3146853146853147,9,0.8181818181818182,1,0
47,"For the tilde - nc - nmt ( unconstrained NMT ) systems , we performed model averaging of the best four models .",System Overview,Full Workflow,machine-translation,5,23,0.3150684931506849,46,0.3216783216783216,10,0.9090909090909092,1,0
48,"For the tilde - c - nmt - comb Estonian - English system , we performed majority voting ( see Section 4.3 ) of translations produced by six different runs of different constrained systems ( using best BLEU models , averaged models , ensembled averaged models , ensembled models , and larger beam search ( 10 instead of 5 ) ) .",System Overview,Full Workflow,machine-translation,5,24,0.3287671232876712,47,0.3286713286713286,11,1.0,1,0
49,Data Filtering,System Overview,,machine-translation,5,25,0.3424657534246575,48,0.3356643356643357,0,0.0,1,0
50,"As NMT systems are sensitive to noise in parallel data , all parallel data were filtered using the parallel data filtering methods described by .",System Overview,Data Filtering,machine-translation,5,26,0.3561643835616438,49,0.3426573426573426,1,0.0714285714285714,1,0
51,"The parallel corpora filtering methods remove sentence pairs that have indications of data corruption or low parallelity ( e.g. , source - target length ratio , content overlap , digit mismatch , language adherence , etc. ) issues .",System Overview,Data Filtering,machine-translation,5,27,0.3698630136986301,50,0.3496503496503496,2,0.1428571428571428,1,0
52,"Contrary to Tilde 's submissions for WMT 2017 , isolated sentence pair filtering for the WMT 2018 submissions was supplemented with a maximum content overlap filter ( i.e. only one target sentence for each source sentence was preserved and vice versa based on the content overlap filter 's score for each sentence pair ) .",System Overview,Data Filtering,machine-translation,5,28,0.3835616438356164,51,0.3566433566433566,3,0.2142857142857142,1,0
53,"For filtering , we required probabilistic dictionaries , which were obtained from the parallel corpora ( different dictionaries for the constrained and unconstrained scenarios ) using fast align .",System Overview,Data Filtering,machine-translation,5,29,0.3972602739726027,52,0.3636363636363636,4,0.2857142857142857,1,0
54,The dictionaries were filtered using the transliteration - based probabilistic dictionary filtering method by .,System Overview,Data Filtering,machine-translation,5,30,0.410958904109589,53,0.3706293706293706,5,0.3571428571428571,1,0
55,"During filtering , we identified that one of the corpora that were provided by the organisers contained a significant amount of data corruption .",System Overview,Data Filtering,machine-translation,5,31,0.4246575342465753,54,0.3776223776223776,6,0.4285714285714285,1,0
56,It was the Estonian ?,System Overview,Data Filtering,machine-translation,5,32,0.4383561643835616,55,0.3846153846153846,7,0.5,1,0
57,English ParaCrawl corpus,System Overview,Data Filtering,machine-translation,5,33,0.4520547945205479,56,0.3916083916083916,8,0.5714285714285714,1,0
58,3 .,System Overview,Data Filtering,machine-translation,5,34,0.4657534246575342,57,0.3986013986013986,9,0.6428571428571429,1,0
59,The corpus consisted of 1.30 million sentence pairs out of which 0.77 million were identified as being corrupt .,System Overview,Data Filtering,machine-translation,5,35,0.4794520547945205,58,0.4055944055944055,10,0.7142857142857143,1,0
60,"To reduce the high level of noise , this corpus was filtered using stricter content overlap ( a threshold of 0.3 instead of 0.1 ) and language adherence filters ( both the language detection and the valid alphabet filters had to validate a sentence pair instead of just one of the filters ) than all other corpora .",System Overview,Data Filtering,machine-translation,5,36,0.4931506849315068,59,0.4125874125874126,11,0.7857142857142857,1,0
61,"As a result , only 0.17 million sentence pairs from the ParaCrawl corpus were used for training of the constrained systems .",System Overview,Data Filtering,machine-translation,5,37,0.5068493150684932,60,0.4195804195804196,12,0.8571428571428571,1,0
62,"Due to the quality concerns , the corpus was not used for training of the unconstrained systems .",System Overview,Data Filtering,machine-translation,5,38,0.5205479452054794,61,0.4265734265734265,13,0.9285714285714286,1,0
63,The corpora statistics before and after filtering are provided in .,System Overview,Data Filtering,machine-translation,5,39,0.5342465753424658,62,0.4335664335664335,14,1.0,1,0
64,Data Pre-processing,System Overview,,machine-translation,5,40,0.547945205479452,63,0.4405594405594406,0,0.0,1,0
65,All corpora were pre-processed using the parallel data pre-processing workflow from the Tilde MT platform ) that performs the following pre-processing steps :,System Overview,Data Pre-processing,machine-translation,5,41,0.5616438356164384,64,0.4475524475524475,1,0.0769230769230769,1,0
66,"First , parallel corpora are cleaned by removing HTML and XML tags , decoding escaped symbols , normalising whitespaces and punctuation marks , replacing control characters with spaces , etc .",System Overview,Data Pre-processing,machine-translation,5,42,0.5753424657534246,65,0.4545454545454545,2,0.1538461538461538,1,0
67,This step is performed only on the training data .,System Overview,Data Pre-processing,machine-translation,5,43,0.589041095890411,66,0.4615384615384615,3,0.2307692307692307,1,0
68,"Then , non-translatable entities , such as email addresses , URLs , file paths , etc. are identified and replaced with place - holders .",System Overview,Data Pre-processing,machine-translation,5,44,0.6027397260273972,67,0.4685314685314685,4,0.3076923076923077,1,0
69,This allows reducing data sparsity where it is not needed .,System Overview,Data Pre-processing,machine-translation,5,45,0.6164383561643836,68,0.4755244755244755,5,0.3846153846153846,1,0
70,"Then , the data are tokenised using the Tilde MT regular expression - based tokeniser .",System Overview,Data Pre-processing,machine-translation,5,46,0.6301369863013698,69,0.4825174825174825,6,0.4615384615384615,1,0
71,The Moses truecasing script truecase .,System Overview,Data Pre-processing,machine-translation,5,47,0.6438356164383562,70,0.4895104895104895,7,0.5384615384615384,1,0
72,perl is used to truecase the first word of every sentence .,System Overview,Data Pre-processing,machine-translation,5,48,0.6575342465753424,71,0.4965034965034965,8,0.6153846153846154,1,0
73,"Then , tokens are split into sub - word units using byte - pair encoding ( BPE ) .",System Overview,Data Pre-processing,machine-translation,5,49,0.6712328767123288,72,0.5034965034965035,9,0.6923076923076923,1,0
74,"For the constrained and unconstrained systems , we use BPE models consisting of 24,500 and 49,500 merging operations respectively .",System Overview,Data Pre-processing,machine-translation,5,50,0.684931506849315,73,0.5104895104895105,10,0.7692307692307693,1,0
75,"Finally , data for the constrained systems are factored using an averaged perceptron - based morpho-syntactic tagger for Estonian and the lexicalized probabilistic parser , we introduce also a factor indicating a word part 's position in a word ( beginning , middle , end , or the word part represents the whole word - B , I , E , or O ) .",System Overview,Data Pre-processing,machine-translation,5,51,0.6986301369863014,74,0.5174825174825175,11,0.8461538461538461,1,0
76,"As a result , the Estonian data consist of the the following factors : word part , position , lemma , and morpho-syntactic tag .",System Overview,Data Pre-processing,machine-translation,5,52,0.7123287671232876,75,0.5244755244755245,12,0.9230769230769232,1,0
77,"The English data consist of the following factors : word part , position , lemma , part - of - speech tag , and syntactic function .",System Overview,Data Pre-processing,machine-translation,5,53,0.726027397260274,76,0.5314685314685315,13,1.0,1,0
78,Synthetic Data,System Overview,,machine-translation,5,54,0.7397260273972602,77,0.5384615384615384,0,0.0,1,0
79,"Similarly to Tilde 's 2017 systems , we submitted systems that were trained using synthetic data :",System Overview,Synthetic Data,machine-translation,5,55,0.7534246575342466,78,0.5454545454545454,1,0.1,1,0
80,"1 ) back - translated data , and 2 ) data infused with unknown token identifiers .",System Overview,Synthetic Data,machine-translation,5,56,0.7671232876712328,79,0.5524475524475524,2,0.2,1,0
81,"The back - translated data allow performing domain adaptation and the second type of synthetic data allow training NMT models that are robust to unknown phenomena ( e.g. , code - mixed content , target language words in the source text , rare or unseen words , etc . ) .",System Overview,Synthetic Data,machine-translation,5,57,0.7808219178082192,80,0.5594405594405595,3,0.3,1,0
82,"To create the synthetic corpora with unknown phenomena , we extracted fast align the parallel corpora and randomly replaced one to three unambiguously ( one - to - one ) aligned content words with unknown word identifiers .",System Overview,Synthetic Data,machine-translation,5,58,0.7945205479452054,81,0.5664335664335665,4,0.4,1,0
83,"These synthetic corpora were added to the parallel corpora , thereby almost doubling the sizes of the available training data .",System Overview,Synthetic Data,machine-translation,5,59,0.8082191780821918,82,0.5734265734265734,5,0.5,1,0
84,The back - translated data were acquired from two sources :,System Overview,Synthetic Data,machine-translation,5,60,0.821917808219178,83,0.5804195804195804,6,0.6,1,0
85,"1 ) the constrained system data were acquired from initial Transformer - based NMT systems that were trained on the filtered and preprocessed parallel data , which were supplemented with the unknown phenomena infused data , and 2 ) the unconstrained system data were acquired from pre-existing unconstrained MLSTM - based NMT systems - the NMT systems that were developed by Tilde for the Estonian EU Council Presidency in 2017 .",System Overview,Synthetic Data,machine-translation,5,61,0.8356164383561644,84,0.5874125874125874,7,0.7,1,0
86,"In order to limit noise , the back - translated data were filtered using the same parallel data filtering methods that were described in Section 3.1.1 ( although with a higher threshold for the content overlap filter ) .",System Overview,Synthetic Data,machine-translation,5,62,0.8493150684931506,85,0.5944055944055944,8,0.8,1,0
87,"Furthermore , in order to train the final systems , we also generated unknown phenomena infused data for the back - translated filtered data , thereby also almost doubling the sizes of the back - translated data .",System Overview,Synthetic Data,machine-translation,5,63,0.863013698630137,86,0.6013986013986014,9,0.9,1,0
88,The synthetic corpora statistics and the sizes of the total training data are given in .,System Overview,Synthetic Data,machine-translation,5,64,0.8767123287671232,87,0.6083916083916084,10,1.0,1,0
89,Light Workflow,System Overview,,machine-translation,5,65,0.8904109589041096,88,0.6153846153846154,0,0.0,1,0
90,The light workflow was used to produce the tilde - c - nmt - 2 bt ( constrained NMT with two sets of back - translated data ) systems .,System Overview,Light Workflow,machine-translation,5,66,0.9041095890410958,89,0.6223776223776224,1,0.04,1,0
91,"First , we trained baseline models using only filtered parallel datasets ( Parallel - only in ) .",System Overview,Light Workflow,machine-translation,5,67,0.9178082191780822,90,0.6293706293706294,2,0.08,1,0
92,"Then , we back - translated the first batches of monolingual news data and trained intermediate NMT systems ( Parallel + First Back - translated ) .",System Overview,Light Workflow,machine-translation,5,68,0.9315068493150684,91,0.6363636363636364,3,0.12,1,0
93,"Finally , we used the intermediate NMT systems to backtranslate the second batches of monolingual news data and trained final NMT systems ( Parallel + Second Back - translated ) .",System Overview,Light Workflow,machine-translation,5,69,0.9452054794520548,92,0.6433566433566433,4,0.16,1,0
94,"The training progress in shows that the English - Estonian system benefits from the additional data , but the system in the other direction - not so much .",System Overview,Light Workflow,machine-translation,5,70,0.958904109589041,93,0.6503496503496503,5,0.2,1,0
95,"For the final translations , we used a postprocessing script to replace consecutive repeating n-grams and repeating ngrams that have a preposition between them ( i.e. , victim of the victim ) with a single n-gram .",System Overview,Light Workflow,machine-translation,5,71,0.9726027397260274,94,0.6573426573426573,6,0.24,1,0
96,"This problem was more apparent in RNN - based NMT systems , but it was also noticable in our Transformer model outputs .",System Overview,Light Workflow,machine-translation,5,72,0.9863013698630136,95,0.6643356643356644,7,0.28,1,0
97,NMT,System Overview,,machine-translation,5,73,1.0,96,0.6713286713286714,8,0.32,1,0
98,Systems,,,machine-translation,5,0,0.0,97,0.6783216783216783,9,0.36,1,0
99,"In order to train the NMT systems , we used the Nematus ( Sennrich et al. , 2017 b ) ( for MLSTM models ) and Sockeye ) ( for Transformer models ) toolkits .",Systems,Systems,machine-translation,5,1,0.0625,98,0.6853146853146853,10,0.4,1,0
100,"All models were trained until convergence ( i.e. , until an early stopping criterion was met ) .",Systems,Systems,machine-translation,5,2,0.125,99,0.6923076923076923,11,0.44,1,0
101,Figure 1 : NMT system training progress ( BLEU scores on the validation set ) for English - Estonian ( left ) and,Systems,Systems,machine-translation,5,3,0.1875,100,0.6993006993006993,12,0.48,1,0
102,Estonian - English ( right ) .,Systems,Systems,machine-translation,5,4,0.25,101,0.7062937062937062,13,0.52,1,0
103,"Note that batch size may differ between different architectures and BLEU scores are calculated on raw ( token level ) pre-processed validation sets , therefore , the scores are slightly higher than evaluation results for the final translations !",Systems,Systems,machine-translation,5,5,0.3125,102,0.7132867132867133,14,0.56,1,0
104,Automatic Post - editing of Named Entities,Systems,,machine-translation,5,6,0.375,103,0.7202797202797203,15,0.6,1,0
105,"NMT models so far have struggled with translating rare or unseen words ( not different surface forms , but rather different words ) correctly .",Systems,Automatic Post - editing of Named Entities,machine-translation,5,7,0.4375,104,0.7272727272727273,16,0.64,1,0
106,"Named entities and non-translatable entities ( various product names , identifiers , etc. ) are often rare or unknown .",Systems,Automatic Post - editing of Named Entities,machine-translation,5,8,0.5,105,0.7342657342657343,17,0.68,1,0
107,"In order to aid the NMT model in translating such tokens better , we extracted named entity and non-translatable token dictionaries from the parallel corpora .",Systems,Automatic Post - editing of Named Entities,machine-translation,5,9,0.5625,106,0.7412587412587412,18,0.72,1,0
108,"This was done by performing word alignment of the parallel corpora using fast align and searching ( in a language - agnostic manner ) for transliterated source - target word pairs using a similarity metric based on Levenshtein distance , which start with upper-case letters .",Systems,Automatic Post - editing of Named Entities,machine-translation,5,10,0.625,107,0.7482517482517482,19,0.76,1,0
109,The dictionaries consist of 15.6 ( 94.7 ) thousand and 6.2 ( 149.8 ) thousand entries for the constrained ( unconstrained ) English - Estonian and Estonian - English NMT systems respectively .,Systems,Automatic Post - editing of Named Entities,machine-translation,5,11,0.6875,108,0.7552447552447552,20,0.8,1,0
110,"When the NMT systems had translated a sentence , source - to - target word alignment was extracted from the source sentence and the translation .",Systems,Automatic Post - editing of Named Entities,machine-translation,5,12,0.75,109,0.7622377622377622,21,0.84,1,0
111,"Then named entity recognition ( based on dictionary look - up ) was performed on the source text and , if a named entity was found , the target translation was validated against the entries in the dic-tionary .",Systems,Automatic Post - editing of Named Entities,machine-translation,5,13,0.8125,110,0.7692307692307693,22,0.88,1,0
112,"In order to capture different surface forms , a stemming tool was used .",Systems,Automatic Post - editing of Named Entities,machine-translation,5,14,0.875,111,0.7762237762237763,23,0.92,1,0
113,"If a translation was contradicting the entries in the dictionary , it was replaced with the closest matching ( by looking for the longest matching suffix ) translation from the dictionary .",Systems,Automatic Post - editing of Named Entities,machine-translation,5,15,0.9375,112,0.7832167832167832,24,0.96,1,0
114,"The automatic post-editing method for named entities has a marginal impact on translation quality , however , manual analysis showed that more named entities were corrected than ruined .",Systems,Automatic Post - editing of Named Entities,machine-translation,5,16,1.0,113,0.7902097902097902,25,1.0,1,0
115,System Combination,,,machine-translation,5,0,0.0,114,0.7972027972027972,0,0.0,1,0
116,We attempted to increase the quality of existing translations by employing a voting scheme in which multiple machine translation outputs are combined to produce a single translation .,System Combination,System Combination,machine-translation,5,1,0.0909090909090909,115,0.8041958041958042,1,0.0909090909090909,1,0
117,We used a custom implementation of the majority voting algorithm to combine six of our best - scoring outputs in the Estonian - English translation direction in the constrained scenario .,System Combination,System Combination,machine-translation,5,2,0.1818181818181818,116,0.8111888111888111,2,0.1818181818181818,1,0
118,We did not perform the combination for English - Estonian due to lack of support for alignment extraction for Estonian in Meteor .,System Combination,System Combination,machine-translation,5,3,0.2727272727272727,117,0.8181818181818182,3,0.2727272727272727,1,0
119,MT system translation combination happens on the sentence level .,System Combination,System Combination,machine-translation,5,4,0.3636363636363636,118,0.8251748251748252,4,0.3636363636363636,1,0
120,The majority voting scheme assumes a single base translation hypothesis ( primary hypothesis ) which is aligned at the word level to each of the other hypotheses ( secondary hypotheses ) .,System Combination,System Combination,machine-translation,5,5,0.4545454545454545,119,0.8321678321678322,5,0.4545454545454545,1,0
121,The alignments are used to generate a table of all possible word translations relative to each position in the primary hypothesis .,System Combination,System Combination,machine-translation,5,6,0.5454545454545454,120,0.8391608391608392,6,0.5454545454545454,1,0
122,The table is then used to count the number of occurrences of different translations .,System Combination,System Combination,machine-translation,5,7,0.6363636363636364,121,0.8461538461538461,7,0.6363636363636364,1,0
123,The word translations with the highest count at each position constitute the resulting combined hypothesis .,System Combination,System Combination,machine-translation,5,8,0.7272727272727273,122,0.8531468531468531,8,0.7272727272727273,1,0
124,To acquire the necessary word alignments we used Meteor .,System Combination,System Combination,machine-translation,5,9,0.8181818181818182,123,0.8601398601398601,9,0.8181818181818182,1,0
125,Meteor outputs were then converted to a more easily manageable form using the Jane toolkit ) ( we used an awk script distributed with Jane ) .,System Combination,System Combination,machine-translation,5,10,0.9090909090909092,124,0.8671328671328671,10,0.9090909090909092,1,0
126,The majority voting algorithm was implemented in Python .,System Combination,System Combination,machine-translation,5,11,1.0,125,0.8741258741258742,11,1.0,1,0
127,Results,,,machine-translation,5,0,0.0,126,0.8811188811188811,0,0.0,1,0
128,We performed automatic evaluation of the NMT systems using the SacreBLEU evaluation tool .,Results,Results,machine-translation,5,1,0.125,127,0.8881118881118881,1,0.125,1,0
129,The results ( see ) show that the Transformer models achieved better results than the MLSTM - based models .,Results,Results,machine-translation,5,2,0.25,128,0.8951048951048951,2,0.25,1,1
130,"For the constrained scenarios , both ensembles of averaged models achieved higher scores than each individual averaged model .",Results,Results,machine-translation,5,3,0.375,129,0.902097902097902,3,0.375,1,1
131,It is also evident that the unconstrained models ( tilde - nc - nmt ) achieved the best results .,Results,Results,machine-translation,5,4,0.5,130,0.9090909090909092,4,0.5,1,1
132,"Although the unconstrained models were not trained on factored data , the datasets were 17 times larger than the constrained datasets .",Results,Results,machine-translation,5,5,0.625,131,0.916083916083916,5,0.625,1,0
133,"However , the difference is rather minimal and shows that the current NMT architectures may notable to learn effectively from large datasets .",Results,Results,machine-translation,5,6,0.75,132,0.9230769230769232,6,0.75,1,0
134,The official human evaluation results ( see Table 5 ) from the WMT 2018 shared task on news translation our unconstrained scenario systems ( tilde - nc - nmt ) ranked significantly higher than any other submission for both translation directions .,Results,Results,machine-translation,5,7,0.875,133,0.93006993006993,7,0.875,1,0
135,"Our best constrained systems were the second highest ranked systems among all constrained scenario systems , at the same time sharing the same cluster with the highest ranked systems .",Results,Results,machine-translation,5,8,1.0,134,0.9370629370629372,8,1.0,1,0
136,Conclusion,,,machine-translation,5,0,0.0,135,0.944055944055944,0,0.0,1,0
137,The paper described the development process of the Tilde 's NMT systems that were submitted for the WMT 2018 shared task on news translation .,Conclusion,Conclusion,machine-translation,5,1,0.1428571428571428,136,0.951048951048951,1,0.1428571428571428,0,0
138,We compared Transformer models to MLSTMbased models and showed that the Transformer models outperform the older NMT architecture .,Conclusion,Conclusion,machine-translation,5,2,0.2857142857142857,137,0.958041958041958,2,0.2857142857142857,0,0
139,We also showed that double back - translation may improve translation quality further than single back - translation .,Conclusion,Conclusion,machine-translation,5,3,0.4285714285714285,138,0.965034965034965,3,0.4285714285714285,0,0
140,"In terms of model ensembling and averaging , we showed that the best results in the constrained scenario were achieved by en - :",Conclusion,Conclusion,machine-translation,5,4,0.5714285714285714,139,0.972027972027972,4,0.5714285714285714,0,0
141,Top three systems for the constrained ( C ) and unconstrained ( U ) scenarios according to the official results of the WMT 2018 shared task on news translation ; ordered by the direct assessment ( DA ) standardized mean score sembling different run averaged models .,Conclusion,Conclusion,machine-translation,5,5,0.7142857142857143,140,0.9790209790209792,5,0.7142857142857143,0,0
142,"In total , seven systems were submitted by Tilde for the English ?",Conclusion,Conclusion,machine-translation,5,6,0.8571428571428571,141,0.986013986013986,6,0.8571428571428571,0,0
143,Estonian language pair .,Conclusion,Conclusion,machine-translation,5,7,1.0,142,0.993006993006993,7,1.0,0,0
1,title,,,machine-translation,6,0,0.0,0,0.0,0,0.0,1,0
2,FRAGE : Frequency - Agnostic Word Representation,title,title,machine-translation,6,1,0.0,1,0.0034364261168384,1,0.0,1,1
3,abstract,,,machine-translation,6,0,0.0,2,0.0068728522336769,0,0.0,1,0
4,Continuous word representation ( aka word embedding ) is a basic building block in many neural network - based models used in natural language processing tasks .,abstract,abstract,machine-translation,6,1,0.1666666666666666,3,0.0103092783505154,1,0.1666666666666666,1,0
5,"Although it is widely accepted that words with similar semantics should be close to each other in the embedding space , we find that word embeddings learned in several tasks are biased towards word frequency : the embeddings of highfrequency and low - frequency words lie in different subregions of the embedding space , and the embedding of a rare word and a popular word can be far from each other even if they are semantically similar .",abstract,abstract,machine-translation,6,2,0.3333333333333333,4,0.0137457044673539,2,0.3333333333333333,1,1
6,"This makes learned word embeddings ineffective , especially for rare words , and consequently limits the performance of these neural network models .",abstract,abstract,machine-translation,6,3,0.5,5,0.0171821305841924,3,0.5,1,0
7,"In this paper , we develop a neat , simple yet effective way to learn FRequency - AGnostic word Embedding ( FRAGE ) using adversarial training .",abstract,abstract,machine-translation,6,4,0.6666666666666666,6,0.0206185567010309,4,0.6666666666666666,1,0
8,"We conducted comprehensive studies on ten datasets across four natural language processing tasks , including word similarity , language modeling , machine translation and text classification .",abstract,abstract,machine-translation,6,5,0.8333333333333334,7,0.0240549828178694,5,0.8333333333333334,1,0
9,"Results show that with FRAGE , we achieve higher performance than the baselines in all tasks .",abstract,abstract,machine-translation,6,6,1.0,8,0.0274914089347079,6,1.0,1,0
10,Introduction,,,machine-translation,6,0,0.0,9,0.0309278350515463,0,0.0,1,0
11,"Word embeddings , which are distributed and continuous vector representations for word tokens , have been one of the basic building blocks for many neural network - based models used in natural language processing ( NLP ) tasks , such as language modeling , text classification and machine translation .",Introduction,Introduction,machine-translation,6,1,0.0294117647058823,10,0.0343642611683848,1,0.0294117647058823,1,0
12,"Different from classic one - hot representation , the learned word embeddings contain semantic information which can measure the semantic similarity between words , and can also be transferred into other learning tasks .",Introduction,Introduction,machine-translation,6,2,0.0588235294117647,11,0.0378006872852233,2,0.0588235294117647,1,0
13,"In deep learning approaches for NLP tasks , word embeddings act as the inputs of the neural network and are usually trained together with neural network parameters .",Introduction,Introduction,machine-translation,6,3,0.088235294117647,12,0.0412371134020618,3,0.088235294117647,1,0
14,"As the inputs of the neural network , word embeddings carryall the information of words that will be further processed by the network , and the quality of embeddings is critical and highly impacts the final performance of the learning task .",Introduction,Introduction,machine-translation,6,4,0.1176470588235294,13,0.0446735395189003,4,0.1176470588235294,1,0
15,"Unfortunately , we find the word embeddings learned by many deep learning approaches are far from perfect .",Introduction,Introduction,machine-translation,6,5,0.1470588235294117,14,0.0481099656357388,5,0.1470588235294117,1,0
16,"As shown in ( a ) and 1 ( b ) , in the embedding space learned by word2 vec model , the nearest neighbors of word "" Peking "" includes "" quickest "" , "" multicellular "" , and "" epigenetic "" , which are not semantically similar , while semantically related words such as "" Beijing "" and "" China "" are far from it .",Introduction,Introduction,machine-translation,6,6,0.1764705882352941,15,0.0515463917525773,6,0.1764705882352941,1,0
17,Similar phenomena are observed from the word embeddings learned from translation tasks .,Introduction,Introduction,machine-translation,6,7,0.2058823529411764,16,0.0549828178694158,7,0.2058823529411764,1,0
18,"With a careful study , we find a more general problem which is rooted in low - frequency words in the text corpus .",Introduction,Introduction,machine-translation,6,8,0.2352941176470588,17,0.0584192439862542,8,0.2352941176470588,1,0
19,"Without any confusion , we also call high - frequency words as popular words and call low - frequency words as rare words .",Introduction,Introduction,machine-translation,6,9,0.2647058823529412,18,0.0618556701030927,9,0.2647058823529412,1,0
20,"As is well known , the frequency distribution of words roughly follows a simple mathematical form known as Zipf 's law .",Introduction,Introduction,machine-translation,6,10,0.2941176470588235,19,0.0652920962199312,10,0.2941176470588235,1,0
21,"When the size of a text corpus grows , the frequency of rare words is much smaller than popular words while the number of unique rare words is much larger than popular words .",Introduction,Introduction,machine-translation,6,11,0.3235294117647059,20,0.0687285223367697,11,0.3235294117647059,1,0
22,"Interestingly , the learned embeddings of rare words and popular words behave differently .",Introduction,Introduction,machine-translation,6,12,0.3529411764705882,21,0.0721649484536082,12,0.3529411764705882,1,1
23,"In the embedding space , a popular word usually has semantically related neighbors , while a rare word usually does not .",Introduction,Introduction,machine-translation,6,13,0.3823529411764705,22,0.0756013745704467,13,0.3823529411764705,1,0
24,"Moreover , the nearest neighbors of more than 85 % rare words are rare words .",Introduction,Introduction,machine-translation,6,14,0.4117647058823529,23,0.0790378006872852,14,0.4117647058823529,1,0
25,Word embeddings encode frequency information .,Introduction,Introduction,machine-translation,6,15,0.4411764705882353,24,0.0824742268041237,15,0.4411764705882353,1,0
26,"As shown in ( a ) and 1 ( b ) , the embeddings of rare words and popular words actually lie in different subregions of the space .",Introduction,Introduction,machine-translation,6,16,0.4705882352941176,25,0.0859106529209622,16,0.4705882352941176,1,1
27,Such a phenomenon is also observed in .,Introduction,Introduction,machine-translation,6,17,0.5,26,0.0893470790378006,17,0.5,1,0
28,We argue that the different behaviors of the embeddings of popular words and rare words are problematic .,Introduction,Introduction,machine-translation,6,18,0.5294117647058824,27,0.0927835051546391,18,0.5294117647058824,1,1
29,"First , such embeddings will affect the semantic understanding of words .",Introduction,Introduction,machine-translation,6,19,0.5588235294117647,28,0.0962199312714776,19,0.5588235294117647,1,0
30,We observe more than half of the rare words are nouns or variants of popular words .,Introduction,Introduction,machine-translation,6,20,0.5882352941176471,29,0.0996563573883161,20,0.5882352941176471,1,0
31,Those rare words should have similar meanings or share the same topics with popular words .,Introduction,Introduction,machine-translation,6,21,0.6176470588235294,30,0.1030927835051546,21,0.6176470588235294,1,0
32,"Second , the neighbors of a large number of rare words are semantically unrelated rare words .",Introduction,Introduction,machine-translation,6,22,0.6470588235294118,31,0.1065292096219931,22,0.6470588235294118,1,0
33,"To some extent , those word embeddings encode more frequency information than semantic information which is not good from the view of semantic understanding .",Introduction,Introduction,machine-translation,6,23,0.6764705882352942,32,0.1099656357388316,23,0.6764705882352942,1,0
34,It will consequently limit the performance of down - stream tasks using the embeddings .,Introduction,Introduction,machine-translation,6,24,0.7058823529411765,33,0.1134020618556701,24,0.7058823529411765,1,0
35,"For example , in text classification , it can not be well guaranteed that the label of a sentence does not change when you replace one popular / rare word in the sentence by its rare / popular alternatives .",Introduction,Introduction,machine-translation,6,25,0.7352941176470589,34,0.1168384879725085,25,0.7352941176470589,1,0
36,"To address this problem , in this paper , we propose an adversarial training method to learn FRequency - AGnostic word Embedding ( FRAGE ) .",Introduction,Introduction,machine-translation,6,26,0.7647058823529411,35,0.120274914089347,26,0.7647058823529411,1,1
37,"For a given NLP task , in addition to minimize the task - specific loss by optimizing the task - specific parameters together with word embeddings , we introduce another discriminator , which takes a word embedding as input and classifies whether it is a popular / rare word .",Introduction,Introduction,machine-translation,6,27,0.7941176470588235,36,0.1237113402061855,27,0.7941176470588235,1,1
38,"The discriminator optimizes its parameters to maximize its classification accuracy , while word embeddings are optimized towards a low task - dependent loss as well as fooling the discriminator to mis-classify the popular and rare words .",Introduction,Introduction,machine-translation,6,28,0.8235294117647058,37,0.127147766323024,28,0.8235294117647058,1,1
39,"When the whole training process converges and the system achieves an equilibrium , the discriminator can not well differentiate popular words from rare words .",Introduction,Introduction,machine-translation,6,29,0.8529411764705882,38,0.1305841924398625,29,0.8529411764705882,1,1
40,"Consequently , rare words lie in the same region as and are mixed with popular words in the embedding space .",Introduction,Introduction,machine-translation,6,30,0.8823529411764706,39,0.134020618556701,30,0.8823529411764706,1,0
41,Then FRAGE will catch better semantic information and help the task - specific model to perform better .,Introduction,Introduction,machine-translation,6,31,0.9117647058823528,40,0.1374570446735395,31,0.9117647058823528,1,0
42,"We conduct experiments on four types of NLP tasks , including three word similarity tasks , two language modeling tasks , three sentiment classification tasks and two machine translation tasks to test our method .",Introduction,Introduction,machine-translation,6,32,0.9411764705882352,41,0.140893470790378,32,0.9411764705882352,1,0
43,"In all tasks , FRAGE outperforms the baselines .",Introduction,Introduction,machine-translation,6,33,0.9705882352941176,42,0.1443298969072164,33,0.9705882352941176,1,0
44,"Specifically , in language modeling and machine translation , we achieve better performance than the state - of - the - art results on PTB , WT2 and WMT14 English - German datasets .",Introduction,Introduction,machine-translation,6,34,1.0,43,0.1477663230240549,34,1.0,1,0
45,Background,,,machine-translation,6,0,0.0,44,0.1512027491408934,0,0.0,1,0
46,Word Representation,Background,,machine-translation,6,1,0.1666666666666666,45,0.1546391752577319,0,0.0,0,0
47,"Words are the basic units of natural languages , and distributed word representations ( i.e. , word embeddings ) are the basic units of many models in NLP tasks including language modeling and machine translation .",Background,Word Representation,machine-translation,6,2,0.3333333333333333,46,0.1580756013745704,1,0.2,0,0
48,It has been demonstrated that word representations learned from one task can be transferred to other tasks and achieve competitive performance .,Background,Word Representation,machine-translation,6,3,0.5,47,0.1615120274914089,2,0.4,0,0
49,"While word embeddings play an important role in neural network - based models in NLP and achieve great success , one technical challenge is that the embeddings of rare words are difficult to train due to their low frequency of occurrences .",Background,Word Representation,machine-translation,6,4,0.6666666666666666,48,0.1649484536082474,3,0.6,0,0
50,develops a novel way to split word into sub- word units which is widely used in neural machine translation .,Background,Word Representation,machine-translation,6,5,0.8333333333333334,49,0.1683848797250859,4,0.8,0,0
51,"However , the low - frequency sub- word units are still difficult to train : provides a comprehensive study which shows that the rare ( sub ) words are usually under-estimated in neural machine translation : during inference step , the model tends to choose popular words over their rare alternatives .",Background,Word Representation,machine-translation,6,6,1.0,50,0.1718213058419244,5,1.0,0,0
52,Adversarial Training,,,machine-translation,6,0,0.0,51,0.1752577319587628,0,0.0,1,0
53,"The basic idea of our work to address the above problem is adversarial training , in which two or more models learn together by pursuing competing goals .",Adversarial Training,Adversarial Training,machine-translation,6,1,0.0833333333333333,52,0.1786941580756013,1,0.1111111111111111,1,0
54,"A representative example of adversarial training is Generative Adversarial Networks ( GANs ) for image generation , in which a discriminator and a generator compete with each other : the generator aims to generate images similar to the natural ones , and the discriminator aims to detect the generated ones from the natural ones .",Adversarial Training,Adversarial Training,machine-translation,6,2,0.1666666666666666,53,0.1821305841924398,2,0.2222222222222222,1,0
55,"Recently , adversarial training has been successfully applied to NLP tasks .",Adversarial Training,Adversarial Training,machine-translation,6,3,0.25,54,0.1855670103092783,3,0.3333333333333333,1,0
56,introduce an additional discriminator to differentiate the semantics learned from different languages in non-parallel bilingual data .,Adversarial Training,Adversarial Training,machine-translation,6,4,0.3333333333333333,55,0.1890034364261168,4,0.4444444444444444,1,0
57,develops a discriminator to classify whether a sentence is created by human or generated by a model .,Adversarial Training,Adversarial Training,machine-translation,6,5,0.4166666666666667,56,0.1924398625429553,5,0.5555555555555556,1,0
58,Our proposed method is under the adversarial training framework but not exactly the conventional generator - discriminator approach since there is no generator in our scenario .,Adversarial Training,Adversarial Training,machine-translation,6,6,0.5,57,0.1958762886597938,6,0.6666666666666666,1,0
59,"For an NLP task and its neural network model ( including word embeddings ) , we introduce a discriminator to differentiate embeddings of popular words and rare words ; while the NN model aims to fool the discriminator and minimize the task - specific loss simultaneously .",Adversarial Training,Adversarial Training,machine-translation,6,7,0.5833333333333334,58,0.1993127147766323,7,0.7777777777777778,1,0
60,Our work is also weakly related to adversarial domain adaptation which attempts to mitigate the negative effects of domain shift between training and testing .,Adversarial Training,Adversarial Training,machine-translation,6,8,0.6666666666666666,59,0.2027491408934708,8,0.8888888888888888,1,0
61,"The difference between this work and adversarial domain adaptation is that we do not target at the mismatch between training and testing ; instead , we aim to improve the effectiveness of word embeddings and consequently improve the performance of end - to - end NLP tasks .",Adversarial Training,Adversarial Training,machine-translation,6,9,0.75,60,0.2061855670103092,9,1.0,1,0
62,Empirical Study,Adversarial Training,,machine-translation,6,10,0.8333333333333334,61,0.2096219931271477,0,0.0,1,0
63,"In this section , we study the embeddings of popular words and rare words based on the models trained from Google News corpora using word2vec 1 and trained from WMT14 English - German translation task using Transformer .",Adversarial Training,Empirical Study,machine-translation,6,11,0.9166666666666666,62,0.2130584192439862,1,0.5,1,0
64,The implementation details can be found in the supplementary material ( part A ) .,Adversarial Training,Empirical Study,machine-translation,6,12,1.0,63,0.2164948453608247,2,1.0,1,0
65,Experimental Design,,,machine-translation,6,0,0.0,64,0.2199312714776632,0,0.0,1,0
66,"In both tasks , we simply set the top 20 % frequent words in vocabulary as popular words and denote the rest as rare words ( roughly speaking , we set a word as a rare word if it s relative frequency is lower than 10 ? 6 in WMT14 dataset and 10 ? 7 in Google News dataset ) .",Experimental Design,Experimental Design,machine-translation,6,1,0.0526315789473684,65,0.2233676975945017,1,0.0625,1,0
67,We have tried other thresholds such as 10 % or 25 % and found the observations are similar .,Experimental Design,Experimental Design,machine-translation,6,2,0.1052631578947368,66,0.2268041237113402,2,0.125,1,0
68,We study whether the semantic relationship between two words is reasonable .,Experimental Design,Experimental Design,machine-translation,6,3,0.1578947368421052,67,0.2302405498281786,3,0.1875,1,0
69,"To achieve this , we randomly sampled some rare / popular words and checked the embeddings trained from different tasks .",Experimental Design,Experimental Design,machine-translation,6,4,0.2105263157894736,68,0.2336769759450171,4,0.25,1,0
70,"For each sampled word , we determined its nearest neighbors based on the cosine similarity between its embeddings and others '.",Experimental Design,Experimental Design,machine-translation,6,5,0.2631578947368421,69,0.2371134020618556,5,0.3125,1,0
71,We also manually chose words which are semantically similar to it .,Experimental Design,Experimental Design,machine-translation,6,6,0.3157894736842105,70,0.2405498281786941,6,0.375,1,0
72,"For simplicity , for each word , we call the nearest words predicted from the embeddings as model - predicted neighbors , and call our chosen words as semantic neighbors .",Experimental Design,Experimental Design,machine-translation,6,7,0.3684210526315789,71,0.2439862542955326,7,0.4375,1,0
73,Observation,Experimental Design,,machine-translation,6,8,0.4210526315789473,72,0.2474226804123711,8,0.5,1,0
74,"To visualize word embeddings , we reduce their dimensionalities by SVD and plot two cases in .",Experimental Design,Observation,machine-translation,6,9,0.4736842105263157,73,0.2508591065292096,9,0.5625,1,0
75,More cases and other studies without dimensionality reduction can be found in the supplementary material ( part C ) .,Experimental Design,Observation,machine-translation,6,10,0.5263157894736842,74,0.2542955326460481,10,0.625,1,0
76,We find that the embeddings trained from different tasks share some common patterns .,Experimental Design,Observation,machine-translation,6,11,0.5789473684210527,75,0.2577319587628865,11,0.6875,1,0
77,"For both tasks , more than 90 % of model - predicted neighbors of rare words are rare words .",Experimental Design,Observation,machine-translation,6,12,0.631578947368421,76,0.2611683848797251,12,0.75,1,0
78,"For each rare word , the model - predicted neighbor is usually not semantically related to this word , and semantic neighbors we chose are faraway from it in the embedding space .",Experimental Design,Observation,machine-translation,6,13,0.6842105263157895,77,0.2646048109965636,13,0.8125,1,0
79,"In contrast , the model - predicted neighbors of popular words are very reasonable .",Experimental Design,Observation,machine-translation,6,14,0.7368421052631579,78,0.268041237113402,14,0.875,1,0
80,"As the patterns in rare words are different from that of popular words , we further check the whole embedding matrix to make a general understanding .",Experimental Design,Observation,machine-translation,6,15,0.7894736842105263,79,0.2714776632302405,15,0.9375,1,0
81,We also visualize the word embeddings using SVD by keeping the two directions with top - 2 largest eigenvalues as in and plot them in,Experimental Design,Observation,machine-translation,6,16,0.8421052631578947,80,0.274914089347079,16,1.0,1,0
82,Input Tokens Word Embeddings,Experimental Design,,machine-translation,6,17,0.8947368421052632,81,0.2783505154639175,0,0.0,1,0
83,Task - specific Outputs,Experimental Design,Input Tokens Word Embeddings,machine-translation,6,18,0.9473684210526316,82,0.281786941580756,1,0.3333333333333333,1,0
84,Task - specific,Experimental Design,Input Tokens Word Embeddings,machine-translation,6,19,1.0,83,0.2852233676975945,2,0.6666666666666666,1,0
85,Model,,,machine-translation,6,0,0.0,84,0.2886597938144329,3,1.0,1,0
86,Loss,Model,,machine-translation,6,1,0.0434782608695652,85,0.2920962199312715,0,0.0,1,0
87,Rare / Popular Labels Discriminator,Model,,machine-translation,6,2,0.0869565217391304,86,0.2955326460481099,1,0.0769230769230769,1,0
88,Loss predict predict :,Model,Rare / Popular Labels Discriminator,machine-translation,6,3,0.1304347826086956,87,0.2989690721649484,2,0.1538461538461538,1,0
89,"The proposed learning framework includes a task - specific predictor and a discriminator , whose function is to classify rare and popular words .",Model,Rare / Popular Labels Discriminator,machine-translation,6,4,0.1739130434782608,88,0.3024054982817869,3,0.2307692307692307,1,0
90,Both modules use word embeddings as the input .,Model,Rare / Popular Labels Discriminator,machine-translation,6,5,0.217391304347826,89,0.3058419243986254,4,0.3076923076923077,1,0
91,"a certain degree : the rare words and popular words lie in different regions after this linear projection , and thus they occupy different regions in the original embedding space .",Model,Rare / Popular Labels Discriminator,machine-translation,6,6,0.2608695652173913,90,0.3092783505154639,5,0.3846153846153846,1,0
92,This strange phenomenon is also observed in other learned embeddings ( e.g. CBOW and GLOVE ) and mentioned in .,Model,Rare / Popular Labels Discriminator,machine-translation,6,7,0.3043478260869565,91,0.3127147766323024,6,0.4615384615384615,1,0
93,Explanation,Model,,machine-translation,6,8,0.3478260869565217,92,0.3161512027491409,7,0.5384615384615384,1,0
94,"From the empirical study above , we can see that the occupied spaces of popular words and rare words are different and here we intuitively explain a possible reason .",Model,Explanation,machine-translation,6,9,0.391304347826087,93,0.3195876288659793,8,0.6153846153846154,1,0
95,We simply take word2vec as an example which is trained by stochastic gradient descent .,Model,Explanation,machine-translation,6,10,0.4347826086956521,94,0.3230240549828179,9,0.6923076923076923,1,0
96,"During training , the sample rate of a popular word is high and the embedding of a popular word updates frequently .",Model,Explanation,machine-translation,6,11,0.4782608695652174,95,0.3264604810996563,10,0.7692307692307693,1,0
97,"For a rare word , the sample rate is low and its embedding rarely updates .",Model,Explanation,machine-translation,6,12,0.5217391304347826,96,0.3298969072164948,11,0.8461538461538461,1,0
98,"According to our study , on average , the moving distance of the embedding fora popular word is twice longer than that of a rare word during training .",Model,Explanation,machine-translation,6,13,0.5652173913043478,97,0.3333333333333333,12,0.9230769230769232,1,0
99,"As all word embeddings are usually initialized around the origin with a small variance , we observe in the final model , the embeddings of rare words are still around the origin and the popular words have moved faraway .",Model,Explanation,machine-translation,6,14,0.6086956521739131,98,0.3367697594501718,13,1.0,1,0
100,Discussion,Model,,machine-translation,6,15,0.6521739130434783,99,0.3402061855670103,0,0.0,1,0
101,We have strong evidence that the current phenomena are problematic .,Model,Discussion,machine-translation,6,16,0.6956521739130435,100,0.3436426116838488,1,0.125,1,0
102,"First , according to our study , in both tasks , more than half of the rare words are nouns , e.g. , company names , city names .",Model,Discussion,machine-translation,6,17,0.7391304347826086,101,0.3470790378006873,2,0.25,1,0
103,"They may share some similar topics to popular entities , e.g. , big companies and cities ; around 10 % percent of rare words include a hyphen ( which is usually used to join popular words ) , and over 30 % rare words are different PoSs of popular words .",Model,Discussion,machine-translation,6,18,0.782608695652174,102,0.3505154639175257,3,0.375,1,0
104,These words should have mixed or similar semantics to some popular words .,Model,Discussion,machine-translation,6,19,0.8260869565217391,103,0.3539518900343643,4,0.5,1,0
105,"These facts show that rare words and popular words should lie in the same region of the embedding space , which is different from what we observed .",Model,Discussion,machine-translation,6,20,0.8695652173913043,104,0.3573883161512027,5,0.625,1,0
106,"Second , as we can see from the cases , for rare words , model - predicted neighbors are usually not semantically related words but frequency - related words ( rare words ) .",Model,Discussion,machine-translation,6,21,0.9130434782608696,105,0.3608247422680412,6,0.75,1,0
107,"This shows , for rare words , the embeddings encode more frequency information than semantic information .",Model,Discussion,machine-translation,6,22,0.9565217391304348,106,0.3642611683848797,7,0.875,1,0
108,"It is not good to use such word embeddings into semantic understanding tasks , e.g. , text classification , language modeling , language understanding and translation .",Model,Discussion,machine-translation,6,23,1.0,107,0.3676975945017182,8,1.0,1,0
109,Our Method,,,machine-translation,6,0,0.0,108,0.3711340206185567,0,0.0,1,0
110,"In this section , we present our method to improve word representations .",Our Method,Our Method,machine-translation,6,1,0.0212765957446808,109,0.3745704467353952,1,0.0212765957446808,1,0
111,"As we have a strong prior that many rare words should share the same region in the embedding space as popular words , the basic idea of our algorithm is to train the word embeddings in an adversarial framework :",Our Method,Our Method,machine-translation,6,2,0.0425531914893617,110,0.3780068728522336,2,0.0425531914893617,1,0
112,We introduce a discriminator to categorize word embeddings into two classes : popular ones or rare ones .,Our Method,Our Method,machine-translation,6,3,0.0638297872340425,111,0.3814432989690721,3,0.0638297872340425,1,0
113,We hope the learned word embeddings not only minimize the task - specific training loss but also fool the discriminator .,Our Method,Our Method,machine-translation,6,4,0.0851063829787234,112,0.3848797250859107,4,0.0851063829787234,1,0
114,"By doing so , the frequency information is removed from the embedding and we call our method frequency - agnostic word embedding ( FRAGE ) .",Our Method,Our Method,machine-translation,6,5,0.1063829787234042,113,0.3883161512027491,5,0.1063829787234042,1,0
115,We first define some notations and then introduce our algorithm .,Our Method,Our Method,machine-translation,6,6,0.1276595744680851,114,0.3917525773195876,6,0.1276595744680851,1,0
116,"We develop three types of notations : embeddings , task - specific parameters / loss , and discriminator parameters / loss .",Our Method,Our Method,machine-translation,6,7,0.1489361702127659,115,0.3951890034364261,7,0.1489361702127659,1,0
117,Denote ? emb ?,Our Method,Our Method,machine-translation,6,8,0.1702127659574468,116,0.3986254295532646,8,0.1702127659574468,1,0
118,"R d|V | as the word embedding matrix to be learned , where dis the dimension of the embedding vectors and | V | is the vocabulary size .",Our Method,Our Method,machine-translation,6,9,0.1914893617021276,117,0.4020618556701031,9,0.1914893617021276,1,0
119,Let V pop denote the set of popular words and V rare = V \ V pop denote the set of rare words .,Our Method,Our Method,machine-translation,6,10,0.2127659574468085,118,0.4054982817869416,10,0.2127659574468085,1,0
120,Then the embedding matrix ?,Our Method,Our Method,machine-translation,6,11,0.2340425531914893,119,0.40893470790378,11,0.2340425531914893,1,0
121,emb can be divided into two parts : ?,Our Method,Our Method,machine-translation,6,12,0.2553191489361702,120,0.4123711340206185,12,0.2553191489361702,1,0
122,emb pop for popular words and ?,Our Method,Our Method,machine-translation,6,13,0.2765957446808511,121,0.415807560137457,13,0.2765957446808511,1,0
123,emb rare for rare words .,Our Method,Our Method,machine-translation,6,14,0.2978723404255319,122,0.4192439862542955,14,0.2978723404255319,1,0
124,Let ?,Our Method,Our Method,machine-translation,6,15,0.3191489361702128,123,0.422680412371134,15,0.3191489361702128,1,0
125,emb w denote the embedding of word w .,Our Method,Our Method,machine-translation,6,16,0.3404255319148936,124,0.4261168384879725,16,0.3404255319148936,1,0
126,Let ?,Our Method,Our Method,machine-translation,6,17,0.3617021276595745,125,0.4295532646048109,17,0.3617021276595745,1,0
127,model denote all the other task - specific parameters except word embeddings .,Our Method,Our Method,machine-translation,6,18,0.3829787234042553,126,0.4329896907216495,18,0.3829787234042553,1,0
128,"For instance , for language modeling , ?",Our Method,Our Method,machine-translation,6,19,0.4042553191489361,127,0.436426116838488,19,0.4042553191489361,1,0
129,"model is the parameters of the RNN or LSTM ; for neural machine translation , ?",Our Method,Our Method,machine-translation,6,20,0.425531914893617,128,0.4398625429553264,20,0.425531914893617,1,0
130,"model is the parameters of the encoder , attention module and decoder .",Our Method,Our Method,machine-translation,6,21,0.4468085106382978,129,0.4432989690721649,21,0.4468085106382978,1,0
131,"Let L T ( S ; ? model , ? emb ) denote the task - specific loss over a dataset S. Taking language modeling as an example , the loss L T ( S ; ? model , ? emb ) is defined as the negative log likelihood of the data :",Our Method,Our Method,machine-translation,6,22,0.4680851063829787,130,0.4467353951890034,22,0.4680851063829787,1,0
132,where y is a sentence .,Our Method,Our Method,machine-translation,6,23,0.4893617021276595,131,0.4501718213058419,23,0.4893617021276595,1,0
133,Let f ?,Our Method,Our Method,machine-translation,6,24,0.5106382978723404,132,0.4536082474226804,24,0.5106382978723404,1,0
134,D denote a discriminator with parameters ?,Our Method,Our Method,machine-translation,6,25,0.5319148936170213,133,0.4570446735395189,25,0.5319148936170213,1,0
135,"D , which takes a word embedding as input and outputs a confidence score between 0 and 1 indicating how likely the word is a rare word .",Our Method,Our Method,machine-translation,6,26,0.5531914893617021,134,0.4604810996563573,26,0.5531914893617021,1,0
136,"Let L D ( V ; ? D , ? emb ) denote the loss of the discriminator :",Our Method,Our Method,machine-translation,6,27,0.574468085106383,135,0.4639175257731959,27,0.574468085106383,1,0
137,"Following the principle of adversarial training , we develop a minimax objective to train the taskspecific model ( ?",Our Method,Our Method,machine-translation,6,28,0.5957446808510638,136,0.4673539518900343,28,0.5957446808510638,1,0
138,model and ? emb ) and the discriminator ( ?,Our Method,Our Method,machine-translation,6,29,0.6170212765957447,137,0.4707903780068728,29,0.6170212765957447,1,0
139,D ) as below :,Our Method,Our Method,machine-translation,6,30,0.6382978723404256,138,0.4742268041237113,30,0.6382978723404256,1,0
140,where ?,Our Method,Our Method,machine-translation,6,31,0.6595744680851063,139,0.4776632302405498,31,0.6595744680851063,1,0
141,is a coefficient to trade off the two loss terms .,Our Method,Our Method,machine-translation,6,32,0.6808510638297872,140,0.4810996563573883,32,0.6808510638297872,1,0
142,We can see that when the model parameter ?,Our Method,Our Method,machine-translation,6,33,0.7021276595744681,141,0.4845360824742268,33,0.7021276595744681,1,0
143,model and the embedding ?,Our Method,Our Method,machine-translation,6,34,0.723404255319149,142,0.4879725085910653,34,0.723404255319149,1,0
144,"emb are fixed , the optimization of the discriminator ?",Our Method,Our Method,machine-translation,6,35,0.7446808510638298,143,0.4914089347079037,35,0.7446808510638298,1,0
145,D becomes,Our Method,,machine-translation,6,36,0.7659574468085106,144,0.4948453608247423,36,0.7659574468085106,1,0
146,which is to minimize the classification error of popular and rare words .,Our Method,D becomes,machine-translation,6,37,0.7872340425531915,145,0.4982817869415807,37,0.7872340425531915,1,0
147,When the discriminator ?,Our Method,D becomes,machine-translation,6,38,0.8085106382978723,146,0.5017182130584192,38,0.8085106382978723,1,0
148,"Dis fixed , the optimization of ?",Our Method,D becomes,machine-translation,6,39,0.8297872340425532,147,0.5051546391752577,39,0.8297872340425532,1,0
149,model and ?,Our Method,D becomes,machine-translation,6,40,0.851063829787234,148,0.5085910652920962,40,0.851063829787234,1,0
150,emb becomes,Our Method,D becomes,machine-translation,6,41,0.8723404255319149,149,0.5120274914089347,41,0.8723404255319149,1,0
151,"i.e. , to optimize the task performance as well as fooling the discriminator .",Our Method,D becomes,machine-translation,6,42,0.8936170212765957,150,0.5154639175257731,42,0.8936170212765957,1,0
152,We train ?,Our Method,D becomes,machine-translation,6,43,0.9148936170212766,151,0.5189003436426117,43,0.9148936170212766,1,0
153,"model , ?",Our Method,D becomes,machine-translation,6,44,0.9361702127659576,152,0.5223367697594502,44,0.9361702127659576,1,0
154,emb and ?,Our Method,D becomes,machine-translation,6,45,0.9574468085106383,153,0.5257731958762887,45,0.9574468085106383,1,0
155,D iteratively by stochastic gradient descent or its variants .,Our Method,D becomes,machine-translation,6,46,0.9787234042553192,154,0.5292096219931272,46,0.9787234042553192,1,0
156,The general training process is shown in Algorithm 1 .,Our Method,D becomes,machine-translation,6,47,1.0,155,0.5326460481099656,47,1.0,1,0
157,Experiment,,,machine-translation,6,0,0.0,156,0.5360824742268041,0,0.0,1,0
158,"We test our method on a wide range of tasks , including word similarity , language modeling , machine translation and text classification .",Experiment,Experiment,machine-translation,6,1,0.0196078431372549,157,0.5395189003436426,1,0.0588235294117647,1,0
159,"For each task , we choose the state - of - the - art architecture together with the state - of - the - art training method as our baseline .",Experiment,Experiment,machine-translation,6,2,0.0392156862745098,158,0.5429553264604811,2,0.1176470588235294,1,0
160,Sample a minibatch ?,Experiment,Experiment,machine-translation,6,3,0.0588235294117647,159,0.5463917525773195,3,0.1764705882352941,1,0
161,from S.,Experiment,Experiment,machine-translation,6,4,0.0784313725490196,160,0.5498281786941581,4,0.2352941176470588,1,0
162,4 :,Experiment,Experiment,machine-translation,6,5,0.0980392156862745,161,0.5532646048109966,5,0.2941176470588235,1,0
163,Sample a minibatchV = V pop ?,Experiment,Experiment,machine-translation,6,6,0.1176470588235294,162,0.5567010309278351,6,0.3529411764705882,1,0
164,V rare from V .,Experiment,Experiment,machine-translation,6,7,0.1372549019607843,163,0.5601374570446735,7,0.4117647058823529,1,0
165,5 :,Experiment,Experiment,machine-translation,6,8,0.1568627450980392,164,0.563573883161512,8,0.4705882352941176,1,0
166,"Update ? model , ?",Experiment,Experiment,machine-translation,6,9,0.1764705882352941,165,0.5670103092783505,9,0.5294117647058824,1,0
167,emb by gradient descent according to Eqn. ( 5 ) with data ?.,Experiment,Experiment,machine-translation,6,10,0.196078431372549,166,0.570446735395189,10,0.5882352941176471,1,0
168,Update ?,Experiment,Experiment,machine-translation,6,11,0.2156862745098039,167,0.5738831615120275,11,0.6470588235294118,1,0
169,D by gradient ascent according to Eqn. ( 4 ) with vocabulary V .,Experiment,Experiment,machine-translation,6,12,0.2352941176470588,168,0.5773195876288659,12,0.7058823529411765,1,0
170,"7 : until Converge 8 : Output : ? model , ? emb , ?",Experiment,Experiment,machine-translation,6,13,0.2549019607843137,169,0.5807560137457045,13,0.7647058823529411,1,0
171,D .,Experiment,,machine-translation,6,14,0.2745098039215686,170,0.584192439862543,14,0.8235294117647058,1,0
172,"For fair comparisons , for each task , our method shares the same model architecture as the baseline .",Experiment,D .,machine-translation,6,15,0.2941176470588235,171,0.5876288659793815,15,0.8823529411764706,1,0
173,The only difference is that we use the original task - specific loss function with an additional adversarial loss as in Eqn..,Experiment,D .,machine-translation,6,16,0.3137254901960784,172,0.5910652920962199,16,0.9411764705882352,1,0
174,"Due to space limitations , we put dataset description , model description , hyperparameter configuration into supplementary material ( part A ) .",Experiment,D .,machine-translation,6,17,0.3333333333333333,173,0.5945017182130584,17,1.0,1,0
175,Settings,Experiment,,machine-translation,6,18,0.3529411764705882,174,0.5979381443298969,0,0.0,1,0
176,We conduct experiments on the following tasks .,Experiment,Settings,machine-translation,6,19,0.3725490196078431,175,0.6013745704467354,1,0.0303030303030303,1,0
177,"Word Similarity evaluates the performance of the learned word embeddings by calculating the word similarity : it evaluates whether the most similar words of a given word in the embedding space are consistent with the ground - truth , in terms of Spearman 's rank correlation .",Experiment,Settings,machine-translation,6,20,0.392156862745098,176,0.6048109965635738,2,0.0606060606060606,1,0
178,"We use the skip - gram model as our baseline model , and train the embeddings using Enwik9 6 .",Experiment,Settings,machine-translation,6,21,0.4117647058823529,177,0.6082474226804123,3,0.0909090909090909,1,1
179,"We test the baseline and our method on three datasets : RG65 , WS and RW .",Experiment,Settings,machine-translation,6,22,0.4313725490196078,178,0.6116838487972509,4,0.1212121212121212,1,1
180,The RW dataset is a dataset for the evaluation of rare words .,Experiment,Settings,machine-translation,6,23,0.4509803921568627,179,0.6151202749140894,5,0.1515151515151515,1,0
181,"Following common practice , we use cosine distance while computing the similarity between two word embeddings .",Experiment,Settings,machine-translation,6,24,0.4705882352941176,180,0.6185567010309279,6,0.1818181818181818,1,0
182,Language Modeling is a basic task in natural language processing .,Experiment,Settings,machine-translation,6,25,0.4901960784313725,181,0.6219931271477663,7,0.2121212121212121,1,0
183,The goal is to predict the next word conditioned on previous words and the task is evaluated by perplexity .,Experiment,Settings,machine-translation,6,26,0.5098039215686274,182,0.6254295532646048,8,0.2424242424242424,1,1
184,"We do experiments on two widely used datasets , Penn Treebank ( PTB ) and WikiText - 2 ( WT2 ) .",Experiment,Settings,machine-translation,6,27,0.5294117647058824,183,0.6288659793814433,9,0.2727272727272727,1,1
185,"We choose two recent works as our baselines : the AWD - LSTM model and the AWD - LSTM - MoS model , which achieves state - of - the - art performance .",Experiment,Settings,machine-translation,6,28,0.5490196078431373,184,0.6323024054982818,10,0.303030303030303,1,1
186,Machine Translation is a popular task in both deep learning and natural language processing .,Experiment,Settings,machine-translation,6,29,0.5686274509803921,185,0.6357388316151202,11,0.3333333333333333,1,0
187,"We choose two datasets : WMT14 English - German and IWSLT14 German - English datasets , which are evaluated in terms of BLEU score .",Experiment,Settings,machine-translation,6,30,0.5882352941176471,186,0.6391752577319587,12,0.3636363636363636,1,1
188,"We use Transformer as the baseline model , which achieves state - of - the - art accuracy on multiple translation datasets .",Experiment,Settings,machine-translation,6,31,0.6078431372549019,187,0.6426116838487973,13,0.3939393939393939,1,1
189,We use transformer_base and transformer_big configurations following tensor2 tensor .,Experiment,Settings,machine-translation,6,32,0.6274509803921569,188,0.6460481099656358,14,0.4242424242424242,1,0
190,Text Classification is a conventional machine learning task and is evaluated by accuracy .,Experiment,Settings,machine-translation,6,33,0.6470588235294118,189,0.6494845360824743,15,0.4545454545454545,1,0
191,"Following the setting in , we implement a Recurrent CNN - based model and test it on AG 's news corpus ( AGs ) , IMDB movie review dataset ( IMDB ) and 20 Newsgroups ( 20 NG ) .",Experiment,Settings,machine-translation,6,34,0.6666666666666666,190,0.6529209621993127,16,0.4848484848484848,1,1
192,"In all tasks , we simply set the top 20 % frequent words in vocabulary as popular words and denote the rest as rare words , which is the same as our empirical study .",Experiment,Settings,machine-translation,6,35,0.6862745098039216,191,0.6563573883161512,17,0.5151515151515151,1,0
193,"For all the tasks except word embedding , we use full - batch gradient descent to update the discriminator .",Experiment,Settings,machine-translation,6,36,0.7058823529411765,192,0.6597938144329897,18,0.5454545454545454,1,0
194,"For word embedding , mini- batch stochastic gradient descent is used to update the discriminator with a batch size 3000 , since the vocabulary size is large .",Experiment,Settings,machine-translation,6,37,0.7254901960784313,193,0.6632302405498282,19,0.5757575757575758,1,0
195,"For language modeling and machine translation tasks , we use logistic regression as the discriminator .",Experiment,Settings,machine-translation,6,38,0.7450980392156863,194,0.6666666666666666,20,0.6060606060606061,1,0
196,"For other tasks , we find using a shallow neural network with 5 https://github.com/tensorflow/models/blob/master/tutorials/embedding",Experiment,Settings,machine-translation,6,39,0.7647058823529411,195,0.6701030927835051,21,0.6363636363636364,1,0
197,6 http://mattmahoney.net/dc/textdata.html,Experiment,Settings,machine-translation,6,40,0.7843137254901961,196,0.6735395189003437,22,0.6666666666666666,1,0
198,7 https://github.com/salesforce/awd-lstm-lm,Experiment,Settings,machine-translation,6,41,0.803921568627451,197,0.6769759450171822,23,0.696969696969697,1,0
199,8 https://github.com/zihangdai/mos,Experiment,Settings,machine-translation,6,42,0.8235294117647058,198,0.6804123711340206,24,0.7272727272727273,1,0
200,9 https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/multi-bleu.perl,Experiment,Settings,machine-translation,6,43,0.8431372549019608,199,0.6838487972508591,25,0.7575757575757576,1,0
201,"To improve the training for imbalanced labeled data , a common method is to adjust loss function by reweighting the training samples ; To regularize the parameter space , a common method is to use l 2 regularization .",Experiment,Settings,machine-translation,6,44,0.8627450980392157,200,0.6872852233676976,26,0.7878787878787878,1,0
202,We tested these methods in machine translation and found the performance is not good .,Experiment,Settings,machine-translation,6,45,0.8823529411764706,201,0.6907216494845361,27,0.8181818181818182,1,0
203,Detailed analysis is provided in the supplementary material ( part B ) .,Experiment,Settings,machine-translation,6,46,0.9019607843137256,202,0.6941580756013745,28,0.8484848484848485,1,0
204,https://github.com/brightmart/text_classification one hidden layer is more efficient and we set the number of nodes in the hidden layer as 1.5 times embedding size .,Experiment,Settings,machine-translation,6,47,0.9215686274509804,203,0.697594501718213,29,0.8787878787878788,1,0
205,"In all tasks , we set the hyper - parameter ? to 0.1 .",Experiment,Settings,machine-translation,6,48,0.9411764705882352,204,0.7010309278350515,30,0.9090909090909092,1,0
206,We list other hyper - parameters related to different task - specific models in the supplementary material ( part A ) .,Experiment,Settings,machine-translation,6,49,0.9607843137254902,205,0.7044673539518901,31,0.9393939393939394,1,0
207,"In this subsection , we provide the experimental results of all tasks .",Experiment,Settings,machine-translation,6,50,0.9803921568627452,206,0.7079037800687286,32,0.9696969696969696,1,0
208,"For simplicity , we use "" with FRAGE "" as our proposed method in the tables .",Experiment,Settings,machine-translation,6,51,1.0,207,0.711340206185567,33,1.0,1,0
209,Results,,,machine-translation,6,0,0.0,208,0.7147766323024055,0,0.0,1,0
210,RG65,Results,,machine-translation,6,1,0.25,209,0.718213058419244,0,0.0,1,0
211,Word Similarity,Results,,machine-translation,6,2,0.5,210,0.7216494845360825,0,0.0,1,0
212,The results on three word similarity tasks are listed in .,Results,Word Similarity,machine-translation,6,3,0.75,211,0.7250859106529209,1,0.5,1,0
213,""" Paras "" denotes the number of model parameters .",Results,Word Similarity,machine-translation,6,4,1.0,212,0.7285223367697594,2,1.0,1,0
214,Language Modeling,,,machine-translation,6,0,0.0,213,0.7319587628865979,0,0.0,1,0
215,The results of language modeling on PTB and WT2 datasets are presented in .,Language Modeling,Language Modeling,machine-translation,6,1,0.0588235294117647,214,0.7353951890034365,1,0.1666666666666666,1,0
216,"We test our model and the baselines at several checkpoints used in the baseline papers : without finetune , with finetune , with post -process ( continuous cache pointer or dynamic evaluation ) .",Language Modeling,Language Modeling,machine-translation,6,2,0.1176470588235294,215,0.738831615120275,2,0.3333333333333333,1,0
217,"In all these settings , our method outperforms the two baselines .",Language Modeling,Language Modeling,machine-translation,6,3,0.1764705882352941,216,0.7422680412371134,3,0.5,1,1
218,"On PTB dataset , our method improves the AWD - LSTM and AWD - LSTM - MoS baseline by 0.8/1.2/1.0 and 0.76/1.13/1.15 points in test set at different checkpoints .",Language Modeling,Language Modeling,machine-translation,6,4,0.2352941176470588,217,0.7457044673539519,4,0.6666666666666666,1,1
219,"On WT2 dataset , which contains more rare words , our method achieves larger improvements .",Language Modeling,Language Modeling,machine-translation,6,5,0.2941176470588235,218,0.7491408934707904,5,0.8333333333333334,1,1
220,"We improve the results of AWD - LSTM and AWD - LSTM - MoS by 2.3/2.4/2.7 and 1.15/1.72/1.54 in terms of test perplexity , respectively .",Language Modeling,Language Modeling,machine-translation,6,6,0.3529411764705882,219,0.7525773195876289,6,1.0,1,1
221,Machine Translation,Language Modeling,,machine-translation,6,7,0.4117647058823529,220,0.7560137457044673,0,0.0,1,0
222,The results of neural machine translation on WMT14 English - German and IWSLT14 German - English tasks are shown in .,Language Modeling,Machine Translation,machine-translation,6,8,0.4705882352941176,221,0.7594501718213058,1,0.2,1,0
223,We outperform the baselines for 1.06/0.71 in the term of BLEU in transformer_base and transformer_big settings in WMT14 English - German : BLEU scores on test set on WMT2014 English - German and IWSLT German - English tasks .,Language Modeling,Machine Translation,machine-translation,6,9,0.5294117647058824,222,0.7628865979381443,2,0.4,1,1
224,"task , respectively .",Language Modeling,Machine Translation,machine-translation,6,10,0.5882352941176471,223,0.7663230240549829,3,0.6,1,0
225,The model learned from adversarial training also outperforms original one in IWSLT14 German - English task by 0.85 .,Language Modeling,Machine Translation,machine-translation,6,11,0.6470588235294118,224,0.7697594501718213,4,0.8,1,1
226,These results show improving word embeddings can achieve better results in more complicated tasks and larger datasets .,Language Modeling,Machine Translation,machine-translation,6,12,0.7058823529411765,225,0.7731958762886598,5,1.0,1,0
227,Text Classification,Language Modeling,,machine-translation,6,13,0.7647058823529411,226,0.7766323024054983,0,0.0,1,0
228,The results are listed in .,Language Modeling,Text Classification,machine-translation,6,14,0.8235294117647058,227,0.7800687285223368,1,0.25,1,0
229,Our method outperforms the baseline method for 1.26%/0.66%/0.44 % on three different datasets .,Language Modeling,Text Classification,machine-translation,6,15,0.8823529411764706,228,0.7835051546391752,2,0.5,1,1
230,"As a summary , our experiments on four different tasks with 10 datasets verify the effectiveness of our method .",Language Modeling,Text Classification,machine-translation,6,16,0.9411764705882352,229,0.7869415807560137,3,0.75,1,0
231,"We provide some case studies and visualizations of our method in the supplementary material ( part C ) , which show that the semantic similarities are reasonable and the popular / rare words are better mixed together in the embedding space .",Language Modeling,Text Classification,machine-translation,6,17,1.0,230,0.7903780068728522,4,1.0,1,0
232,Conclusion,,,machine-translation,6,0,0.0,231,0.7938144329896907,0,0.0,1,0
233,"In this paper , we find that word embeddings learned in several tasks are biased towards word frequency : the embeddings of high - frequency and low - frequency words lie in different subregions of the embedding space .",Conclusion,Conclusion,machine-translation,6,1,0.0277777777777777,232,0.7972508591065293,1,0.1666666666666666,0,0
234,"This makes learned word embeddings ineffective , especially for rare words , and consequently limits the performance of these neural network models .",Conclusion,Conclusion,machine-translation,6,2,0.0555555555555555,233,0.8006872852233677,2,0.3333333333333333,0,0
235,"We propose a neat , simple yet effective adversarial training method to improve the model performance which is verified in a wide range of tasks .",Conclusion,Conclusion,machine-translation,6,3,0.0833333333333333,234,0.8041237113402062,3,0.5,0,0
236,We will explore several directions in the future .,Conclusion,Conclusion,machine-translation,6,4,0.1111111111111111,235,0.8075601374570447,4,0.6666666666666666,0,0
237,"First , we will investigate the theoretical aspects of word embedding learning and our adversarial training method .",Conclusion,Conclusion,machine-translation,6,5,0.1388888888888889,236,0.8109965635738832,5,0.8333333333333334,0,0
238,"Second , we will study more applications which have the similar problem even beyond NLP .",Conclusion,Conclusion,machine-translation,6,6,0.1666666666666666,237,0.8144329896907216,6,1.0,0,0
239,A Experimental settings A.1 Dataset Description,Conclusion,,machine-translation,6,7,0.1944444444444444,238,0.8178694158075601,0,0.0,0,0
240,"For word similarity , we use three test datasets .",Conclusion,A Experimental settings A.1 Dataset Description,machine-translation,6,8,0.2222222222222222,239,0.8213058419243986,1,0.0344827586206896,0,0
241,"WordSim-353 ( WS ) dataset consists of 353 pairs of commonly used verbs and nouns ; The rare - words ( RW ) dataset contains rarely used words ; The RG65 dataset contains 65 word pairs , and the similarity values in the dataset are the means of judgments made by 51 subjects .",Conclusion,A Experimental settings A.1 Dataset Description,machine-translation,6,9,0.25,240,0.8247422680412371,2,0.0689655172413793,0,0
242,"For language modeling tasks , we use Penn Treebank dataset and WikiText - 2 dataset .",Conclusion,A Experimental settings A.1 Dataset Description,machine-translation,6,10,0.2777777777777778,241,0.8281786941580757,3,0.1034482758620689,0,0
243,The details of the datasets are provided in . :,Conclusion,A Experimental settings A.1 Dataset Description,machine-translation,6,11,0.3055555555555556,242,0.8316151202749141,4,0.1379310344827586,0,0
244,"Statistics of the Penn Treebank , and WikiText - 2 dataset used in language modeling .",Conclusion,A Experimental settings A.1 Dataset Description,machine-translation,6,12,0.3333333333333333,243,0.8350515463917526,5,0.1724137931034483,0,0
245,The out of vocabulary ( OOV ) words will be replaced by < unk > during training and testing .,Conclusion,A Experimental settings A.1 Dataset Description,machine-translation,6,13,0.3611111111111111,244,0.8384879725085911,6,0.2068965517241379,0,0
246,"For machine translation , we use WMT14 English - German and IWSLT14 German - English datasets .",Conclusion,A Experimental settings A.1 Dataset Description,machine-translation,6,14,0.3888888888888889,245,0.8419243986254296,7,0.2413793103448276,0,0
247,The training set of WMT14 English - German task consists of 4.5 M sentence pairs .,Conclusion,A Experimental settings A.1 Dataset Description,machine-translation,6,15,0.4166666666666667,246,0.845360824742268,8,0.2758620689655172,0,0
248,Source and target tokens are processed into 37 K shared sub - word units based on byte - pair encoding ( BPE ) .,Conclusion,A Experimental settings A.1 Dataset Description,machine-translation,6,16,0.4444444444444444,247,0.8487972508591065,9,0.3103448275862069,0,0
249,We use the concatenation of newstest2012 and newstest2013 as the validation set and use newstest2014 as the test set following all previous works .,Conclusion,A Experimental settings A.1 Dataset Description,machine-translation,6,17,0.4722222222222222,248,0.852233676975945,10,0.3448275862068966,0,0
250,IWSLT14 German - English dataset contains 160K training sentence pairs and 7K validation sentence pairs .,Conclusion,A Experimental settings A.1 Dataset Description,machine-translation,6,18,0.5,249,0.8556701030927835,11,0.3793103448275862,0,0
251,Tokens are processed using BPE and eventually we obtain a shared vocabulary of about 32 K tokens .,Conclusion,A Experimental settings A.1 Dataset Description,machine-translation,6,19,0.5277777777777778,250,0.8591065292096219,12,0.4137931034482758,0,0
252,"We use the concatenation of dev2010 , tst2010 , tst2011 and tst 2011 as the test set , which is widely adopted in .",Conclusion,A Experimental settings A.1 Dataset Description,machine-translation,6,20,0.5555555555555556,251,0.8625429553264605,13,0.4482758620689655,0,0
253,"For text classification tasks , we use three datasets : AG 's News , IMDB and 20 NG .",Conclusion,A Experimental settings A.1 Dataset Description,machine-translation,6,21,0.5833333333333334,252,0.865979381443299,14,0.4827586206896552,0,0
254,"AG 's news corpus is a news article corpus with categorized articles from more than 2,000 news .",Conclusion,A Experimental settings A.1 Dataset Description,machine-translation,6,22,0.6111111111111112,253,0.8694158075601375,15,0.5172413793103449,0,0
255,IMDB movie review dataset is a sentiment classification dataset .,Conclusion,A Experimental settings A.1 Dataset Description,machine-translation,6,23,0.6388888888888888,254,0.872852233676976,16,0.5517241379310345,0,0
256,It consists of movie review comments with binary sentiment labels .,Conclusion,A Experimental settings A.1 Dataset Description,machine-translation,6,24,0.6666666666666666,255,0.8762886597938144,17,0.5862068965517241,0,0
257,"20 Newsgroups is an email collection dataset , in which the emails are categorized into 20 different groups .",Conclusion,A Experimental settings A.1 Dataset Description,machine-translation,6,25,0.6944444444444444,256,0.8797250859106529,18,0.6206896551724138,0,0
258,"We use the bydate version and select 4 major categories ( comp , politics , rec , and religion ) following . :",Conclusion,A Experimental settings A.1 Dataset Description,machine-translation,6,26,0.7222222222222222,257,0.8831615120274914,19,0.6551724137931034,0,0
259,Detailed statistics about text classification datasets .,Conclusion,A Experimental settings A.1 Dataset Description,machine-translation,6,27,0.75,258,0.8865979381443299,20,0.6896551724137931,0,0
260,A.2 Hyper - parameter configurations,Conclusion,A Experimental settings A.1 Dataset Description,machine-translation,6,28,0.7777777777777778,259,0.8900343642611683,21,0.7241379310344828,0,0
261,The hyper -parameters used for AWD - LSTM with / without MoS in language modeling experiment is shown in .,Conclusion,A Experimental settings A.1 Dataset Description,machine-translation,6,29,0.8055555555555556,260,0.8934707903780069,22,0.7586206896551724,0,0
262,"For machine translation tasks , we choose Adam optimizer with ? 1 = 0.9 , ? 2 = 0.98 , ? = 10 ?9 , and follow the learning rate schedule in .",Conclusion,A Experimental settings A.1 Dataset Description,machine-translation,6,30,0.8333333333333334,261,0.8969072164948454,23,0.7931034482758621,0,0
263,"For evaluation , we use the case - sensitive tokenized BLEU score for WMT14 English - German and case - insensitive tokenized BLEU score for IWSLT14 German - English .",Conclusion,A Experimental settings A.1 Dataset Description,machine-translation,6,31,0.8611111111111112,262,0.9003436426116839,24,0.8275862068965517,0,0
264,The hyper - parameters used in machine translation task are summarized in .,Conclusion,A Experimental settings A.1 Dataset Description,machine-translation,6,32,0.8888888888888888,263,0.9037800687285223,25,0.8620689655172413,0,0
265,The hyper - parameters used in word embedding task are summarized in .,Conclusion,A Experimental settings A.1 Dataset Description,machine-translation,6,33,0.9166666666666666,264,0.9072164948453608,26,0.896551724137931,0,0
266,"For all text classification tasks , we use convolutional kernel with size 2 , 3 , 5 .",Conclusion,A Experimental settings A.1 Dataset Description,machine-translation,6,34,0.9444444444444444,265,0.9106529209621992,27,0.9310344827586208,0,0
267,"We implement batch normalization and shortcut connection , and use Adam optimizer with ? 1 = 0.9 , ? 2 = 0.99 , ? = 10 ?8 .: Hyper- parameter used for word embedding training .",Conclusion,A Experimental settings A.1 Dataset Description,machine-translation,6,35,0.9722222222222222,266,0.9140893470790378,28,0.9655172413793104,0,0
268,AWD - LSTM + MoS,Conclusion,A Experimental settings A.1 Dataset Description,machine-translation,6,36,1.0,267,0.9175257731958762,29,1.0,0,0
269,A.3 Models Description,,,machine-translation,6,0,0.0,268,0.9209621993127148,0,0.0,1,0
270,We use task - specific baseline models .,A.3 Models Description,A.3 Models Description,machine-translation,6,1,0.0454545454545454,269,0.9243986254295532,1,0.0909090909090909,1,0
271,"In language modeling , AWD - LSTM is a weight - dropped LSTM which uses Drop Connect on hidden - to - hidden weights as a means of recurrent regularization .",A.3 Models Description,A.3 Models Description,machine-translation,6,2,0.0909090909090909,270,0.9278350515463918,2,0.1818181818181818,1,0
272,"The model is trained by NT - ASGD , which is a variant of the averaged stochastic gradient method .",A.3 Models Description,A.3 Models Description,machine-translation,6,3,0.1363636363636363,271,0.9312714776632304,3,0.2727272727272727,1,0
273,"The training process has two steps , in the second step , the model is finetuned using another configuration of NT - ASGD .",A.3 Models Description,A.3 Models Description,machine-translation,6,4,0.1818181818181818,272,0.9347079037800688,4,0.3636363636363636,1,0
274,AWD - LSTM - MoS uses the Mixture of Softmaxes structure to the vanilla AWD - LSTM and achieves the state - of - the - art result on PTB and WT2 .,A.3 Models Description,A.3 Models Description,machine-translation,6,5,0.2272727272727272,273,0.9381443298969072,5,0.4545454545454545,1,0
275,"For machine translation , Transformer is a recently developed architecture in which the selfattention network is used during encoding and decoding step .",A.3 Models Description,A.3 Models Description,machine-translation,6,6,0.2727272727272727,274,0.9415807560137456,6,0.5454545454545454,1,0
276,"It achieves the best performances on several machine translation tasks , e.g.",A.3 Models Description,A.3 Models Description,machine-translation,6,7,0.3181818181818182,275,0.9450171821305842,7,0.6363636363636364,1,0
277,"WMT14 English - German , WMT14 English - French datasets .",A.3 Models Description,A.3 Models Description,machine-translation,6,8,0.3636363636363636,276,0.9484536082474226,8,0.7272727272727273,1,0
278,Word2vec is one of the pioneer works on using deep learning to NLP tasks .,A.3 Models Description,A.3 Models Description,machine-translation,6,9,0.4090909090909091,277,0.9518900343642612,9,0.8181818181818182,1,0
279,"Based on the co-occurrence of words , it produces distributed representations of words ( word embeddings ) .",A.3 Models Description,A.3 Models Description,machine-translation,6,10,0.4545454545454545,278,0.9553264604810996,10,0.9090909090909092,1,0
280,"RCNN contains both recurrent and convolutional layers to catch the key components in texts , and is widely used in text classification tasks .",A.3 Models Description,A.3 Models Description,machine-translation,6,11,0.5,279,0.9587628865979382,11,1.0,1,0
281,B Additional Comparisons,A.3 Models Description,,machine-translation,6,12,0.5454545454545454,280,0.9621993127147768,0,0.0,1,0
282,"We compare some other simple methods with ours on machine translation tasks , which include reweighting method and l 2 regularization ( weight decay ) .",A.3 Models Description,B Additional Comparisons,machine-translation,6,13,0.5909090909090909,281,0.9656357388316152,1,0.3333333333333333,1,0
283,Results are listed in .,A.3 Models Description,B Additional Comparisons,machine-translation,6,14,0.6363636363636364,282,0.9690721649484536,2,0.6666666666666666,1,0
284,"We notice that those simple methods do notwork for the tasks , even have negative effects . : BLEU scores on test set of the WMT14 English - German task and IWSLT14 German - English task .",A.3 Models Description,B Additional Comparisons,machine-translation,6,15,0.6818181818181818,283,0.972508591065292,3,1.0,1,0
285,WMT,A.3 Models Description,,machine-translation,6,16,0.7272727272727273,284,0.9759450171821306,0,0.0,1,0
286,"Our method is denoted as "" FRAGE "" , "" Reweighting "" denotes reweighting the loss of each word by reciprocal of its frequency , and "" Weight Decay "" denotes putting weight decay rate ( 0.2 ) on embeddings .",A.3 Models Description,WMT,machine-translation,6,17,0.7727272727272727,285,0.979381443298969,1,0.0,1,0
287,C Case Study on Original Models and Qualitative Analysis of Our Method,A.3 Models Description,WMT,machine-translation,6,18,0.8181818181818182,286,0.9828178694158076,0,0.0,1,0
288,We provide more word similarity cases in to justify our statement in Section 3 .,A.3 Models Description,WMT,machine-translation,6,19,0.8636363636363636,287,0.986254295532646,1,0.25,1,0
289,We also present the effectiveness of our method by showcase and embedding visualizations .,A.3 Models Description,WMT,machine-translation,6,20,0.9090909090909092,288,0.9896907216494846,2,0.5,1,0
290,"From the cases and visualizations in and , we find the word similarities are improved and popular / rare words are better mixed together .",A.3 Models Description,WMT,machine-translation,6,21,0.9545454545454546,289,0.993127147766323,3,0.75,1,0
291,"( a ) ( b ) : These figures show that , in different tasks , the embeddings of rare and popular words are better mixed together after applying our method .",A.3 Models Description,WMT,machine-translation,6,22,1.0,290,0.9965635738831616,4,1.0,1,0
1,title,,,machine-translation,7,0,0.0,0,0.0,0,0.0,1,0
2,OUTRAGEOUSLY LARGE NEURAL NETWORKS : THE SPARSELY - GATED MIXTURE - OF - EXPERTS LAYER,title,title,machine-translation,7,1,0.0,1,0.002680965147453,1,0.0,1,0
3,abstract,,,machine-translation,7,0,0.0,2,0.0053619302949061,0,0.0,1,0
4,The capacity of a neural network to absorb information is limited by its number of parameters .,abstract,abstract,machine-translation,7,1,0.0108695652173913,3,0.0080428954423592,1,0.0204081632653061,1,1
5,"Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation .",abstract,abstract,machine-translation,7,2,0.0217391304347826,4,0.0107238605898123,2,0.0408163265306122,1,1
6,"In practice , however , there are significant algorithmic and performance challenges .",abstract,abstract,machine-translation,7,3,0.0326086956521739,5,0.0134048257372654,3,0.0612244897959183,1,0
7,"In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters .",abstract,abstract,machine-translation,7,4,0.0434782608695652,6,0.0160857908847185,4,0.0816326530612244,1,0
8,"We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks .",abstract,abstract,machine-translation,7,5,0.0543478260869565,7,0.0187667560321715,5,0.1020408163265306,1,0
9,A trainable gating network determines a sparse combination of these experts to use for each example .,abstract,abstract,machine-translation,7,6,0.0652173913043478,8,0.0214477211796246,6,0.1224489795918367,1,0
10,"We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora .",abstract,abstract,machine-translation,7,7,0.0760869565217391,9,0.0241286863270777,7,0.1428571428571428,1,0
11,We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers .,abstract,abstract,machine-translation,7,8,0.0869565217391304,10,0.0268096514745308,8,0.1632653061224489,1,0
12,"On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",abstract,abstract,machine-translation,7,9,0.0978260869565217,11,0.0294906166219839,9,0.1836734693877551,1,0
13,* Equally major contributors Work done as a member of the Google Brain Residency program ( g.co/ brainresidency ),abstract,abstract,machine-translation,7,10,0.108695652173913,12,0.032171581769437,10,0.2040816326530612,1,0
14,INTRODUCTION AND RELATED WORK 1 .,abstract,abstract,machine-translation,7,11,0.1195652173913043,13,0.03485254691689,11,0.2244897959183673,1,0
15,CONDITIONAL COMPUTATION,abstract,abstract,machine-translation,7,12,0.1304347826086956,14,0.0375335120643431,12,0.2448979591836734,1,0
16,Exploiting scale in both training data and model size has been central to the success of deep learning .,abstract,abstract,machine-translation,7,13,0.1413043478260869,15,0.0402144772117962,13,0.2653061224489796,1,1
17,"When datasets are sufficiently large , increasing the capacity ( number of parameters ) of neural networks can give much better prediction accuracy .",abstract,abstract,machine-translation,7,14,0.1521739130434782,16,0.0428954423592493,14,0.2857142857142857,1,0
18,"This has been shown in domains such as text , images , and audio .",abstract,abstract,machine-translation,7,15,0.1630434782608695,17,0.0455764075067024,15,0.3061224489795918,1,0
19,"For typical deep learning models , where the entire model is activated for every example , this leads to a roughly quadratic blow - up in training costs , as both the model size and the number of training examples increase .",abstract,abstract,machine-translation,7,16,0.1739130434782608,18,0.0482573726541554,16,0.3265306122448979,1,0
20,"Unfortunately , the advances in computing power and distributed computation fall short of meeting such demand .",abstract,abstract,machine-translation,7,17,0.1847826086956521,19,0.0509383378016085,17,0.3469387755102041,1,0
21,Various forms of conditional computation have been proposed as away to increase model capacity without a proportional increase in computational costs .,abstract,abstract,machine-translation,7,18,0.1956521739130435,20,0.0536193029490616,18,0.3673469387755102,1,0
22,"In these schemes , large parts of a network are active or inactive on a per-example basis .",abstract,abstract,machine-translation,7,19,0.2065217391304347,21,0.0563002680965147,19,0.3877551020408163,1,0
23,"The gating decisions maybe binary or sparse and continuous , stochastic or deterministic .",abstract,abstract,machine-translation,7,20,0.217391304347826,22,0.0589812332439678,20,0.4081632653061224,1,0
24,Various forms of reinforcement learning and back - propagation are proposed for trarining the gating decisions .,abstract,abstract,machine-translation,7,21,0.2282608695652173,23,0.0616621983914209,21,0.4285714285714285,1,0
25,"While these ideas are promising in theory , no work to date has yet demonstrated massive improvements in model capacity , training time , or model quality .",abstract,abstract,machine-translation,7,22,0.2391304347826087,24,0.064343163538874,22,0.4489795918367347,1,0
26,We blame this on a combination of the following challenges :,abstract,abstract,machine-translation,7,23,0.25,25,0.067024128686327,23,0.4693877551020408,1,0
27,"Modern computing devices , especially GPUs , are much faster at arithmetic than at branching .",abstract,abstract,machine-translation,7,24,0.2608695652173913,26,0.0697050938337801,24,0.4897959183673469,1,0
28,Most of the works above recognize this and propose turning on / off large chunks of the network with each gating decision .,abstract,abstract,machine-translation,7,25,0.2717391304347826,27,0.0723860589812332,25,0.5102040816326531,1,0
29,"Large batch sizes are critical for performance , as they amortize the costs of parameter transfers and updates .",abstract,abstract,machine-translation,7,26,0.2826086956521739,28,0.0750670241286863,26,0.5306122448979592,1,0
30,Conditional computation reduces the batch sizes for the conditionally active chunks of the network .,abstract,abstract,machine-translation,7,27,0.2934782608695652,29,0.0777479892761394,27,0.5510204081632653,1,0
31,Network bandwidth can be a bottleneck .,abstract,abstract,machine-translation,7,28,0.3043478260869565,30,0.0804289544235924,28,0.5714285714285714,1,0
32,A cluster of GPUs may have computational power thousands of times greater than the aggregate inter - device network bandwidth .,abstract,abstract,machine-translation,7,29,0.3152173913043478,31,0.0831099195710455,29,0.5918367346938775,1,0
33,"To be computationally efficient , the relative computational versus network demands of an algorithm must exceed this ratio .",abstract,abstract,machine-translation,7,30,0.3260869565217391,32,0.0857908847184986,30,0.6122448979591837,1,0
34,"Embedding layers , which can be seen as a form of conditional computation , are handicapped by this very problem .",abstract,abstract,machine-translation,7,31,0.3369565217391304,33,0.0884718498659517,31,0.6326530612244898,1,0
35,"Since the embeddings generally need to be sent across the network , the number of ( example , parameter ) interactions is limited by network bandwidth instead of computational capacity .",abstract,abstract,machine-translation,7,32,0.3478260869565217,34,0.0911528150134048,32,0.6530612244897959,1,0
36,"Depending on the scheme , loss terms maybe necessary to achieve the desired level of sparsity per-chunk and / or per example .",abstract,abstract,machine-translation,7,33,0.358695652173913,35,0.0938337801608579,33,0.673469387755102,1,0
37,use three such terms .,abstract,abstract,machine-translation,7,34,0.3695652173913043,36,0.0965147453083109,34,0.6938775510204082,1,0
38,These issues can affect both model quality and load - balancing .,abstract,abstract,machine-translation,7,35,0.3804347826086957,37,0.099195710455764,35,0.7142857142857143,1,0
39,Model capacity is most critical for very large data sets .,abstract,abstract,machine-translation,7,36,0.391304347826087,38,0.1018766756032171,36,0.7346938775510204,1,0
40,"The existing literature on conditional computation deals with relatively small image recognition data sets consisting of up to 600,000 images .",abstract,abstract,machine-translation,7,37,0.4021739130434782,39,0.1045576407506702,37,0.7551020408163265,1,0
41,"It is hard to imagine that the labels of these images provide a sufficient signal to adequately train a model with millions , let alone billions of parameters .",abstract,abstract,machine-translation,7,38,0.4130434782608695,40,0.1072386058981233,38,0.7755102040816326,1,0
42,"In this work , we for the first time address all of the above challenges and finally realize the promise of conditional computation .",abstract,abstract,machine-translation,7,39,0.4239130434782608,41,0.1099195710455764,39,0.7959183673469388,1,0
43,We obtain greater than 1000x improvements in model capacity with only minor losses in computational efficiency and significantly advance the state - of - the - art results on public language modeling and translation data sets .,abstract,abstract,machine-translation,7,40,0.4347826086956521,42,0.1126005361930295,40,0.8163265306122449,1,0
44,OUR APPROACH : THE SPARSELY - GATED MIXTURE - OF - EXPERTS LAYER,abstract,abstract,machine-translation,7,41,0.4456521739130434,43,0.1152815013404825,41,0.8367346938775511,1,0
45,Our approach to conditional computation is to introduce anew type of general purpose neural network component : a Sparsely - Gated Mixture - of - Experts Layer ( MoE ) .,abstract,abstract,machine-translation,7,42,0.4565217391304347,44,0.1179624664879356,42,0.8571428571428571,1,1
46,"The MoE consists of a number of experts , each a simple feed - forward neural network , and a trainable gating network which selects a sparse combination of the experts to process each input ( see ) .",abstract,abstract,machine-translation,7,43,0.4673913043478261,45,0.1206434316353887,43,0.8775510204081632,1,1
47,All parts of the network are trained jointly by back - propagation .,abstract,abstract,machine-translation,7,44,0.4782608695652174,46,0.1233243967828418,44,0.8979591836734694,1,1
48,"While the introduced technique is generic , in this paper we focus on language modeling and machine translation tasks , which are known to benefit from very large models .",abstract,abstract,machine-translation,7,45,0.4891304347826087,47,0.1260053619302949,45,0.9183673469387756,1,0
49,"In particular , we apply a MoE convolutionally between stacked LSTM layers , as in .",abstract,abstract,machine-translation,7,46,0.5,48,0.128686327077748,46,0.9387755102040816,1,0
50,"The MoE is called once for each position in the text , selecting a potentially different combination of experts at each position .",abstract,abstract,machine-translation,7,47,0.5108695652173914,49,0.131367292225201,47,0.9591836734693876,1,0
51,The different experts tend to become highly specialized based on syntax and semantics ( see Appendix E ) .,abstract,abstract,machine-translation,7,48,0.5217391304347826,50,0.1340482573726541,48,0.979591836734694,1,0
52,"On both language modeling and machine translation benchmarks , we improve on best published results at a fraction of the computational cost .",abstract,abstract,machine-translation,7,49,0.532608695652174,51,0.1367292225201072,49,1.0,1,0
53,RELATED WORK ON MIXTURES OF EXPERTS,abstract,abstract,machine-translation,7,50,0.5434782608695652,52,0.1394101876675603,0,0.0,1,0
54,"Since its introduction more than two decades ago , the mixture - of - experts approach has been the subject of much research .",abstract,abstract,machine-translation,7,51,0.5543478260869565,53,0.1420911528150134,1,0.0333333333333333,1,0
55,"Different types of expert architectures hae been proposed such as SVMs , Gaussian Processes , Dirichlet Processes , and deep networks .",abstract,abstract,machine-translation,7,52,0.5652173913043478,54,0.1447721179624665,2,0.0666666666666666,1,0
56,"Other work has focused on different expert configurations such as a hierarchical structure , infinite numbers of experts , and adding experts sequentially .",abstract,abstract,machine-translation,7,53,0.5760869565217391,55,0.1474530831099195,3,0.1,1,0
57,suggest an ensemble model in the format of mixture of experts for machine translation .,abstract,abstract,machine-translation,7,54,0.5869565217391305,56,0.1501340482573726,4,0.1333333333333333,1,0
58,The gating network is trained on a pre-trained ensemble NMT model .,abstract,abstract,machine-translation,7,55,0.5978260869565217,57,0.1528150134048257,5,0.1666666666666666,1,0
59,The works above concern top - level mixtures of experts .,abstract,abstract,machine-translation,7,56,0.6086956521739131,58,0.1554959785522788,6,0.2,1,0
60,The mixture of experts is the whole model .,abstract,abstract,machine-translation,7,57,0.6195652173913043,59,0.1581769436997319,7,0.2333333333333333,1,0
61,introduce the idea of using multiple,abstract,abstract,machine-translation,7,58,0.6304347826086957,60,0.1608579088471849,8,0.2666666666666666,1,0
62,MoEs with their own gating networks as parts of a deep model .,abstract,abstract,machine-translation,7,59,0.6413043478260869,61,0.163538873994638,9,0.3,1,0
63,"It is intuitive that the latter approach is more powerful , since complex problems may contain many sub-problems each requiring different experts .",abstract,abstract,machine-translation,7,60,0.6521739130434783,62,0.1662198391420911,10,0.3333333333333333,1,0
64,"They also allude in their conclusion to the potential to introduce sparsity , turning MoEs into a vehicle for computational computation .",abstract,abstract,machine-translation,7,61,0.6630434782608695,63,0.1689008042895442,11,0.3666666666666666,1,0
65,Our work builds on this use of MoEs as a general purpose neural network component .,abstract,abstract,machine-translation,7,62,0.6739130434782609,64,0.1715817694369973,12,0.4,1,0
66,"While uses two stacked MoEs allowing for two sets of gating decisions , our convolutional application of the MoE allows for different gating decisions at each position in the text .",abstract,abstract,machine-translation,7,63,0.6847826086956522,65,0.1742627345844504,13,0.4333333333333333,1,0
67,We also realize sparse gating and demonstrate its use as a practical way to massively increase model capacity .,abstract,abstract,machine-translation,7,64,0.6956521739130435,66,0.1769436997319035,14,0.4666666666666667,1,0
68,THE STRUCTURE OF THE MIXTURE - OF - EXPERTS LAYER,abstract,abstract,machine-translation,7,65,0.7065217391304348,67,0.1796246648793565,15,0.5,1,0
69,"The Mixture - of - Experts ( MoE ) layer consists of a set of n "" expert networks "" E 1 , , E n , and a "" gating network "" G whose output is a sparse n-dimensional vector .",abstract,abstract,machine-translation,7,66,0.717391304347826,68,0.1823056300268096,16,0.5333333333333333,1,0
70,shows an overview of the MoE module .,abstract,abstract,machine-translation,7,67,0.7282608695652174,69,0.1849865951742627,17,0.5666666666666667,1,0
71,"The experts are themselves neural networks , each with their own parameters .",abstract,abstract,machine-translation,7,68,0.7391304347826086,70,0.1876675603217158,18,0.6,1,0
72,"Although in principle we only require that the experts accept the same sized inputs and produce the same - sized outputs , in our initial investigations in this paper , we restrict ourselves to the case where the models are feed - forward networks with identical architectures , but with separate parameters .",abstract,abstract,machine-translation,7,69,0.75,71,0.1903485254691689,19,0.6333333333333333,1,0
73,Let us denote by G ( x ) and E i ( x ) the output of the gating network and the output of the i - th expert network fora given input x .,abstract,abstract,machine-translation,7,70,0.7608695652173914,72,0.1930294906166219,20,0.6666666666666666,1,0
74,The output y of the MoE module can be written as follows :,abstract,abstract,machine-translation,7,71,0.7717391304347826,73,0.195710455764075,21,0.7,1,0
75,We save computation based on the sparsity of the output of G ( x ) .,abstract,abstract,machine-translation,7,72,0.782608695652174,74,0.1983914209115281,22,0.7333333333333333,1,0
76,"Wherever G (x ) i = 0 , we need not compute E i ( x ) .",abstract,abstract,machine-translation,7,73,0.7934782608695652,75,0.2010723860589812,23,0.7666666666666667,1,0
77,"In our experiments , we have up to thousands of experts , but only need to evaluate a handful of them for every example .",abstract,abstract,machine-translation,7,74,0.8043478260869565,76,0.2037533512064343,24,0.8,1,0
78,"If the number of experts is very large , we can reduce the branching factor by using a two - level hierarchical MoE. Ina hierarchical MoE , a primary gating network chooses a sparse weighted combination of "" experts "" , each of which is itself a secondary mixture - of - experts with its own gating network .",abstract,abstract,machine-translation,7,75,0.8152173913043478,77,0.2064343163538874,25,0.8333333333333334,1,0
79,In the following we focus on ordinary MoEs .,abstract,abstract,machine-translation,7,76,0.8260869565217391,78,0.2091152815013404,26,0.8666666666666667,1,0
80,We provide more details on hierarchical MoEs in Appendix B.,abstract,abstract,machine-translation,7,77,0.8369565217391305,79,0.2117962466487935,27,0.9,1,0
81,Our implementation is related to other models of conditional computation .,abstract,abstract,machine-translation,7,78,0.8478260869565217,80,0.2144772117962466,28,0.9333333333333332,1,0
82,A MoE whose experts are simple weight matrices is similar to the parameterized weight matrix proposed in .,abstract,abstract,machine-translation,7,79,0.8586956521739131,81,0.2171581769436997,29,0.9666666666666668,1,0
83,"A MoE whose experts have one hidden layer is similar to the block - wise dropout described in , where the dropped - out layer is sandwiched between fully - activated layers .",abstract,abstract,machine-translation,7,80,0.8695652173913043,82,0.2198391420911528,30,1.0,1,0
84,GATING NETWORK,abstract,abstract,machine-translation,7,81,0.8804347826086957,83,0.2225201072386059,0,0.0,1,0
85,Softmax Gating :,abstract,abstract,machine-translation,7,82,0.8913043478260869,84,0.225201072386059,1,0.0526315789473684,1,0
86,A simple choice of non-sparse gating function is to multiply the input by a trainable weight matrix W g and then apply the Sof tmax function .,abstract,abstract,machine-translation,7,83,0.9021739130434784,85,0.227882037533512,2,0.1052631578947368,1,0
87,Noisy Top - K,abstract,,machine-translation,7,84,0.9130434782608696,86,0.2305630026809651,3,0.1578947368421052,1,0
88,Gating :,abstract,Noisy Top - K,machine-translation,7,85,0.9239130434782608,87,0.2332439678284182,4,0.2105263157894736,1,0
89,We add two components to the Softmax gating network : sparsity and noise .,abstract,Noisy Top - K,machine-translation,7,86,0.9347826086956522,88,0.2359249329758713,5,0.2631578947368421,1,0
90,"Before taking the softmax function , we add tunable Gaussian noise , then keep only the top k values , setting the rest to ??",abstract,Noisy Top - K,machine-translation,7,87,0.9456521739130436,89,0.2386058981233244,6,0.3157894736842105,1,0
91,( which causes the corresponding gate values to equal 0 ) .,abstract,Noisy Top - K,machine-translation,7,88,0.9565217391304348,90,0.2412868632707774,7,0.3684210526315789,1,0
92,"The sparsity serves to save computation , as described above .",abstract,Noisy Top - K,machine-translation,7,89,0.967391304347826,91,0.2439678284182305,8,0.4210526315789473,1,0
93,"While this form of sparsity creates some theoretically scary discontinuities in the output of gating function , we have not yet observed this to be a problem in practice .",abstract,Noisy Top - K,machine-translation,7,90,0.9782608695652174,92,0.2466487935656836,9,0.4736842105263157,1,0
94,"The noise term helps with load balancing , as will be discussed in Appendix A .",abstract,Noisy Top - K,machine-translation,7,91,0.9891304347826086,93,0.2493297587131367,10,0.5263157894736842,1,0
95,The amount of noise per component is controlled by a second trainable weight matrix W noise .,abstract,Noisy Top - K,machine-translation,7,92,1.0,94,0.2520107238605898,11,0.5789473684210527,1,0
96,Training the Gating Network,,,machine-translation,7,0,0.0,95,0.2546916890080429,12,0.631578947368421,1,0
97,"We train the gating network by simple back - propagation , along with the rest of the model .",Training the Gating Network,Training the Gating Network,machine-translation,7,1,0.0082644628099173,96,0.257372654155496,13,0.6842105263157895,1,0
98,"If we choose k > 1 , the gate values for the top k experts have nonzero derivatives with respect to the weights of the gating network .",Training the Gating Network,Training the Gating Network,machine-translation,7,2,0.0165289256198347,97,0.260053619302949,14,0.7368421052631579,1,0
99,This type of occasionally - sensitive behavior is described in with respect to noisy rectifiers .,Training the Gating Network,Training the Gating Network,machine-translation,7,3,0.024793388429752,98,0.2627345844504021,15,0.7894736842105263,1,0
100,Gradients also backpropagate through the gating network to its inputs .,Training the Gating Network,Training the Gating Network,machine-translation,7,4,0.0330578512396694,99,0.2654155495978552,16,0.8421052631578947,1,0
101,Our method differs here from who use boolean gates and a REINFORCE - style approach to train the gating network .,Training the Gating Network,Training the Gating Network,machine-translation,7,5,0.0413223140495867,100,0.2680965147453083,17,0.8947368421052632,1,0
102,ADDRESSING PERFORMANCE,Training the Gating Network,Training the Gating Network,machine-translation,7,6,0.0495867768595041,101,0.2707774798927614,18,0.9473684210526316,1,0
103,CHALLENGES,Training the Gating Network,,machine-translation,7,7,0.0578512396694214,102,0.2734584450402145,19,1.0,1,0
104,THE SHRINKING BATCH PROBLEM,Training the Gating Network,CHALLENGES,machine-translation,7,8,0.0661157024793388,103,0.2761394101876676,0,0.0,1,0
105,"On modern CPUs and GPUs , large batch sizes are necessary for computational efficiency , so as to amortize the overhead of parameter loads and updates .",Training the Gating Network,CHALLENGES,machine-translation,7,9,0.0743801652892562,104,0.2788203753351206,1,0.032258064516129,1,0
106,"If the gating network chooses k out of n experts for each example , then fora batch of b examples , each expert receives a much smaller batch of approximately kb n b examples .",Training the Gating Network,CHALLENGES,machine-translation,7,10,0.0826446280991735,105,0.2815013404825737,2,0.064516129032258,1,0
107,This causes a naive MoE implementation to become very inefficient as the number of experts increases .,Training the Gating Network,CHALLENGES,machine-translation,7,11,0.0909090909090909,106,0.2841823056300268,3,0.0967741935483871,1,0
108,The solution to this shrinking batch problem is to make the original batch size as large as possible .,Training the Gating Network,CHALLENGES,machine-translation,7,12,0.0991735537190082,107,0.2868632707774799,4,0.1290322580645161,1,0
109,"However , batch size tends to be limited by the memory necessary to store activations between the forwards and backwards passes .",Training the Gating Network,CHALLENGES,machine-translation,7,13,0.1074380165289256,108,0.289544235924933,5,0.1612903225806451,1,0
110,We propose the following techniques for increasing the batch size :,Training the Gating Network,CHALLENGES,machine-translation,7,14,0.1157024793388429,109,0.292225201072386,6,0.1935483870967742,1,0
111,Mixing Data Parallelism and Model Parallelism :,Training the Gating Network,CHALLENGES,machine-translation,7,15,0.1239669421487603,110,0.2949061662198391,7,0.2258064516129032,1,0
112,"Ina conventional distributed training setting , multiple copies of the model on different devices asynchronously process distinct batches of data , and parameters are synchronized through a set of parameter servers .",Training the Gating Network,CHALLENGES,machine-translation,7,16,0.1322314049586777,111,0.2975871313672922,8,0.2580645161290322,1,0
113,"In our technique , these different batches run synchronously so that they can be combined for the MoE layer .",Training the Gating Network,CHALLENGES,machine-translation,7,17,0.140495867768595,112,0.3002680965147453,9,0.2903225806451613,1,0
114,"We distribute the standard layers of the model and the gating network according to conventional data - parallel schemes , but keep only one shared copy of each expert .",Training the Gating Network,CHALLENGES,machine-translation,7,18,0.1487603305785124,113,0.3029490616621983,10,0.3225806451612903,1,0
115,Each expert in the MoE layer receives a combined batch consisting of the relevant examples from all of the data - parallel input batches .,Training the Gating Network,CHALLENGES,machine-translation,7,19,0.1570247933884297,114,0.3056300268096514,11,0.3548387096774194,1,0
116,The same set of devices function as data - parallel replicas ( for the standard layers and the gating networks ) and as model - parallel shards ( each hosting a subset of the experts ) .,Training the Gating Network,CHALLENGES,machine-translation,7,20,0.1652892561983471,115,0.3083109919571045,12,0.3870967741935484,1,0
117,"If the model is distributed over d devices , and each device processes a batch of size b , each expert receives a batch of approximately kbd n examples .",Training the Gating Network,CHALLENGES,machine-translation,7,21,0.1735537190082644,116,0.3109919571045576,13,0.4193548387096774,1,0
118,"Thus , we achieve a factor of d improvement inexpert batch size .",Training the Gating Network,CHALLENGES,machine-translation,7,22,0.1818181818181818,117,0.3136729222520107,14,0.4516129032258064,1,0
119,"In the case of a hierarchical MoE ( Section B ) , the primary gating network employs data parallelism , and the secondary MoEs employ model parallelism .",Training the Gating Network,CHALLENGES,machine-translation,7,23,0.1900826446280991,118,0.3163538873994638,15,0.4838709677419355,1,0
120,Each secondary MoE resides on one device .,Training the Gating Network,CHALLENGES,machine-translation,7,24,0.1983471074380165,119,0.3190348525469169,16,0.5161290322580645,1,0
121,This technique allows us to increase the number of experts ( and hence the number of parameters ) by proportionally increasing the number of devices in the training cluster .,Training the Gating Network,CHALLENGES,machine-translation,7,25,0.2066115702479339,120,0.3217158176943699,17,0.5483870967741935,1,0
122,"The total batch size increases , keeping the batch size per expert constant .",Training the Gating Network,CHALLENGES,machine-translation,7,26,0.2148760330578512,121,0.324396782841823,18,0.5806451612903226,1,0
123,"The memory and bandwidth requirements per device also remain constant , as do the step times , as does the amount of time necessary to process a number of training examples equal to the number of parameters in the model .",Training the Gating Network,CHALLENGES,machine-translation,7,27,0.2231404958677686,122,0.3270777479892761,19,0.6129032258064516,1,0
124,It is our goal to train a trillionparameter model on a trillion - word corpus .,Training the Gating Network,CHALLENGES,machine-translation,7,28,0.2314049586776859,123,0.3297587131367292,20,0.6451612903225806,1,0
125,"We have not scaled our systems this far as of the writing of this paper , but it should be possible by adding more hardware .",Training the Gating Network,CHALLENGES,machine-translation,7,29,0.2396694214876033,124,0.3324396782841823,21,0.6774193548387096,1,0
126,Taking Advantage of Convolutionality :,Training the Gating Network,CHALLENGES,machine-translation,7,30,0.2479338842975206,125,0.3351206434316354,22,0.7096774193548387,1,0
127,"In our language models , we apply the same MoE to each time step of the previous layer .",Training the Gating Network,CHALLENGES,machine-translation,7,31,0.256198347107438,126,0.3378016085790885,23,0.7419354838709677,1,0
128,"If we wait for the previous layer to finish , we can apply the MoE to all the time steps together as one big batch .",Training the Gating Network,CHALLENGES,machine-translation,7,32,0.2644628099173554,127,0.3404825737265415,24,0.7741935483870968,1,0
129,Doing so increases the size of the input batch to the MoE layer by a factor of the number of unrolled time steps .,Training the Gating Network,CHALLENGES,machine-translation,7,33,0.2727272727272727,128,0.3431635388739946,25,0.8064516129032258,1,0
130,Increasing Batch Size fora,Training the Gating Network,,machine-translation,7,34,0.2809917355371901,129,0.3458445040214477,26,0.8387096774193549,1,0
131,Recurrent MoE :,Training the Gating Network,Increasing Batch Size fora,machine-translation,7,35,0.2892561983471074,130,0.3485254691689008,27,0.8709677419354839,1,0
132,We suspect that even more powerful models may involve applying a MoE recurrently .,Training the Gating Network,Increasing Batch Size fora,machine-translation,7,36,0.2975206611570248,131,0.3512064343163539,28,0.9032258064516128,1,0
133,"For example , the weight matrices of a LSTM or other RNN could be replaced by a MoE. Sadly , such models break the convolutional trick from the last paragraph , since the input to the MoE atone timestep depends on the output of the MoE at the previous timestep .",Training the Gating Network,Increasing Batch Size fora,machine-translation,7,37,0.3057851239669421,132,0.353887399463807,29,0.935483870967742,1,0
134,"Gruslys et al . ( 2016 ) describe a technique for drastically reducing the number of stored activations in an unrolled RNN , at the cost of recomputing forward activations .",Training the Gating Network,Increasing Batch Size fora,machine-translation,7,38,0.3140495867768595,133,0.35656836461126,30,0.967741935483871,1,0
135,This would allow fora large increase in batch size .,Training the Gating Network,Increasing Batch Size fora,machine-translation,7,39,0.3223140495867768,134,0.3592493297587131,31,1.0,1,0
136,NETWORK BANDWIDTH,Training the Gating Network,Increasing Batch Size fora,machine-translation,7,40,0.3305785123966942,135,0.3619302949061662,0,0.0,1,0
137,Another major performance concern in distributed computing is network bandwidth .,Training the Gating Network,Increasing Batch Size fora,machine-translation,7,41,0.3388429752066115,136,0.3646112600536193,1,0.1428571428571428,1,0
138,"Since the experts are stationary ( see above ) and the number of gating parameters is small , most of the communication involves sending the inputs and outputs of the experts across the network .",Training the Gating Network,Increasing Batch Size fora,machine-translation,7,42,0.3471074380165289,137,0.3672922252010724,2,0.2857142857142857,1,0
139,"To maintain computational efficiency , the ratio of an expert 's computation to the size of its input and output must exceed the ratio of computational to network capacity of the computing device .",Training the Gating Network,Increasing Batch Size fora,machine-translation,7,43,0.3553719008264462,138,0.3699731903485255,3,0.4285714285714285,1,0
140,"For GPUs , this maybe thousands to one .",Training the Gating Network,Increasing Batch Size fora,machine-translation,7,44,0.3636363636363636,139,0.3726541554959786,4,0.5714285714285714,1,0
141,"In our experiments , we use experts with one hidden layer containing thousands of RELU - activated units .",Training the Gating Network,Increasing Batch Size fora,machine-translation,7,45,0.371900826446281,140,0.3753351206434316,5,0.7142857142857143,1,0
142,"Since the weight matrices in the expert have sizes input_sizehidden_size and hidden_size output_size , the ratio of computation to input and output is equal to the size of the hidden layer .",Training the Gating Network,Increasing Batch Size fora,machine-translation,7,46,0.3801652892561983,141,0.3780160857908847,6,0.8571428571428571,1,0
143,"Conveniently , we can increase computational efficiency simply by using a larger hidden layer , or more hidden layers .",Training the Gating Network,Increasing Batch Size fora,machine-translation,7,47,0.3884297520661157,142,0.3806970509383378,7,1.0,1,0
144,BALANCING EXPERT UTILIZATION,Training the Gating Network,Increasing Batch Size fora,machine-translation,7,48,0.396694214876033,143,0.3833780160857908,0,0.0,1,0
145,We have observed that the gating network tends to converge to a state where it always produces large weights for the same few experts .,Training the Gating Network,Increasing Batch Size fora,machine-translation,7,49,0.4049586776859504,144,0.3860589812332439,1,0.0263157894736842,1,0
146,"This imbalance is self - reinforcing , as the favored experts are trained more rapidly and thus are selected even more by the gating network .",Training the Gating Network,Increasing Batch Size fora,machine-translation,7,50,0.4132231404958678,145,0.388739946380697,2,0.0526315789473684,1,0
147,"describe the same phenomenon , and use a hard constraint at the beginning of training to avoid this local minimum .",Training the Gating Network,Increasing Batch Size fora,machine-translation,7,51,0.4214876033057851,146,0.3914209115281501,3,0.0789473684210526,1,0
148,include a soft constraint on the batch - wise average of each gate .,Training the Gating Network,Increasing Batch Size fora,machine-translation,7,52,0.4297520661157025,147,0.3941018766756032,4,0.1052631578947368,1,0
149,We take a soft constraint approach .,Training the Gating Network,Increasing Batch Size fora,machine-translation,7,53,0.4380165289256198,148,0.3967828418230563,5,0.131578947368421,1,0
150,We define the importance of an expert relative to a batch of training examples to be the batchwise sum of the gate values for that expert .,Training the Gating Network,Increasing Batch Size fora,machine-translation,7,54,0.4462809917355372,149,0.3994638069705093,6,0.1578947368421052,1,0
151,"We define an additional loss L importance , which is added to the overall loss function for the model .",Training the Gating Network,Increasing Batch Size fora,machine-translation,7,55,0.4545454545454545,150,0.4021447721179624,7,0.1842105263157894,1,0
152,"This loss is equal to the square of the coefficient of variation of the set of importance values , multiplied by a hand - tuned scaling factor w importance .",Training the Gating Network,Increasing Batch Size fora,machine-translation,7,56,0.4628099173553719,151,0.4048257372654155,8,0.2105263157894736,1,0
153,This additional loss encourages all experts to have equal importance .,Training the Gating Network,Increasing Batch Size fora,machine-translation,7,57,0.4710743801652892,152,0.4075067024128686,9,0.2368421052631578,1,0
154,L importance ( X ) = w importance CV ( Importance ( X ) ),Training the Gating Network,Increasing Batch Size fora,machine-translation,7,58,0.4793388429752066,153,0.4101876675603217,10,0.2631578947368421,1,0
155,2 . The number of parameters in the LSTM layers of these models vary from 2 million to 151 million .,Training the Gating Network,Increasing Batch Size fora,machine-translation,7,59,0.4876033057851239,154,0.4128686327077748,11,0.2894736842105263,1,0
156,"Quality increases greatly with parameter count , as do computational costs .",Training the Gating Network,Increasing Batch Size fora,machine-translation,7,60,0.4958677685950413,155,0.4155495978552279,12,0.3157894736842105,1,0
157,Results for these models form the top line of - right .,Training the Gating Network,Increasing Batch Size fora,machine-translation,7,61,0.5041322314049587,156,0.4182305630026809,13,0.3421052631578947,1,0
158,MoE Models :,Training the Gating Network,Increasing Batch Size fora,machine-translation,7,62,0.512396694214876,157,0.420911528150134,14,0.3684210526315789,1,0
159,Our models consist of two stacked LSTM layers with a MoE layer between them ( see ) .,Training the Gating Network,Increasing Batch Size fora,machine-translation,7,63,0.5206611570247934,158,0.4235924932975871,15,0.3947368421052631,1,0
160,We vary the sizes of the layers and the number of experts .,Training the Gating Network,Increasing Batch Size fora,machine-translation,7,64,0.5289256198347108,159,0.4262734584450402,16,0.4210526315789473,1,0
161,"For full details on model architecture , training regimen , additional baselines and results , see Appendix C .",Training the Gating Network,Increasing Batch Size fora,machine-translation,7,65,0.5371900826446281,160,0.4289544235924933,17,0.4473684210526316,1,0
162,The results of these models are shown in - left .,Training the Gating Network,Increasing Batch Size fora,machine-translation,7,66,0.5454545454545454,161,0.4316353887399464,18,0.4736842105263157,1,0
163,"The model with 4 always - active experts performed ( unsurprisingly ) similarly to the computationally - matched baseline models , while the largest of the models ( 4096 experts ) achieved an impressive 24 % lower perplexity on the test set .",Training the Gating Network,Increasing Batch Size fora,machine-translation,7,67,0.5537190082644629,162,0.4343163538873995,19,0.5,1,0
164,"Varied Computation , High Capacity :",Training the Gating Network,Increasing Batch Size fora,machine-translation,7,68,0.5619834710743802,163,0.4369973190348525,20,0.5263157894736842,1,0
165,"In addition to the largest model from the previous section , we trained two more MoE models with similarly high capacity ( 4 billion parameters ) , but higher computation budgets .",Training the Gating Network,Increasing Batch Size fora,machine-translation,7,69,0.5702479338842975,164,0.4396782841823056,21,0.5526315789473685,1,0
166,"These models had larger LSTMs , and fewer but larger and experts .",Training the Gating Network,Increasing Batch Size fora,machine-translation,7,70,0.5785123966942148,165,0.4423592493297587,22,0.5789473684210527,1,0
167,Details can be found in Appendix C.2 .,Training the Gating Network,Increasing Batch Size fora,machine-translation,7,71,0.5867768595041323,166,0.4450402144772118,23,0.6052631578947368,1,0
168,Results of these three models form the bottom line of - right .,Training the Gating Network,Increasing Batch Size fora,machine-translation,7,72,0.5950413223140496,167,0.4477211796246649,24,0.631578947368421,1,0
169,compares the results of these models to the best previously - published result on this dataset .,Training the Gating Network,Increasing Batch Size fora,machine-translation,7,73,0.6033057851239669,168,0.450402144772118,25,0.6578947368421053,1,0
170,"Even the fastest of these models beats the best published result ( when controlling for the number of training epochs ) , despite requiring only 6 % of the computation .",Training the Gating Network,Increasing Batch Size fora,machine-translation,7,74,0.6115702479338843,169,0.453083109919571,26,0.6842105263157895,1,0
171,Computational,Training the Gating Network,,machine-translation,7,75,0.6198347107438017,170,0.4557640750670241,27,0.7105263157894737,1,0
172,Efficiency : We trained our models using TensorFlow on clusters containing 16 - 32 Tesla K40 GPUs .,Training the Gating Network,Computational,machine-translation,7,76,0.628099173553719,171,0.4584450402144772,28,0.7368421052631579,1,0
173,"For each of our models , we determine computational efficiency in TFLOPS / GPU by dividing the number of floating point operations required to process one training batch by the observed step time and the number of GPUs in the cluster .",Training the Gating Network,Computational,machine-translation,7,77,0.6363636363636364,172,0.4611260053619302,29,0.7631578947368421,1,0
174,"The operation counts used here are higher than the ones we report in our ops / timestep numbers in that we include the backwards pass , we include the importance - sampling - based training of the softmax layer , and we count a multiply - and - add as two separate operations .",Training the Gating Network,Computational,machine-translation,7,78,0.6446280991735537,173,0.4638069705093833,30,0.7894736842105263,1,0
175,"For all of our MoE models , the floating point operations involved in the experts represent between 37 % and 46 % of the total .",Training the Gating Network,Computational,machine-translation,7,79,0.6528925619834711,174,0.4664879356568364,31,0.8157894736842105,1,0
176,"For our baseline models wtih no MoE , observed computational efficiency ranged from 1.07 - 1.29 TFLOPS / GPU .",Training the Gating Network,Computational,machine-translation,7,80,0.6611570247933884,175,0.4691689008042895,32,0.8421052631578947,1,0
177,"For our low-computation MoE models , computation efficiency ranged from 0.74 - 0.90 TFLOPS / GPU , except for the 4 - expert model which did not make full use of the available parallelism .",Training the Gating Network,Computational,machine-translation,7,81,0.6694214876033058,176,0.4718498659517426,33,0.868421052631579,1,0
178,"Our highest - computation MoE model was more efficient at 1.56 TFLOPS / GPU , likely due to the larger matrices .",Training the Gating Network,Computational,machine-translation,7,82,0.6776859504132231,177,0.4745308310991957,34,0.8947368421052632,1,0
179,These numbers represent a significant fraction of the theoretical maximum of 4.29 TFLOPS / GPU claimed by NVIDIA .,Training the Gating Network,Computational,machine-translation,7,83,0.6859504132231405,178,0.4772117962466488,35,0.9210526315789472,1,0
180,"Detailed results are in Appendix C , .",Training the Gating Network,Computational,machine-translation,7,84,0.6942148760330579,179,0.4798927613941018,36,0.9473684210526316,1,0
181,"On the 1 - billion - word corpus , adding additional capacity seems to produce diminishing returns as the number of parameters in the MoE layer exceeds 1 billion , as can be seen in - left .",Training the Gating Network,Computational,machine-translation,7,85,0.7024793388429752,180,0.4825737265415549,37,0.9736842105263158,1,0
182,"We hypothesized that fora larger training set , even higher capacities would produce significant quality improvements .",Training the Gating Network,Computational,machine-translation,7,86,0.7107438016528925,181,0.485254691689008,38,1.0,1,0
183,100 BILLION WORD GOOGLE NEWS CORPUS,Training the Gating Network,Computational,machine-translation,7,87,0.71900826446281,182,0.4879356568364611,0,0.0,1,0
184,"We constructed a similar training set consisting of shuffled unique sentences from Google 's internal news corpus , totalling roughly 100 billion words .",Training the Gating Network,Computational,machine-translation,7,88,0.7272727272727273,183,0.4906166219839142,1,0.037037037037037,1,0
185,"Similarly to the previous section , we tested a series of models with similar computational costs of about 8 million ops / timestep .",Training the Gating Network,Computational,machine-translation,7,89,0.7355371900826446,184,0.4932975871313673,2,0.074074074074074,1,0
186,"In addition to a baseline LSTM model , we trained models augmented with MoE layers containing 32 , experts .",Training the Gating Network,Computational,machine-translation,7,90,0.743801652892562,185,0.4959785522788204,3,0.1111111111111111,1,1
187,This corresponds to up to 137 billion parameters in the MoE layer .,Training the Gating Network,Computational,machine-translation,7,91,0.7520661157024794,186,0.4986595174262734,4,0.1481481481481481,1,0
188,"Details on architecture , training , and results are given in Appendix D.",Training the Gating Network,Computational,machine-translation,7,92,0.7603305785123967,187,0.5013404825737265,5,0.1851851851851851,1,0
189,Results : shows test perplexity as a function of capacity after training on 10 billion words ( top line ) and 100 billion words ( bottom line ) .,Training the Gating Network,Computational,machine-translation,7,93,0.768595041322314,188,0.5040214477211796,6,0.2222222222222222,1,0
190,"When training over the full 100 billion words , test perplexity improves significantly up to 65536 experts ( 68 billion parameters ) , dropping 39 % lower than the computationally matched baseline , but degrades at 131072 experts , possibly a result of too much sparsity .",Training the Gating Network,Computational,machine-translation,7,94,0.7768595041322314,189,0.5067024128686327,7,0.2592592592592592,1,1
191,The widening gap between the two lines demonstrates ( unsurprisingly ) that increased model capacity helps more on larger training sets .,Training the Gating Network,Computational,machine-translation,7,95,0.7851239669421488,190,0.5093833780160858,8,0.2962962962962963,1,0
192,"Even at 65536 experts ( 99.994 % layer sparsity ) , computational efficiency for the model stays at a respectable 0.72 TFLOPS / GPU .",Training the Gating Network,Computational,machine-translation,7,96,0.7933884297520661,191,0.5120643431635389,9,0.3333333333333333,1,0
193,MACHINE TRANSLATION ( SINGLE LANGUAGE PAIR ),Training the Gating Network,Computational,machine-translation,7,97,0.8016528925619835,192,0.514745308310992,10,0.3703703703703703,1,0
194,Model Architecture :,Training the Gating Network,Computational,machine-translation,7,98,0.8099173553719008,193,0.517426273458445,11,0.4074074074074074,1,0
195,Our model was a modified version of the GNMT model described in .,Training the Gating Network,Computational,machine-translation,7,99,0.8181818181818182,194,0.5201072386058981,12,0.4444444444444444,1,1
196,"To reduce computation , we decreased the number of LSTM layers in the encoder and decoder from 9 and 8 to 3 and 2 respectively .",Training the Gating Network,Computational,machine-translation,7,100,0.8264462809917356,195,0.5227882037533512,13,0.4814814814814814,1,1
197,We inserted MoE layers in both the encoder ( between layers 2 and 3 ) and the decoder ( between layers 1 and 2 ) .,Training the Gating Network,Computational,machine-translation,7,101,0.8347107438016529,196,0.5254691689008043,14,0.5185185185185185,1,1
198,"Each MoE layer contained up to 2048 experts each with about two million parameters , adding a total of about 8 billion parameters to the models .",Training the Gating Network,Computational,machine-translation,7,102,0.8429752066115702,197,0.5281501340482574,15,0.5555555555555556,1,1
199,"Further details on model architecture , testing procedure and results can be found in Appendix E.",Training the Gating Network,Computational,machine-translation,7,103,0.8512396694214877,198,0.5308310991957105,16,0.5925925925925926,1,0
200,Datasets :,Training the Gating Network,Computational,machine-translation,7,104,0.859504132231405,199,0.5335120643431636,17,0.6296296296296297,1,0
201,We benchmarked our method on the WMT ' 14 En? Fr and En ?,Training the Gating Network,Computational,machine-translation,7,105,0.8677685950413223,200,0.5361930294906166,18,0.6666666666666666,1,0
202,"De corpora , whose training sets have 36M sentence pairs and 5 M sentence pairs , respectively .",Training the Gating Network,Computational,machine-translation,7,106,0.8760330578512396,201,0.5388739946380697,19,0.7037037037037037,1,0
203,"The experimental protocols were also similar to those in : newstest2014 was used as the test set to compare against previous work , while the combination of newstest2012 and newstest2013 was used as the development set .",Training the Gating Network,Computational,machine-translation,7,107,0.8842975206611571,202,0.5415549597855228,20,0.7407407407407407,1,0
204,We also tested the same model on a Google 's Production English to French data . 2.79 39.22 214M 278M 6 days/96 k 80s GNMT+RL 2.96 39.92 214M 278M 6 days/96 k80s PBMT 37.0 LSTM ( 6-layer ) 31.5 LSTM ( 6-layer + PosUnk ) 33.1 DeepAtt 37.7 DeepAtt+PosUnk 39.2 5.25 24.91 214M 278M 1 day/96 k80s GNMT + RL 8.08 24.66 214M 278M 1 day/96 k80s PBMT 20.7 DeepAtt 20.6,Training the Gating Network,Computational,machine-translation,7,108,0.8925619834710744,203,0.5442359249329759,21,0.7777777777777778,1,0
205,"Results : show the results of our largest models , compared with published results .",Training the Gating Network,Computational,machine-translation,7,109,0.9008264462809916,204,0.546916890080429,22,0.8148148148148148,1,0
206,Our approach achieved BLEU scores of 40.56 and 26.03 on the WMT ' 14 En?Fr and En ? De benchmarks .,Training the Gating Network,Computational,machine-translation,7,110,0.9090909090909092,205,0.5495978552278821,23,0.8518518518518519,1,1
207,"As our models did not use RL refinement , these results constitute significant gains of 1.34 and 1.12 BLEU score on top of the strong baselines in .",Training the Gating Network,Computational,machine-translation,7,111,0.9173553719008264,206,0.5522788203753352,24,0.8888888888888888,1,0
208,The perplexity scores are also better .,Training the Gating Network,Computational,machine-translation,7,112,0.9256198347107438,207,0.5549597855227882,25,0.925925925925926,1,0
209,2,Training the Gating Network,Computational,machine-translation,7,113,0.9338842975206612,208,0.5576407506702413,26,0.9629629629629628,1,0
210,"On the Google Production dataset , our model achieved 1.01 higher test BLEU score even after training for only one sixth of the time .",Training the Gating Network,Computational,machine-translation,7,114,0.9421487603305784,209,0.5603217158176944,27,1.0,1,1
211,MULTILINGUAL MACHINE TRANSLATION,Training the Gating Network,Computational,machine-translation,7,115,0.950413223140496,210,0.5630026809651475,0,0.0,1,0
212,Results :,Training the Gating Network,Computational,machine-translation,7,116,0.9586776859504132,211,0.5656836461126006,1,0.1666666666666666,1,0
213,"Results for the single - pair GNMT models , the multilingual GNMT model and the multilingual MoE model are given in .",Training the Gating Network,Computational,machine-translation,7,117,0.9669421487603306,212,0.5683646112600537,2,0.3333333333333333,1,0
214,The MoE model achieves 19 % lower perplexity on the dev set than the multilingual GNMT model .,Training the Gating Network,Computational,machine-translation,7,118,0.975206611570248,213,0.5710455764075067,3,0.5,1,1
215,"On BLEU score , the MoE model significantly beats the multilingual GNMT model on 11 of the 12 language pairs ( by as much as 5.84 points ) , and even beats the monolingual GNMT models on 8 of 12 language pairs .",Training the Gating Network,Computational,machine-translation,7,119,0.9834710743801652,214,0.5737265415549598,4,0.6666666666666666,1,1
216,The poor performance on English ?,Training the Gating Network,Computational,machine-translation,7,120,0.9917355371900828,215,0.5764075067024129,5,0.8333333333333334,1,0
217,"Korean seems to be a result of severe overtraining , as for the rarer language pairs a small number of real examples were highly oversampled in the training corpus .",Training the Gating Network,Computational,machine-translation,7,121,1.0,216,0.579088471849866,6,1.0,1,0
218,CONCLUSION,,,machine-translation,7,0,0.0,217,0.5817694369973191,0,0.0,1,0
219,This work is the first to demonstrate major wins from conditional computation in deep networks .,CONCLUSION,CONCLUSION,machine-translation,7,1,0.0064516129032258,218,0.5844504021447721,1,0.03125,0,0
220,We carefully identified the design considerations and challenges of conditional computing and addressed them with a combination of algorithmic and engineering solutions .,CONCLUSION,CONCLUSION,machine-translation,7,2,0.0129032258064516,219,0.5871313672922251,2,0.0625,0,0
221,"While we focused on text , conditional computation may help in other domains as well , provided sufficiently large training sets .",CONCLUSION,CONCLUSION,machine-translation,7,3,0.0193548387096774,220,0.5898123324396782,3,0.09375,0,0
222,We look forward to seeing many novel implementations and applications of conditional computation in the years to come .,CONCLUSION,CONCLUSION,machine-translation,7,4,0.0258064516129032,221,0.5924932975871313,4,0.125,0,0
223,APPENDICES A LOAD - BALANCING LOSS,CONCLUSION,CONCLUSION,machine-translation,7,5,0.032258064516129,222,0.5951742627345844,5,0.15625,0,0
224,"As discussed in section 4 , for load - balancing purposes , we want to define an additional loss function to encourage experts to receive roughly equal numbers of training examples .",CONCLUSION,CONCLUSION,machine-translation,7,6,0.0387096774193548,223,0.5978552278820375,6,0.1875,0,0
225,"Unfortunately , the number of examples received by an expert is a discrete quantity , so it can not be used in backpropagation .",CONCLUSION,CONCLUSION,machine-translation,7,7,0.0451612903225806,224,0.6005361930294906,7,0.21875,0,0
226,"Instead , we define a smooth estimator Load ( X ) of the number of examples assigned to each expert fora batch X of inputs .",CONCLUSION,CONCLUSION,machine-translation,7,8,0.0516129032258064,225,0.6032171581769437,8,0.25,0,0
227,The smoothness allows us to back - propagate gradients through the estimator .,CONCLUSION,CONCLUSION,machine-translation,7,9,0.0580645161290322,226,0.6058981233243967,9,0.28125,0,0
228,This is the purpose of the noise term in the gating function .,CONCLUSION,CONCLUSION,machine-translation,7,10,0.064516129032258,227,0.6085790884718498,10,0.3125,0,0
229,"We define P ( x , i ) as the probability that G (x ) i is nonzero , given anew random choice of noise on element i , but keeping the already - sampled choices of noise on the other elements .",CONCLUSION,CONCLUSION,machine-translation,7,11,0.0709677419354838,228,0.6112600536193029,11,0.34375,0,0
230,"To compute P ( x , i ) , we note that the G ( x ) i is nonzero if and only if H ( x ) i is greater than the k th - greatest element of H ( x ) excluding itself .",CONCLUSION,CONCLUSION,machine-translation,7,12,0.0774193548387096,229,0.613941018766756,12,0.375,0,0
231,The probability works out to be :,CONCLUSION,CONCLUSION,machine-translation,7,13,0.0838709677419354,230,0.6166219839142091,13,0.40625,0,0
232,"Where kth_excluding ( v , k , i ) means the kth highest component of v , excluding component i .",CONCLUSION,CONCLUSION,machine-translation,7,14,0.0903225806451612,231,0.6193029490616622,14,0.4375,0,0
233,"Simplifying , we get :",CONCLUSION,CONCLUSION,machine-translation,7,15,0.0967741935483871,232,0.6219839142091153,15,0.46875,0,0
234,Where ?,CONCLUSION,CONCLUSION,machine-translation,7,16,0.1032258064516129,233,0.6246648793565683,16,0.5,0,0
235,is the CDF of the standard normal distribution .,CONCLUSION,CONCLUSION,machine-translation,7,17,0.1096774193548387,234,0.6273458445040214,17,0.53125,0,0
236,"We can now define the load loss to be the square of the coefficient of variation of the load vector , multiplied by a hand - tuned scaling factor w load .",CONCLUSION,CONCLUSION,machine-translation,7,18,0.1161290322580645,235,0.6300268096514745,18,0.5625,0,0
237,L load ( X ) = w load CV ( Load ( X ) ),CONCLUSION,CONCLUSION,machine-translation,7,19,0.1225806451612903,236,0.6327077747989276,19,0.59375,0,0
238,2 ( 11 ) Initial Load Imbalance :,CONCLUSION,CONCLUSION,machine-translation,7,20,0.1290322580645161,237,0.6353887399463807,20,0.625,0,0
239,"To avoid out - of - memory errors , we need to initialize the network in a state of approximately equal expert load ( since the soft constraints need sometime to work ) .",CONCLUSION,CONCLUSION,machine-translation,7,21,0.1354838709677419,238,0.6380697050938338,21,0.65625,0,0
240,"To accomplish this , we initialize the matrices W g and W noise to all zeros , which yields no signal and some noise .",CONCLUSION,CONCLUSION,machine-translation,7,22,0.1419354838709677,239,0.6407506702412868,22,0.6875,0,0
241,Experiments :,CONCLUSION,CONCLUSION,machine-translation,7,23,0.1483870967741935,240,0.6434316353887399,23,0.71875,0,0
242,"We trained a set of models with identical architecture ( the MoE - 256 model described in Appendix C ) , using different values of w importance and w load .",CONCLUSION,CONCLUSION,machine-translation,7,24,0.1548387096774193,241,0.646112600536193,24,0.75,0,0
243,"We trained each model for 10 epochs , then measured perplexity on the test set .",CONCLUSION,CONCLUSION,machine-translation,7,25,0.1612903225806451,242,0.6487935656836461,25,0.78125,0,0
244,"We also measured the coefficients of variation in Importance and Load , as well as ratio of the load on the most overloaded expert to the average load .",CONCLUSION,CONCLUSION,machine-translation,7,26,0.1677419354838709,243,0.6514745308310992,26,0.8125,0,0
245,This last value is significant for load balancing purposes on distributed hardware .,CONCLUSION,CONCLUSION,machine-translation,7,27,0.1741935483870967,244,0.6541554959785523,27,0.84375,0,0
246,All of these metrics were averaged over several training batches .,CONCLUSION,CONCLUSION,machine-translation,7,28,0.1806451612903225,245,0.6568364611260054,28,0.875,0,0
247,Results :,CONCLUSION,CONCLUSION,machine-translation,7,29,0.1870967741935484,246,0.6595174262734584,29,0.90625,0,0
248,Results are reported in .,CONCLUSION,CONCLUSION,machine-translation,7,30,0.1935483870967742,247,0.6621983914209115,30,0.9375,0,0
249,"All the combinations containing at least one the two losses led to very similar model quality , where having no loss was much worse .",CONCLUSION,CONCLUSION,machine-translation,7,31,0.2,248,0.6648793565683646,31,0.96875,0,0
250,Models with higher values of w load had lower loads on the most overloaded expert .,CONCLUSION,CONCLUSION,machine-translation,7,32,0.2064516129032258,249,0.6675603217158177,32,1.0,0,0
251,B HIERACHICAL MIXTURE OF EXPERTS,CONCLUSION,CONCLUSION,machine-translation,7,33,0.2129032258064516,250,0.6702412868632708,0,0.0,0,0
252,"If the number of experts is very large , we can reduce the branching factor by using a two - level hierarchical MoE. Ina hierarchical MoE , a primary gating network chooses a sparse weighted combination of "" experts "" , each of which is itself a secondary mixture - of - experts with its own gating network .",CONCLUSION,CONCLUSION,machine-translation,7,34,0.2193548387096774,251,0.6729222520107239,1,0.01,0,0
253,3,CONCLUSION,CONCLUSION,machine-translation,7,35,0.2258064516129032,252,0.675603217158177,2,0.02,0,0
254,"If the hierarchical MoE consists of a groups of b experts each , we denote the primary gating network by G primary , the secondary gating networks by , and the expert networks by ( E 0 , 0 , E 0 , 1 ..E a , b ) .",CONCLUSION,CONCLUSION,machine-translation,7,36,0.232258064516129,253,0.67828418230563,3,0.03,0,0
255,The output of the MoE is given by :,CONCLUSION,CONCLUSION,machine-translation,7,37,0.2387096774193548,254,0.6809651474530831,4,0.04,0,0
256,Our metrics of expert utilization change to the following :,CONCLUSION,CONCLUSION,machine-translation,7,38,0.2451612903225806,255,0.6836461126005362,5,0.05,0,0
257,Load primary and Load i deonte the Load functions for the primary gating network and i th secondary gating network respectively .,CONCLUSION,CONCLUSION,machine-translation,7,39,0.2516129032258064,256,0.6863270777479893,6,0.06,0,0
258,X ( i ) denotes the subset of X for which G primary ( x ) i >,CONCLUSION,CONCLUSION,machine-translation,7,40,0.2580645161290322,257,0.6890080428954424,7,0.07,0,0
259,0 .,CONCLUSION,CONCLUSION,machine-translation,7,41,0.2645161290322581,258,0.6916890080428955,8,0.08,0,0
260,"It would seem simpler to let Load H ( X ) i , j = Load i ( X i ) j , but this would not have a gradient with respect to the primary gating network , so we use the formulation above .",CONCLUSION,CONCLUSION,machine-translation,7,42,0.2709677419354839,259,0.6943699731903485,9,0.09,0,0
261,C 1 BILLION WORD LANGUAGE MODELING BENCHMARK - EXPERIMENTAL DETAILS,CONCLUSION,CONCLUSION,machine-translation,7,43,0.2774193548387096,260,0.6970509383378016,10,0.1,0,0
262,C.1 8- MILLION - OPERATIONS - PER - TIMESTEP MODELS,CONCLUSION,CONCLUSION,machine-translation,7,44,0.2838709677419355,261,0.6997319034852547,11,0.11,0,0
263,Model Architecture :,CONCLUSION,CONCLUSION,machine-translation,7,45,0.2903225806451613,262,0.7024128686327078,12,0.12,0,0
264,"Our model consists of five layers : a word embedding layer , a recurrent Long Short - Term Memory ( LSTM ) layer , a MoE layer , a second LSTM layer , and a softmax layer .",CONCLUSION,CONCLUSION,machine-translation,7,46,0.2967741935483871,263,0.7050938337801609,13,0.13,0,0
265,"The dimensionality of the embedding layer , the number of units in each LSTM layer , and the input and output dimensionality of the MoE layer are all equal to 512 .",CONCLUSION,CONCLUSION,machine-translation,7,47,0.3032258064516129,264,0.707774798927614,14,0.14,0,0
266,"For every layer other than the softmax , we apply drouput to the layer output , dropping each activation with probability DropP rob , otherwise dividing by ( 1 ? DropP rob ) .",CONCLUSION,CONCLUSION,machine-translation,7,48,0.3096774193548387,265,0.710455764075067,15,0.15,0,0
267,"After dropout , the output of the previous layer is added to the layer output .",CONCLUSION,CONCLUSION,machine-translation,7,49,0.3161290322580645,266,0.7131367292225201,16,0.16,0,0
268,This residual connection encourages gradient flow .,CONCLUSION,CONCLUSION,machine-translation,7,50,0.3225806451612903,267,0.7158176943699732,17,0.17,0,0
269,"For the hierarchical MoE layers , the first level branching factor was 16 , corresponding to the number of GPUs in our cluster .",CONCLUSION,CONCLUSION,machine-translation,7,51,0.3290322580645161,268,0.7184986595174263,18,0.18,0,0
270,We use Noisy - Top - K Gating ( see Section 2.1 ) with k = 4 for the ordinary MoE layers and k = 2 at each level of the hierarchical MoE layers .,CONCLUSION,CONCLUSION,machine-translation,7,52,0.3354838709677419,269,0.7211796246648794,19,0.19,0,0
271,"Thus , each example is processed by exactly 4 experts fora total of 4M ops / timestep .",CONCLUSION,CONCLUSION,machine-translation,7,53,0.3419354838709677,270,0.7238605898123325,20,0.2,0,0
272,The two LSTM layers contribute 2M ops / timestep each for the desired total of 8 M .,CONCLUSION,CONCLUSION,machine-translation,7,54,0.3483870967741935,271,0.7265415549597856,21,0.21,0,0
273,Computationally - Matched Baselines :,CONCLUSION,CONCLUSION,machine-translation,7,55,0.3548387096774194,272,0.7292225201072386,22,0.22,0,0
274,"The MoE - 4 model does not employ sparsity , since all 4 experts are always used .",CONCLUSION,CONCLUSION,machine-translation,7,56,0.3612903225806451,273,0.7319034852546917,23,0.23,0,0
275,"In addition , we trained four more computationally - matched baseline models with no sparsity :",CONCLUSION,CONCLUSION,machine-translation,7,57,0.3677419354838709,274,0.7345844504021448,24,0.24,0,0
276,MoE - 1 - Wide :,CONCLUSION,CONCLUSION,machine-translation,7,58,0.3741935483870968,275,0.7372654155495979,25,0.25,0,0
277,"The MoE layer consists of a single "" expert "" containing one ReLU - activated hidden layer of size 4096 .",CONCLUSION,CONCLUSION,machine-translation,7,59,0.3806451612903225,276,0.739946380697051,26,0.26,0,0
278,MoE - 1 - Deep :,CONCLUSION,CONCLUSION,machine-translation,7,60,0.3870967741935484,277,0.7426273458445041,27,0.27,0,0
279,"The MoE layer consists of a single "" expert "" containing four ReLU - activated hidden layers , each with size 1024 .",CONCLUSION,CONCLUSION,machine-translation,7,61,0.3935483870967742,278,0.7453083109919572,28,0.28,0,0
280,4xLSTM - 512 : We replace the MoE layer with two additional 512 - unit LSTM layers .,CONCLUSION,CONCLUSION,machine-translation,7,62,0.4,279,0.7479892761394102,29,0.29,0,0
281,LSTM - 2048-512 :,CONCLUSION,CONCLUSION,machine-translation,7,63,0.4064516129032258,280,0.7506702412868632,30,0.3,0,0
282,The model contains one 2048 - unit LSTM layer ( and no MoE ) .,CONCLUSION,CONCLUSION,machine-translation,7,64,0.4129032258064516,281,0.7533512064343163,31,0.31,0,0
283,The output of the LSTM is projected down to 512 dimensions .,CONCLUSION,CONCLUSION,machine-translation,7,65,0.4193548387096774,282,0.7560321715817694,32,0.32,0,0
284,The next timestep of the LSTM receives the projected output .,CONCLUSION,CONCLUSION,machine-translation,7,66,0.4258064516129032,283,0.7587131367292225,33,0.33,0,0
285,This is identical to one of the models published in .,CONCLUSION,CONCLUSION,machine-translation,7,67,0.432258064516129,284,0.7613941018766756,34,0.34,0,0
286,"We re-ran it to account for differences in training regimen , and obtained results very similar to the published ones .",CONCLUSION,CONCLUSION,machine-translation,7,68,0.4387096774193548,285,0.7640750670241286,35,0.35,0,0
287,Training :,CONCLUSION,CONCLUSION,machine-translation,7,69,0.4451612903225806,286,0.7667560321715817,36,0.36,0,0
288,The models were trained on a cluster of 16 K40 GPUs using the synchronous method described in Section 3 .,CONCLUSION,CONCLUSION,machine-translation,7,70,0.4516129032258064,287,0.7694369973190348,37,0.37,0,0
289,"Each batch consisted of a set of sentences totaling roughly 300,000 words .",CONCLUSION,CONCLUSION,machine-translation,7,71,0.4580645161290322,288,0.7721179624664879,38,0.38,0,0
290,"In the interest of time , we limited training to 10 epochs , ( 27,000 steps ) .",CONCLUSION,CONCLUSION,machine-translation,7,72,0.4645161290322581,289,0.774798927613941,39,0.39,0,0
291,"Training took 12 - 16 hours for all models , except for MoE - 4 , which took 18 hours ( since all the expert computation was performed on only 4 of 16 GPUs ) .",CONCLUSION,CONCLUSION,machine-translation,7,73,0.4709677419354838,290,0.7774798927613941,40,0.4,0,0
292,We used the Adam optimizer .,CONCLUSION,CONCLUSION,machine-translation,7,74,0.4774193548387097,291,0.7801608579088471,41,0.41,0,0
293,"The base learning rate was increased linearly for the first 1000 training steps , and decreased after that so as to be proportional to the inverse square root of the step number .",CONCLUSION,CONCLUSION,machine-translation,7,75,0.4838709677419355,292,0.7828418230563002,42,0.42,0,0
294,The Softmax output layer was trained efficiently using importance sampling similarly to the models in .,CONCLUSION,CONCLUSION,machine-translation,7,76,0.4903225806451612,293,0.7855227882037533,43,0.43,0,0
295,"For each model , we performed a hyper - parmeter search to find the best dropout probability , in increments of 0.1 .",CONCLUSION,CONCLUSION,machine-translation,7,77,0.4967741935483871,294,0.7882037533512064,44,0.44,0,0
296,"To ensure balanced expert utilization we set w importance = 0.1 and w load = 0.1 , as described in Section 4 and Appendix A.",CONCLUSION,CONCLUSION,machine-translation,7,78,0.5032258064516129,295,0.7908847184986595,45,0.45,0,0
297,Results :,CONCLUSION,CONCLUSION,machine-translation,7,79,0.5096774193548387,296,0.7935656836461126,46,0.46,0,0
298,"We evaluate our model using perplexity on the holdout dataset , used by .",CONCLUSION,CONCLUSION,machine-translation,7,80,0.5161290322580645,297,0.7962466487935657,47,0.47,0,0
299,We follow the standard procedure and sum overall the words including the end of sentence symbol .,CONCLUSION,CONCLUSION,machine-translation,7,81,0.5225806451612903,298,0.7989276139410187,48,0.48,0,0
300,Results are reported in .,CONCLUSION,CONCLUSION,machine-translation,7,82,0.5290322580645161,299,0.8016085790884718,49,0.49,0,0
301,"For each model , we report the test perplexity , the computational budget , the parameter counts , the value of DropP rob , and the computational efficiency .",CONCLUSION,CONCLUSION,machine-translation,7,83,0.535483870967742,300,0.8042895442359249,50,0.5,0,0
302,We implement several memory optimizations in order to fit up to 1 billion parameters per GPU .,CONCLUSION,CONCLUSION,machine-translation,7,84,0.5419354838709678,301,0.806970509383378,51,0.51,0,0
303,"First , we do not store the activations of the hidden layers of the experts , but instead recompute them on the backwards pass .",CONCLUSION,CONCLUSION,machine-translation,7,85,0.5483870967741935,302,0.8096514745308311,52,0.52,0,0
304,"Secondly , we modify the optimizer on the expert parameters to require less auxiliary storage :",CONCLUSION,CONCLUSION,machine-translation,7,86,0.5548387096774193,303,0.8123324396782842,53,0.53,0,0
305,The Adam optimizer keeps first and second moment estimates of the perparameter gradients .,CONCLUSION,CONCLUSION,machine-translation,7,87,0.5612903225806452,304,0.8150134048257373,54,0.54,0,0
306,This triples the required memory .,CONCLUSION,CONCLUSION,machine-translation,7,88,0.567741935483871,305,0.8176943699731903,55,0.55,0,0
307,"To avoid keeping a first - moment estimator , we set ?",CONCLUSION,CONCLUSION,machine-translation,7,89,0.5741935483870968,306,0.8203753351206434,56,0.56,0,0
308,"1 = 0 . To reduce the size of the second moment estimator , we replace it with a factored approximation .",CONCLUSION,CONCLUSION,machine-translation,7,90,0.5806451612903226,307,0.8230563002680965,57,0.57,0,0
309,"For a matrix of parameters , instead of maintaining a full matrix of second - moment estimators , we maintain vectors of row - wise and column - wise averages of that matrix .",CONCLUSION,CONCLUSION,machine-translation,7,91,0.5870967741935483,308,0.8257372654155496,58,0.58,0,0
310,"At each step , the matrix of estimators is taken to be the outer product of those two vectors divided by the mean of either one .",CONCLUSION,CONCLUSION,machine-translation,7,92,0.5935483870967742,309,0.8284182305630027,59,0.59,0,0
311,This technique could similarly be applied to Adagrad .,CONCLUSION,CONCLUSION,machine-translation,7,93,0.6,310,0.8310991957104558,60,0.6,0,0
312,Results :,CONCLUSION,CONCLUSION,machine-translation,7,94,0.6064516129032258,311,0.8337801608579088,61,0.61,0,0
313,We evaluate our model using perplexity on a holdout dataset .,CONCLUSION,CONCLUSION,machine-translation,7,95,0.6129032258064516,312,0.8364611260053619,62,0.62,0,0
314,Results are reported in .,CONCLUSION,CONCLUSION,machine-translation,7,96,0.6193548387096774,313,0.839142091152815,63,0.63,0,0
315,Perplexity after 100 billion training words is 39 % lower for the 68 - billion - parameter MoE model than for the baseline model .,CONCLUSION,CONCLUSION,machine-translation,7,97,0.6258064516129033,314,0.8418230563002681,64,0.64,0,0
316,It is notable that the measured computational efficiency of the largest model ( 0.30 TFLOPS / GPU ) is very low compared to the other models .,CONCLUSION,CONCLUSION,machine-translation,7,98,0.632258064516129,315,0.8445040214477212,65,0.65,0,0
317,"This is likely a result of the fact that , for purposes of comparison to the other models , we did not increase the training batch size proportionally to the number of GPUs .",CONCLUSION,CONCLUSION,machine-translation,7,99,0.6387096774193548,316,0.8471849865951743,66,0.66,0,0
318,"For comparison , we include results fora computationally matched baseline model consisting of 4 LSTMs , and for an unpruned 5 - gram model with Kneser - Ney smoothing .",CONCLUSION,CONCLUSION,machine-translation,7,100,0.6451612903225806,317,0.8498659517426274,67,0.67,0,0
319,4,CONCLUSION,CONCLUSION,machine-translation,7,101,0.6516129032258065,318,0.8525469168900804,68,0.68,0,0
320,E MACHINE TRANSLATION - EXPERIMENTAL DETAILS,CONCLUSION,CONCLUSION,machine-translation,7,102,0.6580645161290323,319,0.8552278820375335,69,0.69,0,0
321,Model Architecture for Single Language,CONCLUSION,,machine-translation,7,103,0.6645161290322581,320,0.8579088471849866,70,0.7,0,0
322,Pair MoE Models :,CONCLUSION,Model Architecture for Single Language,machine-translation,7,104,0.6709677419354839,321,0.8605898123324397,71,0.71,0,0
323,Our model is a modified version of the GNMT model described in .,CONCLUSION,Model Architecture for Single Language,machine-translation,7,105,0.6774193548387096,322,0.8632707774798928,72,0.72,0,0
324,"To reduce computation , we decrease the number of LSTM layers in the encoder and decoder from 9 and 8 to 3 and 2 respectively .",CONCLUSION,Model Architecture for Single Language,machine-translation,7,106,0.6838709677419355,323,0.8659517426273459,73,0.73,0,0
325,We insert MoE layers in both the encoder ( between layers 2 and 3 ) and the decoder ( between layers 1 and 2 ) .,CONCLUSION,Model Architecture for Single Language,machine-translation,7,107,0.6903225806451613,324,0.868632707774799,74,0.74,0,0
326,"We use an attention mechanism between the encoder and decoder , with the first decoder LSTM receiving output from and providing input for the attention 5 .",CONCLUSION,Model Architecture for Single Language,machine-translation,7,108,0.6967741935483871,325,0.871313672922252,75,0.75,0,0
327,All of the layers in our model have input and output dimensionality of 512 .,CONCLUSION,Model Architecture for Single Language,machine-translation,7,109,0.7032258064516129,326,0.8739946380697051,76,0.76,0,0
328,"Our LSTM layers have 2048 hidden units , with a 512 - dimensional output projection .",CONCLUSION,Model Architecture for Single Language,machine-translation,7,110,0.7096774193548387,327,0.8766756032171582,77,0.77,0,0
329,We add residual connections around all LSTM and MoE layers to encourage gradient flow .,CONCLUSION,Model Architecture for Single Language,machine-translation,7,111,0.7161290322580646,328,0.8793565683646113,78,0.78,0,0
330,"Similar to GNMT , to effectively deal with rare words , we used subword units ( also known as "" wordpieces "" )",CONCLUSION,Model Architecture for Single Language,machine-translation,7,112,0.7225806451612903,329,0.8820375335120644,79,0.79,0,0
331,"( Schuster & Nakajima , 2012 ) for inputs and outputs in our system .",CONCLUSION,Model Architecture for Single Language,machine-translation,7,113,0.7290322580645161,330,0.8847184986595175,80,0.8,0,0
332,We use a shared source and target vocabulary of 32 K wordpieces .,CONCLUSION,Model Architecture for Single Language,machine-translation,7,114,0.7354838709677419,331,0.8873994638069705,81,0.81,0,0
333,We also used the same beam search technique as proposed in Model Architecture for Multilingual MoE Model :,CONCLUSION,Model Architecture for Single Language,machine-translation,7,115,0.7419354838709677,332,0.8900804289544236,82,0.82,0,0
334,"We used the same model architecture as for the single - language - pair models , with the following exceptions :",CONCLUSION,Model Architecture for Single Language,machine-translation,7,116,0.7483870967741936,333,0.8927613941018767,83,0.83,0,0
335,"We used noisy - top - k gating as described in Section 2.1 , not the scheme from Appendix F. The MoE layers in the encoder and decoder are non-hierarchical MoEs with n = 512 experts , and k = 2 .",CONCLUSION,Model Architecture for Single Language,machine-translation,7,117,0.7548387096774194,334,0.8954423592493298,84,0.84,0,0
336,Each expert has a larger hidden layer of size 8192 .,CONCLUSION,Model Architecture for Single Language,machine-translation,7,118,0.7612903225806451,335,0.8981233243967829,85,0.85,0,0
337,"This doubles the amount of computation in the MoE layers , raising the computational budget of the entire model from 85 M to 102M ops / timestep .",CONCLUSION,Model Architecture for Single Language,machine-translation,7,119,0.7677419354838709,336,0.900804289544236,86,0.86,0,0
338,Training :,CONCLUSION,Model Architecture for Single Language,machine-translation,7,120,0.7741935483870968,337,0.903485254691689,87,0.87,0,0
339,We trained our networks using the Adam optimizer .,CONCLUSION,Model Architecture for Single Language,machine-translation,7,121,0.7806451612903226,338,0.906166219839142,88,0.88,0,0
340,"The base learning rate was increased linearly for the first 2000 training steps , held constant for an additional 8000 steps , and decreased after that so as to be proportional to the inverse square root of the step number .",CONCLUSION,Model Architecture for Single Language,machine-translation,7,122,0.7870967741935484,339,0.9088471849865952,89,0.89,0,0
341,"For the single - language - pair models , similarly to , we applied dropout to the output of all embedding , LSTM and MoE layers , using DropP rob = 0.4 .",CONCLUSION,Model Architecture for Single Language,machine-translation,7,123,0.7935483870967742,340,0.9115281501340484,90,0.9,0,0
342,Training was done synchronously on a cluster of up to 64 GPUs as described in section 3 .,CONCLUSION,Model Architecture for Single Language,machine-translation,7,124,0.8,341,0.9142091152815014,91,0.91,0,0
343,Each training batch consisted of a set of sentence pairs containing roughly 16000 words per GPU .,CONCLUSION,Model Architecture for Single Language,machine-translation,7,125,0.8064516129032258,342,0.9168900804289544,92,0.92,0,0
344,"To ensure balanced expert utilization we set w importance = 0.01 and w load = 0.01 , as described in Section 4 and Appendix A.",CONCLUSION,Model Architecture for Single Language,machine-translation,7,126,0.8129032258064516,343,0.9195710455764076,93,0.93,0,0
345,Metrics : We evaluated our models using the perplexity and the standard BLEU score metric .,CONCLUSION,Model Architecture for Single Language,machine-translation,7,127,0.8193548387096774,344,0.9222520107238604,94,0.94,0,0
346,"We reported tokenized BLEU score as computed by the multi -bleu.pl script , downloaded from the public implementation of Moses ( on Github ) , which was also used in .",CONCLUSION,Model Architecture for Single Language,machine-translation,7,128,0.8258064516129032,345,0.9249329758713136,95,0.95,0,0
347,Results : and 4 in Section 5.3 show comparisons of our results to other published methods .,CONCLUSION,Model Architecture for Single Language,machine-translation,7,129,0.832258064516129,346,0.9276139410187668,96,0.96,0,0
348,shows test perplexity as a function of number of words in the ( training data 's ) source sentences processed for models with different numbers of experts .,CONCLUSION,Model Architecture for Single Language,machine-translation,7,130,0.8387096774193549,347,0.9302949061662198,97,0.97,0,0
349,"As can be seen from the as we increased the number of experts to approach 2048 , the test perplexity of our model continued to improve .",CONCLUSION,Model Architecture for Single Language,machine-translation,7,131,0.8451612903225807,348,0.9329758713136728,98,0.98,0,0
350,"We found that the experts indeed become highly specialized by syntax and / or semantics , as can be seen in .",CONCLUSION,Model Architecture for Single Language,machine-translation,7,132,0.8516129032258064,349,0.935656836461126,99,0.99,0,0
351,"For example , one expert is used when the indefinite article "" a "" introduces the direct object in a verb phrase indicating importance or leadership .",CONCLUSION,Model Architecture for Single Language,machine-translation,7,133,0.8580645161290322,350,0.938337801608579,100,1.0,0,0
352,F STRICTLY BALANCED GATING,CONCLUSION,Model Architecture for Single Language,machine-translation,7,134,0.864516129032258,351,0.941018766756032,0,0.0,0,0
353,"Due to some peculiarities in our infrastructure which have since been fixed , at the time we ran some of the machine translation experiments , our models ran faster if every expert received exactly the same batch size .",CONCLUSION,Model Architecture for Single Language,machine-translation,7,135,0.8709677419354839,352,0.9436997319034852,1,0.0769230769230769,0,0
354,"To accommodate this , we used a different gating function which we describe below .",CONCLUSION,Model Architecture for Single Language,machine-translation,7,136,0.8774193548387097,353,0.9463806970509384,2,0.1538461538461538,0,0
355,Recall that we define the softmax gating function to be :,CONCLUSION,Model Architecture for Single Language,machine-translation,7,137,0.8838709677419355,354,0.9490616621983914,3,0.2307692307692307,0,0
356,Sparse Gating ( alternate formulation ) :,CONCLUSION,Model Architecture for Single Language,machine-translation,7,138,0.8903225806451613,355,0.9517426273458444,4,0.3076923076923077,0,0
357,"To obtain a sparse gating vector , we multiply G ?",CONCLUSION,Model Architecture for Single Language,machine-translation,7,139,0.896774193548387,356,0.9544235924932976,5,0.3846153846153846,0,0
358,( x ) component - wise with a sparse mask M ( G ? ( x ) ) and normalize the output .,CONCLUSION,Model Architecture for Single Language,machine-translation,7,140,0.9032258064516128,357,0.9571045576407506,6,0.4615384615384615,0,0
359,"The mask itself is a function of G ? ( x ) and specifies which experts are assigned to each input example : M batchwise ( X , m ) j , i = 1 if X j, i is in the top m values for to expert i 0 otherwise",CONCLUSION,Model Architecture for Single Language,machine-translation,7,141,0.9096774193548388,358,0.9597855227882036,7,0.5384615384615384,0,0
360,"As our experiments suggest and also observed in , using a batchwise function during training ( such as M batchwise ) requires modifications to the inference when we may not have a large batch of examples .",CONCLUSION,Model Architecture for Single Language,machine-translation,7,142,0.9161290322580644,359,0.9624664879356568,8,0.6153846153846154,0,0
361,Our solution to this is to train a vector T of per-expert threshold values to approximate the effects of the batchwise mask .,CONCLUSION,Model Architecture for Single Language,machine-translation,7,143,0.9225806451612903,360,0.96514745308311,9,0.6923076923076923,0,0
362,We use the following mask at inference time :,CONCLUSION,Model Architecture for Single Language,machine-translation,7,144,0.9290322580645162,361,0.967828418230563,10,0.7692307692307693,0,0
363,"To learn the threshold values , we apply an additional loss at training time which is minimized when the batchwise mask and the threshold mask are identical .",CONCLUSION,Model Architecture for Single Language,machine-translation,7,145,0.935483870967742,362,0.970509383378016,11,0.8461538461538461,0,0
364,"L batchwise ( X , T , m ) = | X | j=1 n i=1 ( M threshold ( x , T ) i ?",CONCLUSION,Model Architecture for Single Language,machine-translation,7,146,0.9419354838709676,363,0.9731903485254692,12,0.9230769230769232,0,0
365,"M batchwise ( X , m ) j, i ) ( X j , i ? Ti ) ( 20 )",CONCLUSION,Model Architecture for Single Language,machine-translation,7,147,0.9483870967741936,364,0.9758713136729222,13,1.0,0,0
366,G ATTENTION FUNCTION,CONCLUSION,Model Architecture for Single Language,machine-translation,7,148,0.9548387096774194,365,0.9785522788203752,0,0.0,0,0
367,"The attention mechanism described in GNMT involves a learned "" Attention Function "" A ( x i , y j ) which takes a "" source vector "" x i and a "" target vector "" y j , and must be computed for every source time step i and target time step j .",CONCLUSION,Model Architecture for Single Language,machine-translation,7,149,0.9612903225806452,366,0.9812332439678284,1,0.1428571428571428,0,0
368,"In GNMT , the attention function is implemented as a feed forward neural network with a hidden layer of size n.",CONCLUSION,Model Architecture for Single Language,machine-translation,7,150,0.967741935483871,367,0.9839142091152816,2,0.2857142857142857,0,0
369,It can be expressed as :,CONCLUSION,Model Architecture for Single Language,machine-translation,7,151,0.9741935483870968,368,0.9865951742627346,3,0.4285714285714285,0,0
370,Where U and Ware trainable weight matrices and V is a trainable weight vector .,CONCLUSION,Model Architecture for Single Language,machine-translation,7,152,0.9806451612903224,369,0.9892761394101875,4,0.5714285714285714,0,0
371,"For performance reasons , in our models , we used a slightly different attention function :",CONCLUSION,Model Architecture for Single Language,machine-translation,7,153,0.9870967741935484,370,0.9919571045576407,5,0.7142857142857143,0,0
372,"With our attention function , we can simultaneously compute the attention function on multiple source time steps and multiple target time steps using optimized matrix multiplications .",CONCLUSION,Model Architecture for Single Language,machine-translation,7,154,0.9935483870967742,371,0.9946380697050938,6,0.8571428571428571,0,0
373,We found little difference in quality between the two functions .,CONCLUSION,Model Architecture for Single Language,machine-translation,7,155,1.0,372,0.9973190348525468,7,1.0,0,0
1,title,,,machine-translation,8,1,0.1111111111111111,0,0.0,0,0.0,1,0
2,Published as a conference paper at ICLR 2015 NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE,,,machine-translation,8,2,0.2222222222222222,1,0.0030211480362537,1,0.0,1,1
3,abstract,,,machine-translation,8,3,0.3333333333333333,2,0.0060422960725075,0,0.0,1,0
4,Neural machine translation is a recently proposed approach to machine translation .,,,machine-translation,8,4,0.4444444444444444,3,0.0090634441087613,1,0.1666666666666666,1,0
5,"Unlike the traditional statistical machine translation , the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance .",,,machine-translation,8,5,0.5555555555555556,4,0.0120845921450151,2,0.3333333333333333,1,0
6,The models proposed recently for neural machine translation often belong to a family of encoder - decoders and encode a source sentence into a fixed - length vector from which a decoder generates a translation .,,,machine-translation,8,6,0.6666666666666666,5,0.0151057401812688,3,0.5,1,0
7,"In this paper , we conjecture that the use of a fixed - length vector is a bottleneck in improving the performance of this basic encoder - decoder architecture , and propose to extend this by allowing a model to automatically ( soft - ) search for parts of a source sentence that are relevant to predicting a target word , without having to form these parts as a hard segment explicitly .",,,machine-translation,8,7,0.7777777777777778,6,0.0181268882175226,4,0.6666666666666666,1,1
8,"With this new approach , we achieve a translation performance comparable to the existing state - of - the - art phrase - based system on the task of English - to - French translation .",,,machine-translation,8,8,0.8888888888888888,7,0.0211480362537764,5,0.8333333333333334,1,0
9,"Furthermore , qualitative analysis reveals that the ( soft - ) alignments found by the model agree well with our intuition .",,,machine-translation,8,9,1.0,8,0.0241691842900302,6,1.0,1,0
10,INTRODUCTION,,,machine-translation,8,0,0.0,9,0.0271903323262839,0,0.0,1,0
11,"Neural machine translation is a newly emerging approach to machine translation , recently proposed by , and .",INTRODUCTION,INTRODUCTION,machine-translation,8,1,0.0108695652173913,10,0.0302114803625377,1,0.0232558139534883,1,1
12,"Unlike the traditional phrase - based translation system ( see , e.g. , which consists of many small sub-components that are tuned separately , neural machine translation attempts to build and train a single , large neural network that reads a sentence and outputs a correct translation .",INTRODUCTION,INTRODUCTION,machine-translation,8,2,0.0217391304347826,11,0.0332326283987915,2,0.0465116279069767,1,0
13,"Most of the proposed neural machine translation models belong to a family of encoderdecoders , with an encoder and a decoder for each language , or involve a language - specific encoder applied to each sentence whose outputs are then compared ) .",INTRODUCTION,INTRODUCTION,machine-translation,8,3,0.0326086956521739,12,0.0362537764350453,3,0.0697674418604651,1,0
14,An encoder neural network reads and encodes a source sentence into a fixed - length vector .,INTRODUCTION,INTRODUCTION,machine-translation,8,4,0.0434782608695652,13,0.039274924471299,4,0.0930232558139534,1,0
15,A decoder then outputs a translation from the encoded vector .,INTRODUCTION,INTRODUCTION,machine-translation,8,5,0.0543478260869565,14,0.0422960725075528,5,0.1162790697674418,1,0
16,"The whole encoder - decoder system , which consists of the encoder and the decoder fora language pair , is jointly trained to maximize the probability of a correct translation given a source sentence .",INTRODUCTION,INTRODUCTION,machine-translation,8,6,0.0652173913043478,15,0.0453172205438066,6,0.1395348837209302,1,0
17,A potential issue with this encoder - decoder approach is that a neural network needs to be able to compress all the necessary information of a source sentence into a fixed - length vector .,INTRODUCTION,INTRODUCTION,machine-translation,8,7,0.0760869565217391,16,0.0483383685800604,7,0.1627906976744186,1,0
18,"This may make it difficult for the neural network to cope with long sentences , especially those that are longer than the sentences in the training corpus .",INTRODUCTION,INTRODUCTION,machine-translation,8,8,0.0869565217391304,17,0.0513595166163142,8,0.1860465116279069,1,0
19,showed that indeed the performance of a basic encoder - decoder deteriorates rapidly as the length of an input sentence increases .,INTRODUCTION,INTRODUCTION,machine-translation,8,9,0.0978260869565217,18,0.0543806646525679,9,0.2093023255813953,1,0
20,"In order to address this issue , we introduce an extension to the encoder - decoder model which learns to align and translate jointly .",INTRODUCTION,INTRODUCTION,machine-translation,8,10,0.108695652173913,19,0.0574018126888217,10,0.2325581395348837,1,1
21,"Each time the proposed model generates a word in a translation , it ( soft - ) searches fora set of positions in a source sentence where the most relevant information is concentrated .",INTRODUCTION,INTRODUCTION,machine-translation,8,11,0.1195652173913043,20,0.0604229607250755,11,0.2558139534883721,1,1
22,The model then predicts a target word based on the context vectors associated with these source positions and all the previous generated target words .,INTRODUCTION,INTRODUCTION,machine-translation,8,12,0.1304347826086956,21,0.0634441087613293,12,0.2790697674418604,1,1
23,The most important distinguishing feature of this approach from the basic encoder - decoder is that it does not attempt to encode a whole input sentence into a single fixed - length vector .,INTRODUCTION,INTRODUCTION,machine-translation,8,13,0.1413043478260869,22,0.066465256797583,13,0.3023255813953488,1,0
24,"Instead , it encodes the input sentence into a sequence of vectors and chooses a subset of these vectors adaptively while decoding the translation .",INTRODUCTION,INTRODUCTION,machine-translation,8,14,0.1521739130434782,23,0.0694864048338368,14,0.3255813953488372,1,0
25,"This frees a neural translation model from having to squash all the information of a source sentence , regardless of its length , into a fixed - length vector .",INTRODUCTION,INTRODUCTION,machine-translation,8,15,0.1630434782608695,24,0.0725075528700906,15,0.3488372093023256,1,0
26,We show this allows a model to cope better with long sentences .,INTRODUCTION,INTRODUCTION,machine-translation,8,16,0.1739130434782608,25,0.0755287009063444,16,0.3720930232558139,1,0
27,"In this paper , we show that the proposed approach of jointly learning to align and translate achieves significantly improved translation performance over the basic encoder - decoder approach .",INTRODUCTION,INTRODUCTION,machine-translation,8,17,0.1847826086956521,26,0.0785498489425981,17,0.3953488372093023,1,0
28,"The improvement is more apparent with longer sentences , but can be observed with sentences of any length .",INTRODUCTION,INTRODUCTION,machine-translation,8,18,0.1956521739130435,27,0.0815709969788519,18,0.4186046511627907,1,0
29,"On the task of English - to - French translation , the proposed approach achieves , with a single model , a translation performance comparable , or close , to the conventional phrase - based system .",INTRODUCTION,INTRODUCTION,machine-translation,8,19,0.2065217391304347,28,0.0845921450151057,19,0.4418604651162791,1,0
30,"Furthermore , qualitative analysis reveals that the proposed model finds a linguistically plausible ( soft - ) alignment between a source sentence and the corresponding target sentence .",INTRODUCTION,INTRODUCTION,machine-translation,8,20,0.217391304347826,29,0.0876132930513595,20,0.4651162790697674,1,0
31,BACKGROUND : NEURAL MACHINE TRANSLATION,INTRODUCTION,,machine-translation,8,21,0.2282608695652173,30,0.0906344410876132,21,0.4883720930232558,1,0
32,"From a probabilistic perspective , translation is equivalent to finding a target sentence y that maximizes the conditional probability of y given a source sentence x , i.e. , arg max y p ( y | x ) .",INTRODUCTION,BACKGROUND : NEURAL MACHINE TRANSLATION,machine-translation,8,22,0.2391304347826087,31,0.093655589123867,22,0.5116279069767442,1,0
33,"In neural machine translation , we fit a parameterized model to maximize the conditional probability of sentence pairs using a parallel training corpus .",INTRODUCTION,BACKGROUND : NEURAL MACHINE TRANSLATION,machine-translation,8,23,0.25,32,0.0966767371601208,23,0.5348837209302325,1,0
34,"Once the conditional distribution is learned by a translation model , given a source sentence a corresponding translation can be generated by searching for the sentence that maximizes the conditional probability .",INTRODUCTION,BACKGROUND : NEURAL MACHINE TRANSLATION,machine-translation,8,24,0.2608695652173913,33,0.0996978851963746,24,0.5581395348837209,1,0
35,"Recently , a number of papers have proposed the use of neural networks to directly learn this conditional distribution ( see , e.g. , .",INTRODUCTION,BACKGROUND : NEURAL MACHINE TRANSLATION,machine-translation,8,25,0.2717391304347826,34,0.1027190332326284,25,0.5813953488372093,1,0
36,"This neural machine translation approach typically consists of two components , the first of which encodes a source sentence x and the second decodes to a target sentence y .",INTRODUCTION,BACKGROUND : NEURAL MACHINE TRANSLATION,machine-translation,8,26,0.2826086956521739,35,0.1057401812688821,26,0.6046511627906976,1,0
37,"For instance , two recurrent neural networks ( RNN ) were used by and to encode a variable - length source sentence into a fixed - length vector and to decode the vector into a variable - length target sentence .",INTRODUCTION,BACKGROUND : NEURAL MACHINE TRANSLATION,machine-translation,8,27,0.2934782608695652,36,0.1087613293051359,27,0.627906976744186,1,0
38,"Despite being a quite new approach , neural machine translation has already shown promising results .",INTRODUCTION,BACKGROUND : NEURAL MACHINE TRANSLATION,machine-translation,8,28,0.3043478260869565,37,0.1117824773413897,28,0.6511627906976745,1,0
39,reported that the neural machine translation based on RNNs with long shortterm memory ( LSTM ) units achieves close to the state - of - the - art performance of the conventional phrase - based machine translation system on an English - to - French translation task .,INTRODUCTION,BACKGROUND : NEURAL MACHINE TRANSLATION,machine-translation,8,29,0.3152173913043478,38,0.1148036253776435,29,0.6744186046511628,1,0
40,"1 Adding neural components to existing translation systems , for instance , to score the phrase pairs in the phrase table or to re-rank candidate translations , has allowed to surpass the previous state - of - the - art performance level .",INTRODUCTION,BACKGROUND : NEURAL MACHINE TRANSLATION,machine-translation,8,30,0.3260869565217391,39,0.1178247734138972,30,0.6976744186046512,1,0
41,RNN ENCODER - DECODER,INTRODUCTION,,machine-translation,8,31,0.3369565217391304,40,0.120845921450151,31,0.7209302325581395,1,0
42,"Here , we describe briefly the underlying framework , called RNN Encoder - Decoder , proposed by and upon which we build a novel architecture that learns to align and translate simultaneously .",INTRODUCTION,RNN ENCODER - DECODER,machine-translation,8,32,0.3478260869565217,41,0.1238670694864048,32,0.7441860465116279,1,0
43,"In the Encoder - Decoder framework , an encoder reads the input sentence , a sequence of vectors x = ( x 1 , , x Tx ) , into a vector c. 2 The most common approach is to use an RNN such that",INTRODUCTION,RNN ENCODER - DECODER,machine-translation,8,33,0.358695652173913,42,0.1268882175226586,33,0.7674418604651163,1,0
44,"( 1 ) and c = q ( {h 1 , , h Tx } ) , where ht ?",INTRODUCTION,RNN ENCODER - DECODER,machine-translation,8,34,0.3695652173913043,43,0.1299093655589124,34,0.7906976744186046,1,0
45,"Rn is a hidden state at time t , and c is a vector generated from the sequence of the hidden states .",INTRODUCTION,RNN ENCODER - DECODER,machine-translation,8,35,0.3804347826086957,44,0.1329305135951661,35,0.813953488372093,1,0
46,f and q are some nonlinear functions .,INTRODUCTION,RNN ENCODER - DECODER,machine-translation,8,36,0.391304347826087,45,0.1359516616314199,36,0.8372093023255814,1,0
47,"used an LSTM as f and q ( {h 1 , , h T }) = h T , for instance .",INTRODUCTION,RNN ENCODER - DECODER,machine-translation,8,37,0.4021739130434782,46,0.1389728096676737,37,0.8604651162790697,1,0
48,"The decoder is often trained to predict the next wordy t given the context vector c and all the previously predicted words {y 1 , , y t ?1 }.",INTRODUCTION,RNN ENCODER - DECODER,machine-translation,8,38,0.4130434782608695,47,0.1419939577039275,38,0.8837209302325582,1,0
49,"In other words , the decoder defines a probability over the translation y by decomposing the joint probability into the ordered conditionals :",INTRODUCTION,RNN ENCODER - DECODER,machine-translation,8,39,0.4239130434782608,48,0.1450151057401812,39,0.9069767441860463,1,0
50,"where y = y 1 , , y Ty .",INTRODUCTION,RNN ENCODER - DECODER,machine-translation,8,40,0.4347826086956521,49,0.148036253776435,40,0.9302325581395348,1,0
51,"With an RNN , each conditional probability is modeled as",INTRODUCTION,RNN ENCODER - DECODER,machine-translation,8,41,0.4456521739130434,50,0.1510574018126888,41,0.9534883720930232,1,0
52,"where g is a nonlinear , potentially multi-layered , function that outputs the probability of y t , and st is the hidden state of the RNN .",INTRODUCTION,RNN ENCODER - DECODER,machine-translation,8,42,0.4565217391304347,51,0.1540785498489426,42,0.9767441860465116,1,0
53,It should be noted that other architectures such as a hybrid of an RNN and a de-convolutional neural network can be used .,INTRODUCTION,RNN ENCODER - DECODER,machine-translation,8,43,0.4673913043478261,52,0.1570996978851963,43,1.0,1,0
54,LEARNING TO ALIGN AND TRANSLATE,INTRODUCTION,,machine-translation,8,44,0.4782608695652174,53,0.1601208459214501,0,0.0,1,0
55,"In this section , we propose a novel architecture for neural machine translation .",INTRODUCTION,LEARNING TO ALIGN AND TRANSLATE,machine-translation,8,45,0.4891304347826087,54,0.1631419939577039,1,0.0256410256410256,1,0
56,The new architecture consists of a bidirectional RNN as an encoder ( Sec. 3.2 ) and a decoder that emulates searching through a source sentence during decoding a translation ( Sec. 3.1 ) .,INTRODUCTION,LEARNING TO ALIGN AND TRANSLATE,machine-translation,8,46,0.5,55,0.1661631419939577,2,0.0512820512820512,1,0
57,DECODER : GENERAL DESCRIPTION,INTRODUCTION,,machine-translation,8,47,0.5108695652173914,56,0.1691842900302115,3,0.0769230769230769,1,0
58,x 1 x 2 x 3 x T :,INTRODUCTION,DECODER : GENERAL DESCRIPTION,machine-translation,8,48,0.5217391304347826,57,0.1722054380664652,4,0.1025641025641025,1,0
59,"The graphical illustration of the proposed model trying to generate the t-th target wordy t given a source sentence ( x 1 , x 2 , . . . , x T ) .",INTRODUCTION,DECODER : GENERAL DESCRIPTION,machine-translation,8,49,0.532608695652174,58,0.175226586102719,5,0.1282051282051282,1,0
60,"Ina new model architecture , we define each conditional probability in Eq .",INTRODUCTION,DECODER : GENERAL DESCRIPTION,machine-translation,8,50,0.5434782608695652,59,0.1782477341389728,6,0.1538461538461538,1,0
61,( 2 ) as :,INTRODUCTION,DECODER : GENERAL DESCRIPTION,machine-translation,8,51,0.5543478260869565,60,0.1812688821752265,7,0.1794871794871795,1,0
62,"where s i is an RNN hidden state for time i , computed by",INTRODUCTION,DECODER : GENERAL DESCRIPTION,machine-translation,8,52,0.5652173913043478,61,0.1842900302114803,8,0.2051282051282051,1,0
63,It should be noted that unlike the existing encoder - decoder approach ( see Eq.,INTRODUCTION,DECODER : GENERAL DESCRIPTION,machine-translation,8,53,0.5760869565217391,62,0.1873111782477341,9,0.2307692307692307,1,0
64,"( 2 ) ) , here the probability is conditioned on a distinct context vector c i for each target wordy i .",INTRODUCTION,DECODER : GENERAL DESCRIPTION,machine-translation,8,54,0.5869565217391305,63,0.1903323262839879,10,0.2564102564102564,1,0
65,"The context vector c i depends on a sequence of annotations ( h 1 , , h Tx ) to which an encoder maps the input sentence .",INTRODUCTION,DECODER : GENERAL DESCRIPTION,machine-translation,8,55,0.5978260869565217,64,0.1933534743202417,11,0.282051282051282,1,0
66,Each annotation hi contains information about the whole input sequence with a strong focus on the parts surrounding the i - th word of the input sequence .,INTRODUCTION,DECODER : GENERAL DESCRIPTION,machine-translation,8,56,0.6086956521739131,65,0.1963746223564954,12,0.3076923076923077,1,0
67,We explain in detail how the annotations are computed in the next section .,INTRODUCTION,DECODER : GENERAL DESCRIPTION,machine-translation,8,57,0.6195652173913043,66,0.1993957703927492,13,0.3333333333333333,1,0
68,"The context vector c i is , then , computed as a weighted sum of these annotations hi :",INTRODUCTION,DECODER : GENERAL DESCRIPTION,machine-translation,8,58,0.6304347826086957,67,0.202416918429003,14,0.358974358974359,1,0
69,The weight ?,INTRODUCTION,DECODER : GENERAL DESCRIPTION,machine-translation,8,59,0.6413043478260869,68,0.2054380664652568,15,0.3846153846153846,1,0
70,ij of each annotation h j is computed by,INTRODUCTION,DECODER : GENERAL DESCRIPTION,machine-translation,8,60,0.6521739130434783,69,0.2084592145015106,16,0.4102564102564102,1,0
71,"where e ij = a (s i?1 , h j ) is an alignment model which scores how well the inputs around position j and the output at position i match .",INTRODUCTION,DECODER : GENERAL DESCRIPTION,machine-translation,8,61,0.6630434782608695,70,0.2114803625377643,17,0.4358974358974359,1,0
72,"The score is based on the RNN hidden state s i ?1 ( just before emitting y i , Eq. ( 4 ) ) and the j - th annotation h j of the input sentence .",INTRODUCTION,DECODER : GENERAL DESCRIPTION,machine-translation,8,62,0.6739130434782609,71,0.2145015105740181,18,0.4615384615384615,1,0
73,We parametrize the alignment model a as a feedforward neural network which is jointly trained with all the other components of the proposed system .,INTRODUCTION,DECODER : GENERAL DESCRIPTION,machine-translation,8,63,0.6847826086956522,72,0.2175226586102719,19,0.4871794871794871,1,0
74,"Note that unlike in traditional machine translation , the alignment is not considered to be a latent variable .",INTRODUCTION,DECODER : GENERAL DESCRIPTION,machine-translation,8,64,0.6956521739130435,73,0.2205438066465256,20,0.5128205128205128,1,0
75,"Instead , the alignment model directly computes a soft alignment , which allows the gradient of the cost function to be backpropagated through .",INTRODUCTION,DECODER : GENERAL DESCRIPTION,machine-translation,8,65,0.7065217391304348,74,0.2235649546827794,21,0.5384615384615384,1,0
76,This gradient can be used to train the alignment model as well as the whole translation model jointly .,INTRODUCTION,DECODER : GENERAL DESCRIPTION,machine-translation,8,66,0.717391304347826,75,0.2265861027190332,22,0.5641025641025641,1,0
77,"We can understand the approach of taking a weighted sum of all the annotations as computing an expected annotation , where the expectation is over possible alignments .",INTRODUCTION,DECODER : GENERAL DESCRIPTION,machine-translation,8,67,0.7282608695652174,76,0.229607250755287,23,0.5897435897435898,1,0
78,Let ?,INTRODUCTION,DECODER : GENERAL DESCRIPTION,machine-translation,8,68,0.7391304347826086,77,0.2326283987915408,24,0.6153846153846154,1,0
79,"ij be a probability that the target wordy i is aligned to , or translated from , a source word x j .",INTRODUCTION,DECODER : GENERAL DESCRIPTION,machine-translation,8,69,0.75,78,0.2356495468277945,25,0.6410256410256411,1,0
80,"Then , the i - th context vector c i is the expected annotation overall the annotations with probabilities ? ij .",INTRODUCTION,DECODER : GENERAL DESCRIPTION,machine-translation,8,70,0.7608695652173914,79,0.2386706948640483,26,0.6666666666666666,1,0
81,The probability ?,INTRODUCTION,DECODER : GENERAL DESCRIPTION,machine-translation,8,71,0.7717391304347826,80,0.2416918429003021,27,0.6923076923076923,1,0
82,"ij , or it s associated energy e ij , reflects the importance of the annotation h j with respect to the previous hidden state s i ?1 in deciding the next state s i and generating y i .",INTRODUCTION,DECODER : GENERAL DESCRIPTION,machine-translation,8,72,0.782608695652174,81,0.2447129909365558,28,0.717948717948718,1,0
83,"Intuitively , this implements a mechanism of attention in the decoder .",INTRODUCTION,DECODER : GENERAL DESCRIPTION,machine-translation,8,73,0.7934782608695652,82,0.2477341389728096,29,0.7435897435897436,1,0
84,The decoder decides parts of the source sentence to pay attention to .,INTRODUCTION,DECODER : GENERAL DESCRIPTION,machine-translation,8,74,0.8043478260869565,83,0.2507552870090634,30,0.7692307692307693,1,0
85,"By letting the decoder have an attention mechanism , we relieve the encoder from the burden of having to encode all information in the source sentence into a fixedlength vector .",INTRODUCTION,DECODER : GENERAL DESCRIPTION,machine-translation,8,75,0.8152173913043478,84,0.2537764350453172,31,0.7948717948717948,1,0
86,"With this new approach the information can be spread throughout the sequence of annotations , which can be selectively retrieved by the decoder accordingly .",INTRODUCTION,DECODER : GENERAL DESCRIPTION,machine-translation,8,76,0.8260869565217391,85,0.256797583081571,32,0.8205128205128205,1,0
87,ENCODER : BIDIRECTIONAL RNN FOR ANNOTATING SEQUENCES,INTRODUCTION,,machine-translation,8,77,0.8369565217391305,86,0.2598187311178248,33,0.8461538461538461,1,0
88,"The usual RNN , described in Eq. ( 1 ) , reads an input sequence x in order starting from the first symbol x 1 to the last one x Tx .",INTRODUCTION,ENCODER : BIDIRECTIONAL RNN FOR ANNOTATING SEQUENCES,machine-translation,8,78,0.8478260869565217,87,0.2628398791540785,34,0.8717948717948718,1,0
89,"However , in the proposed scheme , we would like the annotation of each word to summarize not only the preceding words , but also the following words .",INTRODUCTION,ENCODER : BIDIRECTIONAL RNN FOR ANNOTATING SEQUENCES,machine-translation,8,79,0.8586956521739131,88,0.2658610271903323,35,0.8974358974358975,1,0
90,"Hence , we propose to use a bidirectional RNN ( BiRNN , , which has been successfully used recently in speech recognition ( see , e.g. , .",INTRODUCTION,ENCODER : BIDIRECTIONAL RNN FOR ANNOTATING SEQUENCES,machine-translation,8,80,0.8695652173913043,89,0.2688821752265861,36,0.9230769230769232,1,0
91,A BiRNN consists of forward and backward RNN 's .,INTRODUCTION,ENCODER : BIDIRECTIONAL RNN FOR ANNOTATING SEQUENCES,machine-translation,8,81,0.8804347826086957,90,0.2719033232628399,37,0.9487179487179488,1,0
92,The forward RNN ? ?,INTRODUCTION,ENCODER : BIDIRECTIONAL RNN FOR ANNOTATING SEQUENCES,machine-translation,8,82,0.8913043478260869,91,0.2749244712990936,38,0.9743589743589745,1,0
93,f reads the input sequence as it is ordered ( from x 1 to x Tx ) and calculates a sequence of forward hidden states (,INTRODUCTION,ENCODER : BIDIRECTIONAL RNN FOR ANNOTATING SEQUENCES,machine-translation,8,83,0.9021739130434784,92,0.2779456193353474,39,1.0,1,0
94,The backward RNN,INTRODUCTION,ENCODER : BIDIRECTIONAL RNN FOR ANNOTATING SEQUENCES,machine-translation,8,84,0.9130434782608696,93,0.2809667673716012,0,0.0,1,0
95,? ?,INTRODUCTION,ENCODER : BIDIRECTIONAL RNN FOR ANNOTATING SEQUENCES,machine-translation,8,85,0.9239130434782608,94,0.283987915407855,1,0.125,1,0
96,"f reads the sequence in the reverse order ( from x Tx to x 1 ) , resulting in a sequence of backward hidden states (",INTRODUCTION,ENCODER : BIDIRECTIONAL RNN FOR ANNOTATING SEQUENCES,machine-translation,8,86,0.9347826086956522,95,0.2870090634441087,2,0.25,1,0
97,We obtain an annotation for each word x j by concatenating the forward hidden state ? ?,INTRODUCTION,ENCODER : BIDIRECTIONAL RNN FOR ANNOTATING SEQUENCES,machine-translation,8,87,0.9456521739130436,96,0.2900302114803625,3,0.375,1,0
98,h j and the backward one,INTRODUCTION,ENCODER : BIDIRECTIONAL RNN FOR ANNOTATING SEQUENCES,machine-translation,8,88,0.9565217391304348,97,0.2930513595166163,4,0.5,1,0
99,"In this way , the annotation h j contains the summaries of both the preceding words and the following words .",INTRODUCTION,ENCODER : BIDIRECTIONAL RNN FOR ANNOTATING SEQUENCES,machine-translation,8,89,0.967391304347826,98,0.29607250755287,5,0.625,1,0
100,"Due to the tendency of RNNs to better represent recent inputs , the annotation h j will be focused on the words around x j .",INTRODUCTION,ENCODER : BIDIRECTIONAL RNN FOR ANNOTATING SEQUENCES,machine-translation,8,90,0.9782608695652174,99,0.2990936555891239,6,0.75,1,0
101,This sequence of annotations is used by the decoder and the alignment model later to compute the context vector ( Eqs. ( 5 ) - ) .,INTRODUCTION,ENCODER : BIDIRECTIONAL RNN FOR ANNOTATING SEQUENCES,machine-translation,8,91,0.9891304347826086,100,0.3021148036253776,7,0.875,1,0
102,See for the graphical illustration of the proposed model .,INTRODUCTION,ENCODER : BIDIRECTIONAL RNN FOR ANNOTATING SEQUENCES,machine-translation,8,92,1.0,101,0.3051359516616314,8,1.0,1,0
103,EXPERIMENT SETTINGS,,,machine-translation,8,0,0.0,102,0.3081570996978852,0,0.0,1,0
104,We evaluate the proposed approach on the task of English - to - French translation .,EXPERIMENT SETTINGS,EXPERIMENT SETTINGS,machine-translation,8,1,0.0769230769230769,103,0.311178247734139,1,0.1666666666666666,1,0
105,"We use the bilingual , parallel corpora provided by ACL WMT ' 14 .",EXPERIMENT SETTINGS,EXPERIMENT SETTINGS,machine-translation,8,2,0.1538461538461538,104,0.3141993957703927,2,0.3333333333333333,1,0
106,3,EXPERIMENT SETTINGS,EXPERIMENT SETTINGS,machine-translation,8,3,0.2307692307692307,105,0.3172205438066465,3,0.5,1,0
107,"As a comparison , we also report the performance of an RNN Encoder - Decoder which was proposed recently by .",EXPERIMENT SETTINGS,EXPERIMENT SETTINGS,machine-translation,8,4,0.3076923076923077,106,0.3202416918429003,4,0.6666666666666666,1,0
108,We use the same training procedures and the same dataset for both models .,EXPERIMENT SETTINGS,EXPERIMENT SETTINGS,machine-translation,8,5,0.3846153846153846,107,0.323262839879154,5,0.8333333333333334,1,0
109,4,EXPERIMENT SETTINGS,EXPERIMENT SETTINGS,machine-translation,8,6,0.4615384615384615,108,0.3262839879154078,6,1.0,1,0
110,DATASET,EXPERIMENT SETTINGS,,machine-translation,8,7,0.5384615384615384,109,0.3293051359516616,0,0.0,1,0
111,"WMT ' 14 contains the following English - French parallel corpora : Europarl ( 61 M words ) , news commentary ( 5.5 M ) , UN ( 421M ) and two crawled corpora of 90 M and 272.5 M words respectively , totaling 850M words .",EXPERIMENT SETTINGS,DATASET,machine-translation,8,8,0.6153846153846154,110,0.3323262839879154,1,0.1666666666666666,1,0
112,"Following the procedure described in , we reduce the size of the combined corpus to have 348M words using the data selection method by .",EXPERIMENT SETTINGS,DATASET,machine-translation,8,9,0.6923076923076923,111,0.3353474320241691,2,0.3333333333333333,1,0
113,"We do not use any monolingual data other than the mentioned parallel corpora , although it maybe possible to use a much larger monolingual corpus to pretrain an encoder .",EXPERIMENT SETTINGS,DATASET,machine-translation,8,10,0.7692307692307693,112,0.338368580060423,3,0.5,1,0
114,"We concatenate news - test - After a usual tokenization 6 , we use a shortlist of 30,000 most frequent words in each language to train our models .",EXPERIMENT SETTINGS,DATASET,machine-translation,8,11,0.8461538461538461,113,0.3413897280966767,4,0.6666666666666666,1,0
115,Any word not included in the shortlist is mapped to a special token ( [ UNK ] ) .,EXPERIMENT SETTINGS,DATASET,machine-translation,8,12,0.9230769230769232,114,0.3444108761329305,5,0.8333333333333334,1,0
116,"We do not apply any other special preprocessing , such as lowercasing or stemming , to the data .",EXPERIMENT SETTINGS,DATASET,machine-translation,8,13,1.0,115,0.3474320241691843,6,1.0,1,0
117,MODELS,,,machine-translation,8,0,0.0,116,0.3504531722054381,0,0.0,1,0
118,We train two types of models .,MODELS,MODELS,machine-translation,8,1,0.0769230769230769,117,0.3534743202416918,1,0.0769230769230769,1,1
119,"The first one is an RNN Encoder - Decoder ( RNNencdec , , and the other is the proposed model , to which we refer as RNNsearch .",MODELS,MODELS,machine-translation,8,2,0.1538461538461538,118,0.3564954682779456,2,0.1538461538461538,1,1
120,"We train each model twice : first with the sentences of length up to 30 words ( RNNencdec - 30 , RNNsearch - 30 ) and then with the sentences of length up to 50 word ( RNNencdec - 50 , RNNsearch - 50 ) .",MODELS,MODELS,machine-translation,8,3,0.2307692307692307,119,0.3595166163141994,3,0.2307692307692307,1,1
121,The encoder and decoder of the RNNencdec have 1000 hidden units each .,MODELS,MODELS,machine-translation,8,4,0.3076923076923077,120,0.3625377643504531,4,0.3076923076923077,1,1
122,The encoder of the RNNsearch consists of forward and backward recurrent neural networks ( RNN ) each having 1000 hidden units .,MODELS,MODELS,machine-translation,8,5,0.3846153846153846,121,0.3655589123867069,5,0.3846153846153846,1,1
123,It s decoder has 1000 hidden units .,MODELS,MODELS,machine-translation,8,6,0.4615384615384615,122,0.3685800604229607,6,0.4615384615384615,1,1
124,"In both cases , we use a multilayer network with a single maxout hidden layer to compute the conditional probability of each target word .",MODELS,MODELS,machine-translation,8,7,0.5384615384615384,123,0.3716012084592145,7,0.5384615384615384,1,1
125,We use a minibatch stochastic gradient descent ( SGD ) algorithm together with Adadelta to train each model .,MODELS,MODELS,machine-translation,8,8,0.6153846153846154,124,0.3746223564954682,8,0.6153846153846154,1,0
126,Each SGD update direction is computed using a minibatch of 80 sentences .,MODELS,MODELS,machine-translation,8,9,0.6923076923076923,125,0.3776435045317221,9,0.6923076923076923,1,0
127,We trained each model for approximately 5 days .,MODELS,MODELS,machine-translation,8,10,0.7692307692307693,126,0.3806646525679758,10,0.7692307692307693,1,0
128,"Once a model is trained , we use abeam search to find a translation that approximately maximizes the conditional probability ( see , e.g. , .",MODELS,MODELS,machine-translation,8,11,0.8461538461538461,127,0.3836858006042296,11,0.8461538461538461,1,0
129,used this approach to generate translations from their neural machine translation model .,MODELS,MODELS,machine-translation,8,12,0.9230769230769232,128,0.3867069486404834,12,0.9230769230769232,1,0
130,"For more details on the architectures of the models and training procedure used in the experiments , see Appendices A and B.",MODELS,MODELS,machine-translation,8,13,1.0,129,0.3897280966767371,13,1.0,1,0
131,RESULTS,,,machine-translation,8,0,0.0,130,0.3927492447129909,0,0.0,1,0
132,QUANTITATIVE RESULTS,,,machine-translation,8,0,0.0,131,0.3957703927492447,0,0.0,1,0
133,In : Four sample alignments found by RNNsearch - 50 .,QUANTITATIVE RESULTS,QUANTITATIVE RESULTS,machine-translation,8,1,0.0153846153846153,132,0.3987915407854985,1,0.0769230769230769,1,0
134,"The x - axis and y-axis of each plot correspond to the words in the source sentence ( English ) and the generated translation ( French ) , respectively .",QUANTITATIVE RESULTS,QUANTITATIVE RESULTS,machine-translation,8,2,0.0307692307692307,133,0.4018126888217522,2,0.1538461538461538,1,0
135,Each pixel shows the weight ?,QUANTITATIVE RESULTS,QUANTITATIVE RESULTS,machine-translation,8,3,0.0461538461538461,134,0.404833836858006,3,0.2307692307692307,1,0
136,"ij of the annotation of the j - th source word for the i - th target word ( see Eq. ) , in grayscale ( 0 : black , 1 : white ) .",QUANTITATIVE RESULTS,QUANTITATIVE RESULTS,machine-translation,8,4,0.0615384615384615,135,0.4078549848942598,4,0.3076923076923077,1,0
137,( a ) an arbitrary sentence .,QUANTITATIVE RESULTS,QUANTITATIVE RESULTS,machine-translation,8,5,0.0769230769230769,136,0.4108761329305136,5,0.3846153846153846,1,0
138,( b - d ) three randomly selected samples among the sentences without any unknown words and of length between 10 and 20 words from the test set .,QUANTITATIVE RESULTS,QUANTITATIVE RESULTS,machine-translation,8,6,0.0923076923076923,137,0.4138972809667673,6,0.4615384615384615,1,0
139,One of the motivations behind the proposed approach was the use of a fixed - length context vector in the basic encoder - decoder approach .,QUANTITATIVE RESULTS,QUANTITATIVE RESULTS,machine-translation,8,7,0.1076923076923077,138,0.4169184290030212,7,0.5384615384615384,1,0
140,We conjectured that this limitation may make the basic encoder - decoder approach to underperform with long sentences .,QUANTITATIVE RESULTS,QUANTITATIVE RESULTS,machine-translation,8,8,0.123076923076923,139,0.4199395770392749,8,0.6153846153846154,1,0
141,"In , we see that the performance of RNNencdec dramatically drops as the length of the sentences increases .",QUANTITATIVE RESULTS,QUANTITATIVE RESULTS,machine-translation,8,9,0.1384615384615384,140,0.4229607250755287,9,0.6923076923076923,1,1
142,"On the other hand , both RNNsearch - 30 and RNNsearch - 50 are more robust to the length of the sentences .",QUANTITATIVE RESULTS,QUANTITATIVE RESULTS,machine-translation,8,10,0.1538461538461538,141,0.4259818731117825,10,0.7692307692307693,1,1
143,"RNNsearch - 50 , especially , shows no performance deterioration even with sentences of length 50 or more .",QUANTITATIVE RESULTS,QUANTITATIVE RESULTS,machine-translation,8,11,0.1692307692307692,142,0.4290030211480362,11,0.8461538461538461,1,1
144,This superiority of the proposed model over the basic encoder - decoder is further confirmed by the fact that the RNNsearch - 30 even outperforms RNNencdec - 50 ( see ) .,QUANTITATIVE RESULTS,QUANTITATIVE RESULTS,machine-translation,8,12,0.1846153846153846,143,0.43202416918429,12,0.9230769230769232,1,1
145,tokens when only the sentences having no unknown words were evaluated ( last column ) .,QUANTITATIVE RESULTS,QUANTITATIVE RESULTS,machine-translation,8,13,0.2,144,0.4350453172205438,13,1.0,1,0
146,QUALITATIVE ANALYSIS,QUANTITATIVE RESULTS,,machine-translation,8,14,0.2153846153846154,145,0.4380664652567976,0,0.0,1,0
147,ALIGNMENT,QUANTITATIVE RESULTS,,machine-translation,8,15,0.2307692307692307,146,0.4410876132930513,0,0.0,1,0
148,The proposed approach provides an intuitive way to inspect the ( soft - ) alignment between the words in a generated translation and those in a source sentence .,QUANTITATIVE RESULTS,ALIGNMENT,machine-translation,8,16,0.2461538461538461,147,0.4441087613293051,1,0.0909090909090909,1,0
149,This is done by visualizing the annotation weights ?,QUANTITATIVE RESULTS,ALIGNMENT,machine-translation,8,17,0.2615384615384615,148,0.4471299093655589,2,0.1818181818181818,1,0
150,"ij from Eq. , as in .",QUANTITATIVE RESULTS,ALIGNMENT,machine-translation,8,18,0.2769230769230769,149,0.4501510574018127,3,0.2727272727272727,1,0
151,Each row of a matrix in each plot indicates the weights associated with the annotations .,QUANTITATIVE RESULTS,ALIGNMENT,machine-translation,8,19,0.2923076923076923,150,0.4531722054380664,4,0.3636363636363636,1,0
152,From this we see which positions in the source sentence were considered more important when generating the target word .,QUANTITATIVE RESULTS,ALIGNMENT,machine-translation,8,20,0.3076923076923077,151,0.4561933534743202,5,0.4545454545454545,1,0
153,We can see from the alignments in that the alignment of words between English and French is largely monotonic .,QUANTITATIVE RESULTS,ALIGNMENT,machine-translation,8,21,0.3230769230769231,152,0.459214501510574,6,0.5454545454545454,1,0
154,We see strong weights along the diagonal of each matrix .,QUANTITATIVE RESULTS,ALIGNMENT,machine-translation,8,22,0.3384615384615385,153,0.4622356495468278,7,0.6363636363636364,1,0
155,"However , we also observe a number of non-trivial , non-monotonic alignments .",QUANTITATIVE RESULTS,ALIGNMENT,machine-translation,8,23,0.3538461538461538,154,0.4652567975830816,8,0.7272727272727273,1,0
156,"Adjectives and nouns are typically ordered differently between French and English , and we see an example in The strength of the soft - alignment , opposed to a hard - alignment , is evident , for instance , from ].",QUANTITATIVE RESULTS,ALIGNMENT,machine-translation,8,24,0.3692307692307692,155,0.4682779456193353,9,0.8181818181818182,1,0
157,We observe similar behaviors in all the presented cases in .,QUANTITATIVE RESULTS,ALIGNMENT,machine-translation,8,25,0.3846153846153846,156,0.4712990936555891,10,0.9090909090909092,1,0
158,"An additional benefit of the soft alignment is that it naturally deals with source and target phrases of different lengths , without requiring a counter - intuitive way of mapping some words to or from nowhere ( [ NULL ] ) ( see , e.g. , Chapters 4 and 5 of .",QUANTITATIVE RESULTS,ALIGNMENT,machine-translation,8,26,0.4,157,0.4743202416918429,11,1.0,1,0
159,LONG SENTENCES,QUANTITATIVE RESULTS,,machine-translation,8,27,0.4153846153846154,158,0.4773413897280967,0,0.0,1,0
160,As clearly visible from the proposed model ( RNNsearch ) is much better than the conventional model ( RNNencdec ) at translating long sentences .,QUANTITATIVE RESULTS,LONG SENTENCES,machine-translation,8,28,0.4307692307692308,159,0.4803625377643504,1,0.0526315789473684,1,0
161,"This is likely due to the fact that the RNNsearch does not require encoding along sentence into a fixed - length vector perfectly , but only accurately encoding the parts of the input sentence that surround a particular word .",QUANTITATIVE RESULTS,LONG SENTENCES,machine-translation,8,29,0.4461538461538462,160,0.4833836858006042,2,0.1052631578947368,1,0
162,"As an example , consider this source sentence from the test set :",QUANTITATIVE RESULTS,LONG SENTENCES,machine-translation,8,30,0.4615384615384615,161,0.486404833836858,3,0.1578947368421052,1,0
163,"An admitting privilege is the right of a doctor to admit a patient to a hospital or a medical centre to carryout a diagnosis or a procedure , based on his status as a healthcare worker at a hospital .",QUANTITATIVE RESULTS,LONG SENTENCES,machine-translation,8,31,0.4769230769230769,162,0.4894259818731117,4,0.2105263157894736,1,0
164,The RNNencdec - 50 translated this sentence into :,QUANTITATIVE RESULTS,LONG SENTENCES,machine-translation,8,32,0.4923076923076923,163,0.4924471299093655,5,0.2631578947368421,1,0
165,Un privilge d'admission est le droit d'un mdecin de reconnatre un patient l'hpital ou un centre mdical d'un diagnostic ou de prendre un diagnostic en fonction de sontat de sant .,QUANTITATIVE RESULTS,LONG SENTENCES,machine-translation,8,33,0.5076923076923077,164,0.4954682779456193,6,0.3157894736842105,1,0
166,"On the other hand , the RNNsearch - 50 generated the following correct translation , preserving the whole meaning of the input sentence without omitting any details :",QUANTITATIVE RESULTS,LONG SENTENCES,machine-translation,8,34,0.5230769230769231,165,0.4984894259818731,7,0.3684210526315789,1,0
167,"Un privilge d'admission est le droit d'un mdecin d'admettre un patient un hpital ou un centre mdical pour effectuer un diagnostic ou une procdure , selon son statut de travailleur des soins de sant l'hpital .",QUANTITATIVE RESULTS,LONG SENTENCES,machine-translation,8,35,0.5384615384615384,166,0.5015105740181269,8,0.4210526315789473,1,0
168,Let us consider another sentence from the test set :,QUANTITATIVE RESULTS,LONG SENTENCES,machine-translation,8,36,0.5538461538461539,167,0.5045317220543807,9,0.4736842105263157,1,0
169,"This kind of experience is part of Disney 's efforts to "" extend the lifetime of its series and build new relationships with audiences via digital platforms that are becoming evermore important , "" he added .",QUANTITATIVE RESULTS,LONG SENTENCES,machine-translation,8,37,0.5692307692307692,168,0.5075528700906344,10,0.5263157894736842,1,0
170,The translation by the RNNencdec - 50 is,QUANTITATIVE RESULTS,LONG SENTENCES,machine-translation,8,38,0.5846153846153846,169,0.5105740181268882,11,0.5789473684210527,1,0
171,"Ce type d'exprience fait partie des initiatives du Disney pour "" prolonger la dure de vie de ses nouvelles et de dvelopper des liens avec les lecteurs numriques qui deviennent plus complexes .",QUANTITATIVE RESULTS,LONG SENTENCES,machine-translation,8,39,0.6,170,0.513595166163142,12,0.631578947368421,1,0
172,"As with the previous example , the RNNencdec began deviating from the actual meaning of the source sentence after generating approximately 30 words ( see the underlined phrase ) .",QUANTITATIVE RESULTS,LONG SENTENCES,machine-translation,8,40,0.6153846153846154,171,0.5166163141993958,13,0.6842105263157895,1,0
173,"After that point , the quality of the translation deteriorates , with basic mistakes such as the lack of a closing quotation mark .",QUANTITATIVE RESULTS,LONG SENTENCES,machine-translation,8,41,0.6307692307692307,172,0.5196374622356495,14,0.7368421052631579,1,0
174,"Again , the RNNsearch - 50 was able to translate this long sentence correctly :",QUANTITATIVE RESULTS,LONG SENTENCES,machine-translation,8,42,0.6461538461538462,173,0.5226586102719033,15,0.7894736842105263,1,0
175,"Ce genre d'exprience fait partie des efforts de Disney pour "" prolonger la dure de vie de ses sries et crer de nouvelles relations avec des publics via des plateformes numriques de plus en plus importantes "" , a-t - il ajout .",QUANTITATIVE RESULTS,LONG SENTENCES,machine-translation,8,43,0.6615384615384615,174,0.525679758308157,16,0.8421052631578947,1,0
176,"In conjunction with the quantitative results presented already , these qualitative observations confirm our hypotheses that the RNNsearch architecture enables far more reliable translation of long sentences than the standard RNNencdec model .",QUANTITATIVE RESULTS,LONG SENTENCES,machine-translation,8,44,0.676923076923077,175,0.5287009063444109,17,0.8947368421052632,1,0
177,"In Appendix C , we provide a few more sample translations of long source sentences generated by the RNNencdec - 50 , RNNsearch - 50 and Google Translate along with the reference translations .",QUANTITATIVE RESULTS,LONG SENTENCES,machine-translation,8,45,0.6923076923076923,176,0.5317220543806647,18,0.9473684210526316,1,0
178,6 RELATED WORK,QUANTITATIVE RESULTS,LONG SENTENCES,machine-translation,8,46,0.7076923076923077,177,0.5347432024169184,19,1.0,1,0
179,LEARNING TO ALIGN,QUANTITATIVE RESULTS,,machine-translation,8,47,0.7230769230769231,178,0.5377643504531722,0,0.0,1,0
180,A similar approach of aligning an output symbol with an input symbol was proposed recently by in the context of handwriting synthesis .,QUANTITATIVE RESULTS,LEARNING TO ALIGN,machine-translation,8,48,0.7384615384615385,179,0.540785498489426,1,0.1111111111111111,1,0
181,Handwriting synthesis is a task where the model is asked to generate handwriting of a given sequence of characters .,QUANTITATIVE RESULTS,LEARNING TO ALIGN,machine-translation,8,49,0.7538461538461538,180,0.5438066465256798,2,0.2222222222222222,1,0
182,"In his work , he used a mixture of Gaussian kernels to compute the weights of the annotations , where the location , width and mixture coefficient of each kernel was predicted from an alignment model .",QUANTITATIVE RESULTS,LEARNING TO ALIGN,machine-translation,8,50,0.7692307692307693,181,0.5468277945619335,3,0.3333333333333333,1,0
183,"More specifically , his alignment was restricted to predict the location such that the location increases monotonically .",QUANTITATIVE RESULTS,LEARNING TO ALIGN,machine-translation,8,51,0.7846153846153846,182,0.5498489425981873,4,0.4444444444444444,1,0
184,"The main difference from our approach is that , in , the modes of the weights of the annotations only move in one direction .",QUANTITATIVE RESULTS,LEARNING TO ALIGN,machine-translation,8,52,0.8,183,0.552870090634441,5,0.5555555555555556,1,0
185,"In the context of machine translation , this is a severe limitation , as ( long - distance ) reordering is often needed to generate a grammatically correct translation ( for instance , English - to - German ) .",QUANTITATIVE RESULTS,LEARNING TO ALIGN,machine-translation,8,53,0.8153846153846154,184,0.5558912386706949,6,0.6666666666666666,1,0
186,"Our approach , on the other hand , requires computing the annotation weight of every word in the source sentence for each word in the translation .",QUANTITATIVE RESULTS,LEARNING TO ALIGN,machine-translation,8,54,0.8307692307692308,185,0.5589123867069486,7,0.7777777777777778,1,0
187,This drawback is not severe with the task of translation in which most of input and output sentences are only 15 - 40 words .,QUANTITATIVE RESULTS,LEARNING TO ALIGN,machine-translation,8,55,0.8461538461538461,186,0.5619335347432024,8,0.8888888888888888,1,0
188,"However , this may limit the applicability of the proposed scheme to other tasks .",QUANTITATIVE RESULTS,LEARNING TO ALIGN,machine-translation,8,56,0.8615384615384616,187,0.5649546827794562,9,1.0,1,0
189,NEURAL NETWORKS FOR MACHINE TRANSLATION,QUANTITATIVE RESULTS,,machine-translation,8,57,0.8769230769230769,188,0.56797583081571,0,0.0,1,0
190,"Since introduced a neural probabilistic language model which uses a neural network to model the conditional probability of a word given a fixed number of the preceding words , neural networks have widely been used in machine translation .",QUANTITATIVE RESULTS,NEURAL NETWORKS FOR MACHINE TRANSLATION,machine-translation,8,58,0.8923076923076924,189,0.5709969788519638,1,0.125,1,0
191,"However , the role of neural networks has been largely limited to simply providing a single feature to an existing statistical machine translation system or to re-rank a list of candidate translations provided by an existing system .",QUANTITATIVE RESULTS,NEURAL NETWORKS FOR MACHINE TRANSLATION,machine-translation,8,59,0.9076923076923076,190,0.5740181268882175,2,0.25,1,0
192,"For instance , proposed using a feedforward neural network to compute the score of a pair of source and target phrases and to use the score as an additional feature in the phrase - based statistical machine translation system .",QUANTITATIVE RESULTS,NEURAL NETWORKS FOR MACHINE TRANSLATION,machine-translation,8,60,0.9230769230769232,191,0.5770392749244713,3,0.375,1,0
193,"More recently , and reported the successful use of the neural networks as a sub-component of the existing translation system .",QUANTITATIVE RESULTS,NEURAL NETWORKS FOR MACHINE TRANSLATION,machine-translation,8,61,0.9384615384615383,192,0.5800604229607251,4,0.5,1,0
194,"Traditionally , a neural network trained as a target - side language model has been used to rescore or rerank a list of candidate translations ( see , e.g. , .",QUANTITATIVE RESULTS,NEURAL NETWORKS FOR MACHINE TRANSLATION,machine-translation,8,62,0.953846153846154,193,0.5830815709969789,5,0.625,1,0
195,"Although the above approaches were shown to improve the translation performance over the stateof - the - art machine translation systems , we are more interested in a more ambitious objective of designing a completely new translation system based on neural networks .",QUANTITATIVE RESULTS,NEURAL NETWORKS FOR MACHINE TRANSLATION,machine-translation,8,63,0.9692307692307692,194,0.5861027190332326,6,0.75,1,0
196,The neural machine translation approach we consider in this paper is therefore a radical departure from these earlier works .,QUANTITATIVE RESULTS,NEURAL NETWORKS FOR MACHINE TRANSLATION,machine-translation,8,64,0.9846153846153848,195,0.5891238670694864,7,0.875,1,0
197,"Rather than using a neural network as apart of the existing system , our model works on its own and generates a translation from a source sentence directly .",QUANTITATIVE RESULTS,NEURAL NETWORKS FOR MACHINE TRANSLATION,machine-translation,8,65,1.0,196,0.5921450151057401,8,1.0,1,0
198,CONCLUSION,,,machine-translation,8,0,0.0,197,0.595166163141994,0,0.0,1,0
199,"The conventional approach to neural machine translation , called an encoder - decoder approach , encodes a whole input sentence into a fixed - length vector from which a translation will be decoded .",CONCLUSION,CONCLUSION,machine-translation,8,1,0.0666666666666666,198,0.5981873111782477,1,0.0666666666666666,0,0
200,"We conjectured that the use of a fixed - length context vector is problematic for translating long sentences , based on a recent empirical study reported by and .",CONCLUSION,CONCLUSION,machine-translation,8,2,0.1333333333333333,199,0.6012084592145015,2,0.1333333333333333,0,0
201,"In this paper , we proposed a novel architecture that addresses this issue .",CONCLUSION,CONCLUSION,machine-translation,8,3,0.2,200,0.6042296072507553,3,0.2,0,0
202,"We extended the basic encoder - decoder by letting a model ( soft - ) search fora set of input words , or their annotations computed by an encoder , when generating each target word .",CONCLUSION,CONCLUSION,machine-translation,8,4,0.2666666666666666,201,0.6072507552870091,4,0.2666666666666666,0,0
203,"This frees the model from having to encode a whole source sentence into a fixed - length vector , and also lets the model focus only on information relevant to the generation of the next target word .",CONCLUSION,CONCLUSION,machine-translation,8,5,0.3333333333333333,202,0.6102719033232629,5,0.3333333333333333,0,0
204,This has a major positive impact on the ability of the neural machine translation system to yield good results on longer sentences .,CONCLUSION,CONCLUSION,machine-translation,8,6,0.4,203,0.6132930513595166,6,0.4,0,0
205,"Unlike with the traditional machine translation systems , all of the pieces of the translation system , including the alignment mechanism , are jointly trained towards a better log-probability of producing correct translations .",CONCLUSION,CONCLUSION,machine-translation,8,7,0.4666666666666667,204,0.6163141993957704,7,0.4666666666666667,0,0
206,"We tested the proposed model , called RNNsearch , on the task of English - to - French translation .",CONCLUSION,CONCLUSION,machine-translation,8,8,0.5333333333333333,205,0.6193353474320241,8,0.5333333333333333,0,0
207,"The experiment revealed that the proposed RNNsearch outperforms the conventional encoder - decoder model ( RNNencdec ) significantly , regardless of the sentence length and that it is much more robust to the length of a source sentence .",CONCLUSION,CONCLUSION,machine-translation,8,9,0.6,206,0.622356495468278,9,0.6,0,0
208,"From the qualitative analysis where we investigated the ( soft - ) alignment generated by the RNNsearch , we were able to conclude that the model can correctly align each target word with the relevant words , or their annotations , in the source sentence as it generated a correct translation .",CONCLUSION,CONCLUSION,machine-translation,8,10,0.6666666666666666,207,0.6253776435045317,10,0.6666666666666666,0,0
209,"Perhaps more importantly , the proposed approach achieved a translation performance comparable to the existing phrase - based statistical machine translation .",CONCLUSION,CONCLUSION,machine-translation,8,11,0.7333333333333333,208,0.6283987915407855,11,0.7333333333333333,0,0
210,"It is a striking result , considering that the proposed architecture , or the whole family of neural machine translation , has only been proposed as recently as this year .",CONCLUSION,CONCLUSION,machine-translation,8,12,0.8,209,0.6314199395770392,12,0.8,0,0
211,We believe the architecture proposed here is a promising step toward better machine translation and a better understanding of natural languages in general .,CONCLUSION,CONCLUSION,machine-translation,8,13,0.8666666666666667,210,0.6344410876132931,13,0.8666666666666667,0,0
212,"One of challenges left for the future is to better handle unknown , or rare words .",CONCLUSION,CONCLUSION,machine-translation,8,14,0.9333333333333332,211,0.6374622356495468,14,0.9333333333333332,0,0
213,This will be required for the model to be more widely used and to match the performance of current state - of - the - art machine translation systems in all contexts .,CONCLUSION,CONCLUSION,machine-translation,8,15,1.0,212,0.6404833836858006,15,1.0,0,0
214,A MODEL ARCHITECTURE,,,machine-translation,8,0,0.0,213,0.6435045317220544,0,0.0,1,0
215,A.1 ARCHITECTURAL CHOICES,A MODEL ARCHITECTURE,,machine-translation,8,1,0.0416666666666666,214,0.6465256797583081,1,0.3333333333333333,1,0
216,"The proposed scheme in Section 3 is a general framework where one can freely define , for instance , the activation functions f of recurrent neural networks ( RNN ) and the alignment model a .",A MODEL ARCHITECTURE,A.1 ARCHITECTURAL CHOICES,machine-translation,8,2,0.0833333333333333,215,0.649546827794562,2,0.6666666666666666,1,0
217,"Here , we describe the choices we made for the experiments in this paper .",A MODEL ARCHITECTURE,A.1 ARCHITECTURAL CHOICES,machine-translation,8,3,0.125,216,0.6525679758308157,3,1.0,1,0
218,A.1.1 RECURRENT NEURAL NETWORK,A MODEL ARCHITECTURE,,machine-translation,8,4,0.1666666666666666,217,0.6555891238670695,0,0.0,1,0
219,"For the activation function f of an RNN , we use the gated hidden unit recently proposed by .",A MODEL ARCHITECTURE,A.1.1 RECURRENT NEURAL NETWORK,machine-translation,8,5,0.2083333333333333,218,0.6586102719033232,1,0.05,1,0
220,The gated hidden unit is an alternative to the conventional simple units such as an element - wise tanh .,A MODEL ARCHITECTURE,A.1.1 RECURRENT NEURAL NETWORK,machine-translation,8,6,0.25,219,0.6616314199395771,2,0.1,1,0
221,"This gated unit is similar to along short - term memory ( LSTM ) unit proposed earlier by , sharing with it the ability to better model and learn long - term dependencies .",A MODEL ARCHITECTURE,A.1.1 RECURRENT NEURAL NETWORK,machine-translation,8,7,0.2916666666666667,220,0.6646525679758308,3,0.15,1,0
222,This is made possible by having computation paths in the unfolded RNN for which the product of derivatives is close to 1 .,A MODEL ARCHITECTURE,A.1.1 RECURRENT NEURAL NETWORK,machine-translation,8,8,0.3333333333333333,221,0.6676737160120846,4,0.2,1,0
223,These paths allow gradients to flow backward easily without suffering too much from the vanishing effect .,A MODEL ARCHITECTURE,A.1.1 RECURRENT NEURAL NETWORK,machine-translation,8,9,0.375,222,0.6706948640483383,5,0.25,1,0
224,"It is therefore possible to use LSTM units instead of the gated hidden unit described here , as was done in a similar context by .",A MODEL ARCHITECTURE,A.1.1 RECURRENT NEURAL NETWORK,machine-translation,8,10,0.4166666666666667,223,0.6737160120845922,6,0.3,1,0
225,The new state s i of the RNN employing n gated hidden units 8 is computed by,A MODEL ARCHITECTURE,A.1.1 RECURRENT NEURAL NETWORK,machine-translation,8,11,0.4583333333333333,224,0.676737160120846,7,0.35,1,0
226,"where is an element - wise multiplication , and z i is the output of the update gates ( see below ) .",A MODEL ARCHITECTURE,A.1.1 RECURRENT NEURAL NETWORK,machine-translation,8,12,0.5,225,0.6797583081570997,8,0.4,1,0
227,The proposed updated states i is computed b ?,A MODEL ARCHITECTURE,A.1.1 RECURRENT NEURAL NETWORK,machine-translation,8,13,0.5416666666666666,226,0.6827794561933535,9,0.45,1,0
228,where e ( y,A MODEL ARCHITECTURE,A.1.1 RECURRENT NEURAL NETWORK,machine-translation,8,14,0.5833333333333334,227,0.6858006042296072,10,0.5,1,0
229,i?1 ) ?,A MODEL ARCHITECTURE,A.1.1 RECURRENT NEURAL NETWORK,machine-translation,8,15,0.625,228,0.6888217522658611,11,0.55,1,0
230,"R m is an m-dimensional embedding of a wordy i ?1 , and r i is the output of the reset gates ( see below ) .",A MODEL ARCHITECTURE,A.1.1 RECURRENT NEURAL NETWORK,machine-translation,8,16,0.6666666666666666,229,0.6918429003021148,12,0.6,1,0
231,"When y i is represented as a 1 - of - K vector , e ( y i ) is simply a column of an embedding matrix E ?",A MODEL ARCHITECTURE,A.1.1 RECURRENT NEURAL NETWORK,machine-translation,8,17,0.7083333333333334,230,0.6948640483383686,13,0.65,1,0
232,R mK .,A MODEL ARCHITECTURE,A.1.1 RECURRENT NEURAL NETWORK,machine-translation,8,18,0.75,231,0.6978851963746223,14,0.7,1,0
233,"Whenever possible , we omit bias terms to make the equations less cluttered .",A MODEL ARCHITECTURE,A.1.1 RECURRENT NEURAL NETWORK,machine-translation,8,19,0.7916666666666666,232,0.7009063444108762,15,0.75,1,0
234,"The update gates z i allow each hidden unit to maintain its previous activation , and the reset gates r i control how much and what information from the previous state should be reset .",A MODEL ARCHITECTURE,A.1.1 RECURRENT NEURAL NETWORK,machine-translation,8,20,0.8333333333333334,233,0.7039274924471299,16,0.8,1,0
235,We compute them by,A MODEL ARCHITECTURE,A.1.1 RECURRENT NEURAL NETWORK,machine-translation,8,21,0.875,234,0.7069486404833837,17,0.85,1,0
236,where ? ( ) is a logistic sigmoid function .,A MODEL ARCHITECTURE,A.1.1 RECURRENT NEURAL NETWORK,machine-translation,8,22,0.9166666666666666,235,0.7099697885196374,18,0.9,1,0
237,"At each step of the decoder , we compute the output probability ( Eq. ( 4 ) ) as a multi -layered function .",A MODEL ARCHITECTURE,A.1.1 RECURRENT NEURAL NETWORK,machine-translation,8,23,0.9583333333333334,236,0.7129909365558912,19,0.95,1,0
238,We use a single hidden layer of maxout units and normalize the output probabilities ( one for each word ) with a softmax function ( see Eq. ) .,A MODEL ARCHITECTURE,A.1.1 RECURRENT NEURAL NETWORK,machine-translation,8,24,1.0,237,0.716012084592145,20,1.0,1,0
239,A.1.2 ALIGNMENT MODEL,,,machine-translation,8,0,0.0,238,0.7190332326283988,0,0.0,1,0
240,The alignment model should be designed considering that the model needs to be evaluated T x Ty times for each sentence pair of lengths T x and Ty .,A.1.2 ALIGNMENT MODEL,A.1.2 ALIGNMENT MODEL,machine-translation,8,1,0.0204081632653061,239,0.7220543806646526,1,0.125,1,0
241,"In order to reduce computation , we use a singlelayer multilayer perceptron such that",A.1.2 ALIGNMENT MODEL,A.1.2 ALIGNMENT MODEL,machine-translation,8,2,0.0408163265306122,240,0.7250755287009063,2,0.25,1,0
242,where W a ?,A.1.2 ALIGNMENT MODEL,A.1.2 ALIGNMENT MODEL,machine-translation,8,3,0.0612244897959183,241,0.7280966767371602,3,0.375,1,0
243,"R nn , U a ?",A.1.2 ALIGNMENT MODEL,A.1.2 ALIGNMENT MODEL,machine-translation,8,4,0.0816326530612244,242,0.7311178247734139,4,0.5,1,0
244,R n 2n and v a ?,A.1.2 ALIGNMENT MODEL,A.1.2 ALIGNMENT MODEL,machine-translation,8,5,0.1020408163265306,243,0.7341389728096677,5,0.625,1,0
245,Rn are the weight matrices .,A.1.2 ALIGNMENT MODEL,A.1.2 ALIGNMENT MODEL,machine-translation,8,6,0.1224489795918367,244,0.7371601208459214,6,0.75,1,0
246,Since,A.1.2 ALIGNMENT MODEL,A.1.2 ALIGNMENT MODEL,machine-translation,8,7,0.1428571428571428,245,0.7401812688821753,7,0.875,1,0
247,"U ah j does not depend on i , we can pre-compute it in advance to minimize the computational cost .",A.1.2 ALIGNMENT MODEL,A.1.2 ALIGNMENT MODEL,machine-translation,8,8,0.1632653061224489,246,0.743202416918429,8,1.0,1,0
248,A.2 DETAILED DESCRIPTION OF THE MODEL,A.1.2 ALIGNMENT MODEL,,machine-translation,8,9,0.1836734693877551,247,0.7462235649546828,0,0.0,1,0
249,A.2.1 ENCODER,A.1.2 ALIGNMENT MODEL,,machine-translation,8,10,0.2040816326530612,248,0.7492447129909365,1,0.0714285714285714,1,0
250,"In this section , we describe in detail the architecture of the proposed model ( RNNsearch ) used in the experiments ( see .",A.1.2 ALIGNMENT MODEL,A.2.1 ENCODER,machine-translation,8,11,0.2244897959183673,249,0.7522658610271903,2,0.1428571428571428,1,0
251,"From hereon , we omit all bias terms in order to increase readability .",A.1.2 ALIGNMENT MODEL,A.2.1 ENCODER,machine-translation,8,12,0.2448979591836734,250,0.7552870090634441,3,0.2142857142857142,1,0
252,The model takes a source sentence of 1 - of - K coded word vectors as input,A.1.2 ALIGNMENT MODEL,A.2.1 ENCODER,machine-translation,8,13,0.2653061224489796,251,0.7583081570996979,4,0.2857142857142857,1,0
253,"and outputs a translated sentence of 1 - of - K coded word vectors y = ( y 1 , . . . , y Ty ) , y i ? R Ky ,",A.1.2 ALIGNMENT MODEL,A.2.1 ENCODER,machine-translation,8,14,0.2857142857142857,252,0.7613293051359517,5,0.3571428571428571,1,0
254,"where K x and Ky are the vocabulary sizes of source and target languages , respectively .",A.1.2 ALIGNMENT MODEL,A.2.1 ENCODER,machine-translation,8,15,0.3061224489795918,253,0.7643504531722054,6,0.4285714285714285,1,0
255,T x and Ty respectively denote the lengths of source and target sentences .,A.1.2 ALIGNMENT MODEL,A.2.1 ENCODER,machine-translation,8,16,0.3265306122448979,254,0.7673716012084593,7,0.5,1,0
256,"First , the forward states of the bidirectional recurrent neural network ( BiRNN ) are computed :",A.1.2 ALIGNMENT MODEL,A.2.1 ENCODER,machine-translation,8,17,0.3469387755102041,255,0.770392749244713,8,0.5714285714285714,1,0
257,are weight matrices .,A.1.2 ALIGNMENT MODEL,A.2.1 ENCODER,machine-translation,8,18,0.3673469387755102,256,0.7734138972809668,9,0.6428571428571429,1,0
258,"m and n are the word embedding dimensionality and the number of hidden units , respectively . ? ( ) is as usual a logistic sigmoid function .",A.1.2 ALIGNMENT MODEL,A.2.1 ENCODER,machine-translation,8,19,0.3877551020408163,257,0.7764350453172205,10,0.7142857142857143,1,0
259,"The backward states ( ? ? h 1 , , ? ? h Tx ) are computed similarly .",A.1.2 ALIGNMENT MODEL,A.2.1 ENCODER,machine-translation,8,20,0.4081632653061224,258,0.7794561933534743,11,0.7857142857142857,1,0
260,"We share the word embedding matrix E between the forward and backward RNNs , unlike the weight matrices .",A.1.2 ALIGNMENT MODEL,A.2.1 ENCODER,machine-translation,8,21,0.4285714285714285,259,0.7824773413897281,12,0.8571428571428571,1,0
261,"We concatenate the forward and backward states to to obtain the annotations ( h 1 , h 2 , , h Tx ) , where",A.1.2 ALIGNMENT MODEL,A.2.1 ENCODER,machine-translation,8,22,0.4489795918367347,260,0.7854984894259819,13,0.9285714285714286,1,0
262,A.,A.1.2 ALIGNMENT MODEL,,machine-translation,8,23,0.4693877551020408,261,0.7885196374622356,14,1.0,1,0
263,DECODER,A.1.2 ALIGNMENT MODEL,,machine-translation,8,24,0.4897959183673469,262,0.7915407854984894,0,0.0,1,0
264,The hidden state s i of the decoder given the annotations from the encoder is computed by,A.1.2 ALIGNMENT MODEL,DECODER,machine-translation,8,25,0.5102040816326531,263,0.7945619335347432,1,0.04,1,0
265,E is the word embedding matrix for the target language .,A.1.2 ALIGNMENT MODEL,DECODER,machine-translation,8,26,0.5306122448979592,264,0.797583081570997,2,0.08,1,0
266,"W , W z , W r ?",A.1.2 ALIGNMENT MODEL,DECODER,machine-translation,8,27,0.5510204081632653,265,0.8006042296072508,3,0.12,1,0
267,"R nm , U , U z , Ur ? R nn , and C , C z , Cr ?",A.1.2 ALIGNMENT MODEL,DECODER,machine-translation,8,28,0.5714285714285714,266,0.8036253776435045,4,0.16,1,0
268,R n 2n are weights .,A.1.2 ALIGNMENT MODEL,DECODER,machine-translation,8,29,0.5918367346938775,267,0.8066465256797583,5,0.2,1,0
269,"Again , m and n are the word embedding dimensionality and the number of hidden units , respectively .",A.1.2 ALIGNMENT MODEL,DECODER,machine-translation,8,30,0.6122448979591837,268,0.8096676737160121,6,0.24,1,0
270,The initial hidden state s 0 is computed by,A.1.2 ALIGNMENT MODEL,DECODER,machine-translation,8,31,0.6326530612244898,269,0.8126888217522659,7,0.28,1,0
271,The context vector c i are recomputed at each step by the alignment model : :,A.1.2 ALIGNMENT MODEL,DECODER,machine-translation,8,32,0.6530612244897959,270,0.8157099697885196,8,0.32,1,0
272,Learning statistics and relevant information .,A.1.2 ALIGNMENT MODEL,DECODER,machine-translation,8,33,0.673469387755102,271,0.8187311178247734,9,0.36,1,0
273,Each update corresponds to updating the parameters once using a single minibatch .,A.1.2 ALIGNMENT MODEL,DECODER,machine-translation,8,34,0.6938775510204082,272,0.8217522658610272,10,0.4,1,0
274,One epoch is one pass through the training set .,A.1.2 ALIGNMENT MODEL,DECODER,machine-translation,8,35,0.7142857142857143,273,0.824773413897281,11,0.44,1,0
275,NLL is the average conditional log-probabilities of the sentences in either the training set or the development set .,A.1.2 ALIGNMENT MODEL,DECODER,machine-translation,8,36,0.7346938775510204,274,0.8277945619335347,12,0.48,1,0
276,Note that the lengths of the sentences differ .,A.1.2 ALIGNMENT MODEL,DECODER,machine-translation,8,37,0.7551020408163265,275,0.8308157099697885,13,0.52,1,0
277,where,A.1.2 ALIGNMENT MODEL,DECODER,machine-translation,8,38,0.7755102040816326,276,0.8338368580060423,14,0.56,1,0
278,and h j is the j - th annotation in the source sentence ( see Eq. ) .,A.1.2 ALIGNMENT MODEL,DECODER,machine-translation,8,39,0.7959183673469388,277,0.8368580060422961,15,0.6,1,0
279,v a ?,A.1.2 ALIGNMENT MODEL,DECODER,machine-translation,8,40,0.8163265306122449,278,0.8398791540785498,16,0.64,1,0
280,"Rn , W a ?",A.1.2 ALIGNMENT MODEL,DECODER,machine-translation,8,41,0.8367346938775511,279,0.8429003021148036,17,0.68,1,0
281,Rn n and U a ?,A.1.2 ALIGNMENT MODEL,DECODER,machine-translation,8,42,0.8571428571428571,280,0.8459214501510574,18,0.72,1,0
282,Rn 2n are weight matrices .,A.1.2 ALIGNMENT MODEL,DECODER,machine-translation,8,43,0.8775510204081632,281,0.8489425981873112,19,0.76,1,0
283,Note that the model becomes RNN Encoder - Decoder,A.1.2 ALIGNMENT MODEL,DECODER,machine-translation,8,44,0.8979591836734694,282,0.851963746223565,20,0.8,1,0
284,"With the decoder state s i ?1 , the context c i and the last generated wordy i ? 1 , we define the probability of a target wordy i as",A.1.2 ALIGNMENT MODEL,DECODER,machine-translation,8,45,0.9183673469387756,283,0.8549848942598187,21,0.84,1,0
285,"where ti = max t i ,2 j?1 ,t i , 2 j j=1 , ... , l andt i , k is the k - th element of a vectort i which is computed b ?",A.1.2 ALIGNMENT MODEL,DECODER,machine-translation,8,46,0.9387755102040816,284,0.8580060422960725,22,0.88,1,0
286,and Co ?,A.1.2 ALIGNMENT MODEL,DECODER,machine-translation,8,47,0.9591836734693876,285,0.8610271903323263,23,0.92,1,0
287,R 2 l 2n are weight matrices .,A.1.2 ALIGNMENT MODEL,DECODER,machine-translation,8,48,0.979591836734694,286,0.8640483383685801,24,0.96,1,0
288,This can be understood as having a deep output with a single maxout hidden layer .,A.1.2 ALIGNMENT MODEL,DECODER,machine-translation,8,49,1.0,287,0.8670694864048338,25,1.0,1,0
289,A.2.3 MODEL SIZE,,,machine-translation,8,0,0.0,288,0.8700906344410876,0,0.0,1,0
290,"For all the models used in this paper , the size of a hidden layer n is 1000 , the word embedding dimensionality m is 620 and the size of the maxout hidden layer in the deep output l is 500 .",A.2.3 MODEL SIZE,A.2.3 MODEL SIZE,machine-translation,8,1,0.1428571428571428,289,0.8731117824773413,1,0.1428571428571428,1,0
291,The number of hidden units in the alignment model n is 1000 .,A.2.3 MODEL SIZE,A.2.3 MODEL SIZE,machine-translation,8,2,0.2857142857142857,290,0.8761329305135952,2,0.2857142857142857,1,0
292,and ? ?,A.2.3 MODEL SIZE,A.2.3 MODEL SIZE,machine-translation,8,3,0.4285714285714285,291,0.879154078549849,3,0.4285714285714285,1,0
293,Ur as random orthogonal matrices .,A.2.3 MODEL SIZE,A.2.3 MODEL SIZE,machine-translation,8,4,0.5714285714285714,292,0.8821752265861027,4,0.5714285714285714,1,0
294,"For W a and U a , we initialized them by sampling each element from the Gaussian distribution of mean 0 and variance 0.001 2 .",A.2.3 MODEL SIZE,A.2.3 MODEL SIZE,machine-translation,8,5,0.7142857142857143,293,0.8851963746223565,5,0.7142857142857143,1,0
295,All the elements of Va and all the bias vectors were initialized to zero .,A.2.3 MODEL SIZE,A.2.3 MODEL SIZE,machine-translation,8,6,0.8571428571428571,294,0.8882175226586103,6,0.8571428571428571,1,0
296,Any other weight matrix was initialized by sampling from the Gaussian distribution of mean 0 and variance 0.01 2 .,A.2.3 MODEL SIZE,A.2.3 MODEL SIZE,machine-translation,8,7,1.0,295,0.8912386706948641,7,1.0,1,0
297,B.2 TRAINING,,,machine-translation,8,0,0.0,296,0.8942598187311178,0,0.0,1,0
298,We used the stochastic gradient descent ( SGD ) algorithm .,B.2 TRAINING,B.2 TRAINING,machine-translation,8,1,0.0294117647058823,297,0.8972809667673716,1,0.1111111111111111,1,0
299,Adadelta was used to automatically adapt the learning rate of each parameter ( = 10 ?6 and ? = 0.95 ) .,B.2 TRAINING,B.2 TRAINING,machine-translation,8,2,0.0588235294117647,298,0.9003021148036254,2,0.2222222222222222,1,0
300,"We explicitly normalized the L 2 - norm of the gradient of the cost function each time to beat most a predefined threshold of 1 , when the norm was larger than the threshold .",B.2 TRAINING,B.2 TRAINING,machine-translation,8,3,0.088235294117647,299,0.9033232628398792,3,0.3333333333333333,1,0
301,Each SGD update direction was computed with a minibatch of 80 sentences .,B.2 TRAINING,B.2 TRAINING,machine-translation,8,4,0.1176470588235294,300,0.9063444108761328,4,0.4444444444444444,1,0
302,At each update our implementation requires time proportional to the length of the longest sentence in a minibatch .,B.2 TRAINING,B.2 TRAINING,machine-translation,8,5,0.1470588235294117,301,0.9093655589123868,5,0.5555555555555556,1,0
303,"Hence , to minimize the waste of computation , before every 20 - th update , we retrieved 1600 sentence pairs , sorted them according to the lengths and split them into 20 minibatches .",B.2 TRAINING,B.2 TRAINING,machine-translation,8,6,0.1764705882352941,302,0.9123867069486404,6,0.6666666666666666,1,0
304,The training data was shuffled once before training and was traversed sequentially in this manner .,B.2 TRAINING,B.2 TRAINING,machine-translation,8,7,0.2058823529411764,303,0.9154078549848944,7,0.7777777777777778,1,0
305,In Tables 2 we present the statistics related to training all the models used in the experiments .,B.2 TRAINING,B.2 TRAINING,machine-translation,8,8,0.2352941176470588,304,0.918429003021148,8,0.8888888888888888,1,0
306,C TRANSLATIONS OF LONG SENTENCES,B.2 TRAINING,,machine-translation,8,9,0.2647058823529412,305,0.9214501510574018,9,1.0,1,0
307,Source,B.2 TRAINING,C TRANSLATIONS OF LONG SENTENCES,machine-translation,8,10,0.2941176470588235,306,0.9244712990936556,0,0.0,1,0
308,"An admitting privilege is the right of a doctor to admit a patient to a hospital or a medical centre to carryout a diagnosis or a procedure , based on his status as a healthcare worker at a hospital .",B.2 TRAINING,C TRANSLATIONS OF LONG SENTENCES,machine-translation,8,11,0.3235294117647059,307,0.9274924471299094,1,0.0,1,0
309,Reference,B.2 TRAINING,C TRANSLATIONS OF LONG SENTENCES,machine-translation,8,12,0.3529411764705882,308,0.9305135951661632,0,0.0,1,0
310,"Le privilge d'admission est le droit d'un mdecin , en vertu de son statut de membre soignant d'un hpital , d'admettre un patient dans un hpital ou un centre mdical afin d 'y dlivrer un diagnostic ou un traitement .",B.2 TRAINING,C TRANSLATIONS OF LONG SENTENCES,machine-translation,8,13,0.3823529411764705,309,0.9335347432024168,1,0.1428571428571428,1,0
311,RNNenc - 50,B.2 TRAINING,C TRANSLATIONS OF LONG SENTENCES,machine-translation,8,14,0.4117647058823529,310,0.9365558912386708,2,0.2857142857142857,1,0
312,"Ce type d'exprience fait partie des initiatives du Disney pour "" prolonger la dure de vie de ses nouvelles et de dvelopper des liens avec les lecteurs numriques qui deviennent plus complexes .",B.2 TRAINING,C TRANSLATIONS OF LONG SENTENCES,machine-translation,8,15,0.4411764705882353,311,0.9395770392749244,3,0.4285714285714285,1,0
313,RNNsearch - 50,B.2 TRAINING,C TRANSLATIONS OF LONG SENTENCES,machine-translation,8,16,0.4705882352941176,312,0.9425981873111784,4,0.5714285714285714,1,0
314,"Ce genre d'exprience fait partie des efforts de Disney pour "" prolonger la dure de vie de ses sries et crer de nouvelles relations avec des publics via des plateformes numriques de plus en plus importantes "" , a-t - il ajout .",B.2 TRAINING,C TRANSLATIONS OF LONG SENTENCES,machine-translation,8,17,0.5,313,0.945619335347432,5,0.7142857142857143,1,0
315,Google Translate,B.2 TRAINING,C TRANSLATIONS OF LONG SENTENCES,machine-translation,8,18,0.5294117647058824,314,0.9486404833836858,6,0.8571428571428571,1,0
316,"Ce genre d'exprience fait partie des efforts de Disney "" tendre la dure de vie de sa srie et construire de nouvelles relations avec le public par le biais des plates - formes numriques qui deviennent de plus en plus important "" , at - il ajout .",B.2 TRAINING,C TRANSLATIONS OF LONG SENTENCES,machine-translation,8,19,0.5588235294117647,315,0.9516616314199396,7,1.0,1,0
317,Source,B.2 TRAINING,C TRANSLATIONS OF LONG SENTENCES,machine-translation,8,20,0.5882352941176471,316,0.9546827794561934,0,0.0,1,0
318,"Ina press conference on Thursday , Mr Blair stated that there was nothing in this video that might constitute a "" reasonable motive "" that could lead to criminal charges being brought against the mayor .",B.2 TRAINING,C TRANSLATIONS OF LONG SENTENCES,machine-translation,8,21,0.6176470588235294,317,0.9577039274924471,1,0.0,1,0
319,Reference,B.2 TRAINING,C TRANSLATIONS OF LONG SENTENCES,machine-translation,8,22,0.6470588235294118,318,0.9607250755287008,0,0.0,1,0
320,"En confrence de presse , jeudi , M. Blair a affirm qu'il n'y avait rien dans cette vido qui puisse constituer des "" motifs raisonnables "" pouvant mener au dpt d'une accusation criminelle contre le maire .",B.2 TRAINING,C TRANSLATIONS OF LONG SENTENCES,machine-translation,8,23,0.6764705882352942,319,0.9637462235649548,1,0.1,1,0
321,RNNenc - 50,B.2 TRAINING,C TRANSLATIONS OF LONG SENTENCES,machine-translation,8,24,0.7058823529411765,320,0.9667673716012084,2,0.2,1,0
322,"Lors de la confrence de presse de jeudi , M. Blair a dit qu'il n'y avait rien dans cette vido qui pourrait constituer une "" motivation raisonnable "" pouvant entraner des accusations criminelles portes contre le maire .",B.2 TRAINING,C TRANSLATIONS OF LONG SENTENCES,machine-translation,8,25,0.7352941176470589,321,0.9697885196374624,3,0.3,1,0
323,RNNsearch - 50,B.2 TRAINING,C TRANSLATIONS OF LONG SENTENCES,machine-translation,8,26,0.7647058823529411,322,0.972809667673716,4,0.4,1,0
324,"Lors d'une confrence de presse jeudi , M. Blair a dclar qu'il n'y avait rien dans cette vido qui pourrait constituer un "" motif raisonnable "" qui pourrait conduire des accusations criminelles contre le maire .",B.2 TRAINING,C TRANSLATIONS OF LONG SENTENCES,machine-translation,8,27,0.7941176470588235,323,0.9758308157099698,5,0.5,1,0
325,Google Translate,B.2 TRAINING,C TRANSLATIONS OF LONG SENTENCES,machine-translation,8,28,0.8235294117647058,324,0.9788519637462236,6,0.6,1,0
326,"Lors d'une confrence de presse jeudi , M. Blair a dclar qu'il n'y avait rien dans cette vido qui pourrait constituer un "" motif raisonnable "" qui pourrait mener des accusations criminelles portes contre le maire . :",B.2 TRAINING,C TRANSLATIONS OF LONG SENTENCES,machine-translation,8,29,0.8529411764705882,325,0.9818731117824774,7,0.7,1,0
327,The translations generated by RNNenc - 50 and RNNsearch - 50 from long source sentences ( 30 words or more ) selected from the test set .,B.2 TRAINING,C TRANSLATIONS OF LONG SENTENCES,machine-translation,8,30,0.8823529411764706,326,0.9848942598187312,8,0.8,1,0
328,"For each source sentence , we also show the goldstandard translation .",B.2 TRAINING,C TRANSLATIONS OF LONG SENTENCES,machine-translation,8,31,0.9117647058823528,327,0.9879154078549848,9,0.9,1,0
329,The translations by Google Translate were made on 27 August 2014 .,B.2 TRAINING,C TRANSLATIONS OF LONG SENTENCES,machine-translation,8,32,0.9411764705882352,328,0.9909365558912386,10,1.0,1,0
330,Reference,B.2 TRAINING,C TRANSLATIONS OF LONG SENTENCES,machine-translation,8,33,0.9705882352941176,329,0.9939577039274924,0,0.0,1,0
331,"Ce type d'exprience entre dans le cadre des efforts de Disney pour "" tendre la dure de vie de ses sries et construire de nouvelles relations avec son public grce des plateformes numriques qui so nt de plus en plus importantes "" , a-t - il ajout .",B.2 TRAINING,C TRANSLATIONS OF LONG SENTENCES,machine-translation,8,34,1.0,330,0.9969788519637462,1,0.0,1,0
1,title,,,machine-translation,9,0,0.0,0,0.0,0,0.0,1,0
2,Under review as a conference paper at ICLR 2018 COMPRESSING WORD EMBEDDINGS VIA DEEP COMPOSITIONAL CODE LEARNING,title,title,machine-translation,9,1,0.0,1,0.0034843205574912,1,0.0,1,1
3,abstract,,,machine-translation,9,0,0.0,2,0.0069686411149825,0,0.0,1,0
4,"Natural language processing ( NLP ) models often require a massive number of parameters for word embeddings , resulting in a large storage or memory footprint .",abstract,abstract,machine-translation,9,1,0.1,3,0.0104529616724738,1,0.1,1,0
5,Deploying neural NLP models to mobile devices requires compressing the word embeddings without any significant sacrifices in performance .,abstract,abstract,machine-translation,9,2,0.2,4,0.0139372822299651,2,0.2,1,1
6,"For this purpose , we propose to construct the embeddings with few basis vectors .",abstract,abstract,machine-translation,9,3,0.3,5,0.0174216027874564,3,0.3,1,0
7,"For each word , the composition of basis vectors is determined by a hash code .",abstract,abstract,machine-translation,9,4,0.4,6,0.0209059233449477,4,0.4,1,0
8,"To maximize the compression rate , we adopt the multi-codebook quantization approach instead of binary coding scheme .",abstract,abstract,machine-translation,9,5,0.5,7,0.024390243902439,5,0.5,1,0
9,"Each code is composed of multiple discrete numbers , such as ( 3 , 2 , 1 , 8 ) , where the value of each component is limited to a fixed range .",abstract,abstract,machine-translation,9,6,0.6,8,0.0278745644599303,6,0.6,1,0
10,We propose to directly learn the discrete codes in an end - to - end neural network by applying the Gumbel - softmax trick .,abstract,abstract,machine-translation,9,7,0.7,9,0.0313588850174216,7,0.7,1,0
11,Experiments show the compression rate achieves 98 % in a sentiment analysis task and 94 % ? 99 % in machine translation tasks without performance loss .,abstract,abstract,machine-translation,9,8,0.8,10,0.0348432055749128,8,0.8,1,0
12,"In both tasks , the proposed method can improve the model performance by slightly lowering the compression rate .",abstract,abstract,machine-translation,9,9,0.9,11,0.0383275261324041,9,0.9,1,0
13,"Compared to other approaches such as character - level segmentation , the proposed method is language - independent and does not require modifications to the network architecture .",abstract,abstract,machine-translation,9,10,1.0,12,0.0418118466898954,10,1.0,1,0
14,INTRODUCTION,,,machine-translation,9,0,0.0,13,0.0452961672473867,0,0.0,1,0
15,Word embeddings play an important role in neural - based natural language processing ( NLP ) models .,INTRODUCTION,INTRODUCTION,machine-translation,9,1,0.0070422535211267,14,0.048780487804878,1,0.0192307692307692,1,0
16,Neural word embeddings encapsulate the linguistic information of words in continuous vectors .,INTRODUCTION,INTRODUCTION,machine-translation,9,2,0.0140845070422535,15,0.0522648083623693,2,0.0384615384615384,1,0
17,"However , as each word is assigned an independent embedding vector , the number of parameters in the embedding matrix can be huge .",INTRODUCTION,INTRODUCTION,machine-translation,9,3,0.0211267605633802,16,0.0557491289198606,3,0.0576923076923076,1,0
18,"For example , when each embedding has 500 dimensions , the network has to hold 100M embedding parameters to represent 200K words .",INTRODUCTION,INTRODUCTION,machine-translation,9,4,0.028169014084507,17,0.0592334494773519,4,0.0769230769230769,1,0
19,"In practice , fora simple sentiment analysis model , the word embedding parameters account for 98.8 % of the total parameters .",INTRODUCTION,INTRODUCTION,machine-translation,9,5,0.0352112676056338,18,0.0627177700348432,5,0.0961538461538461,1,0
20,"As only a small portion of the word embeddings is selected in the forward pass , the giant embedding matrix usually does not cause a speed issue .",INTRODUCTION,INTRODUCTION,machine-translation,9,6,0.0422535211267605,19,0.0662020905923344,6,0.1153846153846153,1,0
21,"However , the massive number of parameters in the neural network results in a large storage or memory footprint .",INTRODUCTION,INTRODUCTION,machine-translation,9,7,0.0492957746478873,20,0.0696864111498257,7,0.1346153846153846,1,0
22,"When other components of the neural network are also large , the model may fail to fit into GPU memory during training .",INTRODUCTION,INTRODUCTION,machine-translation,9,8,0.056338028169014,21,0.073170731707317,8,0.1538461538461538,1,0
23,"Moreover , as the demand for low - latency neural computation for mobile platforms increases , some neural - based models are expected to run on mobile devices .",INTRODUCTION,INTRODUCTION,machine-translation,9,9,0.0633802816901408,22,0.0766550522648083,9,0.173076923076923,1,0
24,"Thus , it is becoming more important to compress the size of NLP models for deployment to devices with limited memory or storage capacity .",INTRODUCTION,INTRODUCTION,machine-translation,9,10,0.0704225352112676,23,0.0801393728222996,10,0.1923076923076923,1,1
25,"In this study , we attempt to reduce the number of parameters used in word embeddings without hurting the model performance .",INTRODUCTION,INTRODUCTION,machine-translation,9,11,0.0774647887323943,24,0.0836236933797909,11,0.2115384615384615,1,0
26,Neural networks are known for the significant redundancy in the connections .,INTRODUCTION,INTRODUCTION,machine-translation,9,12,0.0845070422535211,25,0.0871080139372822,12,0.2307692307692307,1,0
27,"In this work , we further hypothesize that learning independent embeddings causes more redundancy in the embedding vectors , as the inter-similarity among words is ignored .",INTRODUCTION,INTRODUCTION,machine-translation,9,13,0.0915492957746478,26,0.0905923344947735,13,0.25,1,0
28,Some words are very similar regarding the semantics .,INTRODUCTION,INTRODUCTION,machine-translation,9,14,0.0985915492957746,27,0.0940766550522648,14,0.2692307692307692,1,0
29,"For example , "" dog "" and "" dogs "" have almost the same meaning , except one is plural .",INTRODUCTION,INTRODUCTION,machine-translation,9,15,0.1056338028169014,28,0.0975609756097561,15,0.2884615384615384,1,0
30,"To efficiently represent these two words , it is desirable to share information between the two embeddings .",INTRODUCTION,INTRODUCTION,machine-translation,9,16,0.1126760563380281,29,0.1010452961672473,16,0.3076923076923077,1,0
31,"However , a small portion in both vectors still has to be trained independently to capture the syntactic difference .",INTRODUCTION,INTRODUCTION,machine-translation,9,17,0.1197183098591549,30,0.1045296167247386,17,0.3269230769230769,1,0
32,"Following the intuition of creating partially shared embeddings , instead of assigning each word a unique ID , we represent each word w with a code C w = ( C 1 w , C 2 w , ... , C M w ) .",INTRODUCTION,INTRODUCTION,machine-translation,9,18,0.1267605633802817,31,0.1080139372822299,18,0.3461538461538461,1,1
33,Each component,INTRODUCTION,,machine-translation,9,19,0.1338028169014084,32,0.1114982578397212,19,0.3653846153846153,1,0
34,Ci w is an integer number in .,INTRODUCTION,Each component,machine-translation,9,20,0.1408450704225352,33,0.1149825783972125,20,0.3846153846153846,1,0
35,"Ideally , similar words should have similar codes .",INTRODUCTION,Each component,machine-translation,9,21,0.1478873239436619,34,0.1184668989547038,21,0.4038461538461538,1,0
36,"For example , we may desire C dog = ( 3 , 2 , 4 , 1 ) and C dogs = ( 3 , 2 , 4 , 2 ) .",INTRODUCTION,Each component,machine-translation,9,22,0.1549295774647887,35,0.1219512195121951,22,0.4230769230769231,1,0
37,"Once we have obtained such compact codes for all words in the vocabulary , we use embedding vectors to represent the codes rather than the unique words .",INTRODUCTION,Each component,machine-translation,9,23,0.1619718309859155,36,0.1254355400696864,23,0.4423076923076923,1,1
38,"More specifically , we create M codebooks E 1 , E 2 , ... , EM , each containing K codeword vectors .",INTRODUCTION,Each component,machine-translation,9,24,0.1690140845070422,37,0.1289198606271777,24,0.4615384615384615,1,1
39,The embedding of a word is computed by summing up the codewords corresponding to all the components in the code as,INTRODUCTION,Each component,machine-translation,9,25,0.176056338028169,38,0.1324041811846689,25,0.4807692307692308,1,1
40,( where E i ( C i w ) is the Ci w - th codeword in the codebook E i .,INTRODUCTION,Each component,machine-translation,9,26,0.1830985915492957,39,0.1358885017421602,26,0.5,1,0
41,"In this way , the number of vectors in the embedding matrix will be M K , which is usually much smaller than the vocabulary size .",INTRODUCTION,Each component,machine-translation,9,27,0.1901408450704225,40,0.1393728222996515,27,0.5192307692307693,1,0
42,gives an intuitive comparison between the compositional approach and the conventional approach ( assigning unique IDs ) .,INTRODUCTION,Each component,machine-translation,9,28,0.1971830985915492,41,0.1428571428571428,28,0.5384615384615384,1,0
43,"The codes of all the words can be stored in an integer matrix , denoted by C. Thus , the storage footprint of the embedding layer now depends on the total size of the combined codebook E and the code matrix C.",INTRODUCTION,Each component,machine-translation,9,29,0.204225352112676,42,0.1463414634146341,29,0.5576923076923077,1,0
44,"Although the number of embedding vectors can be greatly reduced by using such coding approach , we want to prevent any serious degradation in performance compared to the models using normal embeddings .",INTRODUCTION,Each component,machine-translation,9,30,0.2112676056338028,43,0.1498257839721254,30,0.5769230769230769,1,0
45,"In other words , given a set of baseline word embeddings ? ( w ) , we wish to find a set of codes ?",INTRODUCTION,Each component,machine-translation,9,31,0.2183098591549295,44,0.1533101045296167,31,0.5961538461538461,1,0
46,and combined codebook that can produce the embeddings with the same effectiveness as ? ( w ) .,INTRODUCTION,Each component,machine-translation,9,32,0.2253521126760563,45,0.156794425087108,32,0.6153846153846154,1,0
47,A safe and straight - forward way is to minimize the squared distance between the baseline embeddings and the composed embeddings as,INTRODUCTION,Each component,machine-translation,9,33,0.2323943661971831,46,0.1602787456445993,33,0.6346153846153846,1,0
48,where | V | is the vocabulary size .,INTRODUCTION,Each component,machine-translation,9,34,0.2394366197183098,47,0.1637630662020905,34,0.6538461538461539,1,0
49,The baseline embeddings can be a set of pre-trained vectors such as word2vec or GloVe embeddings .,INTRODUCTION,Each component,machine-translation,9,35,0.2464788732394366,48,0.1672473867595819,35,0.6730769230769231,1,0
50,"In Eq. 3 , the baseline embedding matrix ?",INTRODUCTION,Each component,machine-translation,9,36,0.2535211267605634,49,0.1707317073170731,36,0.6923076923076923,1,0
51,is approximated by M codewords selected from M codebooks .,INTRODUCTION,Each component,machine-translation,9,37,0.2605633802816901,50,0.1742160278745644,37,0.7115384615384616,1,0
52,The selection of codewords is controlled by the code C w .,INTRODUCTION,Each component,machine-translation,9,38,0.2676056338028169,51,0.1777003484320557,38,0.7307692307692307,1,0
53,"Such problem of learning compact codes with multiple codebooks is formalized and discussed in the research field of compressionbased source coding , known as product quantization and additive quantization .",INTRODUCTION,Each component,machine-translation,9,39,0.2746478873239437,52,0.181184668989547,39,0.75,1,0
54,Previous works learn compositional codes so as to enable an efficient similarity search of vectors .,INTRODUCTION,Each component,machine-translation,9,40,0.2816901408450704,53,0.1846689895470383,40,0.7692307692307693,1,0
55,"In this work , we utilize such codes fora different purpose , that is , constructing word embeddings with drastically fewer parameters .",INTRODUCTION,Each component,machine-translation,9,41,0.2887323943661972,54,0.1881533101045296,41,0.7884615384615384,1,1
56,"Due to the discreteness in the hash codes , it is usually difficult to directly optimize the objective function in Eq.",INTRODUCTION,Each component,machine-translation,9,42,0.2957746478873239,55,0.1916376306620209,42,0.8076923076923077,1,0
57,3 .,INTRODUCTION,Each component,machine-translation,9,43,0.3028169014084507,56,0.1951219512195122,43,0.8269230769230769,1,0
58,"In this paper , we propose a simple and straight - forward method to learn the codes in an end - to - end neural network .",INTRODUCTION,Each component,machine-translation,9,44,0.3098591549295774,57,0.1986062717770035,44,0.8461538461538461,1,0
59,We utilize the Gumbel - softmax trick to find the best discrete codes that minimize the loss .,INTRODUCTION,Each component,machine-translation,9,45,0.3169014084507042,58,0.2020905923344947,45,0.8653846153846154,1,1
60,"Besides the simplicity , this approach also allows one to use any arbitrary differentiable loss function , such as cosine similarity .",INTRODUCTION,Each component,machine-translation,9,46,0.323943661971831,59,0.205574912891986,46,0.8846153846153846,1,0
61,The contribution of this work can be summarized as follows :,INTRODUCTION,Each component,machine-translation,9,47,0.3309859154929577,60,0.2090592334494773,47,0.903846153846154,1,0
62,We propose to utilize the compositional coding approach for constructing the word embeddings with significantly fewer parameters .,INTRODUCTION,Each component,machine-translation,9,48,0.3380281690140845,61,0.2125435540069686,48,0.9230769230769232,1,0
63,"In the experiments , we show that over 98 % of the embedding parameters can be eliminated in sentiment analysis task without affecting performance .",INTRODUCTION,Each component,machine-translation,9,49,0.3450704225352112,62,0.2160278745644599,49,0.9423076923076924,1,0
64,"In machine translation tasks , the loss - free compression rate reaches 94 % ? 99 % . We propose a direct learning approach for the codes in an end - to - end neural network , with a Gumbel - softmax layer to encourage the discreteness .",INTRODUCTION,Each component,machine-translation,9,50,0.352112676056338,63,0.2195121951219512,50,0.9615384615384616,1,0
65,The neural network for learning codes will be packaged into a tool .,INTRODUCTION,Each component,machine-translation,9,51,0.3591549295774648,64,0.2229965156794425,51,0.9807692307692308,1,0
66,"With the learned codes and basis vectors , the computation graph for composing embeddings is fairly easy to implement , and does not require modifications to other parts in the neural network .",INTRODUCTION,Each component,machine-translation,9,52,0.3661971830985915,65,0.2264808362369338,52,1.0,1,0
67,RELATED WORK,INTRODUCTION,Each component,machine-translation,9,53,0.3732394366197183,66,0.229965156794425,0,0.0,1,0
68,"Existing works for compressing neural networks include low - precision computation , quantization and knowledge distillation .",INTRODUCTION,Each component,machine-translation,9,54,0.380281690140845,67,0.2334494773519163,1,0.0344827586206896,1,0
69,"Network quantization such as HashedNet forces the weight matrix to have few real weights , with a hash function to determine the weight assignment .",INTRODUCTION,Each component,machine-translation,9,55,0.3873239436619718,68,0.2369337979094076,2,0.0689655172413793,1,0
70,"To capture the non-uniform nature of the networks , DeepCompression groups weight values into clusters based on pre-trained weight matrices .",INTRODUCTION,Each component,machine-translation,9,56,0.3943661971830985,69,0.2404181184668989,3,0.1034482758620689,1,0
71,The weight assignment for each value is stored in the form of Huffman codes .,INTRODUCTION,Each component,machine-translation,9,57,0.4014084507042254,70,0.2439024390243902,4,0.1379310344827586,1,0
72,"However , as the embedding matrix is tremendously big , the number of hash codes a model need to maintain is still large even with Huffman coding .",INTRODUCTION,Each component,machine-translation,9,58,0.4084507042253521,71,0.2473867595818815,5,0.1724137931034483,1,0
73,Network pruning works in a different way that makes a network sparse .,INTRODUCTION,Each component,machine-translation,9,59,0.4154929577464789,72,0.2508710801393728,6,0.2068965517241379,1,0
74,Iterative pruning prunes a weight value if its absolute value is smaller than a threshold .,INTRODUCTION,Each component,machine-translation,9,60,0.4225352112676056,73,0.2543554006968641,7,0.2413793103448276,1,0
75,The remaining network weights are retrained after pruning .,INTRODUCTION,Each component,machine-translation,9,61,0.4295774647887324,74,0.2578397212543554,8,0.2758620689655172,1,0
76,Some recent works also apply iterative pruning to prune 80 % of the connections for neural machine translation models .,INTRODUCTION,Each component,machine-translation,9,62,0.4366197183098591,75,0.2613240418118467,9,0.3103448275862069,1,0
77,"In this paper , we compare the proposed method with iterative pruning .",INTRODUCTION,Each component,machine-translation,9,63,0.4436619718309859,76,0.2648083623693379,10,0.3448275862068966,1,0
78,"The problem of learning compact codes considered in this paper is closely related to learning to hash , which aims to learn the hash codes for vectors to facilitate the approximate nearest neighbor search .",INTRODUCTION,Each component,machine-translation,9,64,0.4507042253521127,77,0.2682926829268293,11,0.3793103448275862,1,0
79,"Initiated byproduct quantization , subsequent works such as additive quantization explore the use of multiple codebooks for source coding , resulting in compositional codes .",INTRODUCTION,Each component,machine-translation,9,65,0.4577464788732394,78,0.2717770034843205,12,0.4137931034482758,1,0
80,We also adopt the coding scheme of additive quantization for its storage efficiency .,INTRODUCTION,Each component,machine-translation,9,66,0.4647887323943662,79,0.2752613240418118,13,0.4482758620689655,1,0
81,Previous works mainly focus on performing efficient similarity search of image descriptors .,INTRODUCTION,Each component,machine-translation,9,67,0.4718309859154929,80,0.2787456445993031,14,0.4827586206896552,1,0
82,"In this work , we put more focus on reducing the codebook sizes and learning efficient codes to avoid performance loss .",INTRODUCTION,Each component,machine-translation,9,68,0.4788732394366197,81,0.2822299651567944,15,0.5172413793103449,1,0
83,utilizes an improved version of product quantization to compress text classification models .,INTRODUCTION,Each component,machine-translation,9,69,0.4859154929577465,82,0.2857142857142857,16,0.5517241379310345,1,0
84,"However , to match the baseline performance , much longer hash codes are required byproduct quantization .",INTRODUCTION,Each component,machine-translation,9,70,0.4929577464788732,83,0.289198606271777,17,0.5862068965517241,1,0
85,This will be detailed in Section 5.2 .,INTRODUCTION,Each component,machine-translation,9,71,0.5,84,0.2926829268292683,18,0.6206896551724138,1,0
86,"To learn the codebooks and code assignment , additive quantization alternatively optimizes the codebooks and the discrete codes .",INTRODUCTION,Each component,machine-translation,9,72,0.5070422535211268,85,0.2961672473867596,19,0.6551724137931034,1,0
87,The learning of code assignment is performed by Beam Search algorithm when the codebooks are fixed .,INTRODUCTION,Each component,machine-translation,9,73,0.5140845070422535,86,0.2996515679442508,20,0.6896551724137931,1,0
88,"In this work , we propose a straight - forward method to directly learn the code assignment and codebooks simutaneously in an end - to - end neural network .",INTRODUCTION,Each component,machine-translation,9,74,0.5211267605633803,87,0.3031358885017421,21,0.7241379310344828,1,0
89,"Some recent works in learning to hash also utilize neural networks to produce binary codes by applying binary constrains ( e.g. , sigmoid function ) .",INTRODUCTION,Each component,machine-translation,9,75,0.528169014084507,88,0.3066202090592334,22,0.7586206896551724,1,0
90,"In this work , we encourage the discreteness with the Gumbel - Softmax trick for producing compositional codes .",INTRODUCTION,Each component,machine-translation,9,76,0.5352112676056338,89,0.3101045296167247,23,0.7931034482758621,1,0
91,"As an alternative to our approach , one can also reduce the number of unique word types by forcing a character - level segmentation .",INTRODUCTION,Each component,machine-translation,9,77,0.5422535211267606,90,0.313588850174216,24,0.8275862068965517,1,0
92,"proposed a character - based neural language model , which applies a convolutional layer after the character embeddings .",INTRODUCTION,Each component,machine-translation,9,78,0.5492957746478874,91,0.3170731707317073,25,0.8620689655172413,1,0
93,"propose to use char-gram as input features , which are further hashed to save space .",INTRODUCTION,Each component,machine-translation,9,79,0.5563380281690141,92,0.3205574912891986,26,0.896551724137931,1,0
94,"Generally , using characterlevel inputs requires modifications to the model architecture .",INTRODUCTION,Each component,machine-translation,9,80,0.5633802816901409,93,0.3240418118466899,27,0.9310344827586208,1,0
95,"Moreover , some Asian languages such as Japanese and Chinese retain a large vocabulary at the character level , which makes the character - based approach difficult to be applied .",INTRODUCTION,Each component,machine-translation,9,81,0.5704225352112676,94,0.3275261324041811,28,0.9655172413793104,1,0
96,"In contrast , our approach does not suffer from these limitations .",INTRODUCTION,Each component,machine-translation,9,82,0.5774647887323944,95,0.3310104529616725,29,1.0,1,0
97,ADVANTAGE OF COMPOSITIONAL CODES,INTRODUCTION,Each component,machine-translation,9,83,0.5845070422535211,96,0.3344947735191638,0,0.0,1,0
98,"In this section , we formally describe the compositional coding approach and analyze its merits for compressing word embeddings .",INTRODUCTION,Each component,machine-translation,9,84,0.5915492957746479,97,0.337979094076655,1,0.0169491525423728,1,0
99,The coding approach follows the scheme in additive quantization .,INTRODUCTION,Each component,machine-translation,9,85,0.5985915492957746,98,0.3414634146341463,2,0.0338983050847457,1,0
100,"We represent each word w with a compact code C w that is composed of M components such that , which also indicates that M log 2 K bits are required to store each code .",INTRODUCTION,Each component,machine-translation,9,86,0.6056338028169014,99,0.3449477351916376,3,0.0508474576271186,1,0
101,"For convenience , K is selected to be a number of a multiple of 2 , so that the codes can be efficiently stored .",INTRODUCTION,Each component,machine-translation,9,87,0.6126760563380281,100,0.3484320557491289,4,0.0677966101694915,1,0
102,If we restrict each component,INTRODUCTION,Each component,machine-translation,9,88,0.6197183098591549,101,0.3519163763066202,5,0.0847457627118644,1,0
103,"Ci w to values of 0 or 1 , the code for each word C w will be a binary code .",INTRODUCTION,Each component,machine-translation,9,89,0.6267605633802817,102,0.3554006968641115,6,0.1016949152542373,1,0
104,"In this case , the code learning problem is equivalent to a matrix factorization problem with binary components .",INTRODUCTION,Each component,machine-translation,9,90,0.6338028169014085,103,0.3588850174216028,7,0.1186440677966101,1,0
105,"Forcing the compact codes to be binary numbers can be beneficial , as the learning problem is usually easier to solve in the binary case , and some existing optimization algorithms in learning to hash can be reused .",INTRODUCTION,Each component,machine-translation,9,91,0.6408450704225352,104,0.3623693379790941,8,0.135593220338983,1,0
106,"However , the compositional coding approach produces shorter codes and is thus more storage efficient .",INTRODUCTION,Each component,machine-translation,9,92,0.647887323943662,105,0.3658536585365853,9,0.1525423728813559,1,0
107,"As the number of basis vectors is M K regardless of the vocabulary size , the only uncertain factor contributing to the model size is the size of the hash codes , which is proportional to the vocabulary size .",INTRODUCTION,Each component,machine-translation,9,93,0.6549295774647887,106,0.3693379790940767,10,0.1694915254237288,1,0
108,"Therefore , maintaining short codes is cruicial in our work .",INTRODUCTION,Each component,machine-translation,9,94,0.6619718309859155,107,0.3728222996515679,11,0.1864406779661017,1,0
109,Suppose we wish the model to have a set of N basis vectors .,INTRODUCTION,Each component,machine-translation,9,95,0.6690140845070423,108,0.3763066202090592,12,0.2033898305084746,1,0
110,"Then in the binary case , each code will have N / 2 bits .",INTRODUCTION,Each component,machine-translation,9,96,0.676056338028169,109,0.3797909407665505,13,0.2203389830508474,1,0
111,"For the compositional coding approach , if we can find a M K decomposition such that M K = N , then each code will have M log 2 K bits .",INTRODUCTION,Each component,machine-translation,9,97,0.6830985915492958,110,0.3832752613240418,14,0.2372881355932203,1,0
112,"For example , a binary code will have a length of 256 bits to support 512 basis vectors .",INTRODUCTION,Each component,machine-translation,9,98,0.6901408450704225,111,0.3867595818815331,15,0.2542372881355932,1,0
113,"In contrast , a 32 16 compositional coding scheme will produce codes of only 128 bits . :",INTRODUCTION,Each component,machine-translation,9,99,0.6971830985915493,112,0.3902439024390244,16,0.2711864406779661,1,0
114,Comparison of different coding approaches .,INTRODUCTION,Each component,machine-translation,9,100,0.704225352112676,113,0.3937282229965156,17,0.288135593220339,1,0
115,"To support N basis vectors , a binary code will have N / 2 bits and the embedding computation is a summation over N / 2 vectors .",INTRODUCTION,Each component,machine-translation,9,101,0.7112676056338029,114,0.397212543554007,18,0.3050847457627119,1,0
116,"For the compositional approach with M codebooks and K codewords in each codebook , each code has M log 2 K bits , and the computation is a summation over M vectors .",INTRODUCTION,Each component,machine-translation,9,102,0.7183098591549296,115,0.4006968641114982,19,0.3220338983050847,1,0
117,A comparison of different coding approaches is summarized in .,INTRODUCTION,Each component,machine-translation,9,103,0.7253521126760564,116,0.4041811846689895,20,0.3389830508474576,1,0
118,We also report the number of basis vectors required to compute an embedding as a measure of computational cost .,INTRODUCTION,Each component,machine-translation,9,104,0.7323943661971831,117,0.4076655052264808,21,0.3559322033898305,1,0
119,"For the conventional approach , the number of vectors is identical to the vocabulary size and the computation is basically a single indexing operation .",INTRODUCTION,Each component,machine-translation,9,105,0.7394366197183099,118,0.4111498257839721,22,0.3728813559322034,1,0
120,"In the case of binary codes , the computation for constructing an embedding involves a summation over N / 2 basis vectors .",INTRODUCTION,Each component,machine-translation,9,106,0.7464788732394366,119,0.4146341463414634,23,0.3898305084745763,1,0
121,"For the compositional approach , the number of vectors required to construct an embedding vector is M .",INTRODUCTION,Each component,machine-translation,9,107,0.7535211267605634,120,0.4181184668989547,24,0.4067796610169492,1,0
122,Both the binary and compositional approaches have significantly fewer vectors in the embedding matrix .,INTRODUCTION,Each component,machine-translation,9,108,0.7605633802816901,121,0.4216027874564459,25,0.423728813559322,1,0
123,The compositional coding approach provides a better balance with shorter codes and lower computational cost .,INTRODUCTION,Each component,machine-translation,9,109,0.7676056338028169,122,0.4250871080139373,26,0.4406779661016949,1,0
124,CODE LEARNING WITH GUMBEL - SOFTMAX,INTRODUCTION,Each component,machine-translation,9,110,0.7746478873239436,123,0.4285714285714285,27,0.4576271186440678,1,0
125,Let ? ?,INTRODUCTION,Each component,machine-translation,9,111,0.7816901408450704,124,0.4320557491289198,28,0.4745762711864407,1,0
126,"R |V | H be the original embedding matrix , where each embedding vector has H dimensions .",INTRODUCTION,Each component,machine-translation,9,112,0.7887323943661971,125,0.4355400696864111,29,0.4915254237288136,1,0
127,"By using the reconstruction loss as the objective function in Eq. 3 , we are actually finding an approximate matrix factorization ?",INTRODUCTION,Each component,machine-translation,9,113,0.795774647887324,126,0.4390243902439024,30,0.5084745762711864,1,0
128,is a basis matrix for the i - th component .,INTRODUCTION,Each component,machine-translation,9,114,0.8028169014084507,127,0.4425087108013937,31,0.5254237288135594,1,0
129,Di is a | V | K code matrix in which each row is an K-dimensional one - hot vector .,INTRODUCTION,Each component,machine-translation,9,115,0.8098591549295775,128,0.445993031358885,32,0.5423728813559322,1,0
130,If we let d i w be the one - hot vector corresponding to the code component,INTRODUCTION,Each component,machine-translation,9,116,0.8169014084507042,129,0.4494773519163763,33,0.559322033898305,1,0
131,"Ci w for word w , the computation of the word embeddings can be reformulated as",INTRODUCTION,Each component,machine-translation,9,117,0.823943661971831,130,0.4529616724738676,34,0.576271186440678,1,0
132,"Therefore , the problem of learning discrete codes C w can be converted to a problem of finding a set of optimal one - hot vectors d 1 w , ... , d M wand source dictionaries A 1 , ... , AM , that minimize the reconstruction loss .",INTRODUCTION,Each component,machine-translation,9,118,0.8309859154929577,131,0.4564459930313589,35,0.5932203389830508,1,0
133,The Gumbel - softmax reparameterization trick is useful for parameterizing a discrete distribution such as the K-dimensional one - hot vectors d i win Eq.,INTRODUCTION,Each component,machine-translation,9,119,0.8380281690140845,132,0.4599303135888501,36,0.6101694915254238,1,0
134,"5 . By applying the Gumbel - softmax trick , the k - th elemement ind i w is computed as",INTRODUCTION,Each component,machine-translation,9,120,0.8450704225352113,133,0.4634146341463415,37,0.6271186440677966,1,0
135,where,INTRODUCTION,Each component,machine-translation,9,121,0.852112676056338,134,0.4668989547038327,38,0.6440677966101694,1,0
136,Gk is a noise term that is sampled from the Gumbel distribution ? log ( ?,INTRODUCTION,Each component,machine-translation,9,122,0.8591549295774648,135,0.470383275261324,39,0.6610169491525424,1,0
137,"log ( Uniform [ 0 , 1 ] ) ) , whereas ?",INTRODUCTION,Each component,machine-translation,9,123,0.8661971830985915,136,0.4738675958188153,40,0.6779661016949152,1,0
138,is the temperature of the softmax .,INTRODUCTION,Each component,machine-translation,9,124,0.8732394366197183,137,0.4773519163763066,41,0.6949152542372882,1,0
139,"In our model , the vector ?",INTRODUCTION,Each component,machine-translation,9,125,0.8802816901408451,138,0.4808362369337979,42,0.711864406779661,1,0
140,i w is computed by a simple neural network with a single hidden layer as,INTRODUCTION,Each component,machine-translation,9,126,0.8873239436619719,139,0.4843205574912892,43,0.7288135593220338,1,0
141,"In our experiments , the hidden layer h w always has a size of M K /2 .",INTRODUCTION,Each component,machine-translation,9,127,0.8943661971830986,140,0.4878048780487805,44,0.7457627118644068,1,0
142,We found that a fixed temperature of ? = 1 just works well .,INTRODUCTION,Each component,machine-translation,9,128,0.9014084507042254,141,0.4912891986062718,45,0.7627118644067796,1,0
143,The Gumbel - softmax trick is applied to ?,INTRODUCTION,Each component,machine-translation,9,129,0.908450704225352,142,0.494773519163763,46,0.7796610169491526,1,0
144,i w to obtain d i w .,INTRODUCTION,Each component,machine-translation,9,130,0.9154929577464788,143,0.4982578397212543,47,0.7966101694915254,1,0
145,"Then , the model reconstructs the embedding E(C w ) with Eq. 5 and computes the reconstruction loss with Eq.",INTRODUCTION,Each component,machine-translation,9,131,0.9225352112676056,144,0.5017421602787456,48,0.8135593220338984,1,0
146,3 .,INTRODUCTION,Each component,machine-translation,9,132,0.9295774647887324,145,0.5052264808362369,49,0.8305084745762712,1,0
147,"The model architecture of the end - to - end neural network is illustrated in , which is effectively an auto - encoder with a Gumbel - softmax middle layer .",INTRODUCTION,Each component,machine-translation,9,133,0.9366197183098592,146,0.5087108013937283,50,0.847457627118644,1,0
148,"The whole neural network for coding learning has five parameters ( ? , b , ? , b , A ) .",INTRODUCTION,Each component,machine-translation,9,134,0.943661971830986,147,0.5121951219512195,51,0.864406779661017,1,0
149,"Once the coding learning model is trained , the code C w for each word can be easily obtained by applying argmax to the one - hot vectors d 1 w , ... , d M w .",INTRODUCTION,Each component,machine-translation,9,135,0.9507042253521126,148,0.5156794425087108,52,0.8813559322033898,1,0
150,The basis vectors ( codewords ) for composing the embeddings can be found as the row vectors in the weight matrix A.,INTRODUCTION,Each component,machine-translation,9,136,0.9577464788732394,149,0.519163763066202,53,0.8983050847457628,1,0
151,"For general NLP tasks , one can learn the compositional codes from publicly available word vectors such as GloVe vectors .",INTRODUCTION,Each component,machine-translation,9,137,0.9647887323943662,150,0.5226480836236934,54,0.9152542372881356,1,0
152,"However , for some tasks such as machine translation , the word embeddings are usually jointly learned with other parts of the neural network .",INTRODUCTION,Each component,machine-translation,9,138,0.971830985915493,151,0.5261324041811847,55,0.9322033898305084,1,0
153,"For such tasks , one has to first train a normal model to obtain the baseline embeddings .",INTRODUCTION,Each component,machine-translation,9,139,0.9788732394366196,152,0.5296167247386759,56,0.9491525423728814,1,0
154,"Then , based on the trained embedding matrix , one can learn a set of task - specific codes .",INTRODUCTION,Each component,machine-translation,9,140,0.9859154929577464,153,0.5331010452961672,57,0.9661016949152542,1,0
155,"As the reconstructed embeddings E( C w ) are not identical to the original embeddings ? ( w ) , the model parameters other than the embedding matrix have to be retrained again .",INTRODUCTION,Each component,machine-translation,9,141,0.9929577464788732,154,0.5365853658536586,58,0.9830508474576272,1,0
156,The code learning model can not be jointly trained with the machine translation model as it takes far more iterations for the coding layer to converge to one - hot vectors .,INTRODUCTION,Each component,machine-translation,9,142,1.0,155,0.5400696864111498,59,1.0,1,0
157,EXPERIMENTS,,,machine-translation,9,0,0.0,156,0.5435540069686411,0,0.0,1,0
158,"In our experiments , we focus on evaluating the maximum loss - free compression rate of word embeddings on two typical NLP tasks : sentiment analysis and machine translation .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,1,0.0092592592592592,157,0.5470383275261324,1,0.1428571428571428,1,0
159,We compare the model performance and the size of embedding layer with the baseline model and the iterative pruning method .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,2,0.0185185185185185,158,0.5505226480836237,2,0.2857142857142857,1,0
160,Please note that the sizes of other parts in the neural networks are not included in our results .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,3,0.0277777777777777,159,0.554006968641115,3,0.4285714285714285,1,0
161,"For dense matrices , we report the size of dumped numpy arrays .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,4,0.037037037037037,160,0.5574912891986062,4,0.5714285714285714,1,0
162,"For the sparse matrices , we report the size of dumped compressed sparse column matrices ( csc matrix ) in scipy .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,5,0.0462962962962962,161,0.5609756097560976,5,0.7142857142857143,1,0
163,All float numbers take 32 bits storage .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,6,0.0555555555555555,162,0.5644599303135889,6,0.8571428571428571,1,0
164,"We enable the "" compressed "" option when dumping the matrices , without this option , the file size is about 1.1 times bigger .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,7,0.0648148148148148,163,0.5679442508710801,7,1.0,1,0
165,CODE LEARNING,EXPERIMENTS,EXPERIMENTS,machine-translation,9,8,0.074074074074074,164,0.5714285714285714,0,0.0,1,0
166,"To learn efficient compact codes for each word , our proposed method requires a set of baseline embedding vectors .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,9,0.0833333333333333,165,0.5749128919860628,1,0.0833333333333333,1,0
167,"For the sentiment analysis task , we learn the codes based on the publicly available GloVe vectors .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,10,0.0925925925925925,166,0.578397212543554,2,0.1666666666666666,1,0
168,"For the machine translation task , we first train a normal neural machine translation ( NMT ) model to obtain task - specific word embeddings .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,11,0.1018518518518518,167,0.5818815331010453,3,0.25,1,0
169,Then we learn the codes using the pre-trained embeddings .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,12,0.1111111111111111,168,0.5853658536585366,4,0.3333333333333333,1,0
170,We train the end - to - end network described in Section 4 to learn the codes automatically .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,13,0.1203703703703703,169,0.5888501742160279,5,0.4166666666666667,1,0
171,"In each iteration , a small batch of the embeddings is sampled uniformly from the baseline embedding matrix .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,14,0.1296296296296296,170,0.5923344947735192,6,0.5,1,1
172,The network parameters are optimized to minimize the reconstruction loss of the sampled embeddings .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,15,0.1388888888888889,171,0.5958188153310104,7,0.5833333333333334,1,0
173,"In our experiments , the batch size is set to 128 .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,16,0.1481481481481481,172,0.5993031358885017,8,0.6666666666666666,1,1
174,We use Adam optimizer with a fixed learning rate of 0.0001 .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,17,0.1574074074074074,173,0.6027874564459931,9,0.75,1,1
175,The training is run for 200K iterations .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,18,0.1666666666666666,174,0.6062717770034843,10,0.8333333333333334,1,1
176,"Every 1,000 iterations , we examine the loss on a fixed validation set and save the parameters if the loss decreases .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,19,0.1759259259259259,175,0.6097560975609756,11,0.9166666666666666,1,0
177,"We evenly distribute the model training to 4 GPUs using the nccl package , so that one round of code learning takes around 15 minutes to complete .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,20,0.1851851851851851,176,0.6132404181184669,12,1.0,1,1
178,SENTIMENT ANALYSIS,EXPERIMENTS,EXPERIMENTS,machine-translation,9,21,0.1944444444444444,177,0.6167247386759582,0,0.0,1,0
179,"Dataset : For sentiment analysis , we use a standard separation of IMDB movie review dataset , which contains 25 k reviews for training and 25 K reviews for testing purpose .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,22,0.2037037037037037,178,0.6202090592334495,1,0.032258064516129,1,0
180,We lowercase and tokenize all texts with the nltk package .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,23,0.2129629629629629,179,0.6236933797909407,2,0.064516129032258,1,0
181,We choose the 300 - dimensional uncased Glo Ve word vectors ( trained on 42B tokens of Common Crawl data ) as our baseline embeddings .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,24,0.2222222222222222,180,0.627177700348432,3,0.0967741935483871,1,0
182,"The vocabulary for the model training contains all words appears both in the IMDB dataset and the GloVe vocabulary , which results in around 75 K words .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,25,0.2314814814814814,181,0.6306620209059234,4,0.1290322580645161,1,0
183,We truncate the texts of reviews to assure they are not longer than 400 words .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,26,0.2407407407407407,182,0.6341463414634146,5,0.1612903225806451,1,0
184,Model architecture :,EXPERIMENTS,EXPERIMENTS,machine-translation,9,27,0.25,183,0.6376306620209059,6,0.1935483870967742,1,0
185,Both the baseline model and the compressed models have the same computational graph except the embedding layer .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,28,0.2592592592592592,184,0.6411149825783972,7,0.2258064516129032,1,0
186,The model is composed of a single LSTM layer with 150 hidden units and a softmax layer for predicting the binary label .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,29,0.2685185185185185,185,0.6445993031358885,8,0.2580645161290322,1,1
187,"For the baseline model , the embedding layer contains a large 75 K 300 embedding matrix initialized by GloVe embeddings .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,30,0.2777777777777778,186,0.6480836236933798,9,0.2903225806451613,1,1
188,"For the compressed models based on the compositional coding , the embedding layer maintains a matrix of basis vectors .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,31,0.287037037037037,187,0.6515679442508711,10,0.3225806451612903,1,1
189,"Suppose we use a 32 16 coding scheme , the basis matrix will then have a shape of 512 300 , which is initialized by the concatenated weight matrices [ A 1 ; A 2 ; ... ; AM ] in the code learning model .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,32,0.2962962962962963,188,0.6550522648083623,11,0.3548387096774194,1,0
190,The embedding parameters for both models remain fixed during the training .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,33,0.3055555555555556,189,0.6585365853658537,12,0.3870967741935484,1,1
191,"For the models with network pruning , the sparse embedding matrix is finetuned during training .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,34,0.3148148148148148,190,0.662020905923345,13,0.4193548387096774,1,1
192,Training details :,EXPERIMENTS,EXPERIMENTS,machine-translation,9,35,0.324074074074074,191,0.6655052264808362,14,0.4516129032258064,1,0
193,The models are trained with Adam optimizer for 15 epochs with a fixed learning rate of 0.0001 .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,36,0.3333333333333333,192,0.6689895470383276,15,0.4838709677419355,1,1
194,"At the end of each epoch , we evaluate the loss on a small validation set .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,37,0.3425925925925926,193,0.6724738675958188,16,0.5161290322580645,1,0
195,The parameters with lowest validation loss are saved .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,38,0.3518518518518518,194,0.6759581881533101,17,0.5483870967741935,1,0
196,"Results : For different settings of the number of components M and the number of codewords K , we train the code learning network .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,39,0.3611111111111111,195,0.6794425087108014,18,0.5806451612903226,1,0
197,The average reconstruction loss on a fixed validation set is summarized in the left of .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,40,0.3703703703703703,196,0.6829268292682927,19,0.6129032258064516,1,0
198,"For reference , we also report the total size ( MB ) of the embedding layer in the right table , which includes the sizes of the basis matrix and the hash table .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,41,0.3796296296296296,197,0.686411149825784,20,0.6451612903225806,1,0
199,We can see that increasing either M or K can effectively decrease the reconstruction loss .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,42,0.3888888888888889,198,0.6898954703832753,21,0.6774193548387096,1,0
200,"However , setting M to a large number will result in longer hash codes , thus significantly increase the size of the embedding layer .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,43,0.3981481481481481,199,0.6933797909407665,22,0.7096774193548387,1,0
201,"Hence , it is important to choose correct numbers for M and K to balance the performance and model size .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,44,0.4074074074074074,200,0.6968641114982579,23,0.7419354838709677,1,0
202,"To see how the reconstructed loss translates to the classification accuracy , we train the sentiment analysis model for different settings of code schemes and report the results in : Trade - off between the model performance and the size of embedding layer on IMDB sentiment analysis task",EXPERIMENTS,EXPERIMENTS,machine-translation,9,45,0.4166666666666667,201,0.7003484320557491,24,0.7741935483870968,1,0
203,We also show the results using normalized product quantization ( NPQ ) .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,46,0.4259259259259259,202,0.7038327526132404,25,0.8064516129032258,1,0
204,"We quantize the filtered Glo Ve embeddings with the codes provided by the authors , and train the models based on the quantized embeddings .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,47,0.4351851851851852,203,0.7073170731707317,26,0.8387096774193549,1,0
205,"To make the results comparable , we report the codebook size in numpy format .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,48,0.4444444444444444,204,0.710801393728223,27,0.8709677419354839,1,0
206,"For our proposed methods , the maximum loss - free compression rate is achieved by a 16 32 coding scheme .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,49,0.4537037037037037,205,0.7142857142857143,28,0.9032258064516128,1,1
207,"In this case , the total size of the embedding layer is 1.23 MB , which is equivalent to a compression rate of 98.4 % .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,50,0.4629629629629629,206,0.7177700348432056,29,0.935483870967742,1,0
208,We also found the classification accuracy can be substantially improved with a slightly lower compression rate .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,51,0.4722222222222222,207,0.7212543554006968,30,0.967741935483871,1,1
209,The improved model performance maybe a byproduct of the strong regularization .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,52,0.4814814814814814,208,0.7247386759581882,31,1.0,1,0
210,MACHINE TRANSLATION,EXPERIMENTS,EXPERIMENTS,machine-translation,9,53,0.4907407407407407,209,0.7282229965156795,0,0.0,1,0
211,"Dataset : For machine translation tasks , we experiment on IWSLT 2014 German - to - English translation task and ASPEC English - to - Japanese translation task .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,54,0.5,210,0.7317073170731707,1,0.0238095238095238,1,0
212,"The IWSLT14 training data contains 178K sentence pairs , which is a small dataset for machine translation .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,55,0.5092592592592593,211,0.735191637630662,2,0.0476190476190476,1,0
213,We utilize moses toolkit to tokenize and lowercase both sides of the texts .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,56,0.5185185185185185,212,0.7386759581881533,3,0.0714285714285714,1,0
214,Then we concatenate all five TED / TEDx development and test corpus to form a test set containing 6750 sentence pairs .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,57,0.5277777777777778,213,0.7421602787456446,4,0.0952380952380952,1,0
215,We apply byte - pair encoding to transform the texts to subword level so that the vocabulary has a size of 20 K for each language .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,58,0.5370370370370371,214,0.7456445993031359,5,0.119047619047619,1,0
216,"For evaluation , we report tokenized BLEU using "" multi -bleu.perl "" .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,59,0.5462962962962963,215,0.7491289198606271,6,0.1428571428571428,1,0
217,The ASPEC dataset contains 300M bilingual pairs in the training data with the automatically estimated quality scores provided for each pair .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,60,0.5555555555555556,216,0.7526132404181185,7,0.1666666666666666,1,0
218,We only use the first 150M pairs for training the models .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,61,0.5648148148148148,217,0.7560975609756098,8,0.1904761904761904,1,0
219,The English texts are tokenized by moses toolkit whereas the Japanese texts are tokenized by kytea .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,62,0.5740740740740741,218,0.759581881533101,9,0.2142857142857142,1,0
220,The vocabulary size for each language is reduced to 40K using byte - pair encoding .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,63,0.5833333333333334,219,0.7630662020905923,10,0.238095238095238,1,0
221,The evaluation is performed using a standard kytea - based post -processing script for this dataset .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,64,0.5925925925925926,220,0.7665505226480837,11,0.2619047619047619,1,0
222,Model architecture :,EXPERIMENTS,EXPERIMENTS,machine-translation,9,65,0.6018518518518519,221,0.7700348432055749,12,0.2857142857142857,1,0
223,"In our preliminary experiments , we found a 32 16 coding works well fora vanilla NMT model .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,66,0.6111111111111112,222,0.7735191637630662,13,0.3095238095238095,1,0
224,"As it is more meaningful to test on a high - performance model , we applied several techniques to improve the performance .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,67,0.6203703703703703,223,0.7770034843205574,14,0.3333333333333333,1,0
225,The model has a standard bi-directional encoder composed of two LSTM layers similar to .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,68,0.6296296296296297,224,0.7804878048780488,15,0.3571428571428571,1,1
226,The decoder contains two LSTM layers .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,69,0.6388888888888888,225,0.7839721254355401,16,0.3809523809523809,1,1
227,Residual connection with a scaling factor of 1 / 2 is applied to the two decoder states to compute the outputs .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,70,0.6481481481481481,226,0.7874564459930313,17,0.4047619047619047,1,1
228,All LSTMs and embeddings have 256 hidden units in the IWSLT14 task and 1000 hidden units in ASPEC task .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,71,0.6574074074074074,227,0.7909407665505227,18,0.4285714285714285,1,1
229,The decoder states are firstly linearly transformed to 600 - dimensional vectors before computing the final softmax .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,72,0.6666666666666666,228,0.794425087108014,19,0.4523809523809524,1,1
230,Dropout with a rate of 0.2 is applied everywhere except the recurrent computation .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,73,0.6759259259259259,229,0.7979094076655052,20,0.4761904761904761,1,1
231,"We apply Key - Value Attention to the first decoder , where the query is the sum of the feedback embedding and the previous decoder state and the keys are computed by linear transformation of encoder states .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,74,0.6851851851851852,230,0.8013937282229965,21,0.5,1,1
232,Training details :,EXPERIMENTS,EXPERIMENTS,machine-translation,9,75,0.6944444444444444,231,0.8048780487804879,22,0.5238095238095238,1,0
233,All models are trained by Nesterov 's accelerated gradient with an initial learning rate of 0.25 .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,76,0.7037037037037037,232,0.8083623693379791,23,0.5476190476190477,1,1
234,"We evaluate the smoothed BLEU ) on a validation set composed of 50 batches every 7,000 iterations .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,77,0.7129629629629629,233,0.8118466898954704,24,0.5714285714285714,1,0
235,The learning rate is reduced by a factor of 10 if no improvement is observed in 3 validation runs .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,78,0.7222222222222222,234,0.8153310104529616,25,0.5952380952380952,1,0
236,The training ends after the learning rate is reduced three times .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,79,0.7314814814814815,235,0.818815331010453,26,0.6190476190476191,1,0
237,"Similar to the code learning , the training is distributed to 4 GPUs , each GPU computes a mini-batch of 16 samples .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,80,0.7407407407407407,236,0.8222996515679443,27,0.6428571428571429,1,1
238,We firstly train a baseline NMT model to obtain the task - specific embeddings for all in - vocabulary words in both languages .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,81,0.75,237,0.8257839721254355,28,0.6666666666666666,1,0
239,"Then based on these baseline embeddings , we obtain the hash codes and basis vectors by training the code learning model .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,82,0.7592592592592593,238,0.8292682926829268,29,0.6904761904761905,1,0
240,"Finally , the NMT models using compositional coding are retrained by plugging in the reconstructed embeddings .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,83,0.7685185185185185,239,0.8327526132404182,30,0.7142857142857143,1,0
241,"Note that the embedding layer is fixed in this phase , other parameters are retrained from random initial values .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,84,0.7777777777777778,240,0.8362369337979094,31,0.7380952380952381,1,0
242,Results :,EXPERIMENTS,EXPERIMENTS,machine-translation,9,85,0.7870370370370371,241,0.8397212543554007,32,0.7619047619047619,1,0
243,The experimental results are summarized in .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,86,0.7962962962962963,242,0.8432055749128919,33,0.7857142857142857,1,0
244,All translations are decoded by the beam search with abeam size of 5 .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,87,0.8055555555555556,243,0.8466898954703833,34,0.8095238095238095,1,0
245,The performance of iterative pruning varies between tasks .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,88,0.8148148148148148,244,0.8501742160278746,35,0.8333333333333334,1,0
246,The loss - free compression rate reaches 92 % on ASPEC dataset by pruning 90 % of the connections .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,89,0.8240740740740741,245,0.8536585365853658,36,0.8571428571428571,1,1
247,"However , with the same pruning ratio , a modest performance loss is observed in IWSLT14 dataset .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,90,0.8333333333333334,246,0.8571428571428571,37,0.8809523809523809,1,1
248,"For the models using compositional coding , the loss - free compression rate is 94 % for the IWSLT14 dataset and 99 % for the ASPEC dataset .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,91,0.8425925925925926,247,0.8606271777003485,38,0.9047619047619048,1,1
249,"Similar to the sentiment analysis task , a significant performance improvement can be observed by slightly lowering the compression rate .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,92,0.8518518518518519,248,0.8641114982578397,39,0.9285714285714286,1,0
250,"Note that the sizes of NMT models are still quite large due to the big softmax layer and the recurrent layers , which are not reported in the , we show some examples of learned codes based on the 300 - dimensional uncased GloVe embeddings used in the sentiment analysis task .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,93,0.8611111111111112,249,0.867595818815331,40,0.9523809523809524,1,0
251,We can see that the model learned to assign similar codes to the words with similar meanings .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,94,0.8703703703703703,250,0.8710801393728222,41,0.9761904761904762,1,0
252,"Such a code - sharing mechanism can significantly reduce the redundancy of the word embeddings , thus helping to achieve a high compression rate .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,95,0.8796296296296297,251,0.8745644599303136,42,1.0,1,0
253,ANALYSIS OF CODE EFFICIENCY,EXPERIMENTS,EXPERIMENTS,machine-translation,9,96,0.8888888888888888,252,0.8780487804878049,0,0.0,1,0
254,"Besides the performance , we also care about the storage efficiency of the codes .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,97,0.8981481481481481,253,0.8815331010452961,1,0.0833333333333333,1,0
255,"In the ideal situation , all codewords shall be fully utilized to convey a fraction of meaning .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,98,0.9074074074074074,254,0.8850174216027874,2,0.1666666666666666,1,0
256,"However , as the codes are category word 8 8 code 16 16 code dog 0 7 0 1 7 3 7 0 7 7 0 8 3 5 8 5 B 2 E E 0 B 0 A animal cat 7 7 0 1 7 3 7 0 7 7 2 8 B 5 8 CB 2 E E 4 B 0 A penguin 0 7 0 1 7 3 6 0 7 7 E 8 7 6 4 CF DE 3 D 8 0 A go 7 7 0 6 4 3 3 0 2 C C 8 2 C 1 1 B D 0 E 0 B 5 8 verb went 4 0 7 6 4 3 2 0 BC C 6 BC 7 5 B 8 6 E 0 D 0 4 gone 7 7 0 6 4 3 3 0 2 C C 8 0 B 1 5 B D 6 E 0 2 5 A : Examples of learned compositional codes based on Glo Ve embedding vectors automatically learned , it is possible that some codewords are abandoned during the training .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,99,0.9166666666666666,255,0.8885017421602788,3,0.25,1,0
257,"In extreme cases , some "" dead "" codewords can be used by none of the words .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,100,0.925925925925926,256,0.89198606271777,4,0.3333333333333333,1,0
258,"To analyze the code efficiency , we count the number of words that contain a specific subcode in each component .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,101,0.9351851851851852,257,0.8954703832752613,5,0.4166666666666667,1,0
259,gives a visualization of the code balance for three coding schemes .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,102,0.9444444444444444,258,0.8989547038327527,6,0.5,1,0
260,Each column shows the counts of the subcodes of a specific component .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,103,0.9537037037037036,259,0.902439024390244,7,0.5833333333333334,1,0
261,"In our experiments , when using a 8 8 coding scheme , we found 31 % of the words have a subcode "" 0 "" for the first component , while the subcode "" 1 "" is only used by 5 % of the words .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,104,0.9629629629629628,260,0.9059233449477352,8,0.6666666666666666,1,0
262,The assignment of codes is more balanced for larger coding schemes .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,105,0.9722222222222222,261,0.9094076655052264,9,0.75,1,0
263,"In any coding scheme , even the most unpopular codeword is used by about 1000 words .",EXPERIMENTS,EXPERIMENTS,machine-translation,9,106,0.9814814814814816,262,0.9128919860627178,10,0.8333333333333334,1,0
264,This result indicates that the code learning model is capable of assigning codes efficiently without wasting a codeword .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,107,0.9907407407407408,263,0.9163763066202092,11,0.9166666666666666,1,0
265,The results show that any codeword is assigned to more than 1000 words without wasting .,EXPERIMENTS,EXPERIMENTS,machine-translation,9,108,1.0,264,0.9198606271777005,12,1.0,1,0
266,CONCLUSION,,,machine-translation,9,0,0.0,265,0.9233449477351916,0,0.0,1,0
267,"In this work , we propose a novel method for reducing the number of parameters required in word embeddings .",CONCLUSION,CONCLUSION,machine-translation,9,1,0.0476190476190476,266,0.926829268292683,1,0.0476190476190476,0,0
268,"Instead of assigning each unique word an embedding vector , we compose the embedding vectors using a small set of basis vectors .",CONCLUSION,CONCLUSION,machine-translation,9,2,0.0952380952380952,267,0.9303135888501742,2,0.0952380952380952,0,0
269,The selection of basis vectors is governed by the hash code of each word .,CONCLUSION,CONCLUSION,machine-translation,9,3,0.1428571428571428,268,0.9337979094076656,3,0.1428571428571428,0,0
270,We apply the compositional coding approach to maximize the storage efficiency .,CONCLUSION,CONCLUSION,machine-translation,9,4,0.1904761904761904,269,0.9372822299651568,4,0.1904761904761904,0,0
271,The proposed method works by eliminating the redundancy inherent in representing similar words with independent embeddings .,CONCLUSION,CONCLUSION,machine-translation,9,5,0.238095238095238,270,0.940766550522648,5,0.238095238095238,0,0
272,"In our work , we propose a simple way to directly learn the discrete codes in a neural network with Gumbel - softmax trick .",CONCLUSION,CONCLUSION,machine-translation,9,6,0.2857142857142857,271,0.9442508710801394,6,0.2857142857142857,0,0
273,The results show that the size of the embedding layer was reduced by 98 % in IMDB sentiment analysis task and 94 % ? 99 % in machine translation tasks without affecting the performance .,CONCLUSION,CONCLUSION,machine-translation,9,7,0.3333333333333333,272,0.9477351916376306,7,0.3333333333333333,0,0
274,Our approach achieves a high loss - free compression rate by considering the semantic inter-similarity among different words .,CONCLUSION,CONCLUSION,machine-translation,9,8,0.3809523809523809,273,0.951219512195122,8,0.3809523809523809,0,0
275,"In qualitative analysis , we found the learned codes of similar words are very close in Hamming space .",CONCLUSION,CONCLUSION,machine-translation,9,9,0.4285714285714285,274,0.9547038327526132,9,0.4285714285714285,0,0
276,"As our approach maintains a dense basis matrix , it has the potential to be further compressed by applying pruning techniques to the dense matrix .",CONCLUSION,CONCLUSION,machine-translation,9,10,0.4761904761904761,275,0.9581881533101044,10,0.4761904761904761,0,0
277,The advantage of compositional coding approach will be more significant if the size of embedding layer is dominated by the hash codes .,CONCLUSION,CONCLUSION,machine-translation,9,11,0.5238095238095238,276,0.9616724738675958,11,0.5238095238095238,0,0
278,"Huei - Fang Yang , Kevin Lin , and Chu - Song Chen .",CONCLUSION,CONCLUSION,machine-translation,9,12,0.5714285714285714,277,0.9651567944250872,12,0.5714285714285714,0,0
279,Supervised learning of semantics - preserving hash via deep convolutional neural networks .,CONCLUSION,CONCLUSION,machine-translation,9,13,0.6190476190476191,278,0.9686411149825784,13,0.6190476190476191,0,0
280,"IEEE transactions on pattern analysis and machine intelligence , 2017 .",CONCLUSION,CONCLUSION,machine-translation,9,14,0.6666666666666666,279,0.9721254355400696,14,0.6666666666666666,0,0
281,"Xiaowei Zhang , Wei Chen , Feng Wang , Shuang Xu , and Bo Xu.",CONCLUSION,CONCLUSION,machine-translation,9,15,0.7142857142857143,280,0.975609756097561,15,0.7142857142857143,0,0
282,Towards compact and fast neural machine translation using a combined method .,CONCLUSION,CONCLUSION,machine-translation,9,16,0.7619047619047619,281,0.9790940766550522,16,0.7619047619047619,0,0
283,"In EMNLP , 2017 .",CONCLUSION,CONCLUSION,machine-translation,9,17,0.8095238095238095,282,0.9825783972125436,17,0.8095238095238095,0,0
284,"Aojun Zhou , Anbang Yao , Yiwen Guo , Lin Xu , and Yurong Chen .",CONCLUSION,CONCLUSION,machine-translation,9,18,0.8571428571428571,283,0.9860627177700348,18,0.8571428571428571,0,0
285,Incremental network quantization :,CONCLUSION,CONCLUSION,machine-translation,9,19,0.9047619047619048,284,0.989547038327526,19,0.9047619047619048,0,0
286,Towards lossless cnns with low - precision weights .,CONCLUSION,CONCLUSION,machine-translation,9,20,0.9523809523809524,285,0.9930313588850174,20,0.9523809523809524,0,0
287,"CoRR , abs /1702.03044 , 2017 .",CONCLUSION,CONCLUSION,machine-translation,9,21,1.0,286,0.9965156794425089,21,1.0,0,0
1,title,,,named-entity-recognition,0,0,0.0,0,0.0,0,0.0,1,0
2,"0,000 + Times Accelerated Robust Subset Selection ( ARSS )",title,title,named-entity-recognition,0,1,0.0,1,0.003690036900369,1,0.0,1,1
3,abstract,,,named-entity-recognition,0,0,0.0,2,0.007380073800738,0,0.0,1,0
4,Subset selection from massive data with noised information is increasingly popular for various applications .,abstract,abstract,named-entity-recognition,0,1,0.1428571428571428,3,0.011070110701107,1,0.1428571428571428,1,1
5,This problem is still highly challenging as current methods are generally slow in speed and sensitive to outliers .,abstract,abstract,named-entity-recognition,0,2,0.2857142857142857,4,0.014760147601476,2,0.2857142857142857,1,0
6,"To address the above two issues , we propose an accelerated robust subset selection ( ARSS ) method .",abstract,abstract,named-entity-recognition,0,3,0.4285714285714285,5,0.018450184501845,3,0.4285714285714285,1,0
7,"Specifically in the subset selection area , this is the first attempt to employ the p ( 0 < p ? 1 ) - norm based measure for the representation loss , preventing large errors from dominating our objective .",abstract,abstract,named-entity-recognition,0,4,0.5714285714285714,6,0.022140221402214,4,0.5714285714285714,1,1
8,"As a result , the robustness against outlier elements is greatly enhanced .",abstract,abstract,named-entity-recognition,0,5,0.7142857142857143,7,0.025830258302583,5,0.7142857142857143,1,0
9,"Actually , data size is generally much larger than feature length , i.e. N L. Based on this observation , we propose a speedup solver ( via ALM and equivalent derivations ) to highly reduce the computational cost , theoretically from ON 4 to ON 2 L .",abstract,abstract,named-entity-recognition,0,6,0.8571428571428571,8,0.029520295202952,6,0.8571428571428571,1,0
10,"Extensive experiments on ten benchmark datasets verify that our method not only outperforms state of the art methods , but also runs 10,000 + times faster than the most related method .",abstract,abstract,named-entity-recognition,0,7,1.0,9,0.033210332103321,7,1.0,1,0
11,Introduction,,,named-entity-recognition,0,0,0.0,10,0.03690036900369,0,0.0,1,0
12,"Due to the explosive growth of data , subset selection methods are increasingly popular fora wide range of machine learning and computer vision applications .",Introduction,Introduction,named-entity-recognition,0,1,0.010752688172043,11,0.040590405904059,1,0.0909090909090909,1,0
13,This kind of methods offer the potential to select a few highly representative samples or exemplars to describe the entire dataset .,Introduction,Introduction,named-entity-recognition,0,2,0.021505376344086,12,0.044280442804428,2,0.1818181818181818,1,0
14,"By analyzing a few , we can roughly know all .",Introduction,Introduction,named-entity-recognition,0,3,0.032258064516129,13,0.047970479704797,3,0.2727272727272727,1,0
15,"Such case is very important to summarize and visualize huge datasets of texts , images and videos etc . .",Introduction,Introduction,named-entity-recognition,0,4,0.043010752688172,14,0.051660516605166,4,0.3636363636363636,1,0
16,"Besides , by only using the selected exemplars for succeeding tasks , the cost of memories and computational time will be greatly reduced .",Introduction,Introduction,named-entity-recognition,0,5,0.053763440860215,15,0.055350553505535,5,0.4545454545454545,1,0
17,"Additionally , as outliers are generally less representative , the side effect of outliers will be reduced , thus boosting the performance of subsequent applications .",Introduction,Introduction,named-entity-recognition,0,6,0.064516129032258,16,0.059040590405904,6,0.5454545454545454,1,0
18,There have been several subset selection methods .,Introduction,Introduction,named-entity-recognition,0,7,0.075268817204301,17,0.062730627306273,7,0.6363636363636364,1,0
19,The most intuitional method is to randomly select a fixed number of samples .,Introduction,Introduction,named-entity-recognition,0,8,0.086021505376344,18,0.066420664206642,8,0.7272727272727273,1,0
20,"Although highly efficient , there is no guarantee for an effective selection .",Introduction,Introduction,named-entity-recognition,0,9,0.0967741935483871,19,0.070110701107011,9,0.8181818181818182,1,0
21,"For the other methods , depending on the mechanism of representative exemplars , there are mainly three categories of selection methods .",Introduction,Introduction,named-entity-recognition,0,10,0.1075268817204301,20,0.07380073800738,10,0.9090909090909092,1,0
22,One category Data size ( N ) Selection Time,Introduction,Introduction,named-entity-recognition,0,11,0.1182795698924731,21,0.077490774907749,11,1.0,1,0
23,Classifiers,Introduction,,named-entity-recognition,0,12,0.1290322580645161,22,0.081180811808118,0,0.0,1,0
24,Classification Accuracy Performance TED RRSSNie RRSSour ARSSour : Comparisons of four algorithms on Optdigit .,Introduction,Classifiers,named-entity-recognition,0,13,0.1397849462365591,23,0.084870848708487,1,0.0056497175141242,1,0
25,Two conclusions can be drawn .,Introduction,Classifiers,named-entity-recognition,0,14,0.1505376344086021,24,0.088560885608856,2,0.0112994350282485,1,0
26,"First , our method ( ARSSour ) is highly faster than all others ; with the help of an elegant new theorem , RRSSour is significantly faster than the authorial algorithm RRSSNie.",Introduction,Classifiers,named-entity-recognition,0,15,0.1612903225806451,25,0.092250922509225,3,0.0169491525423728,1,0
27,"Second , ARSSour achieves highly promising prediction accuracies .",Introduction,Classifiers,named-entity-recognition,0,16,0.1720430107526881,26,0.0959409594095941,4,0.0225988700564971,1,0
28,relies on the assumption that the data points lie in one or multiple low - dimensional subspaces .,Introduction,Classifiers,named-entity-recognition,0,17,0.1827956989247312,27,0.0996309963099631,5,0.0282485875706214,1,0
29,"Specifically , the Rank Revealing QR ( RRQR ) selects the subsets that give the best conditional sub-matrix .",Introduction,Classifiers,named-entity-recognition,0,18,0.1935483870967742,28,0.1033210332103321,6,0.0338983050847457,1,0
30,"Unfortunately , this method has suboptimal properties , as it is not assured to find the globally optimum in polynomial time .",Introduction,Classifiers,named-entity-recognition,0,19,0.2043010752688172,29,0.1070110701107011,7,0.03954802259887,1,0
31,Another category assumes that the samples are distributed around centers .,Introduction,Classifiers,named-entity-recognition,0,20,0.2150537634408602,30,0.1107011070110701,8,0.0451977401129943,1,0
32,The center or its nearest neighbour are selected as exemplars .,Introduction,Classifiers,named-entity-recognition,0,21,0.2258064516129032,31,0.1143911439114391,9,0.0508474576271186,1,0
33,"Perhaps , Kmeans and Kmedoids are the most typical methods ( Kmedoids is a variant of Kmeans ) .",Introduction,Classifiers,named-entity-recognition,0,22,0.2365591397849462,32,0.1180811808118081,10,0.0564971751412429,1,0
34,Both methods employ an EM - like algorithm .,Introduction,Classifiers,named-entity-recognition,0,23,0.2473118279569892,33,0.1217712177121771,11,0.0621468926553672,1,0
35,"Thus , the results depend tightly on the initialization , and they are highly unstable for large K ( i.e. the number of centers or selected samples ) .",Introduction,Classifiers,named-entity-recognition,0,24,0.2580645161290322,34,0.1254612546125461,12,0.0677966101694915,1,0
36,"Recently , there area few methods that assume exemplars are the samples that can best represent the whole dataset .",Introduction,Classifiers,named-entity-recognition,0,25,0.2688172043010752,35,0.1291512915129151,13,0.0734463276836158,1,0
37,"However , for , the optimization is a combinatorial problem ( NP - hard ) , which is computationally intractable to solve .",Introduction,Classifiers,named-entity-recognition,0,26,0.2795698924731182,36,0.1328413284132841,14,0.0790960451977401,1,0
38,"Besides , the representation loss is measured by the least square measure , which is sensitive to outliers in data .",Introduction,Classifiers,named-entity-recognition,0,27,0.2903225806451613,37,0.1365313653136531,15,0.0847457627118644,1,0
39,"Then improves by employing a robust loss via the 2 , 1 - norm ; the 1 - norm is applied to samples , and the 2 - norm is used for features .",Introduction,Classifiers,named-entity-recognition,0,28,0.3010752688172043,38,0.1402214022140221,16,0.0903954802259887,1,0
40,"In this way , the side effect of outlier samples is relieved .",Introduction,Classifiers,named-entity-recognition,0,29,0.3118279569892473,39,0.1439114391143911,17,0.096045197740113,1,0
41,The solver of ) is theoretically perfect due to its ability of convergence to global optima .,Introduction,Classifiers,named-entity-recognition,0,30,0.3225806451612903,40,0.1476014760147601,18,0.1016949152542373,1,0
42,"Unfortunately , in terms of computational costs , the solver is highly complex .",Introduction,Classifiers,named-entity-recognition,0,31,0.3333333333333333,41,0.1512915129151291,19,0.1073446327683615,1,0
43,It takes ON 4 for one iteration as shown in .,Introduction,Classifiers,named-entity-recognition,0,32,0.3440860215053763,42,0.1549815498154981,20,0.1129943502824858,1,0
44,This is infeasible for the case of large N ( e.g. it takes 2000 + hours fora case of N = 13000 ) .,Introduction,Classifiers,named-entity-recognition,0,33,0.3548387096774194,43,0.1586715867158671,21,0.1186440677966101,1,0
45,"Moreover , the representation loss is only robust against outlier samples .",Introduction,Classifiers,named-entity-recognition,0,34,0.3655913978494624,44,0.1623616236162361,22,0.1242937853107344,1,0
46,"Such case is worth improvement , as there may exist outlier elements in real data .",Introduction,Classifiers,named-entity-recognition,0,35,0.3763440860215054,45,0.1660516605166051,23,0.1299435028248587,1,0
47,Contributions .,Introduction,,named-entity-recognition,0,36,0.3870967741935484,46,0.1697416974169741,24,0.135593220338983,1,0
48,"In this paper , we propose an accelerated robust subset selection method to highly raise the speed on the one hand , and to boost the robustness on the other .",Introduction,Contributions .,named-entity-recognition,0,37,0.3978494623655914,47,0.1734317343173431,25,0.1412429378531073,1,1
49,"To this end , we use the p ( 0 < p ? 1 ) - norm based robust measure for the representation loss , preventing large errors from dominating our objective .",Introduction,Contributions .,named-entity-recognition,0,38,0.4086021505376344,48,0.1771217712177121,26,0.1468926553672316,1,1
50,"As a result , the robustness against outliers is greatly boosted .",Introduction,Contributions .,named-entity-recognition,0,39,0.4193548387096774,49,0.1808118081180811,27,0.1525423728813559,1,0
51,"Then , based on the observation that data size is generally much larger than feature length , i.e. N L , we propose a speedup solver .",Introduction,Contributions .,named-entity-recognition,0,40,0.4301075268817204,50,0.1845018450184501,28,0.1581920903954802,1,1
52,The main acceleration is owing to the Augmented Lagrange Multiplier ( ALM ) and an equivalent derivation .,Introduction,Contributions .,named-entity-recognition,0,41,0.4408602150537634,51,0.1881918819188192,29,0.1638418079096045,1,1
53,"Via them , we reduce the computational complexity from ON 4 to ON 2 L .",Introduction,Contributions .,named-entity-recognition,0,42,0.4516129032258064,52,0.1918819188191882,30,0.1694915254237288,1,0
54,"Extensive results on ten benchmark datasets demonstrate that in average , our method is 10,000 + times faster than Nie 's method .",Introduction,Contributions .,named-entity-recognition,0,43,0.4623655913978494,53,0.1955719557195571,31,0.1751412429378531,1,0
55,The selection quality is highly encouraging as shown in .,Introduction,Contributions .,named-entity-recognition,0,44,0.4731182795698925,54,0.1992619926199262,32,0.1807909604519774,1,0
56,"Additionally , via another equivalent derivation , we give an accelerated solver for Nie 's method , theoretically reducing the computational complexity from ON 4 to ON 2 L + N L 3 as listed in , empirically obtaining a 500 + times speedup compared with the authorial solver .",Introduction,Contributions .,named-entity-recognition,0,45,0.4838709677419355,55,0.2029520295202952,33,0.1864406779661017,1,0
57,Notations .,Introduction,,named-entity-recognition,0,46,0.4946236559139785,56,0.2066420664206642,34,0.192090395480226,1,0
58,We use boldface uppercase letters to denote matrices and boldface lowercase letters to represent vectors .,Introduction,Notations .,named-entity-recognition,0,47,0.5053763440860215,57,0.2103321033210332,35,0.1977401129943503,1,0
59,For a matrix Y = [ Y ln ] ?,Introduction,Notations .,named-entity-recognition,0,48,0.5161290322580645,58,0.2140221402214022,36,0.2033898305084746,1,0
60,"R LN , we denote it s l throw and n th column as y land y n respectively .",Introduction,Notations .,named-entity-recognition,0,49,0.5268817204301075,59,0.2177121771217712,37,0.2090395480225988,1,0
61,"The 2 ,1 - norm of a matrix is defined as",Introduction,Notations .,named-entity-recognition,0,50,0.5376344086021505,60,0.2214022140221402,38,0.2146892655367231,1,0
62,Subset Selection via Self - Representation,Introduction,,named-entity-recognition,0,51,0.5483870967741935,61,0.2250922509225092,39,0.2203389830508474,1,0
63,"In the problem of subset selection , we are often given a set of N unlabelled points X = x 1 , x 2 , , x N | x n ?",Introduction,Subset Selection via Self - Representation,named-entity-recognition,0,52,0.5591397849462365,62,0.2287822878228782,40,0.2259887005649717,1,0
64,"R L , where L is the feature length .",Introduction,Subset Selection via Self - Representation,named-entity-recognition,0,53,0.5698924731182796,63,0.2324723247232472,41,0.231638418079096,1,0
65,The goal is to select the top K ( K N ) most representative and informative samples ( i.e. exemplars ) to effectively describe the entire dataset X .,Introduction,Subset Selection via Self - Representation,named-entity-recognition,0,54,0.5806451612903226,64,0.2361623616236162,42,0.2372881355932203,1,0
66,"By solely using these K exemplars for subsequent tasks , we could greatly reduce the computational costs and largely alleviate the side effects of outlier elements in data .",Introduction,Subset Selection via Self - Representation,named-entity-recognition,0,55,0.5913978494623656,65,0.2398523985239852,43,0.2429378531073446,1,0
67,Such a motivation could be formulated as the Transductive Experimental Design ( TED ) model :,Introduction,Subset Selection via Self - Representation,named-entity-recognition,0,56,0.6021505376344086,66,0.2435424354243542,44,0.2485875706214689,1,0
68,where Q ?,Introduction,Subset Selection via Self - Representation,named-entity-recognition,0,57,0.6129032258064516,67,0.2472324723247232,45,0.2542372881355932,1,0
69,"R LK is the selected subset matrix , whose column vectors all come from X , i.e. q k ?",Introduction,Subset Selection via Self - Representation,named-entity-recognition,0,58,0.6236559139784946,68,0.2509225092250922,46,0.2598870056497175,1,0
70,"X , ?k ?",Introduction,Subset Selection via Self - Representation,named-entity-recognition,0,59,0.6344086021505376,69,0.2546125461254612,47,0.2655367231638418,1,0
71,"{ 1 , , K} ; A = [ a 1 , , a N ] ?",Introduction,Subset Selection via Self - Representation,named-entity-recognition,0,60,0.6451612903225806,70,0.2583025830258302,48,0.2711864406779661,1,0
72,R KN is the corresponding linear combination coefficients .,Introduction,Subset Selection via Self - Representation,named-entity-recognition,0,61,0.6559139784946236,71,0.2619926199261992,49,0.2768361581920904,1,0
73,"By minimizing ( 1 ) , TED could select the highly informative and representative samples , as they have to well represent all the samples in X .",Introduction,Subset Selection via Self - Representation,named-entity-recognition,0,62,0.6666666666666666,72,0.2656826568265683,50,0.2824858757062147,1,0
74,"Although TED ( 1 ) is well modeled - very accurate and intuitive , there are two bottlenecks .",Introduction,Subset Selection via Self - Representation,named-entity-recognition,0,63,0.6774193548387096,73,0.2693726937269373,51,0.288135593220339,1,0
75,"First , the objective is a combinatorial optimization problem .",Introduction,Subset Selection via Self - Representation,named-entity-recognition,0,64,0.6881720430107527,74,0.2730627306273063,52,0.2937853107344633,1,0
76,It is NP - hard to exhaustively search the optimal subset Q from X .,Introduction,Subset Selection via Self - Representation,named-entity-recognition,0,65,0.6989247311827957,75,0.2767527675276753,53,0.2994350282485876,1,0
77,"For this reason , the author approximate ( 1 ) via a sequential optimization problem , which is solved by an inefficient greedy optimization algorithm .",Introduction,Subset Selection via Self - Representation,named-entity-recognition,0,66,0.7096774193548387,76,0.2804428044280442,54,0.3050847457627119,1,0
78,"Second , similar to the existing least square loss based models in machine learning and statistics , ( 1 ) is sensitive to the presence of outliers",Introduction,Subset Selection via Self - Representation,named-entity-recognition,0,67,0.7204301075268817,77,0.2841328413284132,55,0.3107344632768362,1,0
79,where ?,Introduction,Subset Selection via Self - Representation,named-entity-recognition,0,68,0.7311827956989247,78,0.2878228782287823,56,0.3163841807909605,1,0
80,"is a nonnegative parameter ; A is constrained to be row - sparse , and thus to select the most representative and informative samples .",Introduction,Subset Selection via Self - Representation,named-entity-recognition,0,69,0.7419354838709677,79,0.2915129151291513,57,0.3220338983050847,1,0
81,"As the representation loss is accumulated via the 1 - norm among samples , compared with ( 1 ) , the robustness against outlier samples is enhanced .",Introduction,Subset Selection via Self - Representation,named-entity-recognition,0,70,0.7526881720430108,80,0.2952029520295203,58,0.327683615819209,1,0
82,"Equivalently , ( 2 ) is rewritten in the matrix format :",Introduction,Subset Selection via Self - Representation,named-entity-recognition,0,71,0.7634408602150538,81,0.2988929889298893,59,0.3333333333333333,1,0
83,"Since the objective is convex in A , the global minimum maybe found by differentiating ( 3 ) and setting the derivative to zero , resulting in a linear system 1",Introduction,Subset Selection via Self - Representation,named-entity-recognition,0,72,0.7741935483870968,82,0.3025830258302583,60,0.3389830508474576,1,0
84,where V ?,Introduction,Subset Selection via Self - Representation,named-entity-recognition,0,73,0.7849462365591398,83,0.3062730627306273,61,0.3446327683615819,1,0
85,RN,Introduction,,named-entity-recognition,0,74,0.7956989247311828,84,0.3099630996309963,62,0.3502824858757062,1,0
86,N is a diagonal matrix with then th diagonal entry as V nn = 1 2 an 2 and U nn = 1 2 xn ?,Introduction,RN,named-entity-recognition,0,75,0.8064516129032258,85,0.3136531365313653,63,0.3559322033898305,1,0
87,Xan 2 .,Introduction,RN,named-entity-recognition,0,76,0.8172043010752689,86,0.3173431734317343,64,0.3615819209039548,1,0
88,"It seems perfect to use ( 4 ) to solve the objective ( 3 ) , because ( 4 ) looks simple and the global optimum is theoretically guaranteed .",Introduction,RN,named-entity-recognition,0,77,0.8279569892473119,87,0.3210332103321033,65,0.3672316384180791,1,0
89,"Unfortunately , in terms of speed , ( 4 ) is usually infeasible due to the incredible computational demand in the case of large N ( the number of samples ) .",Introduction,RN,named-entity-recognition,0,78,0.8387096774193549,88,0.3247232472324723,66,0.3728813559322034,1,0
90,"At each iteration , the computational complexity of ( 4 ) is up to ON 4 , as analyzed in Remark 1 .",Introduction,RN,named-entity-recognition,0,79,0.8494623655913979,89,0.3284132841328413,67,0.3785310734463277,1,0
91,"According to our experiments , the time cost is up to 2088 hours ( i.e. 87 days ) fora subset selection problem of 13000 samples .",Introduction,RN,named-entity-recognition,0,80,0.8602150537634409,90,0.3321033210332103,68,0.384180790960452,1,0
92,Remark,Introduction,,named-entity-recognition,0,81,0.8709677419354839,91,0.3357933579335793,69,0.3898305084745763,1,0
93,1 . Since U nn X T X +? V ?,Introduction,Remark,named-entity-recognition,0,82,0.8817204301075269,92,0.3394833948339483,70,0.3954802259887006,1,0
94,"RN N , the major computational cost of ( 4 ) focuses on a N N linear system .",Introduction,Remark,named-entity-recognition,0,83,0.8924731182795699,93,0.3431734317343173,71,0.4011299435028249,1,0
95,"If solved by the Cholesky factorization method , it costs 1 3 N 3 for factorization as well as 2N 2 for forward and backward substitution .",Introduction,Remark,named-entity-recognition,0,84,0.9032258064516128,94,0.3468634686346863,72,0.4067796610169492,1,0
96,This amounts to ON 3 in total .,Introduction,Remark,named-entity-recognition,0,85,0.913978494623656,95,0.3505535055350554,73,0.4124293785310734,1,0
97,"By now , we only solve an .",Introduction,Remark,named-entity-recognition,0,86,0.9247311827956988,96,0.3542435424354243,74,0.4180790960451977,1,0
98,"Once solving all the set of {a n } N n= 1 , the total complexity amounts to ON 4 for one iteration step .",Introduction,Remark,named-entity-recognition,0,87,0.935483870967742,97,0.3579335793357933,75,0.423728813559322,1,0
99,Accelerated Robust Subset Selection ( ARSS ),Introduction,Remark,named-entity-recognition,0,88,0.946236559139785,98,0.3616236162361623,76,0.4293785310734463,1,0
100,"Due to the huge computational costs , Nie 's method is infeasible for the case of large N - the computational time is up to 2088 hours fora case of 13000 samples .",Introduction,Remark,named-entity-recognition,0,89,0.956989247311828,99,0.3653136531365313,77,0.4350282485875706,1,0
101,"Besides , Nie 's model imposes the 2 - norm among features , which is prone to outliers in features .",Introduction,Remark,named-entity-recognition,0,90,0.967741935483871,100,0.3690036900369003,78,0.4406779661016949,1,0
102,"To tackle the above two issues , we propose a more robust model in the p ( 0 < p ? 1 ) norm .",Introduction,Remark,named-entity-recognition,0,91,0.978494623655914,101,0.3726937269372694,79,0.4463276836158192,1,0
103,"Although the resulted objective is challenging to solve , a speedup algorithm is proposed to dramatically save the computational costs .",Introduction,Remark,named-entity-recognition,0,92,0.989247311827957,102,0.3763837638376384,80,0.4519774011299435,1,0
104,"For the same task of N = 13000 , it costs our method 1.8 minutes , achieving a 68429 times acceleration compared with the speed of Nie 's method .",Introduction,Remark,named-entity-recognition,0,93,1.0,103,0.3800738007380074,81,0.4576271186440678,1,0
105,Modeling .,,,named-entity-recognition,0,0,0.0,104,0.3837638376383764,82,0.4632768361581921,1,0
106,"To boost the robustness against outliers in both samples and features , we formulate the discrepancy between X and XA via the p ( 0 < p < 1 ) - norm .",Modeling .,Modeling .,named-entity-recognition,0,1,0.0105263157894736,105,0.3874538745387454,83,0.4689265536723164,1,0
107,"There are theoretical and empirical evidences to verify that compared with 2 or 1 norms , the p - norm is more able to prevent outlier elements from dominating the objective , enhancing the robustness ) .",Modeling .,Modeling .,named-entity-recognition,0,2,0.0210526315789473,106,0.3911439114391143,84,0.4745762711864407,1,0
108,"Thus , we have the following objective min",Modeling .,Modeling .,named-entity-recognition,0,3,0.031578947368421,107,0.3948339483394834,85,0.480225988700565,1,0
109,where ?,Modeling .,Modeling .,named-entity-recognition,0,4,0.0421052631578947,108,0.3985239852398524,86,0.4858757062146893,1,0
110,"is a balancing parameter ; A is a row sparse matrix , used to select the most informative and representative samples .",Modeling .,Modeling .,named-entity-recognition,0,5,0.0526315789473684,109,0.4022140221402214,87,0.4915254237288136,1,0
111,"By minimizing the energy of ( 5 ) , we could capture the most essential properties of the dataset X.",Modeling .,Modeling .,named-entity-recognition,0,6,0.0631578947368421,110,0.4059040590405904,88,0.4971751412429379,1,0
112,"After obtaining the optimal A , the row indexes are sorted by the row - sum value of the absolute A in decreasing order .",Modeling .,Modeling .,named-entity-recognition,0,7,0.0736842105263157,111,0.4095940959409594,89,0.5028248587570622,1,0
113,The samples specified by the top K indexes are selected as exemplars .,Modeling .,Modeling .,named-entity-recognition,0,8,0.0842105263157894,112,0.4132841328413284,90,0.5084745762711864,1,0
114,Note that the model ( 5 ) could be applied to the unsupervised feature selection problem by only transposing the data matrix X .,Modeling .,Modeling .,named-entity-recognition,0,9,0.0947368421052631,113,0.4169741697416974,91,0.5141242937853108,1,0
115,"In this case , A is a L L row sparse matrix , used to select the most representative features .",Modeling .,Modeling .,named-entity-recognition,0,10,0.1052631578947368,114,0.4206642066420664,92,0.519774011299435,1,0
116,"Accelerated Solver for the ARSS Objective in Although objective ( 5 ) is challenging to solve , we propose an effective and highly efficient solver .",Modeling .,Modeling .,named-entity-recognition,0,11,0.1157894736842105,115,0.4243542435424354,93,0.5254237288135594,1,0
117,The acceleration owes to the ALM and an equivalent derivation .,Modeling .,Modeling .,named-entity-recognition,0,12,0.1263157894736842,116,0.4280442804428044,94,0.5310734463276836,1,0
118,ALM,Modeling .,,named-entity-recognition,0,13,0.1368421052631579,117,0.4317343173431734,95,0.536723163841808,1,0
119,"The most intractable challenge of ( 5 ) is that , the p ( 0 < p ? 1 ) - norm is non-convex , non-smooth and notdifferentiable at the zero point .",Modeling .,ALM,named-entity-recognition,0,14,0.1473684210526315,118,0.4354243542435424,96,0.5423728813559322,1,0
120,"Therefore , it is beneficial to use the Augmented Lagrangian Method ( ALM ) to solve ( 5 ) , resulting in several easily tackled unconstrained subproblems .",Modeling .,ALM,named-entity-recognition,0,15,0.1578947368421052,119,0.4391143911439114,97,0.5480225988700564,1,0
121,"By solving them iteratively , the solutions of subproblems could eventually converge to a minimum .",Modeling .,ALM,named-entity-recognition,0,16,0.1684210526315789,120,0.4428044280442804,98,0.5536723163841808,1,0
122,"Specifically , we introduce an auxiliary variable E = X ? XA ?",Modeling .,ALM,named-entity-recognition,0,17,0.1789473684210526,121,0.4464944649446494,99,0.559322033898305,1,0
123,R LN .,Modeling .,ALM,named-entity-recognition,0,18,0.1894736842105263,122,0.4501845018450184,100,0.5649717514124294,1,0
124,"Thus , the objective ( 5 ) becomes : min",Modeling .,ALM,named-entity-recognition,0,19,0.2,123,0.4538745387453874,101,0.5706214689265536,1,0
125,"To deal with the equality constraint in , the most convenient method is to add a penalty , resulting in",Modeling .,ALM,named-entity-recognition,0,20,0.2105263157894736,124,0.4575645756457565,102,0.576271186440678,1,0
126,where is a penalty parameter .,Modeling .,ALM,named-entity-recognition,0,21,0.2210526315789473,125,0.4612546125461255,103,0.5819209039548022,1,0
127,"To guarantee the equality constraint , it requires approaching infinity , which may cause bad numerical conditions .",Modeling .,ALM,named-entity-recognition,0,22,0.231578947368421,126,0.4649446494464944,104,0.5875706214689266,1,0
128,"Instead , once introducing a Lagrangian multiplier , it is no longer requiring ? ?.",Modeling .,ALM,named-entity-recognition,0,23,0.2421052631578947,127,0.4686346863468634,105,0.5932203389830508,1,0
129,"Thus , we rewrite into the standard ALM formulation as :",Modeling .,ALM,named-entity-recognition,0,24,0.2526315789473684,128,0.4723247232472324,106,0.5988700564971752,1,0
130,where ?,Modeling .,ALM,named-entity-recognition,0,25,0.2631578947368421,129,0.4760147601476014,107,0.6045197740112994,1,0
131,consists of L N Lagrangian multipliers .,Modeling .,ALM,named-entity-recognition,0,26,0.2736842105263158,130,0.4797047970479705,108,0.6101694915254238,1,0
132,"In the following , a highly efficient solver will be given .",Modeling .,ALM,named-entity-recognition,0,27,0.2842105263157894,131,0.4833948339483395,109,0.615819209039548,1,0
133,The updating rule for ?,Modeling .,ALM,named-entity-recognition,0,28,0.2947368421052631,132,0.4870848708487085,110,0.6214689265536724,1,0
134,"Similar to the iterative thresholding ( IT ) in , the degree of violations of the L N equality constraints are used to update the Lagrangian multiplier :",Modeling .,ALM,named-entity-recognition,0,29,0.3052631578947368,133,0.4907749077490775,111,0.6271186440677966,1,0
135,where is a monotonically increasing parameter over iteration steps .,Modeling .,ALM,named-entity-recognition,0,30,0.3157894736842105,134,0.4944649446494465,112,0.632768361581921,1,0
136,"For example , ? ? , where 1 < ? < 2 is a predefined parameter .",Modeling .,ALM,named-entity-recognition,0,31,0.3263157894736842,135,0.4981549815498155,113,0.6384180790960452,1,0
137,"Efficient solver for E Removing irrelevant terms with E from ( 8 ) , we have",Modeling .,ALM,named-entity-recognition,0,32,0.3368421052631579,136,0.5018450184501845,114,0.6440677966101694,1,0
138,where H = X ? XA ? ? ?,Modeling .,ALM,named-entity-recognition,0,33,0.3473684210526316,137,0.5055350553505535,115,0.6497175141242938,1,0
139,R LN .,Modeling .,ALM,named-entity-recognition,0,34,0.3578947368421052,138,0.5092250922509225,116,0.655367231638418,1,0
140,"According to the definition of the p - norm and the Frobenius - norm , ( 10 ) could be decoupled into L N independent and unconstrained subproblems .",Modeling .,ALM,named-entity-recognition,0,35,0.3684210526315789,139,0.5129151291512916,117,0.6610169491525424,1,0
141,The standard form of these subproblems is,Modeling .,ALM,named-entity-recognition,0,36,0.3789473684210526,140,0.5166051660516605,118,0.6666666666666666,1,0
142,"where ? = 1 is a given positive parameter , y is the scalar variable need to deal with , c is a known scalar constant .",Modeling .,ALM,named-entity-recognition,0,37,0.3894736842105263,141,0.5202952029520295,119,0.672316384180791,1,0
143,Zuo et al. ) has recently proposed a generalized iterative shrinkage algorithm to solve ( 11 ) .,Modeling .,ALM,named-entity-recognition,0,38,0.4,142,0.5239852398523985,120,0.6779661016949152,1,0
144,This algorithm is easy to implement and able to achieve more accurate solutions than current methods .,Modeling .,ALM,named-entity-recognition,0,39,0.4105263157894737,143,0.5276752767527675,121,0.6836158192090396,1,0
145,"Thus , we use it for our problem as :",Modeling .,ALM,named-entity-recognition,0,40,0.4210526315789473,144,0.5313653136531366,122,0.6892655367231638,1,0
146,is obtained by solving the following equation :,Modeling .,ALM,named-entity-recognition,0,41,0.431578947368421,145,0.5350553505535055,123,0.6949152542372882,1,0
147,which could be solved efficiently via an iterative algorithm .,Modeling .,ALM,named-entity-recognition,0,42,0.4421052631578947,146,0.5387453874538746,124,0.7005649717514124,1,0
148,"In this manner , ( 10 ) could be sovled extremely fast .",Modeling .,ALM,named-entity-recognition,0,43,0.4526315789473684,147,0.5424354243542435,125,0.7062146892655368,1,0
149,"Accelerated solver for A The main acceleration focuses on the solver of A. Removing irrelevant terms with A from ( 8 ) , we have",Modeling .,ALM,named-entity-recognition,0,44,0.4631578947368421,148,0.5461254612546126,126,0.711864406779661,1,0
150,where ? = ?,Modeling .,ALM,named-entity-recognition,0,45,0.4736842105263157,149,0.5498154981549815,127,0.7175141242937854,1,0
151,"is a nonnegative parameter , P = X ? E ? ? ?",Modeling .,ALM,named-entity-recognition,0,46,0.4842105263157895,150,0.5535055350553506,128,0.7231638418079096,1,0
152,"R LN . Since ( 13 ) is convex in A , the optimum could be found by differentiating ( 13 ) and setting the derivative to zero .",Modeling .,ALM,named-entity-recognition,0,47,0.4947368421052631,151,0.5571955719557196,129,0.7288135593220338,1,0
153,This amounts to tackling the following linear system 2 :,Modeling .,ALM,named-entity-recognition,0,48,0.5052631578947369,152,0.5608856088560885,130,0.7344632768361582,1,0
154,As V + ? X TX ?,Modeling .,ALM,named-entity-recognition,0,49,0.5157894736842106,153,0.5645756457564576,131,0.7401129943502824,1,0
155,"RN N , is mainly a N N linear system .",Modeling .,ALM,named-entity-recognition,0,50,0.5263157894736842,154,0.5682656826568265,132,0.7457627118644068,1,0
156,"Once solved by the Cholesky factorization , the computational complexity is highly up to ON 3 .",Modeling .,ALM,named-entity-recognition,0,51,0.5368421052631579,155,0.5719557195571956,133,0.751412429378531,1,0
157,This is by no means a good choice for real applications with large N .,Modeling .,ALM,named-entity-recognition,0,52,0.5473684210526316,156,0.5756457564575646,134,0.7570621468926554,1,0
158,"In the following , an equivalent derivation of ( 14 ) will be proposed to significantly save the computational complexity .",Modeling .,ALM,named-entity-recognition,0,53,0.5578947368421052,157,0.5793357933579336,135,0.7627118644067796,1,0
159,Theorem,Modeling .,,named-entity-recognition,0,54,0.5684210526315789,158,0.5830258302583026,136,0.768361581920904,1,0
160,2 . The N N linear system is equivalent to the following L L linear system :,Modeling .,Theorem,named-entity-recognition,0,55,0.5789473684210527,159,0.5867158671586716,137,0.7740112994350282,1,0
161,where IL is a L L identity matrix .,Modeling .,Theorem,named-entity-recognition,0,56,0.5894736842105263,160,0.5904059040590406,138,0.7796610169491526,1,0
162,Proof .,Modeling .,,named-entity-recognition,0,57,0.6,161,0.5940959409594095,139,0.7853107344632768,1,0
163,"Note that V is a N N diagonal and positive - definite matrix , the exponent of V is efficient to achieve , i.e. V ? = { V ? nn } N n=1 , ?? ?",Modeling .,Proof .,named-entity-recognition,0,58,0.6105263157894737,162,0.5977859778597786,140,0.7909604519774012,1,0
164,R.,Modeling .,,named-entity-recognition,0,59,0.6210526315789474,163,0.6014760147601476,141,0.7966101694915254,1,0
165,We have the following equations,Modeling .,R.,named-entity-recognition,0,60,0.631578947368421,164,0.6051660516605166,142,0.8022598870056498,1,0
166,"where Z = XV ? 1 2 , IN is a N N identity matrix .",Modeling .,R.,named-entity-recognition,0,61,0.6421052631578947,165,0.6088560885608856,143,0.807909604519774,1,0
167,The following equation holds for any conditions,Modeling .,R.,named-entity-recognition,0,62,0.6526315789473685,166,0.6125461254612546,144,0.8135593220338984,1,0
168,"Multiplying with IN + ?Z T Z ?1 on the left and IL + ? ZZ T ? 1 on the right of both sides of the equal - sign , we have the equation as :",Modeling .,R.,named-entity-recognition,0,63,0.6631578947368421,167,0.6162361623616236,145,0.8192090395480226,1,0
169,"Therefore , substituting ( 18 ) and Z = XV ? 1 2 into ( 16 ) , we have the simplified updating rule as :",Modeling .,R.,named-entity-recognition,0,64,0.6736842105263158,168,0.6199261992619927,146,0.8248587570621468,1,0
170,"When N L , the most complex operation is the matrix multiplications , not the L L linear system .",Modeling .,R.,named-entity-recognition,0,65,0.6842105263157895,169,0.6236162361623616,147,0.8305084745762712,1,0
171,Corollary,Modeling .,,named-entity-recognition,0,66,0.6947368421052632,170,0.6273062730627307,148,0.8361581920903954,1,0
172,3 . We have two equivalent updating rules and ( 15 ) for the objective ( 13 ) .,Modeling .,Corollary,named-entity-recognition,0,67,0.7052631578947368,171,0.6309963099630996,149,0.8418079096045198,1,0
173,If using ( 14 ) when N ?,Modeling .,Corollary,named-entity-recognition,0,68,0.7157894736842105,172,0.6346863468634686,150,0.847457627118644,1,0
174,"L , and otherwise using ( 15 ) as shown in Algorithm 1 , the computational complexity of solvers for ( 13 ) is ON 2 L .",Modeling .,Corollary,named-entity-recognition,0,69,0.7263157894736842,173,0.6383763837638377,151,0.8531073446327684,1,0
175,"Due to N L , we have highly reduced the complexity from ON 4 to ON 2 L compared with Nie 's method .",Modeling .,Corollary,named-entity-recognition,0,70,0.7368421052631579,174,0.6420664206642066,152,0.8587570621468926,1,0
176,"Algorithm 1 for ( 13 ) : A * = ARSS A ( X , V , P , IL , ?)",Modeling .,Corollary,named-entity-recognition,0,71,0.7473684210526316,175,0.6457564575645757,153,0.864406779661017,1,0
177,"Input : X , V , P , IL , ? 1 : if N ?",Modeling .,Corollary,named-entity-recognition,0,72,0.7578947368421053,176,0.6494464944649446,154,0.8700564971751412,1,0
178,L then 2 :,Modeling .,Corollary,named-entity-recognition,0,73,0.7684210526315789,177,0.6531365313653137,155,0.8757062146892656,1,0
179,"update A via the updating rule , that is 3 : update ? by the updating rule ( 9 ) , ? ?.",Modeling .,Corollary,named-entity-recognition,0,74,0.7789473684210526,178,0.6568265682656826,156,0.8813559322033898,1,0
180,8 : until convergence,Modeling .,Corollary,named-entity-recognition,0,75,0.7894736842105263,179,0.6605166051660517,157,0.8870056497175142,1,0
181,Output : A The solver to update A is given in Algorithm,Modeling .,Corollary,named-entity-recognition,0,76,0.8,180,0.6642066420664207,158,0.8926553672316384,1,0
182,1 . The overall solver for our model ( 5 ) is summarized in Algorithm,Modeling .,Corollary,named-entity-recognition,0,77,0.8105263157894737,181,0.6678966789667896,159,0.8983050847457628,1,0
183,2 .,Modeling .,Corollary,named-entity-recognition,0,78,0.8210526315789474,182,0.6715867158671587,160,0.903954802259887,1,0
184,"According to Theorem 2 and Corollary 3 , the solver for our model ( 13 ) is highly simplified , as feature length is generally much smaller than data size , i.e L N .",Modeling .,Corollary,named-entity-recognition,0,79,0.8315789473684211,183,0.6752767527675276,161,0.9096045197740112,1,0
185,"Similarly ,",Modeling .,,named-entity-recognition,0,80,0.8421052631578947,184,0.6789667896678967,162,0.9152542372881356,1,0
186,"Nie 's method could be highly accelerated by Theorem 4 , obtaining 500 + times speedup , as shown in and .",Modeling .,"Similarly ,",named-entity-recognition,0,81,0.8526315789473684,185,0.6826568265682657,163,0.9209039548022598,1,0
187,Theorem,Modeling .,,named-entity-recognition,0,82,0.8631578947368421,186,0.6863468634686347,164,0.9265536723163842,1,0
188,4 . Nie 's N N solver ( 20 ) ) is equivalent to the following L L linear system ( 21 ) an = U nn U nn X TX + ? V,Modeling .,Theorem,named-entity-recognition,0,83,0.8736842105263158,187,0.6900369003690037,165,0.9322033898305084,1,0
189,?1 X T x n ( 20 ),Modeling .,Theorem,named-entity-recognition,0,84,0.8842105263157894,188,0.6937269372693727,166,0.9378531073446328,1,0
190,"?n ? { 1 , 2 , , N } , where IL is a L L identity matrix .",Modeling .,Theorem,named-entity-recognition,0,85,0.8947368421052632,189,0.6974169741697417,167,0.943502824858757,1,0
191,Proof .,Modeling .,,named-entity-recognition,0,86,0.9052631578947368,190,0.7011070110701108,168,0.9491525423728814,1,0
192,"Based on ( 20 ) , we have the following equalities :",Modeling .,Proof .,named-entity-recognition,0,87,0.9157894736842104,191,0.7047970479704797,169,0.9548022598870056,1,0
193,The derivations are equivalent ; their results are equal .,Modeling .,Proof .,named-entity-recognition,0,88,0.9263157894736842,192,0.7084870848708487,170,0.96045197740113,1,0
194,2 V ?,Modeling .,Proof .,named-entity-recognition,0,89,0.936842105263158,193,0.7121771217712177,171,0.9661016949152542,1,0
195,RN,Modeling .,,named-entity-recognition,0,90,0.9473684210526316,194,0.7158671586715867,172,0.9717514124293786,1,0
196,"N is a positive and diagonal matrix with then th diagonal entry as Vnn = 1 ? an 2 2 + > 0 , where is a small value to avoid singular failures Corollary 5 .",Modeling .,RN,named-entity-recognition,0,91,0.9578947368421052,195,0.7195571955719557,173,0.9774011299435028,1,0
197,"Since feature length is generally much smaller than data size , i.e. L N , our accelerated solver ( 20 ) for Nie 's model ( 3 ) is highly faster than the authorial solver ( 21 ) .",Modeling .,RN,named-entity-recognition,0,92,0.968421052631579,196,0.7232472324723247,174,0.9830508474576272,1,0
198,"Theoretically , we reduce the computational complexity from ON 4 to ON 2 L + N L 3 , while maintaining the same solution .",Modeling .,RN,named-entity-recognition,0,93,0.9789473684210528,197,0.7269372693726938,175,0.9887005649717514,1,0
199,"That is , like Nie 's solver ( 20 ) , our speedup solver ( 21 ) can reach the global optimum .",Modeling .,RN,named-entity-recognition,0,94,0.9894736842105264,198,0.7306273062730627,176,0.9943502824858758,1,0
200,Extensive empirical results will verify the huge acceleration,Modeling .,RN,named-entity-recognition,0,95,1.0,199,0.7343173431734318,177,1.0,1,0
201,Experiments Experimental Settings,,,named-entity-recognition,0,0,0.0,200,0.7380073800738007,0,0.0,1,0
202,"In this part , the experimental settings are introduced .",Experiments Experimental Settings,Experiments Experimental Settings,named-entity-recognition,0,1,0.0172413793103448,201,0.7416974169741697,1,0.125,1,0
203,"All experiments are conducted on a server with 64 - core Intel Xeon E7-4820 @ 2.00 GHz , 18 Mb Cache and 0.986 TB RAM , using Matlab 2012 .",Experiments Experimental Settings,Experiments Experimental Settings,named-entity-recognition,0,2,0.0344827586206896,202,0.7453874538745388,2,0.25,1,1
204,"Brief descriptions often benchmark datasets are summarized in , where ' Total ( N * ) ' denotes the total set of samples in each data .",Experiments Experimental Settings,Experiments Experimental Settings,named-entity-recognition,0,3,0.0517241379310344,203,0.7490774907749077,3,0.375,1,1
205,"Due to the high computational complexity , other methods can only handle small datasets ( while our method can handle the total set ) .",Experiments Experimental Settings,Experiments Experimental Settings,named-entity-recognition,0,4,0.0689655172413793,204,0.7527675276752768,4,0.5,1,0
206,"Thus , we randomly choose the candidate set from the total set to reduce the sample size , i.e. N < N * ( cf. ' Total ( N * ) ' and ' candid . ( N ) ' in ) .",Experiments Experimental Settings,Experiments Experimental Settings,named-entity-recognition,0,5,0.0862068965517241,205,0.7564575645756457,5,0.625,1,0
207,The remainder ( except candidate set ) are used for test .,Experiments Experimental Settings,Experiments Experimental Settings,named-entity-recognition,0,6,0.1034482758620689,206,0.7601476014760148,6,0.75,1,0
208,"Specifically , to simulate the varying quality of samples , ten percentage of candidate samples from each class are randomly selected and arbitrarily added one of the following three kinds of noise : "" Gaussian "" , "" Laplace "" and "" Salt & pepper "" respectively .",Experiments Experimental Settings,Experiments Experimental Settings,named-entity-recognition,0,7,0.1206896551724138,207,0.7638376383763837,7,0.875,1,0
209,"Ina word , all experiment settings are same and fair for all the methods .",Experiments Experimental Settings,Experiments Experimental Settings,named-entity-recognition,0,8,0.1379310344827586,208,0.7675276752767528,8,1.0,1,0
210,Speed Comparisons,Experiments Experimental Settings,,named-entity-recognition,0,9,0.1551724137931034,209,0.7712177121771218,0,0.0,1,0
211,There are two parts of speed comparisons .,Experiments Experimental Settings,Speed Comparisons,named-entity-recognition,0,10,0.1724137931034483,210,0.7749077490774908,1,0.0434782608695652,1,0
212,"First , how speed varies with increasing N is illustrated in .",Experiments Experimental Settings,Speed Comparisons,named-entity-recognition,0,11,0.1896551724137931,211,0.7785977859778598,2,0.0869565217391304,1,0
213,Then the comparison of specific speed is summarized in .,Experiments Experimental Settings,Speed Comparisons,named-entity-recognition,0,12,0.2068965517241379,212,0.7822878228782287,3,0.1304347826086956,1,0
214,Note that TED and RRSS Nie denote the authorial solver ( via authorial codes ) ; RRSS our is our accelerated solver for Nie 's model via Theorem 4 ; ARSS is the proposed method .,Experiments Experimental Settings,Speed Comparisons,named-entity-recognition,0,13,0.2241379310344827,213,0.7859778597785978,4,0.1739130434782608,1,0
215,"Speed vs. increasing N To verify the great superiority of our method over the state - of - the - art methods in speed , three experiments are conducted .",Experiments Experimental Settings,Speed Comparisons,named-entity-recognition,0,14,0.2413793103448276,214,0.7896678966789668,5,0.217391304347826,1,0
216,"The results are illustrated in , where there are three sub-figures showing the speed of four methods on the benchmark datasets of Letter , MNIST and Waveform respectively .",Experiments Experimental Settings,Speed Comparisons,named-entity-recognition,0,15,0.2586206896551724,215,0.7933579335793358,6,0.2608695652173913,1,0
217,"As we shall see , both selection time of TED and RRSS Nie increases dramatically as N increases .",Experiments Experimental Settings,Speed Comparisons,named-entity-recognition,0,16,0.2758620689655172,216,0.7970479704797048,7,0.3043478260869565,1,0
218,"No surprisingly , RRSS Nie is incredibly time - consuming as N grows the order of curves looks higher than quadratic .",Experiments Experimental Settings,Speed Comparisons,named-entity-recognition,0,17,0.293103448275862,217,0.8007380073800738,8,0.3478260869565217,1,0
219,"Actually , the theoretical complexity of RRSS Nie is highly up to ON 4 as analyzed in Remark",Experiments Experimental Settings,Speed Comparisons,named-entity-recognition,0,18,0.3103448275862069,218,0.8044280442804428,9,0.391304347826087,1,0
220,1 .,Experiments Experimental Settings,Speed Comparisons,named-entity-recognition,0,19,0.3275862068965517,219,0.8081180811808119,10,0.4347826086956521,1,0
221,"Compared with TED and RRSS Nie , the curve of ARSS is surprisingly lower and highly stable against increasing N ; there is almost no rise of selection time over growing N .",Experiments Experimental Settings,Speed Comparisons,named-entity-recognition,0,20,0.3448275862068966,220,0.8118081180811808,11,0.4782608695652174,1,1
222,This is owing to the speedup techniques of ALM and equivalent derivations .,Experiments Experimental Settings,Speed Comparisons,named-entity-recognition,0,21,0.3620689655172414,221,0.8154981549815498,12,0.5217391304347826,1,0
223,"Via them , we reduce the computational cost from ON 4 to ON 2 L , as analyzed in Theorem 2 and Corollary",Experiments Experimental Settings,Speed Comparisons,named-entity-recognition,0,22,0.3793103448275862,222,0.8191881918819188,13,0.5652173913043478,1,0
224,3 .,Experiments Experimental Settings,Speed Comparisons,named-entity-recognition,0,23,0.396551724137931,223,0.8228782287822878,14,0.6086956521739131,1,0
225,"Moreover , with the help of Theorem 4 , RRSS our is the second faster algorithm that is significantly accelerated compared with the authorial algorithm RRSS Nie .",Experiments Experimental Settings,Speed Comparisons,named-entity-recognition,0,24,0.4137931034482758,224,0.8265682656826568,15,0.6521739130434783,1,0
226,"Speed with fixed N The speed of four algorithms is summarized in , where each row shows the results on one dataset and the last row displays the average results .",Experiments Experimental Settings,Speed Comparisons,named-entity-recognition,0,25,0.4310344827586206,225,0.8302583025830258,16,0.6956521739130435,1,0
227,Four conclusions can be drawn from .,Experiments Experimental Settings,Speed Comparisons,named-entity-recognition,0,26,0.4482758620689655,226,0.8339483394833949,17,0.7391304347826086,1,0
228,"First , ARSS is the fastest algorithm , and RRSS our is the second fastest algorithm .",Experiments Experimental Settings,Speed Comparisons,named-entity-recognition,0,27,0.4655172413793103,227,0.8376383763837638,18,0.782608695652174,1,1
229,"Second , with the help of Theorem 4 , RRSS our is highly faster than RRSS Nie , averagely obtaining a 559 times acceleration .",Experiments Experimental Settings,Speed Comparisons,named-entity-recognition,0,28,0.4827586206896552,228,0.8413284132841329,19,0.8260869565217391,1,0
230,"Third , ARSS is dramatically faster than :",Experiments Experimental Settings,Speed Comparisons,named-entity-recognition,0,29,0.5,229,0.8450184501845018,20,0.8695652173913043,1,0
231,"Performances of TED , RRSS and ARSS : ( left - a ) speed in seconds , prediction accuracies .",Experiments Experimental Settings,Speed Comparisons,named-entity-recognition,0,30,0.5172413793103449,230,0.8487084870848709,21,0.9130434782608696,1,0
232,"In terms of speed , with the help of Theorem 4 , RRSSour is averagely 559 + times faster than the authorial algorithm , i.e. RRSSNie ; ARSS achieves surprisingly 23275 + times acceleration compared with RRSSNie .",Experiments Experimental Settings,Speed Comparisons,named-entity-recognition,0,31,0.5344827586206896,231,0.8523985239852399,22,0.9565217391304348,1,0
233,"Due to the more robust loss in the p -norm , the prediction accuracy of ARSS is highly encouraging .",Experiments Experimental Settings,Speed Comparisons,named-entity-recognition,0,32,0.5517241379310345,232,0.8560885608856088,23,1.0,1,0
234,Datasets,Experiments Experimental Settings,,named-entity-recognition,0,33,0.5689655172413793,233,0.8597785977859779,0,0.0,1,0
235,"Speed 1 ' ARSS ( N * ) ' means the task of selecting samples from the whole dataset ( with N * samples as shown in the 2 nd column in ) , while ' TED ' to ' ARSS ' indicate the problem of dealing with the candidate sample sets ( with N samples as shown in the 3 rd column in ) .",Experiments Experimental Settings,Datasets,named-entity-recognition,0,34,0.5862068965517241,234,0.8634686346863468,1,0.1666666666666666,1,0
236,verify an average acceleration of 23275 times faster than RRSS Nie and 281 times faster than TED .,Experiments Experimental Settings,Datasets,named-entity-recognition,0,35,0.603448275862069,235,0.8671586715867159,2,0.3333333333333333,1,0
237,"This means that for example if it takes RRSS Nie 100 years to do a subset selection task , it only takes our method 1.6 days to address the same problem .",Experiments Experimental Settings,Datasets,named-entity-recognition,0,36,0.6206896551724138,236,0.8708487084870848,3,0.5,1,0
238,"Finally , we apply ARSS to the whole sample set of each data .",Experiments Experimental Settings,Datasets,named-entity-recognition,0,37,0.6379310344827587,237,0.8745387453874539,4,0.6666666666666666,1,0
239,RRSS Nie and TED ; the results in,Experiments Experimental Settings,Datasets,named-entity-recognition,0,38,0.6551724137931034,238,0.8782287822878229,5,0.8333333333333334,1,0
240,"The results are displayed in the 6 th column in , showing its capability to process very large datasets .",Experiments Experimental Settings,Datasets,named-entity-recognition,0,39,0.6724137931034483,239,0.8819188191881919,6,1.0,1,0
241,Prediction Accuracy,Experiments Experimental Settings,,named-entity-recognition,0,40,0.6896551724137931,240,0.8856088560885609,0,0.0,1,0
242,Accuracy comparison,Experiments Experimental Settings,,named-entity-recognition,0,41,0.7068965517241379,241,0.8892988929889298,1,0.0555555555555555,1,0
243,We conduct experiments on ten benchmark datasets .,Experiments Experimental Settings,Accuracy comparison,named-entity-recognition,0,42,0.7241379310344828,242,0.8929889298892989,2,0.1111111111111111,1,0
244,"For each dataset , the top 200 representative samples are selected for training .",Experiments Experimental Settings,Accuracy comparison,named-entity-recognition,0,43,0.7413793103448276,243,0.8966789667896679,3,0.1666666666666666,1,0
245,"The prediction accuracies are reported in , including the results of two popular classifiers .",Experiments Experimental Settings,Accuracy comparison,named-entity-recognition,0,44,0.7586206896551724,244,0.9003690036900369,4,0.2222222222222222,1,0
246,Three observations can be drawn from this table .,Experiments Experimental Settings,Accuracy comparison,named-entity-recognition,0,45,0.7758620689655172,245,0.904059040590406,5,0.2777777777777778,1,0
247,"First , Linear SVM generally outperforms KNN .",Experiments Experimental Settings,Accuracy comparison,named-entity-recognition,0,46,0.7931034482758621,246,0.9077490774907748,6,0.3333333333333333,1,0
248,"Second , in general , our method performs the best ; fora few cases , our method achieves comparable results with the best performances .",Experiments Experimental Settings,Accuracy comparison,named-entity-recognition,0,47,0.8103448275862069,247,0.911439114391144,7,0.3888888888888889,1,0
249,"Third , compared with TED , both RRSS and ARSS achieve an appreciable advantage .",Experiments Experimental Settings,Accuracy comparison,named-entity-recognition,0,48,0.8275862068965517,248,0.915129151291513,8,0.4444444444444444,1,1
250,The above analyses are better illustrated in the last row of .,Experiments Experimental Settings,Accuracy comparison,named-entity-recognition,0,49,0.8448275862068966,249,0.918819188191882,9,0.5,1,0
251,These results demonstrate that the p loss in our model is well suited to select exemplars from the sample sets of various quality .,Experiments Experimental Settings,Accuracy comparison,named-entity-recognition,0,50,0.8620689655172413,250,0.922509225092251,10,0.5555555555555556,1,0
252,"Prediction accuracies vs. increasing K To give a more detailed comparison , shows the prediction accuracies versus growing K ( the number of selected samples ) .",Experiments Experimental Settings,Accuracy comparison,named-entity-recognition,0,51,0.8793103448275862,251,0.92619926199262,11,0.6111111111111112,1,0
253,There are two rows and four columns of sub-figures .,Experiments Experimental Settings,Accuracy comparison,named-entity-recognition,0,52,0.896551724137931,252,0.9298892988929888,12,0.6666666666666666,1,0
254,"The top row shows the results of KNN , and the bottom one shows results of SVM .",Experiments Experimental Settings,Accuracy comparison,named-entity-recognition,0,53,0.913793103448276,253,0.933579335793358,13,0.7222222222222222,1,0
255,Each column gives the result on one dataset .,Experiments Experimental Settings,Accuracy comparison,named-entity-recognition,0,54,0.9310344827586208,254,0.9372693726937268,14,0.7777777777777778,1,0
256,"As we shall see , the prediction accuracies generally increase as K increases .",Experiments Experimental Settings,Accuracy comparison,named-entity-recognition,0,55,0.9482758620689656,255,0.940959409594096,15,0.8333333333333334,1,1
257,Such case is consistent with the common view that more training data will boost the prediction accuracy .,Experiments Experimental Settings,Accuracy comparison,named-entity-recognition,0,56,0.9655172413793104,256,0.9446494464944648,16,0.8888888888888888,1,0
258,"For each sub-figure , ARSS is generally among the best .",Experiments Experimental Settings,Accuracy comparison,named-entity-recognition,0,57,0.9827586206896552,257,0.948339483394834,17,0.9444444444444444,1,1
259,This case implies that our robust objective ( 5 ) via the p - norm is feasible to select subsets from the data of varying qualities .,Experiments Experimental Settings,Accuracy comparison,named-entity-recognition,0,58,1.0,258,0.9520295202952028,18,1.0,1,0
260,Conclusion,,,named-entity-recognition,0,0,0.0,259,0.955719557195572,0,0.0,1,0
261,"To deal with tremendous data of varying quality , we propose an accelerated robust subset selection ( ARSS ) method .",Conclusion,Conclusion,named-entity-recognition,0,1,0.0909090909090909,260,0.959409594095941,1,0.0909090909090909,0,0
262,The p - norm is exploited to enhance the robustness against both outlier samples and outlier features .,Conclusion,Conclusion,named-entity-recognition,0,2,0.1818181818181818,261,0.96309963099631,2,0.1818181818181818,0,0
263,"Although the resulted objective is complex to solve , we propose a highly efficient solver via two techniques : ALM and equivalent derivations .",Conclusion,Conclusion,named-entity-recognition,0,3,0.2727272727272727,262,0.966789667896679,3,0.2727272727272727,0,0
264,"Via them , we greatly reduce the computational complexity from ON 4 to ON 2 L .",Conclusion,Conclusion,named-entity-recognition,0,4,0.3636363636363636,263,0.970479704797048,4,0.3636363636363636,0,0
265,"Here feature length L is much smaller than data size N , i.e. L N .",Conclusion,Conclusion,named-entity-recognition,0,5,0.4545454545454545,264,0.974169741697417,5,0.4545454545454545,0,0
266,"Extensive results on ten benchmark datasets verify that our method not only runs 10,000 + times faster than the most related method , but also outperforms state of the art methods .",Conclusion,Conclusion,named-entity-recognition,0,6,0.5454545454545454,265,0.977859778597786,6,0.5454545454545454,0,0
267,"Moreover , we propose an accelerated solver to highly speedup Nie 's method , theoretically reducing the computational complexity from ON 4 to ON 2 L + N L 3 .",Conclusion,Conclusion,named-entity-recognition,0,7,0.6363636363636364,266,0.981549815498155,7,0.6363636363636364,0,0
268,"Empirically , our accelerated solver could achieve equal results and 500 + times acceleration compared with the authorial solver .",Conclusion,Conclusion,named-entity-recognition,0,8,0.7272727272727273,267,0.985239852398524,8,0.7272727272727273,0,0
269,Limitation .,Conclusion,,named-entity-recognition,0,9,0.8181818181818182,268,0.988929889298893,9,0.8181818181818182,0,0
270,"Our efficient algorithm build on the observation that the number of samples is generally larger than feature length , i.e. N > L. For the case of N ?",Conclusion,Limitation .,named-entity-recognition,0,10,0.9090909090909092,269,0.992619926199262,10,0.9090909090909092,0,0
271,"L , the acceleration will be inapparent .",Conclusion,Limitation .,named-entity-recognition,0,11,1.0,270,0.996309963099631,11,1.0,0,0
1,title,,,named-entity-recognition,1,0,0.0,0,0.0,0,0.0,1,0
2,Neural Architectures for Named Entity Recognition,title,,named-entity-recognition,1,1,0.0,1,0.0048309178743961,1,0.0,1,1
3,abstract,,,named-entity-recognition,1,0,0.0,2,0.0096618357487922,0,0.0,1,0
4,"State - of - the - art named entity recognition systems rely heavily on hand - crafted features and domain - specific knowledge in order to learn effectively from the small , supervised training corpora that are available .",abstract,abstract,named-entity-recognition,1,1,0.2,3,0.0144927536231884,1,0.2,1,1
5,"In this paper , we introduce two new neural architectures - one based on bidirectional LSTMs and conditional random fields , and the other that constructs and labels segments using a transition - based approach inspired by shift - reduce parsers .",abstract,abstract,named-entity-recognition,1,2,0.4,4,0.0193236714975845,2,0.4,1,0
6,Our models rely on two sources of information about words : character - based word representations learned from the supervised corpus and unsupervised word representations learned from unannotated corpora .,abstract,abstract,named-entity-recognition,1,3,0.6,5,0.0241545893719806,3,0.6,1,0
7,Our models obtain state - of - the - art performance in NER in four languages without resorting to any language - specific knowledge or resources such as gazetteers .,abstract,abstract,named-entity-recognition,1,4,0.8,6,0.0289855072463768,4,0.8,1,1
8,1,abstract,abstract,named-entity-recognition,1,5,1.0,7,0.0338164251207729,5,1.0,1,0
9,Introduction,,,named-entity-recognition,1,0,0.0,8,0.038647342995169,0,0.0,1,0
10,Named entity recognition ( NER ) is a challenging learning problem .,Introduction,Introduction,named-entity-recognition,1,1,0.0588235294117647,9,0.0434782608695652,1,0.05,1,0
11,"One the one hand , inmost languages and domains , there is only a very small amount of supervised training data available .",Introduction,Introduction,named-entity-recognition,1,2,0.1176470588235294,10,0.0483091787439613,2,0.1,1,0
12,"On the other , there are few constraints on the kinds of words that can be names , so generalizing from this small sample of data is difficult .",Introduction,Introduction,named-entity-recognition,1,3,0.1764705882352941,11,0.0531400966183574,3,0.15,1,0
13,"As a result , carefully constructed orthographic features and language - specific knowledge resources , such as gazetteers , are widely used for solving this task .",Introduction,Introduction,named-entity-recognition,1,4,0.2352941176470588,12,0.0579710144927536,4,0.2,1,0
14,"Unfortunately , languagespecific resources and features are costly to develop in new languages and new domains , making NER a challenge to adapt .",Introduction,Introduction,named-entity-recognition,1,5,0.2941176470588235,13,0.0628019323671497,5,0.25,1,0
15,Unsupervised learning from unannotated corpora offers an alternative strategy for obtaining better generalization from small amounts of supervision .,Introduction,Introduction,named-entity-recognition,1,6,0.3529411764705882,14,0.0676328502415458,6,0.3,1,0
16,"However , even systems that have relied extensively on unsupervised features have used these to augment , rather than replace , hand - engineered features ( e.g. , knowledge about capitalization patterns and character classes in a particular language ) and specialized knowledge resources ( e.g. , gazetteers ) .",Introduction,Introduction,named-entity-recognition,1,7,0.4117647058823529,15,0.072463768115942,7,0.35,1,0
17,"In this paper , we present neural architectures for NER that use no language - specific resources or features beyond a small amount of supervised training data and unlabeled corpora .",Introduction,Introduction,named-entity-recognition,1,8,0.4705882352941176,16,0.0772946859903381,8,0.4,1,0
18,Our models are designed to capture two intuitions .,Introduction,Introduction,named-entity-recognition,1,9,0.5294117647058824,17,0.0821256038647343,9,0.45,1,0
19,"First , since names often consist of multiple tokens , reasoning jointly over tagging decisions for each token is important .",Introduction,Introduction,named-entity-recognition,1,10,0.5882352941176471,18,0.0869565217391304,10,0.5,1,0
20,"We compare two models here , ( i ) a bidirectional LSTM with a sequential conditional random layer above it ( LSTM - CRF ; 2 ) , and ( ii ) anew model that constructs and labels chunks of input sentences using an algorithm inspired by transition - based parsing with states represented by stack LSTMs ( S - LSTM ; 3 ) .",Introduction,Introduction,named-entity-recognition,1,11,0.6470588235294118,19,0.0917874396135265,11,0.55,1,1
21,"Second , token - level evidence for "" being a name "" includes both orthographic evidence ( what does the word being tagged as a name look like ? ) and distributional evidence ( where does the word being tagged tend to occur in a corpus ? ) .",Introduction,Introduction,named-entity-recognition,1,12,0.7058823529411765,20,0.0966183574879227,12,0.6,1,0
22,"To capture orthographic sensitivity , we use character - based word representation model to capture distributional sensitivity , we combine these representations with distributional representations .",Introduction,Introduction,named-entity-recognition,1,13,0.7647058823529411,21,0.1014492753623188,13,0.65,1,1
23,"Our word representations combine both of these , and dropout training is used to encourage the model to learn to trust both sources of evidence ( 4 ) .",Introduction,Introduction,named-entity-recognition,1,14,0.8235294117647058,22,0.1062801932367149,14,0.7,1,0
24,"Experiments in English , Dutch , German , and Spanish show that we are able to obtain state - of - the - art NER performance with the LSTM - CRF model in Dutch , German , and Spanish , and very near the state - of - the - art in English without any hand - engineered features or gazetteers ( 5 ) .",Introduction,Introduction,named-entity-recognition,1,15,0.8823529411764706,23,0.1111111111111111,15,0.75,1,0
25,"The transition - based algorithm likewise surpasses the best previously published results in several languages , although it performs less well than the LSTM - CRF model .",Introduction,Introduction,named-entity-recognition,1,16,0.9411764705882352,24,0.1159420289855072,16,0.8,1,0
26,LSTM - CRF,Introduction,Introduction,named-entity-recognition,1,17,1.0,25,0.1207729468599033,17,0.85,1,0
27,Model,,,named-entity-recognition,1,0,0.0,26,0.1256038647342995,18,0.9,1,0
28,"We provide a brief description of LSTMs and CRFs , and present a hybrid tagging architecture .",Model,Model,named-entity-recognition,1,1,0.0454545454545454,27,0.1304347826086956,19,0.95,1,0
29,This architecture is similar to the ones presented by .,Model,Model,named-entity-recognition,1,2,0.0909090909090909,28,0.1352657004830917,20,1.0,1,0
30,LSTM,Model,,named-entity-recognition,1,3,0.1363636363636363,29,0.1400966183574879,0,0.0,1,0
31,Recurrent neural networks ( RNNs ) area family of neural networks that operate on sequential data .,Model,LSTM,named-entity-recognition,1,4,0.1818181818181818,30,0.144927536231884,1,0.0263157894736842,1,0
32,"They take as input a sequence of vectors ( x 1 , x 2 , . . . , x n ) and return another sequence ( h 1 , h 2 , . . . , h n ) that represents some information about the sequence at every step in the input .",Model,LSTM,named-entity-recognition,1,5,0.2272727272727272,31,0.1497584541062802,2,0.0526315789473684,1,0
33,"Although RNNs can , in theory , learn long dependencies , in practice they fail to do so and tend to be biased towards their most recent inputs in the sequence .",Model,LSTM,named-entity-recognition,1,6,0.2727272727272727,32,0.1545893719806763,3,0.0789473684210526,1,0
34,Long Short - term Memory Networks ( LSTMs ) have been designed to combat this issue by incorporating a memory - cell and have been shown to capture long - range dependencies .,Model,LSTM,named-entity-recognition,1,7,0.3181818181818182,33,0.1594202898550724,4,0.1052631578947368,1,0
35,"They do so using several gates that control the proportion of the input to give to the memory cell , and the proportion from the previous state to forget .",Model,LSTM,named-entity-recognition,1,8,0.3636363636363636,34,0.1642512077294686,5,0.131578947368421,1,0
36,We use the following implementation :,Model,LSTM,named-entity-recognition,1,9,0.4090909090909091,35,0.1690821256038647,6,0.1578947368421052,1,0
37,where ?,Model,LSTM,named-entity-recognition,1,10,0.4545454545454545,36,0.1739130434782608,7,0.1842105263157894,1,0
38,"is the element - wise sigmoid function , and is the element - wise product .",Model,LSTM,named-entity-recognition,1,11,0.5,37,0.178743961352657,8,0.2105263157894736,1,0
39,"For a given sentence ( x 1 , x 2 , . . . , x n ) containing n words , each represented as a d-dimensional vector , an LSTM computes a representation ? ?",Model,LSTM,named-entity-recognition,1,12,0.5454545454545454,38,0.1835748792270531,9,0.2368421052631578,1,0
40,ht of the left context of the sentence at every word t.,Model,LSTM,named-entity-recognition,1,13,0.5909090909090909,39,0.1884057971014492,10,0.2631578947368421,1,0
41,"Naturally , generating a representation of the right context ? ?",Model,LSTM,named-entity-recognition,1,14,0.6363636363636364,40,0.1932367149758454,11,0.2894736842105263,1,0
42,ht as well should add useful information .,Model,LSTM,named-entity-recognition,1,15,0.6818181818181818,41,0.1980676328502415,12,0.3157894736842105,1,0
43,This can be achieved using a second LSTM that reads the same sequence in reverse .,Model,LSTM,named-entity-recognition,1,16,0.7272727272727273,42,0.2028985507246377,13,0.3421052631578947,1,0
44,We will refer to the former as the forward LSTM and the latter as the backward LSTM .,Model,LSTM,named-entity-recognition,1,17,0.7727272727272727,43,0.2077294685990338,14,0.3684210526315789,1,0
45,These are two distinct networks with different parameters .,Model,LSTM,named-entity-recognition,1,18,0.8181818181818182,44,0.2125603864734299,15,0.3947368421052631,1,0
46,This forward and backward LSTM pair is referred to as a bidirectional LSTM .,Model,LSTM,named-entity-recognition,1,19,0.8636363636363636,45,0.217391304347826,16,0.4210526315789473,1,0
47,"The representation of a word using this model is obtained by concatenating its left and right context representations ,",Model,LSTM,named-entity-recognition,1,20,0.9090909090909092,46,0.2222222222222222,17,0.4473684210526316,1,0
48,"These representations effectively include a representation of a word in context , which is useful for numerous tagging applications .",Model,LSTM,named-entity-recognition,1,21,0.9545454545454546,47,0.2270531400966183,18,0.4736842105263157,1,0
49,CRF,Model,,named-entity-recognition,1,22,1.0,48,0.2318840579710145,19,0.5,1,0
50,Tagging Models,,,named-entity-recognition,1,0,0.0,49,0.2367149758454106,20,0.5263157894736842,1,0
51,Avery simple - but surprisingly effective - tagging model is to use the ht 's as features to make independent tagging decisions for each output y t.,Tagging Models,Tagging Models,named-entity-recognition,1,1,0.0555555555555555,50,0.2415458937198067,21,0.5526315789473685,1,0
52,"Despite this model 's success in simple problems like POS tagging , its independent classification decisions are limiting when there are strong dependencies across output labels .",Tagging Models,Tagging Models,named-entity-recognition,1,2,0.1111111111111111,51,0.2463768115942029,22,0.5789473684210527,1,0
53,"NER is one such task , since the "" grammar "" that characterizes interpretable sequences of tags imposes several hard constraints ( e.g. , I - PER can not follow B - LOC ; see 2.4 for details ) that would be impossible to model with independence assumptions .",Tagging Models,Tagging Models,named-entity-recognition,1,3,0.1666666666666666,52,0.251207729468599,23,0.6052631578947368,1,0
54,"Therefore , instead of modeling tagging decisions independently , we model them jointly using a conditional random field .",Tagging Models,Tagging Models,named-entity-recognition,1,4,0.2222222222222222,53,0.2560386473429952,24,0.631578947368421,1,0
55,For an input sentence,Tagging Models,Tagging Models,named-entity-recognition,1,5,0.2777777777777778,54,0.2608695652173913,25,0.6578947368421053,1,0
56,we consider P to be the matrix of scores output by the bidirectional LSTM network .,Tagging Models,Tagging Models,named-entity-recognition,1,6,0.3333333333333333,55,0.2657004830917874,26,0.6842105263157895,1,0
57,"P is of size n k , where k is the number of distinct tags , and P i , j corresponds to the score of the j th tag of the i th word in a sentence .",Tagging Models,Tagging Models,named-entity-recognition,1,7,0.3888888888888889,56,0.2705314009661835,27,0.7105263157894737,1,0
58,"For a sequence of predictions y = ( y 1 , y 2 , . . . , y n ) , we define it s score to be",Tagging Models,Tagging Models,named-entity-recognition,1,8,0.4444444444444444,57,0.2753623188405797,28,0.7368421052631579,1,0
59,"where A is a matrix of transition scores such that A i , j represents the score of a transition from the tag i to tag j. y 0 and y n are the start and end tags of a sentence , that we add to the set of possible tags .",Tagging Models,Tagging Models,named-entity-recognition,1,9,0.5,58,0.2801932367149758,29,0.7631578947368421,1,0
60,A is therefore a square matrix of size k + 2 .,Tagging Models,Tagging Models,named-entity-recognition,1,10,0.5555555555555556,59,0.2850241545893719,30,0.7894736842105263,1,0
61,A softmax overall possible tag sequences yields a probability for the sequence y:,Tagging Models,Tagging Models,named-entity-recognition,1,11,0.6111111111111112,60,0.2898550724637681,31,0.8157894736842105,1,0
62,"y ?Y Xe s ( X , y) .",Tagging Models,Tagging Models,named-entity-recognition,1,12,0.6666666666666666,61,0.2946859903381642,32,0.8421052631578947,1,0
63,"During training , we maximize the log-probability of the correct tag sequence :",Tagging Models,Tagging Models,named-entity-recognition,1,13,0.7222222222222222,62,0.2995169082125604,33,0.868421052631579,1,0
64,where Y X represents all possible tag sequences ( even those that do not verify the IOB format ) fora sentence X .,Tagging Models,Tagging Models,named-entity-recognition,1,14,0.7777777777777778,63,0.3043478260869565,34,0.8947368421052632,1,0
65,"From the formulation above , it is evident that we encourage our network to produce a valid sequence of output labels .",Tagging Models,Tagging Models,named-entity-recognition,1,15,0.8333333333333334,64,0.3091787439613526,35,0.9210526315789472,1,0
66,"While decoding , we predict the output sequence that obtains the maximum score given by :",Tagging Models,Tagging Models,named-entity-recognition,1,16,0.8888888888888888,65,0.3140096618357488,36,0.9473684210526316,1,0
67,"Since we are only modeling bigram interactions between outputs , both the summation in Eq. 1 and the maximum a posteriori sequence y * in Eq.",Tagging Models,Tagging Models,named-entity-recognition,1,17,0.9444444444444444,66,0.3188405797101449,37,0.9736842105263158,1,0
68,2 can be computed using dynamic programming .,Tagging Models,Tagging Models,named-entity-recognition,1,18,1.0,67,0.323671497584541,38,1.0,1,0
69,Parameterization and Training,,,named-entity-recognition,1,0,0.0,68,0.3285024154589372,0,0.0,1,0
70,"The scores associated with each tagging decision for each token ( i.e. , the P i , y 's ) are defined to be the dot product between the embedding of a wordin - context computed with a bidirectional LSTMexactly the same as the POS tagging model of and these are combined with bigram compatibility scores ( i.e. , the A y , y 's ) .",Parameterization and Training,Parameterization and Training,named-entity-recognition,1,1,0.0125,69,0.3333333333333333,1,0.0833333333333333,1,0
71,This architecture is shown in figure,Parameterization and Training,Parameterization and Training,named-entity-recognition,1,2,0.025,70,0.3381642512077294,2,0.1666666666666666,1,0
72,"1 . Circles represent observed variables , diamonds are deterministic functions of their parents , and double circles are random variables .",Parameterization and Training,Parameterization and Training,named-entity-recognition,1,3,0.0375,71,0.3429951690821256,3,0.25,1,0
73,"The parameters of this model are thus the matrix of bigram compatibility scores A , and the parameters that give rise to the matrix P , namely the parameters of the bidirectional LSTM , the linear feature weights , and the word embeddings .",Parameterization and Training,Parameterization and Training,named-entity-recognition,1,4,0.05,72,0.3478260869565217,4,0.3333333333333333,1,0
74,"As in part 2.2 , let x i denote the sequence of word embeddings for every word in a sentence , and y i be their associated tags .",Parameterization and Training,Parameterization and Training,named-entity-recognition,1,5,0.0625,73,0.3526570048309179,5,0.4166666666666667,1,0
75,We return to a discussion of how the embeddings xi are modeled in Section 4 .,Parameterization and Training,Parameterization and Training,named-entity-recognition,1,6,0.075,74,0.357487922705314,6,0.5,1,0
76,"The sequence of word embeddings is given as input to a bidirectional LSTM , which returns a representation of the left and right context for each word as explained in 2.1 .",Parameterization and Training,Parameterization and Training,named-entity-recognition,1,7,0.0875,75,0.3623188405797101,7,0.5833333333333334,1,0
77,These representations are concatenated ( c i ) and linearly projected onto a layer whose size is equal to the number of distinct tags .,Parameterization and Training,Parameterization and Training,named-entity-recognition,1,8,0.1,76,0.3671497584541063,8,0.6666666666666666,1,0
78,"Instead of using the softmax output from this layer , we use a CRF as previously described to take into account neighboring tags , yielding the final predictions for every wordy i .",Parameterization and Training,Parameterization and Training,named-entity-recognition,1,9,0.1125,77,0.3719806763285024,9,0.75,1,0
79,"Additionally , we observed that adding a hidden layer between c i and the CRF layer marginally improved our results .",Parameterization and Training,Parameterization and Training,named-entity-recognition,1,10,0.125,78,0.3768115942028985,10,0.8333333333333334,1,0
80,All results reported with this model incorporate this extra-layer .,Parameterization and Training,Parameterization and Training,named-entity-recognition,1,11,0.1375,79,0.3816425120772946,11,0.9166666666666666,1,0
81,"The parameters are trained to maximize Eq. 1 of observed sequences of NER tags in an annotated corpus , given the observed words .",Parameterization and Training,Parameterization and Training,named-entity-recognition,1,12,0.15,80,0.3864734299516908,12,1.0,1,0
82,Tagging Schemes,Parameterization and Training,,named-entity-recognition,1,13,0.1625,81,0.391304347826087,0,0.0,1,0
83,The task of named entity recognition is to assign a named entity label to every word in a sentence .,Parameterization and Training,Tagging Schemes,named-entity-recognition,1,14,0.175,82,0.3961352657004831,1,0.0625,1,0
84,A single named entity could span several tokens within a sentence .,Parameterization and Training,Tagging Schemes,named-entity-recognition,1,15,0.1875,83,0.4009661835748792,2,0.125,1,0
85,"Sentences are usually represented in the IOB format ( Inside , Outside , Beginning ) where every token is labeled as B- label if the token is the beginning of a named entity , I-label if it is inside a named entity but not the first token within the named entity , or O otherwise .",Parameterization and Training,Tagging Schemes,named-entity-recognition,1,16,0.2,84,0.4057971014492754,3,0.1875,1,0
86,"However , we decided to use the IOBES tagging scheme , a variant of IOB commonly used for named entity recognition , which encodes information about singleton entities ( S ) and explicitly marks the end of named entities ( E ) .",Parameterization and Training,Tagging Schemes,named-entity-recognition,1,17,0.2125,85,0.4106280193236715,4,0.25,1,0
87,"Using this scheme , tagging a word as I-label with high - confidence narrows down the choices for the subsequent word to I-label or E-label , however , the IOB scheme is only capable of determining that the subsequent word can not be the interior of another label .",Parameterization and Training,Tagging Schemes,named-entity-recognition,1,18,0.225,86,0.4154589371980676,5,0.3125,1,0
88,Ratinov and Roth and showed that using a more expressive tagging scheme like IOBES improves model performance marginally .,Parameterization and Training,Tagging Schemes,named-entity-recognition,1,19,0.2375,87,0.4202898550724637,6,0.375,1,0
89,"However , we did not observe a significant improvement over the IOB tagging scheme .",Parameterization and Training,Tagging Schemes,named-entity-recognition,1,20,0.25,88,0.4251207729468599,7,0.4375,1,0
90,Transition - Based Chunking Model,Parameterization and Training,,named-entity-recognition,1,21,0.2625,89,0.429951690821256,8,0.5,1,0
91,"As an alternative to the LSTM - CRF discussed in the previous section , we explore anew architecture that chunks and labels a sequence of inputs using an algorithm similar to transition - based dependency parsing .",Parameterization and Training,Transition - Based Chunking Model,named-entity-recognition,1,22,0.275,90,0.4347826086956521,9,0.5625,1,0
92,"This model directly constructs representations of the multi-token names ( e.g. , the name Mark Watney is composed into a single representation ) .",Parameterization and Training,Transition - Based Chunking Model,named-entity-recognition,1,23,0.2875,91,0.4396135265700483,10,0.625,1,0
93,This model relies on a stack data structure to incrementally construct chunks of the input .,Parameterization and Training,Transition - Based Chunking Model,named-entity-recognition,1,24,0.3,92,0.4444444444444444,11,0.6875,1,0
94,"To obtain representations of this stack used for predicting subsequent actions , we use the Stack - LSTM presented by , in which the LSTM is augmented with a "" stack pointer . """,Parameterization and Training,Transition - Based Chunking Model,named-entity-recognition,1,25,0.3125,93,0.4492753623188406,12,0.75,1,0
95,"While sequential LSTMs model sequences from left to right , stack LSTMs permit embedding of a stack of objects that are both added to ( using a push operation ) and removed from ( using a pop operation ) .",Parameterization and Training,Transition - Based Chunking Model,named-entity-recognition,1,26,0.325,94,0.4541062801932367,13,0.8125,1,0
96,"This allows the Stack - LSTM to work like a stack that maintains a "" summary embedding "" of its contents .",Parameterization and Training,Transition - Based Chunking Model,named-entity-recognition,1,27,0.3375,95,0.4589371980676328,14,0.875,1,0
97,We refer to this model as Stack - LSTM or S - LSTM model for simplicity .,Parameterization and Training,Transition - Based Chunking Model,named-entity-recognition,1,28,0.35,96,0.463768115942029,15,0.9375,1,0
98,"Finally , we refer interested readers to the original paper for details about the Stack - LSTM model since in this paper we merely use the same architecture through anew transition - based algorithm presented in the following Section .",Parameterization and Training,Transition - Based Chunking Model,named-entity-recognition,1,29,0.3625,97,0.4685990338164251,16,1.0,1,0
99,Chunking Algorithm,Parameterization and Training,,named-entity-recognition,1,30,0.375,98,0.4734299516908212,0,0.0,1,0
100,"We designed a transition inventory which is given in that is inspired by transition - based parsers , in particular the arc-standard parser of .",Parameterization and Training,Chunking Algorithm,named-entity-recognition,1,31,0.3875,99,0.4782608695652174,1,0.0714285714285714,1,0
101,"In this algorithm , we make use of two stacks ( designated output and stack representing , respectively , completed chunks and scratch space ) and a buffer that contains the words that have yet to be processed .",Parameterization and Training,Chunking Algorithm,named-entity-recognition,1,32,0.4,100,0.4830917874396135,2,0.1428571428571428,1,0
102,The transition inventory contains the following transitions :,Parameterization and Training,Chunking Algorithm,named-entity-recognition,1,33,0.4125,101,0.4879227053140096,3,0.2142857142857142,1,0
103,"The SHIFT transition moves a word from the buffer to the stack , the OUT transition moves a word from the buffer directly into the output stack while the REDUCE ( y ) transition pops all items from the top of the stack creating a "" chunk , "" labels this with label y , and pushes a representation of this chunk onto the output stack .",Parameterization and Training,Chunking Algorithm,named-entity-recognition,1,34,0.425,102,0.4927536231884058,4,0.2857142857142857,1,0
104,The algorithm completes when the stack and buffer are both empty .,Parameterization and Training,Chunking Algorithm,named-entity-recognition,1,35,0.4375,103,0.4975845410628019,5,0.3571428571428571,1,0
105,"The algorithm is depicted in , which shows the sequence of operations required to process the sentence Mark Watney visited Mars .",Parameterization and Training,Chunking Algorithm,named-entity-recognition,1,36,0.45,104,0.5024154589371981,6,0.4285714285714285,1,0
106,"The model is parameterized by defining a probability distribution over actions at each time step , given the current contents of the stack , buffer , and output , as well as the history of actions taken .",Parameterization and Training,Chunking Algorithm,named-entity-recognition,1,37,0.4625,105,0.5072463768115942,7,0.5,1,0
107,"Following , we use stack LSTMs to compute a fixed dimensional embedding of each of these , and take a concatenation of these to obtain the full algorithm state .",Parameterization and Training,Chunking Algorithm,named-entity-recognition,1,38,0.475,106,0.5120772946859904,8,0.5714285714285714,1,0
108,This representation is used to define a distribution over the possible actions that can betaken at each time step .,Parameterization and Training,Chunking Algorithm,named-entity-recognition,1,39,0.4875,107,0.5169082125603864,9,0.6428571428571429,1,0
109,The model is trained to maximize the conditional probability of sequences of reference actions ( extracted from a labeled training corpus ) given the input sentences .,Parameterization and Training,Chunking Algorithm,named-entity-recognition,1,40,0.5,108,0.5217391304347826,10,0.7142857142857143,1,0
110,"To label anew input sequence attest time , the maximum probability action is chosen greedily until the algorithm reaches a termination state .",Parameterization and Training,Chunking Algorithm,named-entity-recognition,1,41,0.5125,109,0.5265700483091788,11,0.7857142857142857,1,0
111,"Although this is not guaranteed to find a global optimum , it is effective in practice .",Parameterization and Training,Chunking Algorithm,named-entity-recognition,1,42,0.525,110,0.5314009661835749,12,0.8571428571428571,1,0
112,"Since each token is either moved directly to the output ( 1 action ) or first to the stack and then the output ( 2 actions ) , the total number of actions fora sequence of length n is maximally 2n .",Parameterization and Training,Chunking Algorithm,named-entity-recognition,1,43,0.5375,111,0.5362318840579711,13,0.9285714285714286,1,0
113,It is worth noting that the nature of this algorithm,Parameterization and Training,Chunking Algorithm,named-entity-recognition,1,44,0.55,112,0.5410628019323671,14,1.0,1,0
114,Representing Labeled Chunks,Parameterization and Training,,named-entity-recognition,1,45,0.5625,113,0.5458937198067633,0,0.0,1,0
115,"When the REDUCE ( y ) operation is executed , the algorithm shifts a sequence of tokens ( together with their vector embeddings ) from the stack to the output buffer as a single completed chunk .",Parameterization and Training,Representing Labeled Chunks,named-entity-recognition,1,46,0.575,114,0.5507246376811594,1,0.25,1,0
116,"To compute an embedding of this sequence , we run a bidirectional LSTM over the embeddings of its constituent tokens together with a token representing the type of the chunk being identified ( i.e. , y ) .",Parameterization and Training,Representing Labeled Chunks,named-entity-recognition,1,47,0.5875,115,0.5555555555555556,2,0.5,1,0
117,"This function is given as g ( u , . . . , v , r y ) , where r y is a learned embedding of a label type .",Parameterization and Training,Representing Labeled Chunks,named-entity-recognition,1,48,0.6,116,0.5603864734299517,3,0.75,1,0
118,"Thus , the output buffer contains a single vector representation for each labeled chunk that is generated , regardless of its length .",Parameterization and Training,Representing Labeled Chunks,named-entity-recognition,1,49,0.6125,117,0.5652173913043478,4,1.0,1,0
119,Input Word Embeddings,Parameterization and Training,,named-entity-recognition,1,50,0.625,118,0.5700483091787439,0,0.0,1,0
120,The input layers to both of our models are vector representations of individual words .,Parameterization and Training,Input Word Embeddings,named-entity-recognition,1,51,0.6375,119,0.5748792270531401,1,0.0434782608695652,1,0
121,Learning independent representations for word types from the limited NER training data is a difficult problem : there are simply too many parameters to reliably estimate .,Parameterization and Training,Input Word Embeddings,named-entity-recognition,1,52,0.65,120,0.5797101449275363,2,0.0869565217391304,1,0
122,"Since many languages have orthographic or morphological evidence that something is a name ( or not a name ) , we want representations that are sensitive to the spelling of words .",Parameterization and Training,Input Word Embeddings,named-entity-recognition,1,53,0.6625,121,0.5845410628019324,3,0.1304347826086956,1,0
123,We therefore use a model that constructs representations of words from representations of the characters they are composed of ( 4.1 ) .,Parameterization and Training,Input Word Embeddings,named-entity-recognition,1,54,0.675,122,0.5893719806763285,4,0.1739130434782608,1,0
124,"Our second intuition is that names , which may individually be quite varied , appear in regular contexts in large corpora .",Parameterization and Training,Input Word Embeddings,named-entity-recognition,1,55,0.6875,123,0.5942028985507246,5,0.217391304347826,1,0
125,Therefore we use embed - dings learned from a large corpus that are sensitive to word order ( 4.2 ) .,Parameterization and Training,Input Word Embeddings,named-entity-recognition,1,56,0.7,124,0.5990338164251208,6,0.2608695652173913,1,0
126,"Finally , to prevent the models from depending on one representation or the other too strongly , we use dropout training and find this is crucial for good generalization performance ( 4.3 ) .",Parameterization and Training,Input Word Embeddings,named-entity-recognition,1,57,0.7125,125,0.6038647342995169,7,0.3043478260869565,1,0
127,Character - based models of words,Parameterization and Training,Input Word Embeddings,named-entity-recognition,1,58,0.725,126,0.6086956521739131,8,0.3478260869565217,1,0
128,An important distinction of our work from most previous approaches is that we learn character - level features while training instead of hand - engineering prefix and suffix information about words .,Parameterization and Training,Input Word Embeddings,named-entity-recognition,1,59,0.7375,127,0.6135265700483091,9,0.391304347826087,1,0
129,Learning character - level embeddings has the advantage of learning representations specific to the task and domain at hand .,Parameterization and Training,Input Word Embeddings,named-entity-recognition,1,60,0.75,128,0.6183574879227053,10,0.4347826086956521,1,0
130,They have been found useful for morphologically rich languages and to handle the outof - vocabulary problem for tasks like part - of - speech tagging and language modeling or dependency parsing . describes our architecture to generate a word embedding fora word from its characters .,Parameterization and Training,Input Word Embeddings,named-entity-recognition,1,61,0.7625,129,0.6231884057971014,11,0.4782608695652174,1,0
131,A character lookup table initialized at random contains an embedding for every character .,Parameterization and Training,Input Word Embeddings,named-entity-recognition,1,62,0.775,130,0.6280193236714976,12,0.5217391304347826,1,0
132,The character embeddings corresponding to every character in a word are given indirect and reverse order to a forward and a backward LSTM .,Parameterization and Training,Input Word Embeddings,named-entity-recognition,1,63,0.7875,131,0.6328502415458938,13,0.5652173913043478,1,0
133,The embedding fora word derived from its characters is the concatenation of its forward and backward representations from the bidirectional LSTM .,Parameterization and Training,Input Word Embeddings,named-entity-recognition,1,64,0.8,132,0.6376811594202898,14,0.6086956521739131,1,0
134,This character - level representation is then concatenated with a word - level representation from a word lookup - table .,Parameterization and Training,Input Word Embeddings,named-entity-recognition,1,65,0.8125,133,0.642512077294686,15,0.6521739130434783,1,0
135,"During testing , words that do not have an embedding in the lookup table are mapped to a UNK embedding .",Parameterization and Training,Input Word Embeddings,named-entity-recognition,1,66,0.825,134,0.6473429951690821,16,0.6956521739130435,1,0
136,"To train the UNK embedding , we replace singletons with the UNK embedding with a probability 0.5 .",Parameterization and Training,Input Word Embeddings,named-entity-recognition,1,67,0.8375,135,0.6521739130434783,17,0.7391304347826086,1,0
137,"In all our experiments , the hidden dimension of the forward and backward character LSTMs are 25 each , which results in our character - based representation of words being of dimension 50 .",Parameterization and Training,Input Word Embeddings,named-entity-recognition,1,68,0.85,136,0.6570048309178744,18,0.782608695652174,1,0
138,"Recurrent models like RNNs and LSTMs are capable of encoding very long sequences , however , they have a representation biased towards their most recent inputs .",Parameterization and Training,Input Word Embeddings,named-entity-recognition,1,69,0.8625,137,0.6618357487922706,19,0.8260869565217391,1,0
139,"As a result , we expect the final representation of the forward LSTM to bean accurate representation of the suffix of the word , and the final state of the backward LSTM to be a better representation of its prefix .",Parameterization and Training,Input Word Embeddings,named-entity-recognition,1,70,0.875,138,0.6666666666666666,20,0.8695652173913043,1,0
140,Alternative approachesmost notably like convolutional networks - have been proposed to learn representations of words from their characters .,Parameterization and Training,Input Word Embeddings,named-entity-recognition,1,71,0.8875,139,0.6714975845410628,21,0.9130434782608696,1,0
141,"However , convnets are designed to discover position - invariant features of their inputs .",Parameterization and Training,Input Word Embeddings,named-entity-recognition,1,72,0.9,140,0.6763285024154589,22,0.9565217391304348,1,0
142,"While this is appropriate for many problems , e.g. , image recognition ( a cat can appear anywhere in a picture ) , we argue that important information is position dependent ( e.g. , prefixes and suffixes encode different information than stems ) , making LSTMs an a priori better function class for modeling the relationship between words and their characters .",Parameterization and Training,Input Word Embeddings,named-entity-recognition,1,73,0.9125,141,0.6811594202898551,23,1.0,1,0
143,Pretrained embeddings,Parameterization and Training,,named-entity-recognition,1,74,0.925,142,0.6859903381642513,0,0.0,1,0
144,"As in , we use pretrained word embeddings to initialize our lookup table .",Parameterization and Training,Pretrained embeddings,named-entity-recognition,1,75,0.9375,143,0.6908212560386473,1,0.1666666666666666,1,0
145,We observe significant improvements using pretrained word embeddings over randomly initialized ones .,Parameterization and Training,Pretrained embeddings,named-entity-recognition,1,76,0.95,144,0.6956521739130435,2,0.3333333333333333,1,0
146,"Embeddings are pretrained using skip - n- gram ( Ling et al. , 2015 a ) , a variation of word2vec that accounts for word order .",Parameterization and Training,Pretrained embeddings,named-entity-recognition,1,77,0.9625,145,0.7004830917874396,3,0.5,1,0
147,These embeddings are fine - tuned during training .,Parameterization and Training,Pretrained embeddings,named-entity-recognition,1,78,0.975,146,0.7053140096618358,4,0.6666666666666666,1,0
148,"Word embeddings for Spanish , Dutch , German and English are trained using the Spanish Gigaword version 3 , the Leipzig corpora collection , the German monolingual training data from the 2010 Machine Translation Workshop and the English Gigaword version 4 ( with the LA Times and NY Times portions removed ) respectively .",Parameterization and Training,Pretrained embeddings,named-entity-recognition,1,79,0.9875,147,0.7101449275362319,5,0.8333333333333334,1,0
149,"We use an embedding dimension of 100 for English , 64 for other languages , a minimum word frequency cutoff of 4 , and a window size of 8 .",Parameterization and Training,Pretrained embeddings,named-entity-recognition,1,80,1.0,148,0.714975845410628,6,1.0,1,0
150,Dropout training,,,named-entity-recognition,1,0,0.0,149,0.7198067632850241,0,0.0,1,0
151,Initial experiments showed that character - level embeddings did not improve our overall performance when used in conjunction with pretrained word representations .,Dropout training,Dropout training,named-entity-recognition,1,1,0.3333333333333333,150,0.7246376811594203,1,0.3333333333333333,1,0
152,"To encourage the model to depend on both representations , we use dropout training , applying a dropout mask to the final embedding layer just before the input to the bidirectional LSTM in .",Dropout training,Dropout training,named-entity-recognition,1,2,0.6666666666666666,151,0.7294685990338164,2,0.6666666666666666,1,0
153,We observe a significant improvement in our model 's performance after using dropout ( see ) .,Dropout training,Dropout training,named-entity-recognition,1,3,1.0,152,0.7342995169082126,3,1.0,1,0
154,Experiments,,,named-entity-recognition,1,0,0.0,153,0.7391304347826086,0,0.0,1,0
155,"This section presents the methods we use to train our models , the results we obtained on various tasks and the impact of our networks ' configuration on model performance .",Experiments,Experiments,named-entity-recognition,1,1,0.0,154,0.7439613526570048,1,0.0,1,0
156,Training,,,named-entity-recognition,1,0,0.0,155,0.748792270531401,0,0.0,1,0
157,"For both models presented , we train our networks using the back - propagation algorithm updating our parameters on every training example , one at a time , using stochastic gradient descent ( SGD ) with a learning rate of 0.01 and a gradient clipping of 5.0 .",Training,Training,named-entity-recognition,1,1,0.0333333333333333,156,0.7536231884057971,1,0.0833333333333333,1,1
158,"Several methods have been proposed to enhance the performance of SGD , such as Adadelta or Adam ( Kingma and Ba , 2014 ) .",Training,Training,named-entity-recognition,1,2,0.0666666666666666,157,0.7584541062801933,2,0.1666666666666666,1,0
159,"Although we observe faster convergence using these methods , none of them perform as well as SGD with gradient clipping .",Training,Training,named-entity-recognition,1,3,0.1,158,0.7632850241545893,3,0.25,1,0
160,Our LSTM - CRF model uses a single layer for the forward and backward LSTMs whose dimensions are set to 100 .,Training,Training,named-entity-recognition,1,4,0.1333333333333333,159,0.7681159420289855,4,0.3333333333333333,1,1
161,Tuning this dimension did not significantly impact model performance .,Training,Training,named-entity-recognition,1,5,0.1666666666666666,160,0.7729468599033816,5,0.4166666666666667,1,0
162,We set the dropout rate to 0.5 .,Training,Training,named-entity-recognition,1,6,0.2,161,0.7777777777777778,6,0.5,1,1
163,"Using higher rates negatively impacted our results , while smaller rates led to longer training time .",Training,Training,named-entity-recognition,1,7,0.2333333333333333,162,0.782608695652174,7,0.5833333333333334,1,0
164,The stack - LSTM model uses two layers each of dimension 100 for each stack .,Training,Training,named-entity-recognition,1,8,0.2666666666666666,163,0.7874396135265701,8,0.6666666666666666,1,1
165,"The embeddings of the actions used in the composition functions have 16 dimensions each , and the output embedding is of dimension 20 .",Training,Training,named-entity-recognition,1,9,0.3,164,0.7922705314009661,9,0.75,1,1
166,We experimented with different dropout rates and reported the scores using the best dropout rate for each language .,Training,Training,named-entity-recognition,1,10,0.3333333333333333,165,0.7971014492753623,10,0.8333333333333334,1,0
167,3,Training,Training,named-entity-recognition,1,11,0.3666666666666666,166,0.8019323671497585,11,0.9166666666666666,1,0
168,"It is a greedy model that apply locally optimal actions until the entire sentence is processed , further improvements might be obtained with beam search or training with exploration .",Training,Training,named-entity-recognition,1,12,0.4,167,0.8067632850241546,12,1.0,1,0
169,Data Sets,Training,,named-entity-recognition,1,13,0.4333333333333333,168,0.8115942028985508,0,0.0,1,0
170,We test our model on different datasets for named entity recognition .,Training,Data Sets,named-entity-recognition,1,14,0.4666666666666667,169,0.8164251207729468,1,0.0588235294117647,1,0
171,"To demonstrate our model 's ability to generalize to different languages , we present results on the ) that contain independent named entity labels for English , Spanish , German and Dutch .",Training,Data Sets,named-entity-recognition,1,15,0.5,170,0.821256038647343,2,0.1176470588235294,1,0
172,"All datasets contain four different types of named entities : locations , persons , organizations , and miscellaneous entities that do not belong in any of the three previous categories .",Training,Data Sets,named-entity-recognition,1,16,0.5333333333333333,171,0.8260869565217391,3,0.1764705882352941,1,0
173,"Although POS tags were made available for all datasets , we did not include them in our models .",Training,Data Sets,named-entity-recognition,1,17,0.5666666666666667,172,0.8309178743961353,4,0.2352941176470588,1,0
174,"We did not perform any dataset preprocessing , apart from replacing every digit with a zero in the English NER dataset .",Training,Data Sets,named-entity-recognition,1,18,0.6,173,0.8357487922705314,5,0.2941176470588235,1,0
175,presents our comparisons with other models for named entity recognition in English .,Training,Data Sets,named-entity-recognition,1,19,0.6333333333333333,174,0.8405797101449275,6,0.3529411764705882,1,0
176,"To make the comparison between our model and others fair , we report the scores of other models with and without the use of external labeled data such as gazetteers and knowledge bases .",Training,Data Sets,named-entity-recognition,1,20,0.6666666666666666,175,0.8454106280193237,7,0.4117647058823529,1,0
177,Our models do not use gazetteers or any external labeled resources .,Training,Data Sets,named-entity-recognition,1,21,0.7,176,0.8502415458937198,8,0.4705882352941176,1,0
178,The best score reported on this task is by .,Training,Data Sets,named-entity-recognition,1,22,0.7333333333333333,177,0.855072463768116,9,0.5294117647058824,1,0
179,They obtained a F 1 of 91.2 by jointly modeling the NER and entity linking tasks .,Training,Data Sets,named-entity-recognition,1,23,0.7666666666666667,178,0.8599033816425121,10,0.5882352941176471,1,0
180,"Their model uses a lot of hand - engineered features including spelling features , WordNet clusters , Brown clusters , POS tags , chunks tags , as well as stemming and external knowledge bases like Freebase and Wikipedia .",Training,Data Sets,named-entity-recognition,1,24,0.8,179,0.8647342995169082,11,0.6470588235294118,1,0
181,"Our LSTM - CRF model outperforms all other systems , including the ones using external labeled data like gazetteers .",Training,Data Sets,named-entity-recognition,1,25,0.8333333333333334,180,0.8695652173913043,12,0.7058823529411765,1,1
182,"Our Stack - LSTM model also outperforms all previous models that do not incorporate external features , apart from the one presented by .",Training,Data Sets,named-entity-recognition,1,26,0.8666666666666667,181,0.8743961352657005,13,0.7647058823529411,1,1
183,"and 4 present our results on NER for German , Dutch and Spanish respectively in comparison to other models .",Training,Data Sets,named-entity-recognition,1,27,0.9,182,0.8792270531400966,14,0.8235294117647058,1,1
184,"On these three languages , the LSTM - CRF model significantly outperforms all previous methods , including the ones using external labeled data .",Training,Data Sets,named-entity-recognition,1,28,0.9333333333333332,183,0.8840579710144928,15,0.8823529411764706,1,1
185,"The only exception is Dutch , where the model of can perform better by leveraging the information from other NER datasets .",Training,Data Sets,named-entity-recognition,1,29,0.9666666666666668,184,0.8888888888888888,16,0.9411764705882352,1,1
186,The Stack - LSTM also consistently presents statethe - art ( or close to ) results compared to systems that do not use external data .,Training,Data Sets,named-entity-recognition,1,30,1.0,185,0.893719806763285,17,1.0,1,1
187,Results,,,named-entity-recognition,1,0,0.0,186,0.8985507246376812,0,0.0,1,0
188,"As we can see in the tables , the Stack - LSTM model is more dependent on character - based representations to achieve competitive performance ; we hypothesize that the LSTM - CRF model requires less orthographic information since it gets more contextual information out of the bidirectional LSTMs ; however , the Stack - LSTM model consumes the words one by one and it just relies on the word representations when it chunks words .",Results,Results,named-entity-recognition,1,1,0.0,187,0.9033816425120772,1,0.0,1,0
189,Network architectures,,,named-entity-recognition,1,0,0.0,188,0.9082125603864736,0,0.0,1,0
190,Our models had several components that we could tweak to understand their impact on the overall performance .,Network architectures,Network architectures,named-entity-recognition,1,1,0.0769230769230769,189,0.9130434782608696,1,0.0769230769230769,1,0
191,"We explored the impact that the CRF , the character - level representations , pretraining of our presented a model similar to our LSTM - CRF , but using hand - crafted spelling features .",Network architectures,Network architectures,named-entity-recognition,1,2,0.1538461538461538,190,0.9178743961352656,2,0.1538461538461538,1,0
192,also used a similar model and adapted it to the semantic role labeling task .,Network architectures,Network architectures,named-entity-recognition,1,3,0.2307692307692307,191,0.9227053140096618,3,0.2307692307692307,1,0
193,"used a linear chain CRF with L 2 regularization , they added phrase cluster features extracted from the web data and spelling features .",Network architectures,Network architectures,named-entity-recognition,1,4,0.3076923076923077,192,0.927536231884058,4,0.3076923076923077,1,0
194,also used a linear chain CRF with spelling features and gazetteers .,Network architectures,Network architectures,named-entity-recognition,1,5,0.3846153846153846,193,0.932367149758454,5,0.3846153846153846,1,0
195,Language independent,Network architectures,,named-entity-recognition,1,6,0.4615384615384615,194,0.9371980676328504,6,0.4615384615384615,1,0
196,NER models like ours have also been proposed in the past .,Network architectures,Language independent,named-entity-recognition,1,7,0.5384615384615384,195,0.9420289855072465,7,0.5384615384615384,1,0
197,present semi-supervised bootstrapping algorithms for named entity recognition by co-training character - level ( word - internal ) and token - level ( context ) features .,Network architectures,Language independent,named-entity-recognition,1,8,0.6153846153846154,196,0.9468599033816424,8,0.6153846153846154,1,0
198,use Bayesian nonparametrics to construct a database of named entities in an almost unsupervised setting .,Network architectures,Language independent,named-entity-recognition,1,9,0.6923076923076923,197,0.9516908212560388,9,0.6923076923076923,1,0
199,Ratinov and Roth quantitatively compare several approaches for NER and build their own supervised model using a regularized average perceptron and aggregating context information .,Network architectures,Language independent,named-entity-recognition,1,10,0.7692307692307693,198,0.9565217391304348,10,0.7692307692307693,1,0
200,"Finally , there is currently a lot of interest in models for NER that use letter - based representations .",Network architectures,Language independent,named-entity-recognition,1,11,0.8461538461538461,199,0.961352657004831,11,0.8461538461538461,1,0
201,model the task of sequencelabeling as a sequence to sequence learning problem and incorporate character - based representations into their encoder model .,Network architectures,Language independent,named-entity-recognition,1,12,0.9230769230769232,200,0.966183574879227,12,0.9230769230769232,1,0
202,"employ an architecture similar to ours , but instead use CNNs to learn character - level features , in away similar to the work by .",Network architectures,Language independent,named-entity-recognition,1,13,1.0,201,0.9710144927536232,13,1.0,1,0
203,Conclusion,,,named-entity-recognition,1,0,0.0,202,0.9758454106280192,0,0.0,1,0
204,"This paper presents two neural architectures for sequence labeling that provide the best NER results ever reported in standard evaluation settings , even compared with models that use external resources , such as gazetteers .",Conclusion,Conclusion,named-entity-recognition,1,1,0.25,203,0.9806763285024156,1,0.25,0,0
205,"A key aspect of our models are that they model output label dependencies , either via a simple CRF architecture , or using a transition - based algorithm to explicitly construct and label chunks of the input .",Conclusion,Conclusion,named-entity-recognition,1,2,0.5,204,0.9855072463768116,2,0.5,0,0
206,"Word representations are also crucially important for success : we use both pre-trained word representations and "" character - based "" representations that capture morphological and orthographic information .",Conclusion,Conclusion,named-entity-recognition,1,3,0.75,205,0.9903381642512076,3,0.75,0,0
207,"To prevent the learner from depending too heavily on one representation class , dropout is used .",Conclusion,Conclusion,named-entity-recognition,1,4,1.0,206,0.9951690821256038,4,1.0,0,0
1,title,,,named-entity-recognition,2,0,0.0,0,0.0,0,0.0,1,0
2,Fast and Accurate Entity Recognition with Iterated Dilated Convolutions,title,,named-entity-recognition,2,1,0.0,1,0.0046948356807511,1,0.0,1,1
3,abstract,,,named-entity-recognition,2,0,0.0,2,0.0093896713615023,0,0.0,1,0
4,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs .",abstract,abstract,named-entity-recognition,2,1,0.1428571428571428,3,0.0140845070422535,1,0.1428571428571428,1,0
5,Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) .,abstract,abstract,named-entity-recognition,2,2,0.2857142857142857,4,0.0187793427230046,2,0.2857142857142857,1,0
6,"Though expressive and accurate , these models fail to fully exploit GPU parallelism , limiting their computational efficiency .",abstract,abstract,named-entity-recognition,2,3,0.4285714285714285,5,0.0234741784037558,3,0.4285714285714285,1,0
7,"This paper proposes a faster alternative to Bi - LSTMs for NER : Iterated Dilated Convolutional Neural Networks ( ID - CNNs ) , which have better capacity than traditional CNNs for large context and structured prediction .",abstract,abstract,named-entity-recognition,2,4,0.5714285714285714,6,0.028169014084507,4,0.5714285714285714,1,1
8,"Unlike LSTMs whose sequential processing on sentences of length N requires O(N ) time even in the face of parallelism , ID - CNNs permit fixed - depth convolutions to run in parallel across entire documents .",abstract,abstract,named-entity-recognition,2,5,0.7142857142857143,7,0.0328638497652582,5,0.7142857142857143,1,0
9,"We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF .",abstract,abstract,named-entity-recognition,2,6,0.8571428571428571,8,0.0375586854460093,6,0.8571428571428571,1,0
10,"Moreover , ID - CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8 x faster test time speeds .",abstract,abstract,named-entity-recognition,2,7,1.0,9,0.0422535211267605,7,1.0,1,0
11,Introduction,,,named-entity-recognition,2,0,0.0,10,0.0469483568075117,0,0.0,1,0
12,"In order to democratize large - scale NLP and information extraction while minimizing our environmental footprint , we require fast , resource - efficient methods for sequence tagging tasks such as part - of - speech tagging and named entity recognition ( NER ) .",Introduction,Introduction,named-entity-recognition,2,1,0.0294117647058823,11,0.0516431924882629,1,0.0175438596491228,1,1
13,Speed is not sufficient of course : they must also be expressive enough to tolerate the tremendous lexical variation in input data .,Introduction,Introduction,named-entity-recognition,2,2,0.0588235294117647,12,0.056338028169014,2,0.0350877192982456,1,0
14,The massively parallel computation facilitated by GPU hardware has led to a surge of successful neural network architectures for sequence labeling .,Introduction,Introduction,named-entity-recognition,2,3,0.088235294117647,13,0.0610328638497652,3,0.0526315789473684,1,0
15,"While these models are expressive and accurate , they fail to fully exploit the parallelism opportunities of a GPU , and thus their speed is limited .",Introduction,Introduction,named-entity-recognition,2,4,0.1176470588235294,14,0.0657276995305164,4,0.0701754385964912,1,0
16,"Specifically , they employ either recurrent neural networks ( RNNs ) for feature extraction , or Viterbi inference in a structured output model , both of which require sequential computation across the length of the input .",Introduction,Introduction,named-entity-recognition,2,5,0.1470588235294117,15,0.0704225352112676,5,0.087719298245614,1,0
17,"Instead , parallelized runtime independent of the length of the sequence saves time and energy costs , maximizing GPU resource usage and minimizing the amount of time it takes to train and evaluate models .",Introduction,Introduction,named-entity-recognition,2,6,0.1764705882352941,16,0.0751173708920187,6,0.1052631578947368,1,0
18,Convolutional neural networks ( CNNs ) provide exactly this property .,Introduction,Introduction,named-entity-recognition,2,7,0.2058823529411764,17,0.0798122065727699,7,0.1228070175438596,1,0
19,"Rather than composing representations incrementally over each token in a sequence , they apply filters in parallel across the entire sequence at once .",Introduction,Introduction,named-entity-recognition,2,8,0.2352941176470588,18,0.0845070422535211,8,0.1403508771929824,1,0
20,"Their computational cost grows with the number of layers , but not the input size , up to the memory and threading limitations of the hardware .",Introduction,Introduction,named-entity-recognition,2,9,0.2647058823529412,19,0.0892018779342723,9,0.1578947368421052,1,0
21,"This provides , for example , audio generation models that can be trained in parallel ( van den .",Introduction,Introduction,named-entity-recognition,2,10,0.2941176470588235,20,0.0938967136150234,10,0.175438596491228,1,0
22,"Despite the clear computational advantages of CNNs , RNNs have become the standard method for composing deep representations of text .",Introduction,Introduction,named-entity-recognition,2,11,0.3235294117647059,21,0.0985915492957746,11,0.1929824561403508,1,0
23,"This is because a token encoded by a bidirectional RNN will incorporate evidence from the entire input sequence , but the CNN 's representation is limited by the effective input width 1 of the network : the size of the input context which is observed , directly or indirectly , by the representation of a token at a given layer in the network .",Introduction,Introduction,named-entity-recognition,2,12,0.3529411764705882,22,0.1032863849765258,12,0.2105263157894736,1,0
24,"Specifically , in a network composed of a series of stacked convolutional layers of convolution width w , the number r of context tokens incorporated into a token 's representation at a given layer l , is given by r = l ( w ?",Introduction,Introduction,named-entity-recognition,2,13,0.3823529411764705,23,0.107981220657277,13,0.2280701754385964,1,0
25,1 ) + 1 . The number of layers required to incorporate the entire input context grows linearly with the length of the sequence .,Introduction,Introduction,named-entity-recognition,2,14,0.4117647058823529,24,0.1126760563380281,14,0.2456140350877192,1,0
26,"To avoid this scaling , one could pool representations across the sequence , but this is not appropriate for sequence labeling , since it reduces the output resolution of the representation .",Introduction,Introduction,named-entity-recognition,2,15,0.4411764705882353,25,0.1173708920187793,15,0.2631578947368421,1,0
27,"In response , this paper presents an application of dilated convolutions for sequence labeling ) .",Introduction,Introduction,named-entity-recognition,2,16,0.4705882352941176,26,0.1220657276995305,16,0.2807017543859649,1,1
28,"For dilated convolutions , the effective input width can grow exponentially with the depth , with no loss in resolution at each layer and with a modest number of parameters to estimate .",Introduction,Introduction,named-entity-recognition,2,17,0.5,27,0.1267605633802817,17,0.2982456140350877,1,0
29,"Like typical CNN layers , dilated convolutions operate on a sliding window of context over the sequence , but unlike conventional convolutions , the context need not be consecutive ; the dilated window skips over every dilation width d inputs .",Introduction,Introduction,named-entity-recognition,2,18,0.5294117647058824,28,0.1314553990610328,18,0.3157894736842105,1,1
30,"By stacking layers of dilated convolutions of exponentially increasing dilation width , we can expand the size of the effective input width to cover the entire length of most sequences using only a few layers :",Introduction,Introduction,named-entity-recognition,2,19,0.5588235294117647,29,0.136150234741784,19,0.3333333333333333,1,1
31,The size of the effective input width fora token at layer l is now given by 2 l +1 ?1 .,Introduction,Introduction,named-entity-recognition,2,20,0.5882352941176471,30,0.1408450704225352,20,0.3508771929824561,1,0
32,"More concretely , just four stacked dilated convolutions of width 3 produces token representations with an effective input width of 31 tokens - longer than the average sentence length ( 23 ) in the Penn TreeBank .",Introduction,Introduction,named-entity-recognition,2,21,0.6176470588235294,31,0.1455399061032863,21,0.3684210526315789,1,0
33,Our overall iterated dilated CNN architecture ( ID - CNN ) repeatedly applies the same block of dilated convolutions to token - wise representations .,Introduction,Introduction,named-entity-recognition,2,22,0.6470588235294118,32,0.1502347417840375,22,0.3859649122807017,1,1
34,This parameter sharing prevents overfitting and also provides opportunities to inject supervision on intermediate activations of the network .,Introduction,Introduction,named-entity-recognition,2,23,0.6764705882352942,33,0.1549295774647887,23,0.4035087719298245,1,1
35,"Similar to models that use logits produced by an RNN , the ID - CNN provides two methods for performing prediction : we can predict each token 's label independently , or by running Viterbi inference in a chain structured graphical model .",Introduction,Introduction,named-entity-recognition,2,24,0.7058823529411765,34,0.1596244131455399,24,0.4210526315789473,1,1
36,"In experiments on CoNLL 2003 and OntoNotes 1 What we call effective input width here is known as the receptive field in the vision literature , drawing an analogy to the visual receptive field of a neuron in the retina . :",Introduction,Introduction,named-entity-recognition,2,25,0.7352941176470589,35,0.1643192488262911,25,0.4385964912280701,1,0
37,A dilated CNN block with maximum dilation width 4 and filter width 3 .,Introduction,Introduction,named-entity-recognition,2,26,0.7647058823529411,36,0.1690140845070422,26,0.4561403508771929,1,0
38,Neurons contributing to a single highlighted neuron in the last layer are also highlighted .,Introduction,Introduction,named-entity-recognition,2,27,0.7941176470588235,37,0.1737089201877934,27,0.4736842105263157,1,0
39,"5.0 English NER , we demonstrate significant speed gains of our ID - CNNs over various recurrent models , while maintaining similar F1 performance .",Introduction,Introduction,named-entity-recognition,2,28,0.8235294117647058,38,0.1784037558685446,28,0.4912280701754385,1,0
40,"When performing prediction using independent classification , the ID - CNN consistently outperforms a bidirectional LSTM ( Bi - LSTM ) , and performs on par with inference in a CRF with logits from a Bi - LSTM ( Bi - LSTM - CRF ) .",Introduction,Introduction,named-entity-recognition,2,29,0.8529411764705882,39,0.1830985915492957,29,0.5087719298245614,1,0
41,"As an extractor of per-token logits fora CRF , our model out - performs the Bi - LSTM - CRF .",Introduction,Introduction,named-entity-recognition,2,30,0.8823529411764706,40,0.1877934272300469,30,0.5263157894736842,1,0
42,"We also apply ID - CNNs to entire documents , where independent token classification is as accurate as the Bi - LSTM - CRF while decoding almost 8 faster .",Introduction,Introduction,named-entity-recognition,2,31,0.9117647058823528,41,0.1924882629107981,31,0.543859649122807,1,0
43,The clear accuracy gains resulting from incorporating broader context suggest that these models could similarly benefit many other contextsensitive NLP tasks which have until now been limited by the computational complexity of existing context - rich models .,Introduction,Introduction,named-entity-recognition,2,32,0.9411764705882352,42,0.1971830985915492,32,0.5614035087719298,1,0
44,2 2 Background,Introduction,Introduction,named-entity-recognition,2,33,0.9705882352941176,43,0.2018779342723004,33,0.5789473684210527,1,0
45,Conditional Probability,Introduction,,named-entity-recognition,2,34,1.0,44,0.2065727699530516,34,0.5964912280701754,1,0
46,Models for Tagging,,,named-entity-recognition,2,0,0.0,45,0.2112676056338028,35,0.6140350877192983,1,0
47,"Let x = [ x 1 , . . . , x T ] be our input text and y = [ y 1 , . . . , y T ] be per-token output tags .",Models for Tagging,Models for Tagging,named-entity-recognition,2,1,0.0222222222222222,46,0.215962441314554,36,0.631578947368421,1,0
48,Let D be the domain size of each y i .,Models for Tagging,Models for Tagging,named-entity-recognition,2,2,0.0444444444444444,47,0.2206572769953051,37,0.6491228070175439,1,0
49,"We predict the most likely y , given a conditional model P ( y|x ) .",Models for Tagging,Models for Tagging,named-entity-recognition,2,3,0.0666666666666666,48,0.2253521126760563,38,0.6666666666666666,1,0
50,This paper considers two factorizations of the conditional distribution .,Models for Tagging,Models for Tagging,named-entity-recognition,2,4,0.0888888888888888,49,0.2300469483568075,39,0.6842105263157895,1,0
51,"First , we have",Models for Tagging,Models for Tagging,named-entity-recognition,2,5,0.1111111111111111,50,0.2347417840375587,40,0.7017543859649122,1,0
52,where the tags are conditionally independent given some features for x .,Models for Tagging,Models for Tagging,named-entity-recognition,2,6,0.1333333333333333,51,0.2394366197183098,41,0.7192982456140351,1,0
53,"Given these features , O ( D ) prediction is simple and parallelizable across the length of the sequence .",Models for Tagging,Models for Tagging,named-entity-recognition,2,7,0.1555555555555555,52,0.244131455399061,42,0.7368421052631579,1,0
54,"However , feature extraction may not necessarily be parallelizable .",Models for Tagging,Models for Tagging,named-entity-recognition,2,8,0.1777777777777777,53,0.2488262910798122,43,0.7543859649122807,1,0
55,"For example , RNN - based features require iterative passes along the length of x .",Models for Tagging,Models for Tagging,named-entity-recognition,2,9,0.2,54,0.2535211267605634,44,0.7719298245614035,1,0
56,We also consider a linear - chain CRF model that couples all of y together :,Models for Tagging,Models for Tagging,named-entity-recognition,2,10,0.2222222222222222,55,0.2582159624413145,45,0.7894736842105263,1,0
57,where ?,Models for Tagging,Models for Tagging,named-entity-recognition,2,11,0.2444444444444444,56,0.2629107981220657,46,0.8070175438596491,1,0
58,"t is a local factor , ?",Models for Tagging,Models for Tagging,named-entity-recognition,2,12,0.2666666666666666,57,0.2676056338028169,47,0.8245614035087719,1,0
59,"p is a pairwise factor that scores consecutive tags , and Z x is the partition function .",Models for Tagging,Models for Tagging,named-entity-recognition,2,13,0.2888888888888888,58,0.272300469483568,48,0.8421052631578947,1,0
60,"To avoid overfitting , ?",Models for Tagging,Models for Tagging,named-entity-recognition,2,14,0.3111111111111111,59,0.2769953051643192,49,0.8596491228070176,1,0
61,p does not depend on the timestep tor the input x in our experiments .,Models for Tagging,Models for Tagging,named-entity-recognition,2,15,0.3333333333333333,60,0.2816901408450704,50,0.8771929824561403,1,0
62,Prediction in this model requires global search using the O ( D 2 T ),Models for Tagging,Models for Tagging,named-entity-recognition,2,16,0.3555555555555555,61,0.2863849765258216,51,0.8947368421052632,1,0
63,Viterbi algorithm .,Models for Tagging,Models for Tagging,named-entity-recognition,2,17,0.3777777777777777,62,0.2910798122065727,52,0.912280701754386,1,0
64,"CRF prediction explicitly reasons about interactions among neighboring output tags , whereas prediction in the first model compiles this reasoning into the feature extraction step .",Models for Tagging,Models for Tagging,named-entity-recognition,2,18,0.4,63,0.2957746478873239,53,0.9298245614035088,1,0
65,The suitability of such compilation depends on the properties and quantity of the data .,Models for Tagging,Models for Tagging,named-entity-recognition,2,19,0.4222222222222222,64,0.3004694835680751,54,0.9473684210526316,1,0
66,"While CRF prediction requires non-trivial search in output space , it can guarantee that certain output constraints , such as for IOB tagging , will always be satisfied .",Models for Tagging,Models for Tagging,named-entity-recognition,2,20,0.4444444444444444,65,0.3051643192488263,55,0.9649122807017544,1,0
67,"It may also have better sample complexity , as it imposes more prior knowledge about the structure of the interactions among the tags .",Models for Tagging,Models for Tagging,named-entity-recognition,2,21,0.4666666666666667,66,0.3098591549295774,56,0.9824561403508772,1,0
68,"However , it has worse computational complexity than independent prediction .",Models for Tagging,Models for Tagging,named-entity-recognition,2,22,0.4888888888888889,67,0.3145539906103286,57,1.0,1,0
69,Dilated Convolutions,Models for Tagging,,named-entity-recognition,2,23,0.5111111111111111,68,0.3192488262910798,0,0.0,1,0
70,"CNNs in NLP are typically one - dimensional , applied to a sequence of vectors representing tokens rather than to a two -dimensional grid of vectors representing pixels .",Models for Tagging,Dilated Convolutions,named-entity-recognition,2,24,0.5333333333333333,69,0.323943661971831,1,0.0454545454545454,1,0
71,"In this setting , a convolutional neural network layer is equivalent to applying an affine transformation , W c to a sliding window of width r tokens on either side of each token in the sequence .",Models for Tagging,Dilated Convolutions,named-entity-recognition,2,25,0.5555555555555556,70,0.3286384976525822,2,0.0909090909090909,1,0
72,"Here , and throughout the paper , we do not explicitly write the bias terms in affine transformations .",Models for Tagging,Dilated Convolutions,named-entity-recognition,2,26,0.5777777777777777,71,0.3333333333333333,3,0.1363636363636363,1,0
73,The convolutional operator applied to each token x t with output ct is defined as :,Models for Tagging,Dilated Convolutions,named-entity-recognition,2,27,0.6,72,0.3380281690140845,4,0.1818181818181818,1,0
74,where ?,Models for Tagging,Dilated Convolutions,named-entity-recognition,2,28,0.6222222222222222,73,0.3427230046948357,5,0.2272727272727272,1,0
75,is vector concatenation .,Models for Tagging,Dilated Convolutions,named-entity-recognition,2,29,0.6444444444444445,74,0.3474178403755869,6,0.2727272727272727,1,0
76,"Dilated convolutions perform the same operation , except rather than transforming adjacent in - puts , the convolution is defined over a wider effective input width by skipping over ?",Models for Tagging,Dilated Convolutions,named-entity-recognition,2,30,0.6666666666666666,75,0.352112676056338,7,0.3181818181818182,1,0
77,"inputs at a time , where ?",Models for Tagging,Dilated Convolutions,named-entity-recognition,2,31,0.6888888888888889,76,0.3568075117370892,8,0.3636363636363636,1,0
78,is the dilation width .,Models for Tagging,Dilated Convolutions,named-entity-recognition,2,32,0.7111111111111111,77,0.3615023474178404,9,0.4090909090909091,1,0
79,We define the dilated convolution operator :,Models for Tagging,Dilated Convolutions,named-entity-recognition,2,33,0.7333333333333333,78,0.3661971830985915,10,0.4545454545454545,1,0
80,A dilated convolution of width 1 is equivalent to a simple convolution .,Models for Tagging,Dilated Convolutions,named-entity-recognition,2,34,0.7555555555555555,79,0.3708920187793427,11,0.5,1,0
81,"Using the same number of parameters as a simple convolution with the same radius ( i.e. W c has the same dimensionality ) , the ? > 1 dilated convolution incorporates broader context into the representation of a token than a simple convolution .",Models for Tagging,Dilated Convolutions,named-entity-recognition,2,35,0.7777777777777778,80,0.3755868544600939,12,0.5454545454545454,1,0
82,Multi - Scale Context Aggregation,Models for Tagging,,named-entity-recognition,2,36,0.8,81,0.380281690140845,13,0.5909090909090909,1,0
83,We can leverage the ability of dilated convolutions to incorporate global context without losing important local information by stacking dilated convolutions of increasing width .,Models for Tagging,Multi - Scale Context Aggregation,named-entity-recognition,2,37,0.8222222222222222,82,0.3849765258215962,14,0.6363636363636364,1,0
84,"First described for pixel classification in computer vision , achieve state - of - the - art results on image segmentation benchmarks by stacking dilated convolutions with exponentially increasing rates of dilation , a technique they refer to as multiscale context aggregation .",Models for Tagging,Multi - Scale Context Aggregation,named-entity-recognition,2,38,0.8444444444444444,83,0.3896713615023474,15,0.6818181818181818,1,0
85,"By feeding the outputs of each dilated convolution as the input to the next , increasingly non-local information is incorporated into each pixel 's representation .",Models for Tagging,Multi - Scale Context Aggregation,named-entity-recognition,2,39,0.8666666666666667,84,0.3943661971830985,16,0.7272727272727273,1,0
86,Performing a dilation - 1 convolution in the first layer ensures that no pixels within the effective input width of any pixel are excluded .,Models for Tagging,Multi - Scale Context Aggregation,named-entity-recognition,2,40,0.8888888888888888,85,0.3990610328638497,17,0.7727272727272727,1,0
87,"By doubling the dilation width at each layer , the size of the effective input width grows exponentially while the number of parameters grows only linearly with the number of layers , so a pixel representation quickly incorporates rich global evidence from the entire image .",Models for Tagging,Multi - Scale Context Aggregation,named-entity-recognition,2,41,0.9111111111111112,86,0.4037558685446009,18,0.8181818181818182,1,0
88,"Unfortunately , simply increasing the depth of stacked dilated CNNs causes considerable overfitting in our experiments .",Models for Tagging,Multi - Scale Context Aggregation,named-entity-recognition,2,42,0.9333333333333332,87,0.4084507042253521,19,0.8636363636363636,1,0
89,"In response , we present Iterated Dilated CNNs ( ID - CNNs ) , which instead apply the same small stack of dilated convolutions multiple times , each iterate taking as input the result of the last application .",Models for Tagging,Multi - Scale Context Aggregation,named-entity-recognition,2,43,0.9555555555555556,88,0.4131455399061033,20,0.9090909090909092,1,0
90,Repeatedly employing the same parameters in a recurrent fashion provides both broad effective input width and desirable generalization capabilities .,Models for Tagging,Multi - Scale Context Aggregation,named-entity-recognition,2,44,0.9777777777777776,89,0.4178403755868544,21,0.9545454545454546,1,0
91,"We also obtain significant accuracy gains with a training objective that strives for accurate labeling after each iterate , allowing follow - on iterations to observe and resolve dependency violations .",Models for Tagging,Multi - Scale Context Aggregation,named-entity-recognition,2,45,1.0,90,0.4225352112676056,22,1.0,1,0
92,Model Architecture,,,named-entity-recognition,2,0,0.0,91,0.4272300469483568,0,0.0,1,0
93,"The network takes as input a sequence of T vectors x t , and outputs a sequence of per-class scores ht , which serve either as the local conditional distributions of Eqn. ( 1 ) or the local factors ?",Model Architecture,Model Architecture,named-entity-recognition,2,1,0.0833333333333333,92,0.431924882629108,1,0.0833333333333333,1,0
94,t of Eqn..,Model Architecture,Model Architecture,named-entity-recognition,2,2,0.1666666666666666,93,0.4366197183098591,2,0.1666666666666666,1,0
95,We denote the jth dilated convolutional layer of dilation width ? as D ( j ) ? .,Model Architecture,Model Architecture,named-entity-recognition,2,3,0.25,94,0.4413145539906103,3,0.25,1,0
96,The first layer in the network is a dilation - 1 convolution D ( 0 ) 1 that transforms the input to a representation it :,Model Architecture,Model Architecture,named-entity-recognition,2,4,0.3333333333333333,95,0.4460093896713615,4,0.3333333333333333,1,0
97,"Next , L c layers of dilated convolutions of exponentially increasing dilation width are applied to it , folding in increasingly broader context into the embedded representation of x tat each layer .",Model Architecture,Model Architecture,named-entity-recognition,2,5,0.4166666666666667,96,0.4507042253521127,5,0.4166666666666667,1,0
98,Let r ( ) denote the ReLU activation function .,Model Architecture,Model Architecture,named-entity-recognition,2,6,0.5,97,0.4553990610328638,6,0.5,1,0
99,Beginning with ct ( 0 ) = it we define the stack of layers with the following recurrence :,Model Architecture,Model Architecture,named-entity-recognition,2,7,0.5833333333333334,98,0.460093896713615,7,0.5833333333333334,1,0
100,and add a final dilation - 1 layer to the stack :,Model Architecture,Model Architecture,named-entity-recognition,2,8,0.6666666666666666,99,0.4647887323943662,8,0.6666666666666666,1,0
101,"We refer to this stack of dilated convolutions as a block B ( ) , which has output resolution equal to its input resolution .",Model Architecture,Model Architecture,named-entity-recognition,2,9,0.75,100,0.4694835680751174,9,0.75,1,0
102,"To incorporate even broader context without over - fitting , we avoid making B deeper , and instead iteratively apply B L b times , introducing no extra parameters .",Model Architecture,Model Architecture,named-entity-recognition,2,10,0.8333333333333334,101,0.4741784037558685,10,0.8333333333333334,1,0
103,Starting,Model Architecture,,named-entity-recognition,2,11,0.9166666666666666,102,0.4788732394366197,11,0.9166666666666666,1,0
104,We apply a simple affine transformation W o to this final representation to obtain per-class scores for each token x t :,Model Architecture,Starting,named-entity-recognition,2,12,1.0,103,0.4835680751173709,12,1.0,1,0
105,Training,,,named-entity-recognition,2,0,0.0,104,0.488262910798122,0,0.0,1,0
106,"Our main focus is to apply the ID - CNN an encoder to produce per-token logits for the first conditional model described in Sec. 2.1 , where tags are conditionally independent given deep features , since this will enable prediction that is parallelizable across the length of the input sequence .",Training,Training,named-entity-recognition,2,1,0.05,105,0.4929577464788732,1,0.05,1,0
107,"Here , maximum likelihood training is straightforward because the likelihood decouples into the sum of the likelihoods of independent logistic regression problems for every tag , with natural parameters given by Eqn. :",Training,Training,named-entity-recognition,2,2,0.1,106,0.4976525821596244,2,0.1,1,0
108,"We can also use the ID - CNN as logits for the CRF model ( Eqn. ( 2 ) ) , where the partition function and its gradient are computed using the forward - backward algorithm .",Training,Training,named-entity-recognition,2,3,0.15,107,0.5023474178403756,3,0.15,1,0
109,We next present an alternative training method that helps bridge the gap between these two techniques .,Training,Training,named-entity-recognition,2,4,0.2,108,0.5070422535211268,4,0.2,1,0
110,Sec. 2.1 identifies that the CRF has preferable sample complexity and accuracy since prediction directly reasons in the space of structured outputs .,Training,Training,named-entity-recognition,2,5,0.25,109,0.5117370892018779,5,0.25,1,0
111,"In response , we compile some of this reasoning in output space into ID - CNN feature extraction .",Training,Training,named-entity-recognition,2,6,0.3,110,0.5164319248826291,6,0.3,1,0
112,"Instead of explicit reasoning over output labels during inference , we train the network such that each block is predictive of output labels .",Training,Training,named-entity-recognition,2,7,0.35,111,0.5211267605633803,7,0.35,1,0
113,"Subsequent blocks learn to correct dependency violations of their predecessors , refining the final sequence prediction .",Training,Training,named-entity-recognition,2,8,0.4,112,0.5258215962441315,8,0.4,1,0
114,"To do so , we first define predictions of the model after each of the L b applications of the block .",Training,Training,named-entity-recognition,2,9,0.45,113,0.5305164319248826,9,0.45,1,0
115,"Let ht ( k ) be the result of applying the matrix W o from ( 9 ) to b t ( k ) , the output of block k.",Training,Training,named-entity-recognition,2,10,0.5,114,0.5352112676056338,10,0.5,1,0
116,We minimize the average of the losses for each application of the block :,Training,Training,named-entity-recognition,2,11,0.55,115,0.539906103286385,11,0.55,1,0
117,"By rewarding accurate predictions after each application of the block , we learn a model where later blocks are used to refine initial predictions .",Training,Training,named-entity-recognition,2,12,0.6,116,0.5446009389671361,12,0.6,1,0
118,The loss also helps reduce the vanishing gradient problem for deep architectures .,Training,Training,named-entity-recognition,2,13,0.65,117,0.5492957746478874,13,0.65,1,0
119,"Such an approach has been applied in a variety of contexts for training very deep networks in computer vision , but not to our knowledge in NLP .",Training,Training,named-entity-recognition,2,14,0.7,118,0.5539906103286385,14,0.7,1,0
120,We apply dropout to the raw inputs x t and to each block 's output b t ( b ) to help prevent overfitting .,Training,Training,named-entity-recognition,2,15,0.75,119,0.5586854460093896,15,0.75,1,0
121,The version of dropout typically used in practice has the undesirable property that the randomized predictor used at train time differs from the fixed one used attest time .,Training,Training,named-entity-recognition,2,16,0.8,120,0.5633802816901409,16,0.8,1,0
122,"present dropout with expectationlinear regularization , which explicitly regularizes these two predictors to behave similarly .",Training,Training,named-entity-recognition,2,17,0.85,121,0.568075117370892,17,0.85,1,0
123,All of our best reported results include such regularization .,Training,Training,named-entity-recognition,2,18,0.9,122,0.5727699530516432,18,0.9,1,0
124,"This is the first investigation of the technique 's effectiveness for NLP , including for RNNs .",Training,Training,named-entity-recognition,2,19,0.95,123,0.5774647887323944,19,0.95,1,0
125,We encourage its further application .,Training,Training,named-entity-recognition,2,20,1.0,124,0.5821596244131455,20,1.0,1,0
126,Related work,,,named-entity-recognition,2,0,0.0,125,0.5868544600938967,0,0.0,1,0
127,"The state - of - the art models for sequence labeling include an inference step that searches the space of possible output sequences of a chain - structured graphical model , or approximates this search with abeam .",Related work,Related work,named-entity-recognition,2,1,0.0434782608695652,126,0.5915492957746479,1,0.0434782608695652,0,0
128,"These outperform similar systems that use the same features , but independent local predictions .",Related work,Related work,named-entity-recognition,2,2,0.0869565217391304,127,0.596244131455399,2,0.0869565217391304,0,0
129,"On the other hand , the greedy sequential prediction ) approach of , which employs lexicalized features , gazetteers , and word clusters , outperforms CRFs with similar features .",Related work,Related work,named-entity-recognition,2,3,0.1304347826086956,128,0.6009389671361502,3,0.1304347826086956,0,0
130,LSTMs were used for NER as early as the CoNLL shared task in 2003 .,Related work,Related work,named-entity-recognition,2,4,0.1739130434782608,129,0.6056338028169014,4,0.1739130434782608,0,0
131,"More recently , a wide variety of neural network architectures for NER have been proposed .",Related work,Related work,named-entity-recognition,2,5,0.217391304347826,130,0.6103286384976526,5,0.217391304347826,0,0
132,"employ a one - layer CNN with pre-trained word embeddings , capitalization and lexicon features , and CRF - based prediction .",Related work,Related work,named-entity-recognition,2,6,0.2608695652173913,131,0.6150234741784038,6,0.2608695652173913,0,0
133,"achieved state - of - the - art accuracy on partof - speech , chunking and NER using a Bi - LSTM - CRF. proposed two models which incorporated Bi - LSTM - composed character embeddings alongside words : a Bi - LSTM - CRF , and a greedy stack LSTM which uses a simple shift - reduce grammar to compose words into labeled entities .",Related work,Related work,named-entity-recognition,2,7,0.3043478260869565,132,0.6197183098591549,7,0.3043478260869565,0,0
134,Their Bi - LSTM - CRF obtained the state - of - the - art on four languages without word shape or lexicon features .,Related work,Related work,named-entity-recognition,2,8,0.3478260869565217,133,0.6244131455399061,8,0.3478260869565217,0,0
135,"use CNNs rather than LSTMs to compose characters in a Bi - LSTM - CRF , achieving state - of the - art performance on part - of - speech tagging and CoNLL NER without lexicons .",Related work,Related work,named-entity-recognition,2,9,0.391304347826087,134,0.6291079812206573,9,0.391304347826087,0,0
136,"Chiu and Nichols ( 2016 ) evaluate a similar network but propose a novel method for encoding lexicon matches , presenting results on CoNLL and OntoNotes NER .",Related work,Related work,named-entity-recognition,2,10,0.4347826086956521,135,0.6338028169014085,10,0.4347826086956521,0,0
137,use GRU - CRFs with GRUcomposed character embeddings of words to train a single network on many tasks and languages .,Related work,Related work,named-entity-recognition,2,11,0.4782608695652174,136,0.6384976525821596,11,0.4782608695652174,0,0
138,"In general , distributed representations for text can provide useful generalization capabilities for NER systems , since they can leverage unsupervised pre-training of distributed word representations .",Related work,Related work,named-entity-recognition,2,12,0.5217391304347826,137,0.6431924882629108,12,0.5217391304347826,0,0
139,"Though our models would also likely benefit from additional features such as character representations and lexicons , we focus on simpler models which use word - embeddings alone , leaving more elaborate input representations to future work .",Related work,Related work,named-entity-recognition,2,13,0.5652173913043478,138,0.647887323943662,13,0.5652173913043478,0,0
140,"In these NER approaches , CNNs were used for low - level feature extraction that feeds into alternative architectures .",Related work,Related work,named-entity-recognition,2,14,0.6086956521739131,139,0.6525821596244131,14,0.6086956521739131,0,0
141,"Overall , end - to - end CNNs have mainly been used in NLP for sentence classification , where the output representation is lower resolution than that of the input Kim Our work draws on the use of dilated convolutions for image segmentation in the computer vision community .",Related work,Related work,named-entity-recognition,2,15,0.6521739130434783,140,0.6572769953051644,15,0.6521739130434783,0,0
142,"Similar to our block , Yu and Koltun ( 2016 ) employ a context - module of stacked dilated convolutions of exponentially increasing dilation width .",Related work,Related work,named-entity-recognition,2,16,0.6956521739130435,141,0.6619718309859155,16,0.6956521739130435,0,0
143,"Dilated convolutions were recently applied to the task of speech generation , and concurrent with this work , posted a pre-print describing the similar ByteNet network for machine translation that uses dilated convolutions in the encoder and decoder components .",Related work,Related work,named-entity-recognition,2,17,0.7391304347826086,142,0.6666666666666666,17,0.7391304347826086,0,0
144,"Our basic model architecture is similar to that of the ByteNet encoder , except that the inputs to our model are tokens and not bytes .",Related work,Related work,named-entity-recognition,2,18,0.782608695652174,143,0.6713615023474179,18,0.782608695652174,0,0
145,"Additionally , we present a novel loss and parameter sharing scheme to facilitate training models on much smaller datasets than those used by .",Related work,Related work,named-entity-recognition,2,19,0.8260869565217391,144,0.676056338028169,19,0.8260869565217391,0,0
146,We are the first to use dilated convolutions for sequence labeling .,Related work,Related work,named-entity-recognition,2,20,0.8695652173913043,145,0.6807511737089202,20,0.8695652173913043,0,0
147,The broad effective input width of the ID - CNN helps aggregate document - level context .,Related work,Related work,named-entity-recognition,2,21,0.9130434782608696,146,0.6854460093896714,21,0.9130434782608696,0,0
148,"incorporate document context in their greedy model by adding features based on tagged entities within a large , fixed window of tokens .",Related work,Related work,named-entity-recognition,2,22,0.9565217391304348,147,0.6901408450704225,22,0.9565217391304348,0,0
149,Prior work has also posed a structured model that couples predictions across the whole document .,Related work,Related work,named-entity-recognition,2,23,1.0,148,0.6948356807511737,23,1.0,0,0
150,Experimental Results,,,named-entity-recognition,2,0,0.0,149,0.6995305164319249,0,0.0,1,0
151,We describe experiments on two benchmark English named entity recognition datasets .,Experimental Results,Experimental Results,named-entity-recognition,2,1,0.3333333333333333,150,0.704225352112676,1,0.3333333333333333,1,0
152,"On CoNLL - 2003 English NER , our ID - CNN performs on par with a Bi - LSTM not only when used to produce per-token logits for structured inference , but the ID - CNN with greedy decoding also performs on - par with the Bi - LSTM - CRF while running at more than 14 times the speed .",Experimental Results,Experimental Results,named-entity-recognition,2,2,0.6666666666666666,151,0.7089201877934272,2,0.6666666666666666,1,0
153,"We also observe a performance boost in almost all models when broadening the context to incorporate entire documents , achieving an average F1 of 90.65 on CoNLL - 2003 , out - performing the sentence - level model while still decoding at nearly 8 times the speed of the Bi - LSTM - CRF .",Experimental Results,Experimental Results,named-entity-recognition,2,3,1.0,152,0.7136150234741784,3,1.0,1,0
154,Data and Evaluation,,,named-entity-recognition,2,0,0.0,153,0.7183098591549296,0,0.0,1,0
155,We evaluate using labeled data from the CoNLL - 2003 shared task ( Tjong and OntoNotes 5.0 .,Data and Evaluation,Data and Evaluation,named-entity-recognition,2,1,0.1666666666666666,154,0.7230046948356808,1,0.1666666666666666,1,0
156,"Following previous work , we use the same OntoNotes data split used for co-reference resolution in the CoNLL - 2012 shared task .",Data and Evaluation,Data and Evaluation,named-entity-recognition,2,2,0.3333333333333333,155,0.7276995305164319,2,0.3333333333333333,1,0
157,"For both datasets , we convert the IOB boundary encoding to BILOU as previous work found this encoding to result in improved performance .",Data and Evaluation,Data and Evaluation,named-entity-recognition,2,3,0.5,156,0.7323943661971831,3,0.5,1,0
158,As in previous work we evaluate the performance of our models using segment - level micro -averaged F1 score .,Data and Evaluation,Data and Evaluation,named-entity-recognition,2,4,0.6666666666666666,157,0.7370892018779343,4,0.6666666666666666,1,0
159,Hyperparameters that resulted in the best performance on the validation set were selected via grid search .,Data and Evaluation,Data and Evaluation,named-entity-recognition,2,5,0.8333333333333334,158,0.7417840375586855,5,0.8333333333333334,1,0
160,"A more detailed description of the data , evaluation , optimization and data pre-processing can be found in the Appendix .",Data and Evaluation,Data and Evaluation,named-entity-recognition,2,6,1.0,159,0.7464788732394366,6,1.0,1,0
161,Baselines,,,named-entity-recognition,2,0,0.0,160,0.7511737089201878,0,0.0,1,0
162,"We compare our ID - CNN against strong LSTM and CNN baselines : a Bi - LSTM with local decoding , and one with CRF decoding ( Bi - LSTM - CRF ) .",Baselines,Baselines,named-entity-recognition,2,1,0.0285714285714285,161,0.755868544600939,1,0.03125,1,1
163,We also compare against a non-dilated CNN architecture with the same number of convolutional layers as our dilated network ( 4 - layer CNN ) and one with enough layers to incorporate an effective input width of the same size as that of the dilated network ( 5 - layer CNN ) to demonstrate that the dilated convolutions more effectively aggregate contextual information than simple convolutions ( i.e. using fewer parameters ) .,Baselines,Baselines,named-entity-recognition,2,2,0.0571428571428571,162,0.7605633802816901,2,0.0625,1,1
164,"We also compare our document - level ID - CNNs to a baseline which does not share parameters between blocks ( noshare ) and one that computes loss only at the last block , rather than after every iterated block of dilated convolutions ( 1 - loss ) .",Baselines,Baselines,named-entity-recognition,2,3,0.0857142857142857,163,0.7652582159624414,3,0.09375,1,0
165,We do not compare with deeper or more elaborate CNN architectures fora number of reasons :,Baselines,Baselines,named-entity-recognition,2,4,0.1142857142857142,164,0.7699530516431925,4,0.125,1,0
166,"1 ) Fast train and test performance are highly desirable for NLP practitioners , and deeper models require more computation time 2 ) more complicated models tend to over - fit on this relatively small dataset and 3 ) most accurate deep CNN architectures repeatedly up - sample and down - sample the inputs .",Baselines,Baselines,named-entity-recognition,2,5,0.1428571428571428,165,0.7746478873239436,5,0.15625,1,0
167,We do not compare to stacked LSTMs for similar reasons - a single LSTM is already slower than a 4 - layer CNN .,Baselines,Baselines,named-entity-recognition,2,6,0.1714285714285714,166,0.7793427230046949,6,0.1875,1,0
168,"Since our task is sequence labeling , we desire a model that maintains the token - level resolution of the input , making dilated convolutions an elegant solution .",Baselines,Baselines,named-entity-recognition,2,7,0.2,167,0.784037558685446,7,0.21875,1,0
169,6.3 CoNLL - 2003 English NER 6.3.1 Sentence - level prediction lists F 1 scores of models predicting with sentence - level context on CoNLL - 2003 .,Baselines,Baselines,named-entity-recognition,2,8,0.2285714285714285,168,0.7887323943661971,8,0.25,1,0
170,"For models that we trained , we report F1 and standard deviation obtained by averaging over 10 random restarts .",Baselines,Baselines,named-entity-recognition,2,9,0.2571428571428571,169,0.7934272300469484,9,0.28125,1,0
171,"The Viterbi - decoding Bi - LSTM - CRF and ID - CNN - CRF and greedy ID - CNN obtain the highest average scores , with the ID - CNN - CRF outperforming the Bi - LSTM - CRF by 0.11 points of F1 on average , and the Bi - LSTM - CRF out - performing the greedy ID - CNN by 0.11 as well .",Baselines,Baselines,named-entity-recognition,2,10,0.2857142857142857,170,0.7981220657276995,10,0.3125,1,1
172,"Our greedy ID - CNN outperforms the Bi - LSTM and the 4 - layer CNN , which uses the same number of parameters as the ID - CNN , and performs similarly to the 5 - layer CNN which uses more parameters but covers the same effective input width .",Baselines,Baselines,named-entity-recognition,2,11,0.3142857142857143,171,0.8028169014084507,11,0.34375,1,1
173,"All CNN models out - perform the Bi-Model F1 86.96 90.33 Bi-LSTM 89.34 0.28 4 - layer CNN 89.97 0.20 5 - layer CNN 90.23 0.16 ID- CNN 90.32 0.26 88.67 90.05 90.20 Bi-LSTM-CRF ( re-impl ) 90.43 0.12 ID-CNN- CRF 90.54 0.18 LSTM when paired with greedy decoding , suggesting that CNNs are better token encoders than Bi - LSTMs for independent logistic regression .",Baselines,Baselines,named-entity-recognition,2,12,0.3428571428571428,172,0.8075117370892019,12,0.375,1,0
174,"When paired with Viterbi decoding , our ID - CNN performs on par with the Bi - LSTM , showing that the ID - CNN is also an effective token encoder for structured inference .",Baselines,Baselines,named-entity-recognition,2,13,0.3714285714285714,173,0.812206572769953,13,0.40625,1,1
175,Our ID - CNN is not only a better token encoder than the Bi - LSTM but it is also faster .,Baselines,Baselines,named-entity-recognition,2,14,0.4,174,0.8169014084507042,14,0.4375,1,0
176,"lists relative decoding times on the CoNLL development set , compared to the Bi - LSTM - CRF .",Baselines,Baselines,named-entity-recognition,2,15,0.4285714285714285,175,0.8215962441314554,15,0.46875,1,0
177,We report decoding times using the fastest batch size for each method .,Baselines,Baselines,named-entity-recognition,2,16,0.4571428571428571,176,0.8262910798122066,16,0.5,1,0
178,The ID - CNN model decodes nearly 50 % faster than the Bi - LSTM .,Baselines,Baselines,named-entity-recognition,2,17,0.4857142857142857,177,0.8309859154929577,17,0.53125,1,0
179,"With Viterbi decoding , the gap closes somewhat but the ID - CNN - CRF still comes out ahead , about 30 % faster than the Bi - LSTM - CRF .",Baselines,Baselines,named-entity-recognition,2,18,0.5142857142857142,178,0.8356807511737089,18,0.5625,1,0
180,"The most vast speed improvements come when comparing the greedy ID - CNN to the Bi - LSTM - CRF - our ID - CNN is more than 14 times faster than the Bi - LSTM - CRF attest time , with comparable accuracy .",Baselines,Baselines,named-entity-recognition,2,19,0.5428571428571428,179,0.8403755868544601,19,0.59375,1,0
181,"The 5 - layer CNN , which observes the same effective input width as the ID - CNN but with more parameters , performs at about the same speed as the ID - CNN in our experiments .",Baselines,Baselines,named-entity-recognition,2,20,0.5714285714285714,180,0.8450704225352113,20,0.625,1,0
182,"With a better implementation of dilated convolutions than currently included in TensorFlow , we would expect the ID - CNN to be notably faster than the 5 - layer CNN .",Baselines,Baselines,named-entity-recognition,2,21,0.6,181,0.8497652582159625,21,0.65625,1,0
183,"We emphasize the importance of the dropout regularizer of in , where we observe increased F1 for every model trained with expectation - linear dropout regularization .",Baselines,Baselines,named-entity-recognition,2,22,0.6285714285714286,182,0.8544600938967136,22,0.6875,1,0
184,"Dropout is important for training neural network models that generalize well , especially on relatively small NLP datasets such as .",Baselines,Baselines,named-entity-recognition,2,23,0.6571428571428571,183,0.8591549295774648,23,0.71875,1,0
185,We recommend this regularizer as a simple and helpful tool for practitioners training neural networks for NLP .,Baselines,Baselines,named-entity-recognition,2,24,0.6857142857142857,184,0.863849765258216,24,0.75,1,0
186,Document - level prediction,Baselines,Baselines,named-entity-recognition,2,25,0.7142857142857143,185,0.8685446009389671,25,0.78125,1,1
187,In we show that adding document - level context improves every model on CoNLL - 2003 .,Baselines,Baselines,named-entity-recognition,2,26,0.7428571428571429,186,0.8732394366197183,26,0.8125,1,1
188,"Incorporating document - level context further improves our greedy ID - CNN model , attaining 90.65 average F1 .",Baselines,Baselines,named-entity-recognition,2,27,0.7714285714285715,187,0.8779342723004695,27,0.84375,1,0
189,"We believe this model sees greater improvement with the addition of document - level context than the Bi - LSTM - CRF due to the ID - CNN learning a feature function better suited for representing broad context , in contrast with the Bi - LSTM which , though better than a simple RNN at encoding long memories of sequences , may reach its limit when provided with sequences more than 1,000 tokens long such as entire documents .",Baselines,Baselines,named-entity-recognition,2,28,0.8,188,0.8826291079812206,28,0.875,1,0
190,We also note that our combination of training objective ( Eqn. 11 ) and tied parameters ( Eqn. : Comparing ID - CNNs with 1 ) backpropagating loss only from the final layer ( 1 - loss ) and 2 ) untied parameters across blocks ( noshare ) 8 ) more effectively learns to aggregate this broad context than a vanilla cross - entropy loss or deep CNN back - propagated from the final neural network layer .,Baselines,Baselines,named-entity-recognition,2,29,0.8285714285714286,189,0.8873239436619719,29,0.90625,1,0
191,compares models trained to incorporate entire document context using the document baselines described in Section 6.2 .,Baselines,Baselines,named-entity-recognition,2,30,0.8571428571428571,190,0.892018779342723,30,0.9375,1,0
192,"In we show that , in addition to being more accurate , our ID - CNN model is also much faster than the Bi - LSTM - CRF when incorporating context from entire documents , decoding at almost 8 times the speed .",Baselines,Baselines,named-entity-recognition,2,31,0.8857142857142857,191,0.8967136150234741,31,0.96875,1,0
193,"On these long sequences , it also tags at more than 4.5 times the speed of the greedy Bi - LSTM , demonstrative of the benefit of our ID - CNNs context - aggregating computation that does not depend on the length of the sequence .",Baselines,Baselines,named-entity-recognition,2,32,0.9142857142857144,192,0.9014084507042254,32,1.0,1,0
194,OntoNotes 5.0 English NER,Baselines,Baselines,named-entity-recognition,2,33,0.9428571428571428,193,0.9061032863849764,0,0.0,1,0
195,We observe similar patterns on OntoNotes as we do on CoNLL. lists overall F 1 scores of our models compared to those in the existing literature .,Baselines,Baselines,named-entity-recognition,2,34,0.9714285714285714,194,0.9107981220657276,1,0.5,1,0
196,The greedy Bi - LSTM out - performs the lex -,Baselines,Baselines,named-entity-recognition,2,35,1.0,195,0.9154929577464788,2,1.0,1,0
197,Model,,,named-entity-recognition,2,0,0.0,196,0.92018779342723,0,0.0,1,0
198,Speed Bi-LSTM-CRF 1 Bi-LSTM 4.60 ID- CNN 7.96 85.76 0.13 21.19 ID-CNN 85.27 0.24 13.21 ID - CNN ( 1 block ) 84.28 0.10 26.01 : F1 score of sentence and document models on OntoNotes .,Model,Model,named-entity-recognition,2,1,0.0833333333333333,197,0.9248826291079812,1,0.0833333333333333,1,0
199,"icalized greedy model of , and our ID - CNN out - performs the Bi - LSTM as well as the more complex model of which leverages the parallel coreference annotation available in the OntoNotes corpus to predict named entities jointly with entity linking and co-reference .",Model,Model,named-entity-recognition,2,2,0.1666666666666666,198,0.9295774647887324,2,0.1666666666666666,1,0
200,"Our greedy model is out - performed by the Bi - LSTM - CRF reported in Chiu and Nichols ( 2016 ) as well as our own re-implementation , which appears to be the new state - of - the - art on this dataset .",Model,Model,named-entity-recognition,2,3,0.25,199,0.9342723004694836,3,0.25,1,1
201,The gap between our greedy model and those using Viterbi decoding is wider than on CoNLL .,Model,Model,named-entity-recognition,2,4,0.3333333333333333,200,0.9389671361502347,4,0.3333333333333333,1,0
202,"We believe this is due to the more diverse set of entities in OntoNotes , which also tend to be much longer - the average length of a multi-token named entity segment in CoNLL is about one token shorter than in OntoNotes .",Model,Model,named-entity-recognition,2,5,0.4166666666666667,201,0.943661971830986,5,0.4166666666666667,1,0
203,These long entities benefit more from explicit structured constraints enforced in Viterbi decoding .,Model,Model,named-entity-recognition,2,6,0.5,202,0.9483568075117372,6,0.5,1,0
204,"Still , our ID - CNN outperforms all other greedy methods , achieving our goal of learning a better token encoder for structured prediction .",Model,Model,named-entity-recognition,2,7,0.5833333333333334,203,0.9530516431924884,7,0.5833333333333334,1,0
205,"Incorporating greater context significantly boosts the score of our greedy model on OntoNotes , whereas the Bi - LSTM - CRF performs more poorly .",Model,Model,named-entity-recognition,2,8,0.6666666666666666,204,0.9577464788732394,8,0.6666666666666666,1,0
206,"In , we also list the F1 of our ID - CNN model and the Bi - LSTM - CRF model trained on entire document context .",Model,Model,named-entity-recognition,2,9,0.75,205,0.9624413145539906,9,0.75,1,0
207,"For the first time , we seethe score decrease when more context is added to the Bi - LSTM - CRF model , though the ID - CNN , whose sentence model a lower score than that of the Bi - LSTM - CRF , sees an increase .",Model,Model,named-entity-recognition,2,10,0.8333333333333334,206,0.9671361502347418,10,0.8333333333333334,1,0
208,"We believe the decrease in the Bi - LSTM - CRF model occurs because of the nature of the OntoNotes dataset compared to contains a particularly high proportion of ambiguous entities , 7 perhaps leading to more benefit from document context that helps with disambiguation .",Model,Model,named-entity-recognition,2,11,0.9166666666666666,207,0.971830985915493,11,0.9166666666666666,1,0
209,"In this scenario , adding the wider context may just add noise to the high - scoring Bi - LSTM - CRF model , whereas the less accurate dilated model can still benefit from the refined predictions of the iterated dilated convolutions .",Model,Model,named-entity-recognition,2,12,1.0,208,0.976525821596244,12,1.0,1,0
210,Conclusion,,,named-entity-recognition,2,0,0.0,209,0.9812206572769951,0,0.0,1,0
211,"We present iterated dilated convolutional neural networks , fast token encoders that efficiently aggregate broad context without losing resolution .",Conclusion,Conclusion,named-entity-recognition,2,1,0.3333333333333333,210,0.9859154929577464,1,0.3333333333333333,0,0
212,"These provide impressive speed improvements for sequence labeling , particularly when processing entire documents at a time .",Conclusion,Conclusion,named-entity-recognition,2,2,0.6666666666666666,211,0.9906103286384976,2,0.6666666666666666,0,0
213,"In the future we hope to extend this work to NLP tasks with richer structured output , such as parsing .",Conclusion,Conclusion,named-entity-recognition,2,3,1.0,212,0.9953051643192488,3,1.0,0,0
1,title,,,named-entity-recognition,3,0,0.0,0,0.0,0,0.0,1,0
2,Semi-supervised sequence tagging with bidirectional language models,title,,named-entity-recognition,3,1,0.0,1,0.0054054054054054,1,0.0,1,1
3,abstract,,,named-entity-recognition,3,0,0.0,2,0.0108108108108108,0,0.0,1,0
4,Pre-trained word embeddings learned from unlabeled text have become a standard component of neural network architectures for NLP tasks .,abstract,abstract,named-entity-recognition,3,1,0.25,3,0.0162162162162162,1,0.25,1,0
5,"However , inmost cases , the recurrent network that operates on word - level representations to produce context sensitive representations is trained on relatively little labeled data .",abstract,abstract,named-entity-recognition,3,2,0.5,4,0.0216216216216216,2,0.5,1,0
6,"In this paper , we demonstrate a general semi-supervised approach for adding pretrained context embeddings from bidirectional language models to NLP systems and apply it to sequence labeling tasks .",abstract,abstract,named-entity-recognition,3,3,0.75,5,0.027027027027027,3,0.75,1,1
7,"We evaluate our model on two standard datasets for named entity recognition ( NER ) and chunking , and in both cases achieve state of the art results , surpassing previous systems that use other forms of transfer or joint learning with additional labeled data and task specific gazetteers .",abstract,abstract,named-entity-recognition,3,4,1.0,6,0.0324324324324324,4,1.0,1,0
8,Introduction,,,named-entity-recognition,3,0,0.0,7,0.0378378378378378,0,0.0,1,0
9,"Due to their simplicity and efficacy , pre-trained word embedding have become ubiquitous in NLP systems .",Introduction,Introduction,named-entity-recognition,3,1,0.0555555555555555,8,0.0432432432432432,1,0.0555555555555555,1,0
10,Many prior studies have shown that they capture useful semantic and syntactic information and including them in NLP systems has been shown to be enormously helpful fora variety of downstream tasks .,Introduction,Introduction,named-entity-recognition,3,2,0.1111111111111111,9,0.0486486486486486,2,0.1111111111111111,1,0
11,"However , in many NLP tasks it is essential to represent not just the meaning of a word , but also the word in context .",Introduction,Introduction,named-entity-recognition,3,3,0.1666666666666666,10,0.054054054054054,3,0.1666666666666666,1,0
12,"For example , in the two phrases "" A Central Bank spokesman "" and "" The Central African Republic "" , the word ' Central ' is used as part of both an Organization and Location .",Introduction,Introduction,named-entity-recognition,3,4,0.2222222222222222,11,0.0594594594594594,4,0.2222222222222222,1,0
13,"Accordingly , current state of the art sequence tagging models typically include a bidirectional re-current neural network ( RNN ) that encodes token sequences into a context sensitive representation before making token specific predictions .",Introduction,Introduction,named-entity-recognition,3,5,0.2777777777777778,12,0.0648648648648648,5,0.2777777777777778,1,0
14,"Although the token representation is initialized with pre-trained embeddings , the parameters of the bidirectional RNN are typically learned only on labeled data .",Introduction,Introduction,named-entity-recognition,3,6,0.3333333333333333,13,0.0702702702702702,6,0.3333333333333333,1,0
15,"Previous work has explored methods for jointly learning the bidirectional RNN with supplemental labeled data from other tasks ( e.g. , .",Introduction,Introduction,named-entity-recognition,3,7,0.3888888888888889,14,0.0756756756756756,7,0.3888888888888889,1,0
16,"In this paper , we explore an alternate semisupervised approach which does not require additional labeled data .",Introduction,Introduction,named-entity-recognition,3,8,0.4444444444444444,15,0.081081081081081,8,0.4444444444444444,1,1
17,"We use a neural language model ( LM ) , pre-trained on a large , unlabeled corpus to compute an encoding of the context at each position in the sequence ( hereafter an LM embedding ) and use it in the supervised sequence tagging model .",Introduction,Introduction,named-entity-recognition,3,9,0.5,16,0.0864864864864864,9,0.5,1,1
18,"Since the LM embeddings are used to compute the probability of future words in a neural LM , they are likely to encode both the semantic and syntactic roles of words in context .",Introduction,Introduction,named-entity-recognition,3,10,0.5555555555555556,17,0.0918918918918919,10,0.5555555555555556,1,0
19,Our main contribution is to show that the context sensitive representation captured in the LM embeddings is useful in the supervised sequence tagging setting .,Introduction,Introduction,named-entity-recognition,3,11,0.6111111111111112,18,0.0972972972972973,11,0.6111111111111112,1,0
20,"When we include the LM embeddings in our system overall performance increases from 90. 87 % to 91.93 % F 1 for the CoNLL 2003 NER task , a more then 1 % absolute F1 increase , and a substantial improvement over the previous state of the art .",Introduction,Introduction,named-entity-recognition,3,12,0.6666666666666666,19,0.1027027027027027,12,0.6666666666666666,1,0
21,We also establish anew state of the art result ( 96.37 % F 1 ) for the CoNLL 2000 Chunking task .,Introduction,Introduction,named-entity-recognition,3,13,0.7222222222222222,20,0.1081081081081081,13,0.7222222222222222,1,0
22,"As a secondary contribution , we show that using both forward and backward LM embeddings boosts performance over a forward only LM .",Introduction,Introduction,named-entity-recognition,3,14,0.7777777777777778,21,0.1135135135135135,14,0.7777777777777778,1,0
23,We also demonstrate that domain specific pre-training is not necessary by applying a LM trained in the news domain to scientific papers .,Introduction,Introduction,named-entity-recognition,3,15,0.8333333333333334,22,0.1189189189189189,15,0.8333333333333334,1,0
24,The main components in our language - modelaugmented sequence tagger ( TagLM ) are illustrated in .,Introduction,Introduction,named-entity-recognition,3,16,0.8888888888888888,23,0.1243243243243243,16,0.8888888888888888,1,0
25,"After pre-training word embeddings and a neural LM on large , unlabeled corpora ( Step 1 ) , we extract the word and LM embeddings for every token in a given input sequence Step 2 ) and use them in the supervised sequence tagging model (",Introduction,Introduction,named-entity-recognition,3,17,0.9444444444444444,24,0.1297297297297297,17,0.9444444444444444,1,0
26,Step 3 ) .,Introduction,,named-entity-recognition,3,18,1.0,25,0.1351351351351351,18,1.0,1,0
27,Baseline sequence tagging model,,,named-entity-recognition,3,0,0.0,26,0.1405405405405405,0,0.0,1,0
28,"Our baseline sequence tagging model is a hierarchical neural tagging model , closely following a number of recent studies ) ( left side of ) .",Baseline sequence tagging model,Baseline sequence tagging model,named-entity-recognition,3,1,0.0238095238095238,27,0.1459459459459459,1,0.05,1,0
29,"Given a sentence of tokens ( t 1 , t 2 , . . . , t N ) it first forms a representation , x k , for each token by concatenating a character based representation ck with a token embedding wk :",Baseline sequence tagging model,Baseline sequence tagging model,named-entity-recognition,3,2,0.0476190476190476,28,0.1513513513513513,2,0.1,1,0
30,The character representation ck captures morphological information and is either a convolutional neural network ( CNN ) or RNN .,Baseline sequence tagging model,Baseline sequence tagging model,named-entity-recognition,3,3,0.0714285714285714,29,0.1567567567567567,3,0.15,1,0
31,"It is parameterized by C ( , ? c ) with parameters ? c .",Baseline sequence tagging model,Baseline sequence tagging model,named-entity-recognition,3,4,0.0952380952380952,30,0.1621621621621621,4,0.2,1,0
32,"The token embeddings , wk , are obtained as a lookup E ( , ? w ) , initialized using pre-trained word embeddings , and fine tuned during training .",Baseline sequence tagging model,Baseline sequence tagging model,named-entity-recognition,3,5,0.119047619047619,31,0.1675675675675675,5,0.25,1,0
33,"To learn a context sensitive representation , we employ multiple layers of bidirectional RNNs .",Baseline sequence tagging model,Baseline sequence tagging model,named-entity-recognition,3,6,0.1428571428571428,32,0.1729729729729729,6,0.3,1,0
34,"For each token position , k , the hidden state h k , i of RNN layer i is formed by concatenating the hidden states from the forward ( ? ? h k , i ) and backward ( ? ? h k , i ) RNNs .",Baseline sequence tagging model,Baseline sequence tagging model,named-entity-recognition,3,7,0.1666666666666666,33,0.1783783783783784,7,0.35,1,0
35,"As a result , the bidirectional RNN is able to use both past and future information to make a prediction at token k.",Baseline sequence tagging model,Baseline sequence tagging model,named-entity-recognition,3,8,0.1904761904761904,34,0.1837837837837838,8,0.4,1,0
36,"More formally , for the first RNN layer that operates on x k to output h k,1 :",Baseline sequence tagging model,Baseline sequence tagging model,named-entity-recognition,3,9,0.2142857142857142,35,0.1891891891891892,9,0.45,1,0
37,Step 2 : Prepare word embedding and LM embedding for each token in the input sequence .,Baseline sequence tagging model,Baseline sequence tagging model,named-entity-recognition,3,10,0.238095238095238,36,0.1945945945945946,10,0.5,1,0
38,"Two representations of the word "" York """,Baseline sequence tagging model,,named-entity-recognition,3,11,0.2619047619047619,37,0.2,11,0.55,1,0
39,Step 3 : Use both word embeddings and LM embeddings in the sequence tagging model .,Baseline sequence tagging model,"Two representations of the word "" York """,named-entity-recognition,3,12,0.2857142857142857,38,0.2054054054054054,12,0.6,1,0
40,New York is located :,Baseline sequence tagging model,"Two representations of the word "" York """,named-entity-recognition,3,13,0.3095238095238095,39,0.2108108108108108,13,0.65,1,0
41,"The main components in TagLM , our language - model - augmented sequence tagging system .",Baseline sequence tagging model,"Two representations of the word "" York """,named-entity-recognition,3,14,0.3333333333333333,40,0.2162162162162162,14,0.7,1,0
42,The language model component ( in orange ) is used to augment the input token representation in a traditional sequence tagging models ( in grey ) .,Baseline sequence tagging model,"Two representations of the word "" York """,named-entity-recognition,3,15,0.3571428571428571,41,0.2216216216216216,15,0.75,1,0
43,"The second RNN layer is similar and uses h k , 1 to output h k ,2 .",Baseline sequence tagging model,"Two representations of the word "" York """,named-entity-recognition,3,16,0.3809523809523809,42,0.227027027027027,16,0.8,1,0
44,"In this paper , we use L = 2 layers of RNNs in all experiments and parameterize R i as either Gated Recurrent Units ( GRU ) or Long Short - Term Memory units ( LSTM ) depending on the task .",Baseline sequence tagging model,"Two representations of the word "" York """,named-entity-recognition,3,17,0.4047619047619047,43,0.2324324324324324,17,0.85,1,0
45,"Finally , the output of the final RNN layer h k,L is used to predict a score for each possible tag using a single dense layer .",Baseline sequence tagging model,"Two representations of the word "" York """,named-entity-recognition,3,18,0.4285714285714285,44,0.2378378378378378,18,0.9,1,0
46,"Due to the dependencies between successive tags in our sequence labeling tasks ( e.g. using the BIOES labeling scheme , it is not possible for I - PER to follow B - LOC ) , it is beneficial to model and decode each sentence jointly instead of independently predicting the label for each token .",Baseline sequence tagging model,"Two representations of the word "" York """,named-entity-recognition,3,19,0.4523809523809524,45,0.2432432432432432,19,0.95,1,0
47,"Accordingly , we add another layer with parameters for each label bigram , computing the sentence conditional random field ( CRF ) loss using the forward - backward algorithm at training time , and using the Viterbi algorithm to find the most likely tag sequence attest time , similar to Collobert et al .",Baseline sequence tagging model,"Two representations of the word "" York """,named-entity-recognition,3,20,0.4761904761904761,46,0.2486486486486486,20,1.0,1,0
48,Bidirectional LM,Baseline sequence tagging model,,named-entity-recognition,3,21,0.5,47,0.254054054054054,0,0.0,1,0
49,"A language model computes the probability of a token sequence ( t 1 , t 2 , . . . , t N )",Baseline sequence tagging model,Bidirectional LM,named-entity-recognition,3,22,0.5238095238095238,48,0.2594594594594595,1,0.0909090909090909,1,0
50,"Recent state of the art neural language models ) use a similar architecture to our baseline sequence tagger where they pass a token representation ( either from a CNN over characters or as token embeddings ) through multiple layers of LSTMs to embed the history ( t 1 , t 2 , . . . , t k ) into a fixed dimensional vector ? ? h LM k .",Baseline sequence tagging model,Bidirectional LM,named-entity-recognition,3,23,0.5476190476190477,49,0.2648648648648649,2,0.1818181818181818,1,0
51,This is the forward LM embedding of the token at position k and is the output of the top LSTM layer in the language model .,Baseline sequence tagging model,Bidirectional LM,named-entity-recognition,3,24,0.5714285714285714,50,0.2702702702702703,3,0.2727272727272727,1,0
52,"Finally , the language model predicts the probability of token t k + 1 using a softmax layer over words in the vocabulary .",Baseline sequence tagging model,Bidirectional LM,named-entity-recognition,3,25,0.5952380952380952,51,0.2756756756756757,4,0.3636363636363636,1,0
53,The need to capture future context in the LM embeddings suggests it is beneficial to also consider a backward LM in additional to the traditional forward LM .,Baseline sequence tagging model,Bidirectional LM,named-entity-recognition,3,26,0.6190476190476191,52,0.2810810810810811,5,0.4545454545454545,1,0
54,A backward LM predicts the previous token given the future context .,Baseline sequence tagging model,Bidirectional LM,named-entity-recognition,3,27,0.6428571428571429,53,0.2864864864864865,6,0.5454545454545454,1,0
55,"Given a sentence with N tokens , it computes",Baseline sequence tagging model,,named-entity-recognition,3,28,0.6666666666666666,54,0.2918918918918919,7,0.6363636363636364,1,0
56,A backward LM can be implemented in an analogous way to a forward LM and produces the backward LM embedding ? ?,Baseline sequence tagging model,"Given a sentence with N tokens , it computes",named-entity-recognition,3,29,0.6904761904761905,55,0.2972972972972973,8,0.7272727272727273,1,0
57,"h LM k , for the sequence ( t k , t k+1 , . . . , t N ) , the output embeddings of the top layer LSTM .",Baseline sequence tagging model,"Given a sentence with N tokens , it computes",named-entity-recognition,3,30,0.7142857142857143,56,0.3027027027027027,9,0.8181818181818182,1,0
58,"In our final system , after pre-training the forward and backward LMs separately , we remove the top layer softmax and concatenate the forward and backward LM embeddings to form bidirectional LM embeddings , i.e. , h LM",Baseline sequence tagging model,"Given a sentence with N tokens , it computes",named-entity-recognition,3,31,0.7380952380952381,57,0.3081081081081081,10,0.9090909090909092,1,0
59,"Note that in our formulation , the forward and backward LMs are independent , without any shared parameters .",Baseline sequence tagging model,"Given a sentence with N tokens , it computes",named-entity-recognition,3,32,0.7619047619047619,58,0.3135135135135135,11,1.0,1,0
60,Combining LM with sequence model,Baseline sequence tagging model,,named-entity-recognition,3,33,0.7857142857142857,59,0.3189189189189189,0,0.0,1,0
61,"Our combined system , TagLM , uses the LM embeddings as additional inputs to the sequence tagging model .",Baseline sequence tagging model,Combining LM with sequence model,named-entity-recognition,3,34,0.8095238095238095,60,0.3243243243243243,1,0.1111111111111111,1,0
62,"In particular , we concatenate the LM embeddings h LM with the output from one of the bidirectional RNN layers in the sequence model .",Baseline sequence tagging model,Combining LM with sequence model,named-entity-recognition,3,35,0.8333333333333334,61,0.3297297297297297,2,0.2222222222222222,1,0
63,"In our experiments , we found that introducing the LM embeddings at the output of the first layer performed the best .",Baseline sequence tagging model,Combining LM with sequence model,named-entity-recognition,3,36,0.8571428571428571,62,0.3351351351351351,3,0.3333333333333333,1,0
64,"More formally , we simply replace ( 2 ) with",Baseline sequence tagging model,Combining LM with sequence model,named-entity-recognition,3,37,0.8809523809523809,63,0.3405405405405405,4,0.4444444444444444,1,0
65,There are alternate possibilities for adding the LM embeddings to the sequence model .,Baseline sequence tagging model,Combining LM with sequence model,named-entity-recognition,3,38,0.9047619047619048,64,0.3459459459459459,5,0.5555555555555556,1,0
66,One pos-sibility adds a non-linear mapping after the concatenation and before the second RNN ( e.g. re -,Baseline sequence tagging model,Combining LM with sequence model,named-entity-recognition,3,39,0.9285714285714286,65,0.3513513513513513,6,0.6666666666666666,1,0
67,where f is a non-linear function ) .,Baseline sequence tagging model,Combining LM with sequence model,named-entity-recognition,3,40,0.9523809523809524,66,0.3567567567567568,7,0.7777777777777778,1,0
68,Another possibility introduces an attention - like mechanism that weights the all LM embeddings in a sentence before including them in the sequence model .,Baseline sequence tagging model,Combining LM with sequence model,named-entity-recognition,3,41,0.9761904761904762,67,0.3621621621621622,8,0.8888888888888888,1,0
69,"Our initial results with the simple concatenation were encouraging so we did not explore these alternatives in this study , preferring to leave them for future work .",Baseline sequence tagging model,Combining LM with sequence model,named-entity-recognition,3,42,1.0,68,0.3675675675675676,9,1.0,1,0
70,Experiments,,,named-entity-recognition,3,0,0.0,69,0.372972972972973,0,0.0,1,0
71,"We evaluate our approach on two well benchmarked sequence tagging tasks , the CoNLL 2003 NER task and the CoNLL 2000 Chunking task .",Experiments,Experiments,named-entity-recognition,3,1,0.0526315789473684,70,0.3783783783783784,1,0.0238095238095238,1,0
72,We report the official evaluation metric ( micro - averaged F 1 ) .,Experiments,Experiments,named-entity-recognition,3,2,0.1052631578947368,71,0.3837837837837838,2,0.0476190476190476,1,0
73,"In both cases , we use the BIOES labeling scheme for the output tags , following previous work which showed it outperforms other options ( e.g. , ) .",Experiments,Experiments,named-entity-recognition,3,3,0.1578947368421052,72,0.3891891891891892,3,0.0714285714285714,1,0
74,"Following , we use the Senna word embeddings and pre-processed the text by lowercasing all tokens and replacing all digits with 0 .",Experiments,Experiments,named-entity-recognition,3,4,0.2105263157894736,73,0.3945945945945946,4,0.0952380952380952,1,0
75,CoNLL 2003 NER .,Experiments,,named-entity-recognition,3,5,0.2631578947368421,74,0.4,5,0.119047619047619,1,0
76,"The CoNLL 2003 NER task consists of newswire from the Reuters RCV1 corpus tagged with four different entity types ( PER , LOC , ORG , MISC ) .",Experiments,CoNLL 2003 NER .,named-entity-recognition,3,6,0.3157894736842105,75,0.4054054054054054,6,0.1428571428571428,1,0
77,"It includes standard train , development and test sets .",Experiments,CoNLL 2003 NER .,named-entity-recognition,3,7,0.3684210526315789,76,0.4108108108108108,7,0.1666666666666666,1,0
78,Following previous work we trained on both the train and development sets after tuning hyperparameters on the development set .,Experiments,CoNLL 2003 NER .,named-entity-recognition,3,8,0.4210526315789473,77,0.4162162162162162,8,0.1904761904761904,1,0
79,The hyperparameters for our baseline model are similar to .,Experiments,CoNLL 2003 NER .,named-entity-recognition,3,9,0.4736842105263157,78,0.4216216216216216,9,0.2142857142857142,1,0
80,We use two bidirectional GRUs with 80 hidden units and 25 dimensional character embeddings for the token character encoder .,Experiments,CoNLL 2003 NER .,named-entity-recognition,3,10,0.5263157894736842,79,0.427027027027027,10,0.238095238095238,1,1
81,The sequence layer uses two bidirectional GRUs with 300 hidden units each .,Experiments,CoNLL 2003 NER .,named-entity-recognition,3,11,0.5789473684210527,80,0.4324324324324324,11,0.2619047619047619,1,1
82,"For regularization , we add 25 % dropout to the input of each GRU , but not to the recurrent connections .",Experiments,CoNLL 2003 NER .,named-entity-recognition,3,12,0.631578947368421,81,0.4378378378378378,12,0.2857142857142857,1,1
83,CoNLL 2000 chunking .,Experiments,,named-entity-recognition,3,13,0.6842105263157895,82,0.4432432432432432,13,0.3095238095238095,1,0
84,The CoNLL 2000 chunking task uses sections 15 - 18 from the Wall Street Journal corpus for training and section 20 for testing .,Experiments,CoNLL 2000 chunking .,named-entity-recognition,3,14,0.7368421052631579,83,0.4486486486486487,14,0.3333333333333333,1,0
85,"It defines 11 syntactic chunk types ( e.g. , NP , VP , ADJP ) in addition to other .",Experiments,CoNLL 2000 chunking .,named-entity-recognition,3,15,0.7894736842105263,84,0.4540540540540541,15,0.3571428571428571,1,0
86,We randomly sampled 1000 sentences from the training set as a held - out development set .,Experiments,CoNLL 2000 chunking .,named-entity-recognition,3,16,0.8421052631578947,85,0.4594594594594595,16,0.3809523809523809,1,0
87,The baseline sequence tagger uses 30 dimensional character embeddings and a CNN with 30 filters of width 3 characters followed by a tanh non-linearity for the token character encoder .,Experiments,CoNLL 2000 chunking .,named-entity-recognition,3,17,0.8947368421052632,86,0.4648648648648649,17,0.4047619047619047,1,1
88,The sequence layer uses two bidirectional LSTMs with 200 hidden units .,Experiments,CoNLL 2000 chunking .,named-entity-recognition,3,18,0.9473684210526316,87,0.4702702702702703,18,0.4285714285714285,1,1
89,"Following we added 50 % dropout to the character embeddings , the input to each LSTM layer ( but not recurrent connections ) and to the output of the final LSTM layer .",Experiments,CoNLL 2000 chunking .,named-entity-recognition,3,19,1.0,88,0.4756756756756757,19,0.4523809523809524,1,1
90,Pre-trained language models .,,,named-entity-recognition,3,0,0.0,89,0.4810810810810811,20,0.4761904761904761,1,0
91,"The primary bidirectional LMs we used in this study were trained on the 1B Word Benchmark , a publicly available benchmark for largescale language modeling .",Pre-trained language models .,Pre-trained language models .,named-entity-recognition,3,1,0.0454545454545454,90,0.4864864864864865,21,0.5,1,0
92,"The training split has approximately 800 million tokens , about a 4000X increase over the number training tokens in the CoNLL datasets .",Pre-trained language models .,Pre-trained language models .,named-entity-recognition,3,2,0.0909090909090909,91,0.4918918918918919,22,0.5238095238095238,1,0
93,explored several model architectures and released their best single model and training recipes .,Pre-trained language models .,Pre-trained language models .,named-entity-recognition,3,3,0.1363636363636363,92,0.4972972972972973,23,0.5476190476190477,1,0
94,"Following , they used linear projection layers at the output of each LSTM layer to reduce the computation time but still maintain a large LSTM state .",Pre-trained language models .,Pre-trained language models .,named-entity-recognition,3,4,0.1818181818181818,93,0.5027027027027027,24,0.5714285714285714,1,0
95,Their single best model took three weeks to train on 32 GPUs and achieved 30.0 test perplexity .,Pre-trained language models .,Pre-trained language models .,named-entity-recognition,3,5,0.2272727272727272,94,0.5081081081081081,25,0.5952380952380952,1,0
96,"It uses a character CNN with 4096 filters for input , followed by two stacked LSTMs , each with 8192 hidden units and a 1024 dimensional projection layer .",Pre-trained language models .,Pre-trained language models .,named-entity-recognition,3,6,0.2727272727272727,95,0.5135135135135135,26,0.6190476190476191,1,0
97,We use CNN - BIG - LSTM to refer to this language model in our results .,Pre-trained language models .,Pre-trained language models .,named-entity-recognition,3,7,0.3181818181818182,96,0.518918918918919,27,0.6428571428571429,1,0
98,"In addition to CNN - BIG - LSTM from , 1 we used the same corpus to train two additional language models with fewer parameters : forward LSTM - 2048-512 and backward LSTM - 2048-512 .",Pre-trained language models .,Pre-trained language models .,named-entity-recognition,3,8,0.3636363636363636,97,0.5243243243243243,28,0.6666666666666666,1,0
99,Both language models use token embeddings as input to a single layer LSTM with 2048 units and a 512 dimension projection layer .,Pre-trained language models .,Pre-trained language models .,named-entity-recognition,3,9,0.4090909090909091,98,0.5297297297297298,29,0.6904761904761905,1,0
100,"We closely followed the procedure outlined in , except we used synchronous parameter updates across four GPUs instead of asynchronous updates across 32 GPUs and ended training after 10 epochs .",Pre-trained language models .,Pre-trained language models .,named-entity-recognition,3,10,0.4545454545454545,99,0.5351351351351351,30,0.7142857142857143,1,0
101,"The test set perplexities for our forward and backward LSTM - 2048 - 512 language models are 47.7 and 47.3 , respectively .",Pre-trained language models .,Pre-trained language models .,named-entity-recognition,3,11,0.5,100,0.5405405405405406,31,0.7380952380952381,1,0
102,2,Pre-trained language models .,Pre-trained language models .,named-entity-recognition,3,12,0.5454545454545454,101,0.5459459459459459,32,0.7619047619047619,1,0
103,Model F 1 std 90.91 0.20 90.94 91.37 Our baseline without LM 90.87 0.13 TagLM 91.93 0.19 Training .,Pre-trained language models .,Pre-trained language models .,named-entity-recognition,3,13,0.5909090909090909,102,0.5513513513513514,33,0.7857142857142857,1,0
104,"All experiments use the Adam optimizer ( Kingma and Ba , 2015 ) with gradient norms clipped at 5.0 .",Pre-trained language models .,Pre-trained language models .,named-entity-recognition,3,14,0.6363636363636364,103,0.5567567567567567,34,0.8095238095238095,1,1
105,"In all experiments , we fine tune the pre-trained Senna word embeddings but fix all weights in the pre-trained language models .",Pre-trained language models .,Pre-trained language models .,named-entity-recognition,3,15,0.6818181818181818,104,0.5621621621621622,35,0.8333333333333334,1,1
106,"In addition to explicit dropout regularization , we also use early stopping to prevent over-fitting and use the following process to determine when to stop training .",Pre-trained language models .,Pre-trained language models .,named-entity-recognition,3,16,0.7272727272727273,105,0.5675675675675675,36,0.8571428571428571,1,1
107,We first train with a constant learning rate ? = 0.001 on the training data and monitor the development set performance at each epoch .,Pre-trained language models .,Pre-trained language models .,named-entity-recognition,3,17,0.7727272727272727,106,0.572972972972973,37,0.8809523809523809,1,1
108,"Then , at the epoch with the highest development performance , we start a simple learning rate annealing schedule : decrease ?",Pre-trained language models .,Pre-trained language models .,named-entity-recognition,3,18,0.8181818181818182,107,0.5783783783783784,38,0.9047619047619048,1,0
109,"an order of magnitude ( i.e. , divide by ten ) , train for five epochs , decrease ?",Pre-trained language models .,Pre-trained language models .,named-entity-recognition,3,19,0.8636363636363636,108,0.5837837837837838,39,0.9285714285714286,1,0
110,"an order of magnitude again , train for five more epochs and stop .",Pre-trained language models .,Pre-trained language models .,named-entity-recognition,3,20,0.9090909090909092,109,0.5891891891891892,40,0.9523809523809524,1,0
111,"Following , we train each final model configuration ten times with different random seeds and report the mean and standard deviation F 1 .",Pre-trained language models .,Pre-trained language models .,named-entity-recognition,3,21,0.9545454545454546,110,0.5945945945945946,41,0.9761904761904762,1,0
112,It is important to estimate the variance of model performance since the test data sets are relatively small .,Pre-trained language models .,Pre-trained language models .,named-entity-recognition,3,22,1.0,111,0.6,42,1.0,1,0
113,Overall system results,,,named-entity-recognition,3,0,0.0,112,0.6054054054054054,0,0.0,1,0
114,Tables 1 and 2 compare results from Tag LM with previously published state of the art results without additional labeled data or task specific gazetteers .,Overall system results,Overall system results,named-entity-recognition,3,1,0.0263157894736842,113,0.6108108108108108,1,0.0,1,0
115,compare results of,Overall system results,Overall system results,named-entity-recognition,3,2,0.0526315789473684,114,0.6162162162162163,0,0.0,1,0
116,Tag LM to other systems that include additional labeled data or gazetteers .,Overall system results,Overall system results,named-entity-recognition,3,3,0.0789473684210526,115,0.6216216216216216,1,0.0909090909090909,1,0
117,"In both tasks , Tag LM establishes anew state of the art using bidirectional LMs ( the forward CNN - BIG - LSTM and the backward LSTM - 2048 - 512 ) .",Overall system results,Overall system results,named-entity-recognition,3,4,0.1052631578947368,116,0.6270270270270271,2,0.1818181818181818,1,0
118,"In the CoNLL 2003 NER task , our model scores 91.93 mean F 1 , which is a statistically significant increase over the previous best result of 91.62 0.33 from that used gazetteers ( at 95 % , two - sided Welch t- test , p = 0.021 ) .",Overall system results,Overall system results,named-entity-recognition,3,5,0.131578947368421,117,0.6324324324324324,3,0.2727272727272727,1,1
119,"In the CoNLL 2000 Chunking task , Tag LM achieves 96.37 mean F 1 , exceeding all previously published results without additional labeled data by more then 1 % absolute F 1 .",Overall system results,Overall system results,named-entity-recognition,3,6,0.1578947368421052,118,0.6378378378378379,4,0.3636363636363636,1,1
120,The improvement over the previous best result of 95.77 in that jointly trains with Penn Treebank ( PTB ) POS tags is statistically significant at 95 % ( p < 0.001 assuming standard deviation of 0.1 ) .,Overall system results,Overall system results,named-entity-recognition,3,7,0.1842105263157894,119,0.6432432432432432,5,0.4545454545454545,1,0
121,"Importantly , the LM embeddings amounts to an average absolute improvement of 1.06 and 1.37 F 1 in the NER and Chunking tasks , respectively .",Overall system results,Overall system results,named-entity-recognition,3,8,0.2105263157894736,120,0.6486486486486487,6,0.5454545454545454,1,0
122,Adding external resources .,Overall system results,,named-entity-recognition,3,9,0.2368421052631578,121,0.654054054054054,7,0.6363636363636364,1,0
123,"Although we do not use external labeled data or gazetteers , we found that TagLM outperforms previous state of the art results in both tasks when external resources ( labeled data or task specific gazetteers ) are available .",Overall system results,Adding external resources .,named-entity-recognition,3,10,0.2631578947368421,122,0.6594594594594595,8,0.7272727272727273,1,0
124,"Furthermore , show that , inmost cases , the improvements we obtain by adding LM embeddings are larger then the improvements previously obtained by adding other forms of transfer or joint learning .",Overall system results,Adding external resources .,named-entity-recognition,3,11,0.2894736842105263,123,0.6648648648648648,9,0.8181818181818182,1,0
125,"For example , noted an improvement of only 0.06 F 1 in the NER task when transfer learning from both CoNLL 2000 chunks and PTB POS tags and reported an increase of 0.71 F 1 when adding gazetteers to their baseline .",Overall system results,Adding external resources .,named-entity-recognition,3,12,0.3157894736842105,124,0.6702702702702703,10,0.9090909090909092,1,0
126,"In the Chunking task , previous work has reported from 0.28 to 0.75 improvement in F 1 when including supervised labels from the PTB POS tags or CoNLL 2003 entities .",Overall system results,Adding external resources .,named-entity-recognition,3,13,0.3421052631578947,125,0.6756756756756757,11,1.0,1,0
127,Analysis,Overall system results,,named-entity-recognition,3,14,0.3684210526315789,126,0.6810810810810811,0,0.0,1,0
128,"To elucidate the characteristics of our LM augmented sequence tagger , we ran a number of additional experiments on the CoNLL 2003 NER task .",Overall system results,Analysis,named-entity-recognition,3,15,0.3947368421052631,127,0.6864864864864865,1,0.0416666666666666,1,0
129,How to use LM embeddings ?,Overall system results,Analysis,named-entity-recognition,3,16,0.4210526315789473,128,0.6918918918918919,2,0.0833333333333333,1,0
130,"In this experiment , we concatenate the LM embeddings at dif - shows that the second alternative performs best .",Overall system results,Analysis,named-entity-recognition,3,17,0.4473684210526316,129,0.6972972972972973,3,0.125,1,0
131,We speculate that the second RNN layer in the sequence tagging model is able to capture interactions between task specific context as expressed in the first RNN layer and general context as expressed in the LM embeddings in away that improves overall system performance .,Overall system results,Analysis,named-entity-recognition,3,18,0.4736842105263157,130,0.7027027027027027,4,0.1666666666666666,1,0
132,These results are consistent with who found that chunking performance was sensitive to the level at which additional POS supervision was added .,Overall system results,Analysis,named-entity-recognition,3,19,0.5,131,0.7081081081081081,5,0.2083333333333333,1,0
133,Does it matter which language model to use ?,Overall system results,Analysis,named-entity-recognition,3,20,0.5263157894736842,132,0.7135135135135136,6,0.25,1,0
134,"In this experiment , we compare six different configurations of the forward and backward language models ( including the baseline model which does not use any language models ) .",Overall system results,Analysis,named-entity-recognition,3,21,0.5526315789473685,133,0.7189189189189189,7,0.2916666666666667,1,0
135,The results are reported in .,Overall system results,,named-entity-recognition,3,22,0.5789473684210527,134,0.7243243243243244,8,0.3333333333333333,1,0
136,"We find that adding backward LM embeddings consistently outperforms forward - only LM embeddings , with F 1 improvements between 0.22 and 0.27 % , even with the relatively small backward LSTM - 2048-512 LM .",Overall system results,The results are reported in .,named-entity-recognition,3,23,0.6052631578947368,135,0.7297297297297297,9,0.375,1,0
137,"LM size is important , and replacing the forward LSTM - 2048 - 512 with CNN - BIG - LSTM ( test perplexities of 47.7 to 30.0 on 1B Word Benchmark ) improves F 1 by 0.26 - 0.31 % , about as much as adding backward LM .",Overall system results,The results are reported in .,named-entity-recognition,3,24,0.631578947368421,136,0.7351351351351352,10,0.4166666666666667,1,0
138,"Accordingly , we hypothesize ( but have not tested ) that replacing the backward LSTM - 2048 - 512 with a backward LM analogous to the CNN - BIG - LSTM would further improve performance .",Overall system results,The results are reported in .,named-entity-recognition,3,25,0.6578947368421053,137,0.7405405405405405,11,0.4583333333333333,1,0
139,"To highlight the importance of including language models trained on a large scale data , we also experimented with training a language model on just the CoNLL 2003 training and development data .",Overall system results,The results are reported in .,named-entity-recognition,3,26,0.6842105263157895,138,0.745945945945946,12,0.5,1,0
140,Due to the much smaller size of this data Including embeddings from these language models decreased performance slightly compared to the baseline system without any LM .,Overall system results,The results are reported in .,named-entity-recognition,3,27,0.7105263157894737,139,0.7513513513513513,13,0.5416666666666666,1,0
141,"This result supports the hypothesis that adding language models help because they learn composition functions ( i.e. , the RNN parameters in the language model ) from much larger data compared to the composition functions in the baseline tagger , which are only learned from labeled data .",Overall system results,The results are reported in .,named-entity-recognition,3,28,0.7368421052631579,140,0.7567567567567568,14,0.5833333333333334,1,0
142,Importance of task specific RNN .,Overall system results,,named-entity-recognition,3,29,0.7631578947368421,141,0.7621621621621621,15,0.625,1,0
143,To understand the importance of including a task specific sequence RNN we ran an experiment that removed the task specific sequence RNN and used only the LM embeddings with a dense layer and CRF to predict output tags .,Overall system results,Importance of task specific RNN .,named-entity-recognition,3,30,0.7894736842105263,142,0.7675675675675676,16,0.6666666666666666,1,0
144,"In this setup , performance was very low , 88.17 F 1 , well below our baseline .",Overall system results,Importance of task specific RNN .,named-entity-recognition,3,31,0.8157894736842105,143,0.772972972972973,17,0.7083333333333334,1,0
145,This result confirms that the RNNs in the baseline tagger encode essential information which is not encoded in the LM embeddings .,Overall system results,Importance of task specific RNN .,named-entity-recognition,3,32,0.8421052631578947,144,0.7783783783783784,18,0.75,1,0
146,Does the LM transfer across domains ?,Overall system results,Importance of task specific RNN .,named-entity-recognition,3,33,0.868421052631579,145,0.7837837837837838,19,0.7916666666666666,1,0
147,One artifact of our evaluation framework is that both the labeled data in the chunking and NER tasks and the unlabeled text in the 1 Billion Word Benchmark used to train the bidirectional LMs are derived from news articles .,Overall system results,Importance of task specific RNN .,named-entity-recognition,3,34,0.8947368421052632,146,0.7891891891891892,20,0.8333333333333334,1,0
148,"To test the sensitivity to the LM training domain , we also applied Tag LM with a LM trained on news articles to the SemEval 2017 Shared Task 10 , Science IE .",Overall system results,Importance of task specific RNN .,named-entity-recognition,3,35,0.9210526315789472,147,0.7945945945945946,21,0.875,1,0
149,"5 Scien - ce IE requires end - to - end joint entity and relationship extraction from scientific publications across three diverse fields ( computer science , material sciences , and physics ) and defines three broad entity types ( Task , Material and Process ) .",Overall system results,Importance of task specific RNN .,named-entity-recognition,3,36,0.9473684210526316,148,0.8,22,0.9166666666666666,1,0
150,"For this task , Tag LM increased F 1 on the development set by 4.12 % ( from 49.93 to to 54.05 % ) for entity extraction over our baseline without LM embeddings and it was a major component in our winning submission to Science IE , Scenario 1 .",Overall system results,Importance of task specific RNN .,named-entity-recognition,3,37,0.9736842105263158,149,0.8054054054054054,23,0.9583333333333334,1,0
151,We conclude that LM embeddings can improve the performance of a sequence tagger even when the data comes from a different domain .,Overall system results,Importance of task specific RNN .,named-entity-recognition,3,38,1.0,150,0.8108108108108109,24,1.0,1,0
152,Related work,,,named-entity-recognition,3,0,0.0,151,0.8162162162162162,0,0.0,1,0
153,Unlabeled data .,Related work,,named-entity-recognition,3,1,0.0909090909090909,152,0.8216216216216217,1,0.0357142857142857,0,0
154,Tag LM was inspired by the widespread use of pre-trained word embeddings in supervised sequence tagging models .,Related work,Unlabeled data .,named-entity-recognition,3,2,0.1818181818181818,153,0.827027027027027,2,0.0714285714285714,0,0
155,"Besides pre-trained word embeddings , our method is most closely related to .",Related work,Unlabeled data .,named-entity-recognition,3,3,0.2727272727272727,154,0.8324324324324325,3,0.1071428571428571,0,0
156,"Instead of using a LM , uses a probabilistic generative model to infer contextsensitive latent variables for each token , which are then used as extra features in a supervised CRF tagger .",Related work,Unlabeled data .,named-entity-recognition,3,4,0.3636363636363636,155,0.8378378378378378,4,0.1428571428571428,0,0
157,"Other semisupervised learning methods for structured prediction problems include co-training , expectation maximization , structural learning ( Ando and Zhang , 2005 ) and maximum discriminant functions .",Related work,Unlabeled data .,named-entity-recognition,3,5,0.4545454545454545,156,0.8432432432432433,5,0.1785714285714285,0,0
158,It is easy to combine Tag LM with any of the above methods by including LM embeddings as additional features in the discriminative components of the model ( except for expectation maximization ) .,Related work,Unlabeled data .,named-entity-recognition,3,6,0.5454545454545454,157,0.8486486486486486,6,0.2142857142857142,0,0
159,A detailed discussion of semisupervised learning methods in NLP can be found in .,Related work,Unlabeled data .,named-entity-recognition,3,7,0.6363636363636364,158,0.8540540540540541,7,0.25,0,0
160,"learned a context encoder from unlabeled data with an objective function similar to a bi-directional LM and applied it to several NLP tasks closely related to the unlabeled objective function : sentence completion , lexical substitution and word sense disambiguation .",Related work,Unlabeled data .,named-entity-recognition,3,8,0.7272727272727273,159,0.8594594594594595,8,0.2857142857142857,0,0
161,"LM embeddings are related to a class of methods ( e.g. , for learning sentence and document encoders from unlabeled data , which can be used for text classification and textual entailment among other tasks .",Related work,Unlabeled data .,named-entity-recognition,3,9,0.8181818181818182,160,0.8648648648648649,9,0.3214285714285714,0,0
162,Dai and Le pre-trained LSTMs using language models and sequence autoencoders then fine tuned the weights for classification tasks .,Related work,Unlabeled data .,named-entity-recognition,3,10,0.9090909090909092,161,0.8702702702702703,10,0.3571428571428571,0,0
163,"In contrast to our method that uses unlabeled data to learn token - in - context embeddings , all of these methods use unlabeled data to learn an encoder for an entire text sequence ( sentence or document ) .",Related work,Unlabeled data .,named-entity-recognition,3,11,1.0,162,0.8756756756756757,11,0.3928571428571428,0,0
164,Neural language models .,,,named-entity-recognition,3,0,0.0,163,0.8810810810810811,12,0.4285714285714285,1,0
165,LMs have always been a critical component in statistical machine translation systems .,Neural language models .,Neural language models .,named-entity-recognition,3,1,0.0625,164,0.8864864864864865,13,0.4642857142857143,1,0
166,"Recently , neural LMs have also been integrated in neural machine translation systems ( e.g. , to score candidate translations .",Neural language models .,Neural language models .,named-entity-recognition,3,2,0.125,165,0.8918918918918919,14,0.5,1,0
167,"In contrast , Tag LM uses neural LMs to encode words in the input sequence .",Neural language models .,Neural language models .,named-entity-recognition,3,3,0.1875,166,0.8972972972972973,15,0.5357142857142857,1,0
168,"Unlike forward LMs , bidirectional LMs have received little prior attention .",Neural language models .,Neural language models .,named-entity-recognition,3,4,0.25,167,0.9027027027027028,16,0.5714285714285714,1,0
169,"Most similar to our formulation , used a bidirectional neural LM in a statistical machine translation system for instance selection .",Neural language models .,Neural language models .,named-entity-recognition,3,5,0.3125,168,0.9081081081081082,17,0.6071428571428571,1,0
170,"They tied the input token embeddings and softmax weights in the forward and backward directions , unlike our approach which uses two distinct models without any shared parameters .",Neural language models .,Neural language models .,named-entity-recognition,3,6,0.375,169,0.9135135135135136,18,0.6428571428571429,1,0
171,Frinken et al. ( 2012 ) also used a bidirectional n-gram LM for handwriting recognition .,Neural language models .,Neural language models .,named-entity-recognition,3,7,0.4375,170,0.918918918918919,19,0.6785714285714286,1,0
172,Interpreting RNN states .,Neural language models .,,named-entity-recognition,3,8,0.5,171,0.9243243243243244,20,0.7142857142857143,1,0
173,"Recently , there has been some interest in interpreting the activations of RNNs .",Neural language models .,Interpreting RNN states .,named-entity-recognition,3,9,0.5625,172,0.9297297297297298,21,0.75,1,0
174,showed that single LSTM units can learn to predict singular - plural distinctions .,Neural language models .,Interpreting RNN states .,named-entity-recognition,3,10,0.625,173,0.9351351351351352,22,0.7857142857142857,1,0
175,"visualized character level LSTM states and showed that individual cells capture long - range dependencies such as line lengths , quotes and brackets .",Neural language models .,Interpreting RNN states .,named-entity-recognition,3,11,0.6875,174,0.9405405405405406,23,0.8214285714285714,1,0
176,Our work complements these studies by showing that LM states are useful for downstream tasks as away of interpreting what they learn .,Neural language models .,Interpreting RNN states .,named-entity-recognition,3,12,0.75,175,0.945945945945946,24,0.8571428571428571,1,0
177,Other sequence tagging models .,Neural language models .,,named-entity-recognition,3,13,0.8125,176,0.9513513513513514,25,0.8928571428571429,1,0
178,Current state of the art results in sequence tagging problems are based on bidirectional RNN models .,Neural language models .,Other sequence tagging models .,named-entity-recognition,3,14,0.875,177,0.9567567567567568,26,0.9285714285714286,1,0
179,"However , many other sequence tagging models have been proposed in the literature for this class of problems ( e.g. , .",Neural language models .,Other sequence tagging models .,named-entity-recognition,3,15,0.9375,178,0.9621621621621622,27,0.9642857142857144,1,0
180,"LM embeddings could also be used as additional features in other models , although it is not clear whether the model complexity would be sufficient to effectively make use of them .",Neural language models .,Other sequence tagging models .,named-entity-recognition,3,16,1.0,179,0.9675675675675676,28,1.0,1,0
181,Conclusion,,,named-entity-recognition,3,0,0.0,180,0.972972972972973,0,0.0,1,0
182,"In this paper , we proposed a simple and general semi-supervised method using pre-trained neural language models to augment token representations in sequence tagging models .",Conclusion,Conclusion,named-entity-recognition,3,1,0.25,181,0.9783783783783784,1,0.25,0,0
183,Our method significantly outperforms current state of the art models in two popular datasets for NER and Chunking .,Conclusion,Conclusion,named-entity-recognition,3,2,0.5,182,0.9837837837837838,2,0.5,0,0
184,Our analysis shows that adding a backward LM in addition to traditional forward LMs consistently improves performance .,Conclusion,Conclusion,named-entity-recognition,3,3,0.75,183,0.9891891891891892,3,0.75,0,0
185,"The proposed method is robust even when the LM is trained on unlabeled data from a different domain , or when the baseline model is trained on a large number of labeled examples .",Conclusion,Conclusion,named-entity-recognition,3,4,1.0,184,0.9945945945945946,4,1.0,0,0
1,title,,,named-entity-recognition,4,0,0.0,0,0.0,0,0.0,1,0
2,Deep contextualized word representations,title,,named-entity-recognition,4,1,0.0,1,0.0036764705882352,1,0.0,1,1
3,abstract,,,named-entity-recognition,4,0,0.0,2,0.0073529411764705,0,0.0,1,0
4,"We introduce anew type of deep contextualized word representation that models both ( 1 ) complex characteristics of word use ( e.g. , syntax and semantics ) , and ( 2 ) how these uses vary across linguistic contexts ( i.e. , to model polysemy ) .",abstract,abstract,named-entity-recognition,4,1,0.25,3,0.0110294117647058,1,0.25,1,0
5,"Our word vectors are learned functions of the internal states of a deep bidirectional language model ( biLM ) , which is pretrained on a large text corpus .",abstract,abstract,named-entity-recognition,4,2,0.5,4,0.0147058823529411,2,0.5,1,0
6,"We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems , including question answering , textual entailment and sentiment analysis .",abstract,abstract,named-entity-recognition,4,3,0.75,5,0.0183823529411764,3,0.75,1,0
7,"We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial , allowing downstream models to mix different types of semi-supervision signals .",abstract,abstract,named-entity-recognition,4,4,1.0,6,0.0220588235294117,4,1.0,1,0
8,Introduction,,,named-entity-recognition,4,0,0.0,7,0.025735294117647,0,0.0,1,0
9,Pre-trained word representations area key component in many neural language understanding models .,Introduction,Introduction,named-entity-recognition,4,1,0.0256410256410256,8,0.0294117647058823,1,0.0263157894736842,1,0
10,"However , learning high quality representations can be challenging .",Introduction,Introduction,named-entity-recognition,4,2,0.0512820512820512,9,0.0330882352941176,2,0.0526315789473684,1,0
11,"They should ideally model both ( 1 ) complex characteristics of word use ( e.g. , syntax and semantics ) , and ( 2 ) how these uses vary across linguistic contexts ( i.e. , to model polysemy ) .",Introduction,Introduction,named-entity-recognition,4,3,0.0769230769230769,10,0.0367647058823529,3,0.0789473684210526,1,0
12,"In this paper , we introduce anew type of deep contextualized word representation that directly addresses both challenges , can be easily integrated into existing models , and significantly improves the state of the art in every considered case across a range of challenging language understanding problems .",Introduction,Introduction,named-entity-recognition,4,4,0.1025641025641025,11,0.0404411764705882,4,0.1052631578947368,1,0
13,Our representations differ from traditional word type embeddings in that each token is assigned a representation that is a function of the entire input sentence .,Introduction,Introduction,named-entity-recognition,4,5,0.1282051282051282,12,0.0441176470588235,5,0.131578947368421,1,1
14,We use vectors derived from a bidirectional LSTM that is trained with a coupled lan - guage model ( LM ) objective on a large text corpus .,Introduction,Introduction,named-entity-recognition,4,6,0.1538461538461538,13,0.0477941176470588,6,0.1578947368421052,1,1
15,"For this reason , we call them ELMo ( Embeddings from Language Models ) representations .",Introduction,Introduction,named-entity-recognition,4,7,0.1794871794871795,14,0.0514705882352941,7,0.1842105263157894,1,1
16,"Unlike previous approaches for learning contextualized word vectors , ELMo representations are deep , in the sense that they area function of all of the internal layers of the biLM .",Introduction,Introduction,named-entity-recognition,4,8,0.2051282051282051,15,0.0551470588235294,8,0.2105263157894736,1,0
17,"More specifically , we learn a linear combination of the vectors stacked above each input word for each end task , which markedly improves performance over just using the top LSTM layer .",Introduction,Introduction,named-entity-recognition,4,9,0.2307692307692307,16,0.0588235294117647,9,0.2368421052631578,1,1
18,Combining the internal states in this manner allows for very rich word representations .,Introduction,Introduction,named-entity-recognition,4,10,0.2564102564102564,17,0.0625,10,0.2631578947368421,1,0
19,"Using intrinsic evaluations , we show that the higher - level LSTM states capture context - dependent aspects of word meaning ( e.g. , they can be used without modification to perform well on supervised word sense disambiguation tasks ) while lowerlevel states model aspects of syntax ( e.g. , they can be used to do part - of - speech tagging ) .",Introduction,Introduction,named-entity-recognition,4,11,0.282051282051282,18,0.0661764705882353,11,0.2894736842105263,1,0
20,"Simultaneously exposing all of these signals is highly beneficial , allowing the learned models select the types of semi-supervision that are most useful for each end task .",Introduction,Introduction,named-entity-recognition,4,12,0.3076923076923077,19,0.0698529411764705,12,0.3157894736842105,1,0
21,Extensive experiments demonstrate that ELMo representations work extremely well in practice .,Introduction,Introduction,named-entity-recognition,4,13,0.3333333333333333,20,0.0735294117647058,13,0.3421052631578947,1,0
22,"We first show that they can be easily added to existing models for six diverse and challenging language understanding problems , including textual entailment , question answering and sentiment analysis .",Introduction,Introduction,named-entity-recognition,4,14,0.358974358974359,21,0.0772058823529411,14,0.3684210526315789,1,0
23,"The addition of ELMo representations alone significantly improves the state of the art in every case , including up to 20 % relative error reductions .",Introduction,Introduction,named-entity-recognition,4,15,0.3846153846153846,22,0.0808823529411764,15,0.3947368421052631,1,0
24,"For tasks where direct comparisons are possible , ELMo outperforms CoVe , which computes contextualized representations using a neural machine translation encoder .",Introduction,Introduction,named-entity-recognition,4,16,0.4102564102564102,23,0.0845588235294117,16,0.4210526315789473,1,0
25,"Finally , an analysis of both ELMo and CoVe reveals that deep representations outperform ar Xiv : 1802.05365v2 [ cs. CL ] 22 Mar 2018 those derived from just the top layer of an LSTM .",Introduction,Introduction,named-entity-recognition,4,17,0.4358974358974359,24,0.088235294117647,17,0.4473684210526316,1,0
26,"Our trained models and code are publicly available , and we expect that ELMo will provide similar gains for many other NLP problems .",Introduction,Introduction,named-entity-recognition,4,18,0.4615384615384615,25,0.0919117647058823,18,0.4736842105263157,1,0
27,1,Introduction,Introduction,named-entity-recognition,4,19,0.4871794871794871,26,0.0955882352941176,19,0.5,1,0
28,"2 Related work Due to their ability to capture syntactic and semantic information of words from large scale unlabeled text , pretrained word vectors area standard component of most state - of the - art NLP architectures , including for question answering , textual entailment and semantic role labeling .",Introduction,Introduction,named-entity-recognition,4,20,0.5128205128205128,27,0.0992647058823529,20,0.5263157894736842,1,0
29,"However , these approaches for learning word vectors only allow a single contextindependent representation for each word .",Introduction,Introduction,named-entity-recognition,4,21,0.5384615384615384,28,0.1029411764705882,21,0.5526315789473685,1,0
30,"Previously proposed methods overcome some of the shortcomings of traditional word vectors by either enriching them with subword information ( e.g. , or learning separate vectors for each word sense ( e.g. , .",Introduction,Introduction,named-entity-recognition,4,22,0.5641025641025641,29,0.1066176470588235,22,0.5789473684210527,1,0
31,"Our approach also benefits from subword units through the use of character convolutions , and we seamlessly incorporate multi-sense information into downstream tasks without explicitly training to predict predefined sense classes .",Introduction,Introduction,named-entity-recognition,4,23,0.5897435897435898,30,0.1102941176470588,23,0.6052631578947368,1,0
32,Other recent work has also focused on learning context - dependent representations .,Introduction,Introduction,named-entity-recognition,4,24,0.6153846153846154,31,0.1139705882352941,24,0.631578947368421,1,0
33,context2vec,Introduction,Introduction,named-entity-recognition,4,25,0.6410256410256411,32,0.1176470588235294,25,0.6578947368421053,1,0
34,"( Melamud et al. , 2016 ) uses a bidirectional Long Short Term Memory ( LSTM ; Hochreiter and Schmidhuber , 1997 ) to encode the context around a pivot word .",Introduction,Introduction,named-entity-recognition,4,26,0.6666666666666666,33,0.1213235294117647,26,0.6842105263157895,1,0
35,Other approaches for learning contextual embeddings include the pivot word itself in the representation and are computed with the encoder of either a supervised neural machine translation ( MT ) system or an unsupervised language model .,Introduction,Introduction,named-entity-recognition,4,27,0.6923076923076923,34,0.125,27,0.7105263157894737,1,0
36,"Both of these approaches benefit from large datasets , although the MT approach is limited by the size of parallel corpora .",Introduction,Introduction,named-entity-recognition,4,28,0.717948717948718,35,0.1286764705882352,28,0.7368421052631579,1,0
37,"In this paper , we take full advantage of access to plentiful monolingual data , and train our biLM on a corpus with approximately 30 million sentences .",Introduction,Introduction,named-entity-recognition,4,29,0.7435897435897436,36,0.1323529411764706,29,0.7631578947368421,1,0
38,"We also generalize these approaches to deep contextual representations , which we show work well across abroad range of diverse NLP tasks .",Introduction,Introduction,named-entity-recognition,4,30,0.7692307692307693,37,0.1360294117647058,30,0.7894736842105263,1,0
39,1 http://allennlp.org/elmo,Introduction,Introduction,named-entity-recognition,4,31,0.7948717948717948,38,0.1397058823529411,31,0.8157894736842105,1,0
40,Previous work has also shown that different layers of deep biRNNs encode different types of information .,Introduction,Introduction,named-entity-recognition,4,32,0.8205128205128205,39,0.1433823529411764,32,0.8421052631578947,1,0
41,"For example , introducing multi-task syntactic supervision ( e.g. , part - of - speech tags ) at the lower levels of a deep LSTM can improve overall performance of higher level tasks such as dependency parsing or CCG super tagging .",Introduction,Introduction,named-entity-recognition,4,33,0.8461538461538461,40,0.1470588235294117,33,0.868421052631579,1,0
42,"In an RNN - based encoder - decoder machine translation system , showed that the representations learned at the first layer in a 2 layer LSTM encoder are better at predicting POS tags then second layer .",Introduction,Introduction,named-entity-recognition,4,34,0.8717948717948718,41,0.150735294117647,34,0.8947368421052632,1,0
43,"Finally , the top layer of an LSTM for encoding word context has been shown to learn representations of word sense .",Introduction,Introduction,named-entity-recognition,4,35,0.8974358974358975,42,0.1544117647058823,35,0.9210526315789472,1,0
44,"We show that similar signals are also induced by the modified language model objective of our ELMo representations , and it can be very beneficial to learn models for downstream tasks that mix these different types of semi-supervision .",Introduction,Introduction,named-entity-recognition,4,36,0.9230769230769232,43,0.1580882352941176,36,0.9473684210526316,1,0
45,and pretrain encoder - decoder pairs using language models and sequence autoencoders and then fine tune with task specific supervision .,Introduction,Introduction,named-entity-recognition,4,37,0.9487179487179488,44,0.1617647058823529,37,0.9736842105263158,1,0
46,"In contrast , after pretraining the biLM with unlabeled data , we fix the weights and add additional taskspecific model capacity , allowing us to leverage large , rich and universal biLM representations for cases where downstream training data size dictates a smaller supervised model .",Introduction,Introduction,named-entity-recognition,4,38,0.9743589743589745,45,0.1654411764705882,38,1.0,1,0
47,Dai and Le,Introduction,,named-entity-recognition,4,39,1.0,46,0.1691176470588235,0,0.0,1,0
48,Bidirectional language models,,,named-entity-recognition,4,0,0.0,47,0.1727941176470588,0,0.0,1,0
49,"Given a sequence of N tokens , ( t 1 , t 2 , ... , t N ) , a forward language model computes the probability of the sequence by modeling the probability of to - ken t k given the history ( t 1 , ... , t k?1 ) :",Bidirectional language models,Bidirectional language models,named-entity-recognition,4,1,0.0175438596491228,48,0.1764705882352941,1,0.1,1,0
50,Recent state - of - the - art neural language models compute a context - independent token representation x LM k ( via token embeddings or a CNN over characters ) then pass it through L layers of forward LSTMs .,Bidirectional language models,Bidirectional language models,named-entity-recognition,4,2,0.0350877192982456,49,0.1801470588235294,2,0.2,1,0
51,"At each position k , each LSTM layer outputs a context - dependent representation ? ? h LM k , j where j = 1 , . . . , L. The top layer LSTM output , ? ? h LM k , L , is used to predict the next token t k +1 with a Softmax layer .",Bidirectional language models,Bidirectional language models,named-entity-recognition,4,3,0.0526315789473684,50,0.1838235294117647,3,0.3,1,0
52,"A backward LM is similar to a forward LM , except it runs over the sequence in reverse , predicting the previous token given the future context :",Bidirectional language models,Bidirectional language models,named-entity-recognition,4,4,0.0701754385964912,51,0.1875,4,0.4,1,0
53,"It can be implemented in an analogous way to a forward LM , with each backward LSTM layer j in a L layer deep model producing representations ? ? h LM k , j oft k given ( t k+1 , . . . , t N ) .",Bidirectional language models,Bidirectional language models,named-entity-recognition,4,5,0.087719298245614,52,0.1911764705882352,5,0.5,1,0
54,A bi LM combines both a forward and backward LM .,Bidirectional language models,Bidirectional language models,named-entity-recognition,4,6,0.1052631578947368,53,0.1948529411764706,6,0.6,1,0
55,Our formulation jointly maximizes the log likelihood of the forward and backward directions :,Bidirectional language models,Bidirectional language models,named-entity-recognition,4,7,0.1228070175438596,54,0.1985294117647058,7,0.7,1,0
56,We tie the parameters for both the token representation (? x ) and Softmax layer (? s ) in the forward and backward direction while maintaining separate parameters for the LSTMs in each direction .,Bidirectional language models,Bidirectional language models,named-entity-recognition,4,8,0.1403508771929824,55,0.2022058823529411,8,0.8,1,0
57,"Overall , this formulation is similar to the approach of Peters et al. , with the exception that we share some weights between directions instead of using completely independent parameters .",Bidirectional language models,Bidirectional language models,named-entity-recognition,4,9,0.1578947368421052,56,0.2058823529411764,9,0.9,1,0
58,"In the next section , we depart from previous work by introducing anew approach for learning word representations that area linear combination of the biLM layers .",Bidirectional language models,Bidirectional language models,named-entity-recognition,4,10,0.175438596491228,57,0.2095588235294117,10,1.0,1,0
59,ELMo,Bidirectional language models,,named-entity-recognition,4,11,0.1929824561403508,58,0.213235294117647,0,0.0,1,0
60,ELMo is a task specific combination of the intermediate layer representations in the biLM .,Bidirectional language models,ELMo,named-entity-recognition,4,12,0.2105263157894736,59,0.2169117647058823,1,0.0909090909090909,1,0
61,"For each token t k , a L-layer biLM computes a set of 2L + 1 representations",Bidirectional language models,ELMo,named-entity-recognition,4,13,0.2280701754385964,60,0.2205882352941176,2,0.1818181818181818,1,0
62,"where h LM k ,0 is the token layer and h LM",Bidirectional language models,ELMo,named-entity-recognition,4,14,0.2456140350877192,61,0.2242647058823529,3,0.2727272727272727,1,0
63,", for each biLSTM layer .",Bidirectional language models,ELMo,named-entity-recognition,4,15,0.2631578947368421,62,0.2279411764705882,4,0.3636363636363636,1,0
64,"For inclusion in a downstream model , ELMo collapses all layers in R into a single vector , ELMo k = E(R k ; ? e ) .",Bidirectional language models,ELMo,named-entity-recognition,4,16,0.2807017543859649,63,0.2316176470588235,5,0.4545454545454545,1,0
65,"In the simplest case , ELMo just selects the top layer , E( R k ) = h LM k , L , as in Tag LM and CoVe .",Bidirectional language models,ELMo,named-entity-recognition,4,17,0.2982456140350877,64,0.2352941176470588,6,0.5454545454545454,1,0
66,"More generally , we compute a task specific weighting of all biLM layers :",Bidirectional language models,ELMo,named-entity-recognition,4,18,0.3157894736842105,65,0.2389705882352941,7,0.6363636363636364,1,0
67,"( 1 ) In ( 1 ) , s task are softmax - normalized weights and the scalar parameter ?",Bidirectional language models,ELMo,named-entity-recognition,4,19,0.3333333333333333,66,0.2426470588235294,8,0.7272727272727273,1,0
68,task allows the task model to scale the entire ELMo vector . ?,Bidirectional language models,ELMo,named-entity-recognition,4,20,0.3508771929824561,67,0.2463235294117647,9,0.8181818181818182,1,0
69,is of practical importance to aid the optimization process ( see supplemental material for details ) .,Bidirectional language models,ELMo,named-entity-recognition,4,21,0.3684210526315789,68,0.25,10,0.9090909090909092,1,0
70,"Considering that the activations of each biLM layer have a different distribution , in some cases it also helped to apply layer normalization to each biLM layer before weighting .",Bidirectional language models,ELMo,named-entity-recognition,4,22,0.3859649122807017,69,0.2536764705882353,11,1.0,1,0
71,Using biLMs for supervised NLP tasks,Bidirectional language models,,named-entity-recognition,4,23,0.4035087719298245,70,0.2573529411764705,0,0.0,1,0
72,"Given a pre-trained biLM and a supervised architecture fora target NLP task , it is a simple process to use the biLM to improve the task model .",Bidirectional language models,Using biLMs for supervised NLP tasks,named-entity-recognition,4,24,0.4210526315789473,71,0.2610294117647059,1,0.0714285714285714,1,0
73,We simply run the biLM and record all of the layer representations for each word .,Bidirectional language models,Using biLMs for supervised NLP tasks,named-entity-recognition,4,25,0.4385964912280701,72,0.2647058823529412,2,0.1428571428571428,1,0
74,"Then , we let the end task model learn a linear combination of these representations , as described below .",Bidirectional language models,Using biLMs for supervised NLP tasks,named-entity-recognition,4,26,0.4561403508771929,73,0.2683823529411764,3,0.2142857142857142,1,0
75,First consider the lowest layers of the supervised model without the biLM .,Bidirectional language models,Using biLMs for supervised NLP tasks,named-entity-recognition,4,27,0.4736842105263157,74,0.2720588235294117,4,0.2857142857142857,1,0
76,"Most supervised NLP models share a common architecture at the lowest layers , allowing us to add ELMo in a consistent , unified manner .",Bidirectional language models,Using biLMs for supervised NLP tasks,named-entity-recognition,4,28,0.4912280701754385,75,0.2757352941176471,5,0.3571428571428571,1,0
77,"Given a sequence of tokens ( t 1 , . . . , t N ) , it is standard to form a context - independent token representation x k for each token position using pre-trained word embeddings and optionally character - based representations .",Bidirectional language models,Using biLMs for supervised NLP tasks,named-entity-recognition,4,29,0.5087719298245614,76,0.2794117647058823,6,0.4285714285714285,1,0
78,"Then , the model forms a context - sensitive representation h k , typically using either bidirectional RNNs , CNNs , or feed forward networks .",Bidirectional language models,Using biLMs for supervised NLP tasks,named-entity-recognition,4,30,0.5263157894736842,77,0.2830882352941176,7,0.5,1,0
79,"To add ELMo to the supervised model , we first freeze the weights of the biLM and then concatenate the ELMo vector ELMo task k with x k and pass the ELMo enhanced representation [ x k ; ELMo task k ] into the task RNN .",Bidirectional language models,Using biLMs for supervised NLP tasks,named-entity-recognition,4,31,0.543859649122807,78,0.2867647058823529,8,0.5714285714285714,1,0
80,"For some tasks ( e.g. , SNLI , SQuAD ) , we observe further improvements by also including ELMo at the output of the task RNN by introducing another set of output specific linear weights and replacing h k with [ h k ; ELMo task k ] .",Bidirectional language models,Using biLMs for supervised NLP tasks,named-entity-recognition,4,32,0.5614035087719298,79,0.2904411764705882,9,0.6428571428571429,1,0
81,"As the remainder of the supervised model remains unchanged , these additions can happen within the context of more complex neural models .",Bidirectional language models,Using biLMs for supervised NLP tasks,named-entity-recognition,4,33,0.5789473684210527,80,0.2941176470588235,10,0.7142857142857143,1,0
82,"For example , seethe SNLI experiments in Sec. 4 where a bi-attention layer follows the biLSTMs , or the coreference resolution experiments where a clustering model is layered on top of the biLSTMs .",Bidirectional language models,Using biLMs for supervised NLP tasks,named-entity-recognition,4,34,0.5964912280701754,81,0.2977941176470588,11,0.7857142857142857,1,0
83,"Finally , we found it beneficial to add a moderate amount of dropout to and in some cases to regularize the ELMo weights by adding ?",Bidirectional language models,Using biLMs for supervised NLP tasks,named-entity-recognition,4,35,0.6140350877192983,82,0.3014705882352941,12,0.8571428571428571,1,0
84,w 2 2 to the loss .,Bidirectional language models,Using biLMs for supervised NLP tasks,named-entity-recognition,4,36,0.631578947368421,83,0.3051470588235294,13,0.9285714285714286,1,0
85,This imposes an inductive bias on the ELMo weights to stay close to an average of all biLM layers .,Bidirectional language models,Using biLMs for supervised NLP tasks,named-entity-recognition,4,37,0.6491228070175439,84,0.3088235294117647,14,1.0,1,0
86,Pre-trained bidirectional language model architecture,Bidirectional language models,,named-entity-recognition,4,38,0.6666666666666666,85,0.3125,0,0.0,1,0
87,"The pre-trained biLMs in this paper are similar to the architectures in Jzefowicz et al. and , but modified to support joint training of both directions and add a residual connection between LSTM layers .",Bidirectional language models,Pre-trained bidirectional language model architecture,named-entity-recognition,4,39,0.6842105263157895,86,0.3161764705882353,1,0.0526315789473684,1,0
88,"We focus on large scale biLMs in this work , as Peters et al .",Bidirectional language models,Pre-trained bidirectional language model architecture,named-entity-recognition,4,40,0.7017543859649122,87,0.3198529411764705,2,0.1052631578947368,1,0
89,highlighted the importance of using biLMs over forward - only LMs and large scale training .,Bidirectional language models,Pre-trained bidirectional language model architecture,named-entity-recognition,4,41,0.7192982456140351,88,0.3235294117647059,3,0.1578947368421052,1,0
90,"To balance overall language model perplexity with model size and computational requirements for downstream tasks while maintaining a purely character - based input representation , we halved all embedding and hidden dimensions from the single best model CNN - BIG - LSTM in Jzefowicz et al ..",Bidirectional language models,Pre-trained bidirectional language model architecture,named-entity-recognition,4,42,0.7368421052631579,89,0.3272058823529412,4,0.2105263157894736,1,0
91,The final model uses L = 2 biLSTM layers with 4096 units and 512 dimension projections and a residual connection from the first to second layer .,Bidirectional language models,Pre-trained bidirectional language model architecture,named-entity-recognition,4,43,0.7543859649122807,90,0.3308823529411764,5,0.2631578947368421,1,0
92,The context insensitive type representation uses 2048 character n-gram convolutional filters followed by two highway layers and a linear projection down to a 512 representation .,Bidirectional language models,Pre-trained bidirectional language model architecture,named-entity-recognition,4,44,0.7719298245614035,91,0.3345588235294117,6,0.3157894736842105,1,0
93,"As a result , the biLM provides three layers of representations for each input token , including those outside the training set due to the purely character input .",Bidirectional language models,Pre-trained bidirectional language model architecture,named-entity-recognition,4,45,0.7894736842105263,92,0.3382352941176471,7,0.3684210526315789,1,0
94,"In contrast , traditional word embedding methods only provide one layer of representation for tokens in a fixed vocabulary .",Bidirectional language models,Pre-trained bidirectional language model architecture,named-entity-recognition,4,46,0.8070175438596491,93,0.3419117647058823,8,0.4210526315789473,1,0
95,"After training for 10 epochs on the 1B Word Benchmark , the average forward and backward perplexities is 39.7 , compared to 30.0 for the forward CNN - BIG - LSTM .",Bidirectional language models,Pre-trained bidirectional language model architecture,named-entity-recognition,4,47,0.8245614035087719,94,0.3455882352941176,9,0.4736842105263157,1,0
96,"Generally , we found the forward and backward perplexities to be approximately equal , with the backward value slightly lower .",Bidirectional language models,Pre-trained bidirectional language model architecture,named-entity-recognition,4,48,0.8421052631578947,95,0.3492647058823529,10,0.5263157894736842,1,0
97,"Once pretrained , the biLM can compute representations for any task .",Bidirectional language models,Pre-trained bidirectional language model architecture,named-entity-recognition,4,49,0.8596491228070176,96,0.3529411764705882,11,0.5789473684210527,1,0
98,"In some cases , fine tuning the biLM on domain specific data leads to significant drops in perplexity and an increase in downstream task performance .",Bidirectional language models,Pre-trained bidirectional language model architecture,named-entity-recognition,4,50,0.8771929824561403,97,0.3566176470588235,12,0.631578947368421,1,0
99,This can be seen as a type of domain transfer for the biLM .,Bidirectional language models,Pre-trained bidirectional language model architecture,named-entity-recognition,4,51,0.8947368421052632,98,0.3602941176470588,13,0.6842105263157895,1,0
100,"As a result , inmost cases we used a fine - tuned biLM in the downstream task .",Bidirectional language models,Pre-trained bidirectional language model architecture,named-entity-recognition,4,52,0.912280701754386,99,0.3639705882352941,14,0.7368421052631579,1,0
101,See supplemental material for details .,Bidirectional language models,,named-entity-recognition,4,53,0.9298245614035088,100,0.3676470588235294,15,0.7894736842105263,1,0
102,shows the performance of ELMo across a diverse set of six benchmark NLP tasks .,Bidirectional language models,See supplemental material for details .,named-entity-recognition,4,54,0.9473684210526316,101,0.3713235294117647,16,0.8421052631578947,1,0
103,"In every task considered , simply adding ELMo establishes anew state - of - the - art result , with relative error reductions ranging from 6 - 20 % over strong base models .",Bidirectional language models,See supplemental material for details .,named-entity-recognition,4,55,0.9649122807017544,102,0.375,17,0.8947368421052632,1,0
104,This is a very general result across a diverse set model architectures and language understanding tasks .,Bidirectional language models,See supplemental material for details .,named-entity-recognition,4,56,0.9824561403508772,103,0.3786764705882353,18,0.9473684210526316,1,0
105,In the remainder of this section we provide high - level sketches of the individual task results ; seethe supplemental material for full experimental details .,Bidirectional language models,See supplemental material for details .,named-entity-recognition,4,57,1.0,104,0.3823529411764705,19,1.0,1,0
106,Evaluation,,,named-entity-recognition,4,0,0.0,105,0.3860294117647059,0,0.0,1,0
107,Question Textual entailment,Evaluation,,named-entity-recognition,4,1,0.008695652173913,106,0.3897058823529412,1,0.0714285714285714,1,0
108,"Textual entailment is the task of determining whether a "" hypothesis "" is true , given a "" premise "" .",Evaluation,Question Textual entailment,named-entity-recognition,4,2,0.017391304347826,107,0.3933823529411764,2,0.1428571428571428,1,0
109,The Stanford Natural Language Inference ( SNLI ) corpus provides approximately 550K hypothesis / premise pairs .,Evaluation,Question Textual entailment,named-entity-recognition,4,3,0.0260869565217391,108,0.3970588235294117,3,0.2142857142857142,1,1
110,"Our baseline , the ESIM sequence model from Chen et al. , uses a biL - STM to encode the premise and hypothesis , followed by a matrix attention layer , a local inference layer , another biLSTM inference composition layer , and finally a pooling operation before the output layer .",Evaluation,Question Textual entailment,named-entity-recognition,4,4,0.0347826086956521,109,0.4007352941176471,4,0.2857142857142857,1,0
111,"Overall , adding ELMo to the ESIM model improves accuracy by an average of 0.7 % across five random seeds .",Evaluation,Question Textual entailment,named-entity-recognition,4,5,0.0434782608695652,110,0.4044117647058823,5,0.3571428571428571,1,1
112,"A five member ensemble pushes the overall accuracy to 89.3 % , exceeding the previous ensemble best of 88.9 % .",Evaluation,Question Textual entailment,named-entity-recognition,4,6,0.0521739130434782,111,0.4080882352941176,6,0.4285714285714285,1,0
113,Semantic role labeling,Evaluation,,named-entity-recognition,4,7,0.0608695652173913,112,0.4117647058823529,7,0.5,1,0
114,"A semantic role labeling ( SRL ) system models the predicate - argument structure of a sentence , and is often described as answering "" Who did what to whom "" .",Evaluation,Semantic role labeling,named-entity-recognition,4,8,0.0695652173913043,113,0.4154411764705882,8,0.5714285714285714,1,0
115,He et al .,Evaluation,,named-entity-recognition,4,9,0.0782608695652174,114,0.4191176470588235,9,0.6428571428571429,1,0
116,"( 2017 ) modeled SRL as a BIO tagging problem and used an 8 - layer deep biLSTM with forward and backward directions interleaved , following .",Evaluation,He et al .,named-entity-recognition,4,10,0.0869565217391304,115,0.4227941176470588,10,0.7142857142857143,1,0
117,As shown in Coreference resolution Coreference resolution is the task of clustering mentions in text that refer to the same underlying real world entities .,Evaluation,He et al .,named-entity-recognition,4,11,0.0956521739130434,116,0.4264705882352941,11,0.7857142857142857,1,0
118,Our baseline model is the end - to - end span - based neural model of .,Evaluation,He et al .,named-entity-recognition,4,12,0.1043478260869565,117,0.4301470588235294,12,0.8571428571428571,1,0
119,It uses a biLSTM and attention mechanism to first compute span representations and then applies a softmax mention ranking model to find coreference chains .,Evaluation,He et al .,named-entity-recognition,4,13,0.1130434782608695,118,0.4338235294117647,13,0.9285714285714286,1,0
120,"In our experiments with the OntoNotes coreference annotations from the CoNLL 2012 shared task , adding ELMo improved the average F 1 by 3.2 % from 67.2 to 70.4 , establishing anew state of the art , again improving over the previous best ensemble result by 1.6 % F 1 .",Evaluation,He et al .,named-entity-recognition,4,14,0.1217391304347826,119,0.4375,14,1.0,1,1
121,Analysis,Evaluation,,named-entity-recognition,4,15,0.1304347826086956,120,0.4411764705882353,0,0.0,1,0
122,This section provides an ablation analysis to validate our chief claims and to elucidate some interesting aspects of ELMo representations .,Evaluation,Analysis,named-entity-recognition,4,16,0.1391304347826087,121,0.4448529411764705,1,0.2,1,0
123,"Sec. 5.1 shows that using deep contextual representations in downstream tasks improves performance over previous work that uses just the top layer , regardless of whether they are produced from a biLM or MT encoder , and that ELMo representations provide the best overall performance .",Evaluation,Analysis,named-entity-recognition,4,17,0.1478260869565217,122,0.4485294117647059,2,0.4,1,0
124,"Sec. 5.3 explores the different types of contextual information captured in biLMs and uses two intrinsic evaluations to show that syntactic information is better represented at lower layers while semantic information is captured a higher layers , consistent with MT encoders .",Evaluation,Analysis,named-entity-recognition,4,18,0.1565217391304348,123,0.4522058823529412,3,0.6,1,0
125,It also shows that our biLM consistently provides richer representations then CoVe .,Evaluation,Analysis,named-entity-recognition,4,19,0.1652173913043478,124,0.4558823529411764,4,0.8,1,0
126,"Additionally , we analyze the sensitivity to where ELMo is included in the task model ( Sec. 5.2 ) , training set size ( Sec. 5.4 ) , and visualize the ELMo learned weights across the tasks ( Sec. 5.5 ) .",Evaluation,Analysis,named-entity-recognition,4,20,0.1739130434782608,125,0.4595588235294117,5,1.0,1,0
127,Alternate layer weighting schemes,Evaluation,,named-entity-recognition,4,21,0.1826086956521739,126,0.4632352941176471,0,0.0,1,0
128,There are many alternatives to Equation 1 for combining the biLM layers .,Evaluation,Alternate layer weighting schemes,named-entity-recognition,4,22,0.1913043478260869,127,0.4669117647058823,1,0.0212765957446808,1,0
129,"Previous work on contextual representations used only the last layer , whether it be from a biLM or an MT encoder ( CoVe ; .",Evaluation,Alternate layer weighting schemes,named-entity-recognition,4,23,0.2,128,0.4705882352941176,2,0.0425531914893617,1,0
130,The choice of the regularization parameter ?,Evaluation,Alternate layer weighting schemes,named-entity-recognition,4,24,0.208695652173913,129,0.4742647058823529,3,0.0638297872340425,1,0
131,"is also important , as large values such as ? = 1 effectively reduce the weighting function to a simple average over the layers , while smaller values ( e.g. , ? = 0.001 ) allow the layer weights to vary .",Evaluation,Alternate layer weighting schemes,named-entity-recognition,4,25,0.217391304347826,130,0.4779411764705882,4,0.0851063829787234,1,0
132,"compares these alternatives for SQuAD , SNLI and SRL .",Evaluation,Alternate layer weighting schemes,named-entity-recognition,4,26,0.2260869565217391,131,0.4816176470588235,5,0.1063829787234042,1,0
133,"Including representations from all layers improves overall performance over just using the last layer , and including contextual representations from the last layer improves performance over the baseline .",Evaluation,Alternate layer weighting schemes,named-entity-recognition,4,27,0.2347826086956521,132,0.4852941176470588,6,0.1276595744680851,1,0
134,"For example , in the case of SQuAD , using just the last biLM layer improves development F 1 by 3.9 % over the baseline .",Evaluation,Alternate layer weighting schemes,named-entity-recognition,4,28,0.2434782608695652,133,0.4889705882352941,7,0.1489361702127659,1,0
135,"Averaging all biLM layers instead of using just the last layer improves F 1 another 0.3 % ( comparing "" Last Only "" to ?= 1 columns ) , and allowing the task model to learn individual layer weights improves F 1 another 0.2 % ( ?= 1 vs. ?= 0.001 ) .",Evaluation,Alternate layer weighting schemes,named-entity-recognition,4,29,0.2521739130434782,134,0.4926470588235294,8,0.1702127659574468,1,0
136,A small ?,Evaluation,Alternate layer weighting schemes,named-entity-recognition,4,30,0.2608695652173913,135,0.4963235294117647,9,0.1914893617021276,1,0
137,"is preferred inmost cases with ELMo , although for NER , a task with a smaller training set , the results are insensitive to ?",Evaluation,Alternate layer weighting schemes,named-entity-recognition,4,31,0.2695652173913043,136,0.5,10,0.2127659574468085,1,0
138,( not shown ) .,Evaluation,Alternate layer weighting schemes,named-entity-recognition,4,32,0.2782608695652174,137,0.5036764705882353,11,0.2340425531914893,1,0
139,The overall trend is similar with CoVe but with smaller increases over the baseline .,Evaluation,Alternate layer weighting schemes,named-entity-recognition,4,33,0.2869565217391304,138,0.5073529411764706,12,0.2553191489361702,1,0
140,"For SNLI , averaging all layers with ?= 1 improves development accuracy from 88.2 to 88.7 % over using just the last layer .",Evaluation,Alternate layer weighting schemes,named-entity-recognition,4,34,0.2956521739130435,139,0.5110294117647058,13,0.2765957446808511,1,0
141,SRL F 1 increased a marginal 0.1 % to 82.2 for the ?= 1 case compared to using the last layer only .,Evaluation,Alternate layer weighting schemes,named-entity-recognition,4,35,0.3043478260869565,140,0.5147058823529411,14,0.2978723404255319,1,0
142,Where to include ELMo ?,Evaluation,Alternate layer weighting schemes,named-entity-recognition,4,36,0.3130434782608696,141,0.5183823529411765,15,0.3191489361702128,1,0
143,All of the task architectures in this paper include word embeddings only as input to the lowest layer biRNN .,Evaluation,Alternate layer weighting schemes,named-entity-recognition,4,37,0.3217391304347826,142,0.5220588235294118,16,0.3404255319148936,1,0
144,"However , we find that including ELMo at the output of the biRNN in task - specific architectures improves overall results for some tasks .",Evaluation,Alternate layer weighting schemes,named-entity-recognition,4,38,0.3304347826086956,143,0.5257352941176471,17,0.3617021276595745,1,0
145,"As shown in , including ELMo at both the input and output layers for SNLI and SQuAD improves over just the input layer , but for SRL ( and coreference resolution , not shown ) performance is highest when it is included at just the input layer .",Evaluation,Alternate layer weighting schemes,named-entity-recognition,4,39,0.3391304347826087,144,0.5294117647058824,18,0.3829787234042553,1,0
146,"One possible explanation for this result is that both the SNLI and SQuAD architectures use attention layers after the biRNN , so introducing ELMo at this layer allows the model to attend directly to the biLM 's internal representations .",Evaluation,Alternate layer weighting schemes,named-entity-recognition,4,40,0.3478260869565217,145,0.5330882352941176,19,0.4042553191489361,1,0
147,"In the SRL case , . . } they were actors who had been handed fat roles in a successful play , and had tale nt enough to fill the roles competently , with nice understatement . the task - specific context representations are likely more important than those from the biLM .",Evaluation,Alternate layer weighting schemes,named-entity-recognition,4,41,0.3565217391304348,146,0.5367647058823529,20,0.425531914893617,1,0
148,What information is captured by the biLM 's representations ?,Evaluation,Alternate layer weighting schemes,named-entity-recognition,4,42,0.3652173913043478,147,0.5404411764705882,21,0.4468085106382978,1,0
149,Since adding,Evaluation,,named-entity-recognition,4,43,0.3739130434782609,148,0.5441176470588235,22,0.4680851063829787,1,0
150,"ELMo improves task performance over word vectors alone , the biLM 's contextual representations must encode information generally useful for NLP tasks that is not captured in word vectors .",Evaluation,Since adding,named-entity-recognition,4,44,0.3826086956521739,149,0.5477941176470589,23,0.4893617021276595,1,0
151,"Intuitively , the biLM must be disambiguating the meaning of words using their context .",Evaluation,Since adding,named-entity-recognition,4,45,0.391304347826087,150,0.5514705882352942,24,0.5106382978723404,1,0
152,"Consider "" play "" , a highly polysemous word .",Evaluation,Since adding,named-entity-recognition,4,46,0.4,151,0.5551470588235294,25,0.5319148936170213,1,0
153,"The top of lists nearest neighbors to "" play "" using GloVe vectors .",Evaluation,Since adding,named-entity-recognition,4,47,0.408695652173913,152,0.5588235294117647,26,0.5531914893617021,1,0
154,"They are spread across several parts of speech ( e.g. , "" played "" , "" playing "" as verbs , and "" player "" , "" game "" as nouns ) but concentrated in the sportsrelated senses of "" play "" .",Evaluation,Since adding,named-entity-recognition,4,48,0.417391304347826,153,0.5625,27,0.574468085106383,1,0
155,"In contrast , the bottom two rows show nearest neighbor sentences from the SemCor dataset ( see below ) using the biLM 's context representation of "" play "" in the source sentence .",Evaluation,Since adding,named-entity-recognition,4,49,0.4260869565217391,154,0.5661764705882353,28,0.5957446808510638,1,0
156,"In these cases , the biLM is able to disambiguate both the part of speech and word sense in the source sentence .",Evaluation,Since adding,named-entity-recognition,4,50,0.4347826086956521,155,0.5698529411764706,29,0.6170212765957447,1,0
157,These observations can be quantified using an intrinsic evaluation of the contextual representations similar to .,Evaluation,Since adding,named-entity-recognition,4,51,0.4434782608695652,156,0.5735294117647058,30,0.6382978723404256,1,0
158,"To isolate the information encoded by the biLM , the representations are used to directly make predictions fora fine grained word sense disambiguation ( WSD ) task and a POS tagging task .",Evaluation,Since adding,named-entity-recognition,4,52,0.4521739130434782,157,0.5772058823529411,31,0.6595744680851063,1,0
159,"Using this approach , it is also possible to compare to CoVe , and across each of the individual layers .",Evaluation,Since adding,named-entity-recognition,4,53,0.4608695652173913,158,0.5808823529411765,32,0.6808510638297872,1,0
160,"Word sense disambiguation Given a sentence , we can use the biLM representations to predict the sense of a target word using a simple 1nearest neighbor approach , similar to .",Evaluation,Since adding,named-entity-recognition,4,54,0.4695652173913043,159,0.5845588235294118,33,0.7021276595744681,1,0
161,"To do so , we first use the biLM to compute representations for all words in Sem - Cor 3.0 , our training corpus , and then take the average representation for each sense .",Evaluation,Since adding,named-entity-recognition,4,55,0.4782608695652174,160,0.5882352941176471,34,0.723404255319149,1,0
162,"At test time , we again use the biLM to compute representations fora given target word and take the nearest neighbor sense from the training set , falling back to the first sense from WordNet for lemmas not observed during training .",Evaluation,Since adding,named-entity-recognition,4,56,0.4869565217391304,161,0.5919117647058824,35,0.7446808510638298,1,0
163,compares WSD results using the evaluation framework from across the same suite of four test sets in .,Evaluation,Since adding,named-entity-recognition,4,57,0.4956521739130435,162,0.5955882352941176,36,0.7659574468085106,1,0
164,"Overall , the biLM top layer rep-resentations have F 1 of 69.0 and are better at WSD then the first layer .",Evaluation,Since adding,named-entity-recognition,4,58,0.5043478260869565,163,0.5992647058823529,37,0.7872340425531915,1,0
165,This is competitive with a state - of - the - art WSD - specific supervised model using hand crafted features and a task specific biLSTM that is also trained with auxiliary coarse - grained semantic labels and POS tags .,Evaluation,Since adding,named-entity-recognition,4,59,0.5130434782608696,164,0.6029411764705882,38,0.8085106382978723,1,0
166,"The CoVe biLSTM layers follow a similar pattern to those from the biLM ( higher overall performance at the second layer compared to the first ) ; however , our biLM outperforms the CoVe biLSTM , which trails the WordNet first sense baseline .",Evaluation,Since adding,named-entity-recognition,4,60,0.5217391304347826,165,0.6066176470588235,39,0.8297872340425532,1,0
167,POS tagging,Evaluation,,named-entity-recognition,4,61,0.5304347826086957,166,0.6102941176470589,40,0.851063829787234,1,0
168,"To examine whether the biLM captures basic syntax , we used the context representations as input to a linear classifier that predicts POS tags with the Wall Street Journal portion of the Penn Treebank ( PTB ) .",Evaluation,POS tagging,named-entity-recognition,4,62,0.5391304347826087,167,0.6139705882352942,41,0.8723404255319149,1,0
169,"As the linear classifier adds only a small amount of model capacity , this is direct test of the biLM 's representations .",Evaluation,POS tagging,named-entity-recognition,4,63,0.5478260869565217,168,0.6176470588235294,42,0.8936170212765957,1,0
170,"Similar to WSD , the biLM representations are competitive with carefully tuned , task specific biLSTMs .",Evaluation,POS tagging,named-entity-recognition,4,64,0.5565217391304348,169,0.6213235294117647,43,0.9148936170212766,1,0
171,"However , unlike WSD , accuracies using the first biLM layer are higher than the top layer , consistent with results from deep biL - STMs in multi-task training and MT .",Evaluation,POS tagging,named-entity-recognition,4,65,0.5652173913043478,170,0.625,44,0.9361702127659576,1,0
172,"CoVe POS tagging accuracies follow the same pattern as those from the biLM , and just like for WSD , the biLM achieves higher accuracies than the CoVe encoder .",Evaluation,POS tagging,named-entity-recognition,4,66,0.5739130434782609,171,0.6286764705882353,45,0.9574468085106383,1,0
173,"Implications for supervised tasks Taken together , these experiments confirm different layers in the biLM represent different types of information and explain why including all biLM layers is important for the highest performance in downstream tasks .",Evaluation,POS tagging,named-entity-recognition,4,67,0.5826086956521739,172,0.6323529411764706,46,0.9787234042553192,1,0
174,"In addition , the biLM 's representations are more transferable to WSD and POS tagging than those in CoVe , helping to illustrate why ELMo outperforms CoVe in downstream tasks .",Evaluation,POS tagging,named-entity-recognition,4,68,0.591304347826087,173,0.6360294117647058,47,1.0,1,0
175,Sample efficiency,Evaluation,,named-entity-recognition,4,69,0.6,174,0.6397058823529411,0,0.0,1,0
176,"Adding ELMo to a model increases the sample efficiency considerably , both in terms of number of parameter updates to reach state - of - the - art performance and the overall training set size .",Evaluation,Sample efficiency,named-entity-recognition,4,70,0.6086956521739131,175,0.6433823529411765,1,0.1,1,0
177,"For example , the SRL model reaches a maximum development F 1 after 486 epochs of training without ELMo .",Evaluation,Sample efficiency,named-entity-recognition,4,71,0.6173913043478261,176,0.6470588235294118,2,0.2,1,0
178,"After adding ELMo , the model exceeds the baseline maximum at epoch 10 , a 98 % relative decrease in the number of updates needed to reach In addition , ELMo - enhanced models use smaller training sets more efficiently than models without ELMo.",Evaluation,Sample efficiency,named-entity-recognition,4,72,0.6260869565217392,177,0.6507352941176471,3,0.3,1,0
179,compares the performance of baselines models with and without ELMo as the percentage of the full training set is varied from 0.1 % to 100 % .,Evaluation,Sample efficiency,named-entity-recognition,4,73,0.6347826086956522,178,0.6544117647058824,4,0.4,1,0
180,Improvements with ELMo are largest for smaller training sets and significantly reduce the amount of training data needed to reach a given level of performance .,Evaluation,Sample efficiency,named-entity-recognition,4,74,0.6434782608695652,179,0.6580882352941176,5,0.5,1,0
181,"In the SRL case , the ELMo model with 1 % of the training set has about the same F 1 as the baseline model with 10 % of the training set .",Evaluation,Sample efficiency,named-entity-recognition,4,75,0.6521739130434783,180,0.6617647058823529,6,0.6,1,0
182,visualizes the softmax - normalized learned layer weights .,Evaluation,Sample efficiency,named-entity-recognition,4,76,0.6608695652173913,181,0.6654411764705882,7,0.7,1,0
183,"At the input layer , the task model favors the first biLSTM layer .",Evaluation,Sample efficiency,named-entity-recognition,4,77,0.6695652173913044,182,0.6691176470588235,8,0.8,1,0
184,"For coreference and SQuAD , the this is strongly favored , but the distribution is less peaked for the other tasks .",Evaluation,Sample efficiency,named-entity-recognition,4,78,0.6782608695652174,183,0.6727941176470589,9,0.9,1,0
185,"The output layer weights are relatively balanced , with a slight preference for the lower layers .",Evaluation,Sample efficiency,named-entity-recognition,4,79,0.6869565217391305,184,0.6764705882352942,10,1.0,1,0
186,Visualization of learned weights,Evaluation,,named-entity-recognition,4,80,0.6956521739130435,185,0.6801470588235294,0,0.0,1,0
187,"We have introduced a general approach for learning high - quality deep context - dependent representations from biLMs , and shown large improvements when applying ELMo to abroad range of NLP tasks .",Evaluation,Visualization of learned weights,named-entity-recognition,4,81,0.7043478260869566,186,0.6838235294117647,1,0.5,1,0
188,"Through ablations and other controlled experiments , we have also confirmed that the biLM layers efficiently encode different types of syntactic and semantic information about wordsin - context , and that using all layers improves overall task performance .",Evaluation,Visualization of learned weights,named-entity-recognition,4,82,0.7130434782608696,187,0.6875,2,1.0,1,0
189,A Supplemental Material to accompany Deep contextualized word representations,Evaluation,,named-entity-recognition,4,83,0.7217391304347827,188,0.6911764705882353,0,0.0,1,0
190,"This supplement contains details of the model architectures , training routines and hyper - parameter choices for the state - of - the - art models in Section 4 .",Evaluation,A Supplemental Material to accompany Deep contextualized word representations,named-entity-recognition,4,84,0.7304347826086957,189,0.6948529411764706,1,0.0625,1,0
191,All of the individual models share a common architecture in the lowest layers with a context independent token representation below several layers of stacked RNNs - LSTMs in every case except the SQuAD model that uses GRUs .,Evaluation,A Supplemental Material to accompany Deep contextualized word representations,named-entity-recognition,4,85,0.7391304347826086,190,0.6985294117647058,2,0.125,1,0
192,A.1,Evaluation,,named-entity-recognition,4,86,0.7478260869565218,191,0.7022058823529411,3,0.1875,1,0
193,Fine tuning biLM,Evaluation,,named-entity-recognition,4,87,0.7565217391304347,192,0.7058823529411765,4,0.25,1,0
194,"As noted in Sec. 3.4 , fine tuning the biLM on task specific data typically resulted in significant drops in perplexity .",Evaluation,Fine tuning biLM,named-entity-recognition,4,88,0.7652173913043478,193,0.7095588235294118,5,0.3125,1,0
195,"To fine tune on a given task , the supervised labels were temporarily ignored , the biLM fine tuned for one epoch on the training split and evaluated on the development split .",Evaluation,Fine tuning biLM,named-entity-recognition,4,89,0.7739130434782608,194,0.7132352941176471,6,0.375,1,0
196,"Once fine tuned , the biLM weights were fixed during task training .",Evaluation,Fine tuning biLM,named-entity-recognition,4,90,0.782608695652174,195,0.7169117647058824,7,0.4375,1,0
197,lists the development set perplexities for the considered tasks .,Evaluation,Fine tuning biLM,named-entity-recognition,4,91,0.7913043478260869,196,0.7205882352941176,8,0.5,1,0
198,"In every case except CoNLL 2012 , fine tuning results in a large improvement in perplexity , e.g. , from 72.1 to 16.8 for SNLI .",Evaluation,Fine tuning biLM,named-entity-recognition,4,92,0.8,197,0.7242647058823529,9,0.5625,1,0
199,The impact of fine tuning on supervised performance is task dependent .,Evaluation,Fine tuning biLM,named-entity-recognition,4,93,0.808695652173913,198,0.7279411764705882,10,0.625,1,0
200,"In the case of SNLI , fine tuning the biLM increased development accuracy 0.6 % from 88.9 % to 89.5 % for our single best model .",Evaluation,Fine tuning biLM,named-entity-recognition,4,94,0.8173913043478261,199,0.7316176470588235,11,0.6875,1,0
201,"However , for sentiment classification development set accuracy is approximately the same regardless whether a fine tuned biLM was used .",Evaluation,Fine tuning biLM,named-entity-recognition,4,95,0.8260869565217391,200,0.7352941176470589,12,0.75,1,0
202,A.2 Importance of ? in Eqn .,Evaluation,Fine tuning biLM,named-entity-recognition,4,96,0.8347826086956521,201,0.7389705882352942,13,0.8125,1,0
203,"The ? parameter in Eqn. ( 1 ) was of practical importance to aid optimization , due to the different distributions between the biLM internal representations and the task specific representations .",Evaluation,Fine tuning biLM,named-entity-recognition,4,97,0.8434782608695652,202,0.7426470588235294,14,0.875,1,0
204,It is especially important in the last - only casein Sec. 5.1 .,Evaluation,Fine tuning biLM,named-entity-recognition,4,98,0.8521739130434782,203,0.7463235294117647,15,0.9375,1,0
205,"Without this parameter , the last - only case performed poorly ( well below the baseline ) for SNLI and training failed completely for SRL .",Evaluation,Fine tuning biLM,named-entity-recognition,4,99,0.8608695652173913,204,0.75,16,1.0,1,0
206,A.3 Textual Entailment,Evaluation,Fine tuning biLM,named-entity-recognition,4,100,0.8695652173913043,205,0.7536764705882353,0,0.0,1,0
207,Our baseline SNLI model is the ESIM sequence model from .,Evaluation,Fine tuning biLM,named-entity-recognition,4,101,0.8782608695652174,206,0.7573529411764706,1,0.1,1,0
208,"Following the original implementation , we used 300 dimensions for all LSTM and feed forward layers and pretrained 300 dimensional GloVe embeddings that were fixed during training .",Evaluation,Fine tuning biLM,named-entity-recognition,4,102,0.8869565217391304,207,0.7610294117647058,2,0.2,1,0
209,"For regularization , we added 50 % variational dropout to the input of each LSTM layer and 50 % dropout at the input to the final two fully connected layers .",Evaluation,Fine tuning biLM,named-entity-recognition,4,103,0.8956521739130435,208,0.7647058823529411,3,0.3,1,0
210,All feed forward layers use ReLU activations .,Evaluation,,named-entity-recognition,4,104,0.9043478260869564,209,0.7683823529411765,4,0.4,1,0
211,"Parameters were optimized using Adam ( Kingma and with gradient norms clipped at 5.0 and initial learning rate 0.0004 , decreasing by half each time accuracy on the development set did not increase in subsequent epochs .",Evaluation,All feed forward layers use ReLU activations .,named-entity-recognition,4,105,0.9130434782608696,210,0.7720588235294118,5,0.5,1,0
212,The batch size was 32 .,Evaluation,,named-entity-recognition,4,106,0.9217391304347826,211,0.7757352941176471,6,0.6,1,0
213,"The best ELMo configuration added ELMo vectors to both the input and output of the lowest layer LSTM , using ( 1 ) with layer normalization and ? = 0.001 .",Evaluation,The batch size was 32 .,named-entity-recognition,4,107,0.9304347826086956,212,0.7794117647058824,7,0.7,1,0
214,"Due to the increased number of parameters in the ELMo model , we added 2 regularization with regularization coefficient 0.0001 to all recurrent and feed forward weight matrices and 50 % dropout after the attention layer .",Evaluation,The batch size was 32 .,named-entity-recognition,4,108,0.9391304347826088,213,0.7830882352941176,8,0.8,1,0
215,compares test set accuracy of our system to previously published systems .,Evaluation,The batch size was 32 .,named-entity-recognition,4,109,0.9478260869565216,214,0.7867647058823529,9,0.9,1,0
216,"Overall , adding ELMo to the ESIM model improved accuracy by 0.7 % establishing anew single model state - of - the - art of 88.7 % , and a five member ensemble pushes the overall accuracy to 89.3 % .",Evaluation,The batch size was 32 .,named-entity-recognition,4,110,0.9565217391304348,215,0.7904411764705882,10,1.0,1,0
217,A.4 Question Answering,Evaluation,The batch size was 32 .,named-entity-recognition,4,111,0.9652173913043478,216,0.7941176470588235,0,0.0,1,0
218,Our QA model is a simplified version of the model from .,Evaluation,The batch size was 32 .,named-entity-recognition,4,112,0.9739130434782608,217,0.7977941176470589,1,0.25,1,0
219,It embeds tokens by concatenating each token 's case - sensitive 300 dimensional Glo Ve word vector with a character - derived embedding produced using a convolutional neural network followed by max - pooling on learned character embeddings .,Evaluation,The batch size was 32 .,named-entity-recognition,4,113,0.982608695652174,218,0.8014705882352942,2,0.5,1,0
220,"The token embeddings are passed through a shared bi-directional GRU , and then the bi-directional attention mechanism from .",Evaluation,The batch size was 32 .,named-entity-recognition,4,114,0.991304347826087,219,0.8051470588235294,3,0.75,1,0
221,The augmented con-,Evaluation,,named-entity-recognition,4,115,1.0,220,0.8088235294117647,4,1.0,1,0
222,Model,,,named-entity-recognition,4,0,0.0,221,0.8125,0,0.0,1,0
223,Acc.,Model,,named-entity-recognition,4,1,0.02,222,0.8161764705882353,1,0.0833333333333333,1,0
224,"Feature based 78.2 DIIN 88.0 BCN+Char+CoVe 88.0 ESIM + TreeLSTM 88.6 ESIM+ELMo 88.7 0.17 DIIN ensemble 88.9 ESIM + ELMo ensemble 89.3 text vectors are then passed through a linear layer with ReLU activations , a residual self - attention layer that uses a GRU followed by the same attention mechanism applied context - to - context , and another linear layer with ReLU activations .",Model,Acc.,named-entity-recognition,4,2,0.04,223,0.8198529411764706,2,0.1666666666666666,1,0
225,"Finally , the results are fed through linear layers to predict the start and end token of the answer .",Model,Acc.,named-entity-recognition,4,3,0.06,224,0.8235294117647058,3,0.25,1,0
226,Variational dropout is used before the input to the GRUs and the linear layers at a rate of 0.2 .,Model,Acc.,named-entity-recognition,4,4,0.08,225,0.8272058823529411,4,0.3333333333333333,1,0
227,"A dimensionality of 90 is used for the GRUs , and 180 for the linear layers .",Model,Acc.,named-entity-recognition,4,5,0.1,226,0.8308823529411765,5,0.4166666666666667,1,0
228,We optimize the model using Adadelta with a batch size of 45 .,Model,Acc.,named-entity-recognition,4,6,0.12,227,0.8345588235294118,6,0.5,1,0
229,At test time we use an exponential moving average of the weights and limit the output span to be of at most size 17 .,Model,Acc.,named-entity-recognition,4,7,0.14,228,0.8382352941176471,7,0.5833333333333334,1,0
230,We do not update the word vectors during training .,Model,Acc.,named-entity-recognition,4,8,0.16,229,0.8419117647058824,8,0.6666666666666666,1,0
231,Performance was highest when adding ELMo without layer normalization to both the input and output of the contextual GRU layer and leaving the ELMo weights unregularized (? = 0 ) .,Model,Acc.,named-entity-recognition,4,9,0.18,230,0.8455882352941176,9,0.75,1,0
232,"compares test set results from the SQuAD leaderboard as of November 17 , 2017 when we submitted our system .",Model,Acc.,named-entity-recognition,4,10,0.2,231,0.8492647058823529,10,0.8333333333333334,1,0
233,"Overall , our submission had the highest single model and ensemble results , improving the previous single model result ( SAN ) by 1.4 % F 1 and our baseline by 4.2 % .",Model,Acc.,named-entity-recognition,4,11,0.22,232,0.8529411764705882,11,0.9166666666666666,1,0
234,"A 11 member ensemble pushes F 1 to 87.4 % , 1.0 % increase over the previous ensemble best .",Model,Acc.,named-entity-recognition,4,12,0.24,233,0.8566176470588235,12,1.0,1,0
235,A.5 Semantic Role Labeling,Model,Acc.,named-entity-recognition,4,13,0.26,234,0.8602941176470589,0,0.0,1,0
236,Our baseline SRL model is an exact reimplementation of .,Model,Acc.,named-entity-recognition,4,14,0.28,235,0.8639705882352942,1,0.0526315789473684,1,0
237,"Words are represented using a concatenation of 100 dimensional vector representations , initialized using GloVe and a binary , per-word predicate feature , represented using an 100 dimensional em-bedding .",Model,Acc.,named-entity-recognition,4,15,0.3,236,0.8676470588235294,2,0.1052631578947368,1,0
238,"This 200 dimensional token representation is then passed through an 8 layer "" interleaved "" biLSTM with a 300 dimensional hidden size , in which the directions of the LSTM layers alternate per layer .",Model,Acc.,named-entity-recognition,4,16,0.32,237,0.8713235294117647,3,0.1578947368421052,1,0
239,This deep LSTM uses Highway connections between layers and variational recurrent dropout .,Model,Acc.,named-entity-recognition,4,17,0.34,238,0.875,4,0.2105263157894736,1,0
240,This deep representation is then projected using a final dense layer followed by a softmax activation to form a distribution overall possible tags .,Model,Acc.,named-entity-recognition,4,18,0.36,239,0.8786764705882353,5,0.2631578947368421,1,0
241,Labels consist of semantic roles from PropBank augmented with a BIO labeling scheme to represent argument spans .,Model,Acc.,named-entity-recognition,4,19,0.38,240,0.8823529411764706,6,0.3157894736842105,1,0
242,"During training , we minimize the negative log likelihood of the tag sequence using Adadelta with a learning rate of 1.0 and ? = 0.95 .",Model,Acc.,named-entity-recognition,4,20,0.4,241,0.8860294117647058,7,0.3684210526315789,1,0
243,"At test time , we perform Viterbi decoding to enforce valid spans using BIO constraints .",Model,Acc.,named-entity-recognition,4,21,0.42,242,0.8897058823529411,8,0.4210526315789473,1,0
244,Variational dropout of 10 % is added to all LSTM hidden layers .,Model,Acc.,named-entity-recognition,4,22,0.44,243,0.8933823529411765,9,0.4736842105263157,1,0
245,Gradients are clipped if their value exceeds 1.0 .,Model,,named-entity-recognition,4,23,0.46,244,0.8970588235294118,10,0.5263157894736842,1,0
246,"Models are trained for 500 epochs or until validation F1 does not improve for 200 epochs , whichever is sooner .",Model,Gradients are clipped if their value exceeds 1.0 .,named-entity-recognition,4,24,0.48,245,0.9007352941176472,11,0.5789473684210527,1,0
247,The pretrained Glo Ve vectors are fine - tuned during training .,Model,Gradients are clipped if their value exceeds 1.0 .,named-entity-recognition,4,25,0.5,246,0.9044117647058824,12,0.631578947368421,1,0
248,The final dense layer and all cells of all LSTMs are initialized to be orthogonal .,Model,Gradients are clipped if their value exceeds 1.0 .,named-entity-recognition,4,26,0.52,247,0.9080882352941176,13,0.6842105263157895,1,0
249,"The forget gate bias is initialized to 1 for all LSTMs , with all other gates initialized to 0 , as per . perparameters exactly following the original implementation .",Model,Gradients are clipped if their value exceeds 1.0 .,named-entity-recognition,4,27,0.54,248,0.9117647058823528,14,0.7368421052631579,1,0
250,The best configuration added ELMo to the input of the lowest layer biLSTM and weighted the biLM layers using ( 1 ) without any regularization ( ? = 0 ) or layer normalization .,Model,Gradients are clipped if their value exceeds 1.0 .,named-entity-recognition,4,28,0.56,249,0.9154411764705882,15,0.7894736842105263,1,0
251,50 % dropout was added to the ELMo representations .,Model,Gradients are clipped if their value exceeds 1.0 .,named-entity-recognition,4,29,0.58,250,0.9191176470588236,16,0.8421052631578947,1,0
252,compares our results with previously published results .,Model,Gradients are clipped if their value exceeds 1.0 .,named-entity-recognition,4,30,0.6,251,0.9227941176470588,17,0.8947368421052632,1,0
253,"Overall , we improve the single model state - of - the - art by 3.2 % average F 1 , and our single model result improves the previous ensemble best by 1.6 % F 1 .",Model,Gradients are clipped if their value exceeds 1.0 .,named-entity-recognition,4,31,0.62,252,0.9264705882352942,18,0.9473684210526316,1,0
254,Adding ELMo to the output from the biLSTM in addition to the biLSTM input reduced F 1 by approximately 0.7 % ( not shown ) .,Model,Gradients are clipped if their value exceeds 1.0 .,named-entity-recognition,4,32,0.64,253,0.9301470588235294,19,1.0,1,0
255,A.7 Named Entity Recognition,Model,Gradients are clipped if their value exceeds 1.0 .,named-entity-recognition,4,33,0.66,254,0.9338235294117648,0,0.0,1,0
256,Our baseline NER model concatenates 50 dimensional pre-trained Senna vectors with a CNN character based representation .,Model,Gradients are clipped if their value exceeds 1.0 .,named-entity-recognition,4,34,0.68,255,0.9375,1,0.0909090909090909,1,0
257,"The character representation uses 16 dimensional character embeddings and 128 convolutional filters of width three characters , a ReLU activation and by max pooling .",Model,Gradients are clipped if their value exceeds 1.0 .,named-entity-recognition,4,35,0.7,256,0.9411764705882352,2,0.1818181818181818,1,0
258,"The token representation is passed through two biLSTM layers , the first with 200 hidden units and the second with 100 hidden units before a final dense layer and softmax layer .",Model,Gradients are clipped if their value exceeds 1.0 .,named-entity-recognition,4,36,0.72,257,0.9448529411764706,3,0.2727272727272727,1,0
259,"During training , we use a CRF loss and attest time perform decoding using the Viterbi algorithm while ensuring that the output tag sequence is valid .",Model,Gradients are clipped if their value exceeds 1.0 .,named-entity-recognition,4,37,0.74,258,0.9485294117647058,4,0.3636363636363636,1,0
260,Variational dropout is added to the input of both biLSTM layers .,Model,Gradients are clipped if their value exceeds 1.0 .,named-entity-recognition,4,38,0.76,259,0.9522058823529412,5,0.4545454545454545,1,0
261,During training the gradients are rescaled if their 2 norm exceeds 5.0 and parameters updated using Adam with constant learning rate of 0.001 .,Model,Gradients are clipped if their value exceeds 1.0 .,named-entity-recognition,4,39,0.78,260,0.9558823529411764,6,0.5454545454545454,1,0
262,The pre-trained Senna embeddings are fine tuned during training .,Model,Gradients are clipped if their value exceeds 1.0 .,named-entity-recognition,4,40,0.8,261,0.9595588235294118,7,0.6363636363636364,1,0
263,We employ early stopping on the development set and report the averaged test set score across five runs with different random seeds .,Model,Gradients are clipped if their value exceeds 1.0 .,named-entity-recognition,4,41,0.82,262,0.9632352941176472,8,0.7272727272727273,1,0
264,ELMo was added to the input of the lowest layer task biLSTM .,Model,Gradients are clipped if their value exceeds 1.0 .,named-entity-recognition,4,42,0.84,263,0.9669117647058824,9,0.8181818181818182,1,0
265,"As the CoNLL 2003 NER data set is relatively small , we found the best performance by constraining the trainable layer weights to be effectively constant by setting ? = 0.1 with ( 1 ) .",Model,Gradients are clipped if their value exceeds 1.0 .,named-entity-recognition,4,43,0.86,264,0.9705882352941176,10,0.9090909090909092,1,0
266,from all layers of the biLM provides a modest improvement .,Model,Gradients are clipped if their value exceeds 1.0 .,named-entity-recognition,4,44,0.88,265,0.9742647058823528,11,1.0,1,0
267,A.8 Sentiment classification,Model,,named-entity-recognition,4,45,0.9,266,0.9779411764705882,0,0.0,1,0
268,"We use almost the same biattention classification network architecture described in , with the exception of replacing the final maxout network with a simpler feedforward network composed of two ReLu layers with dropout .",Model,A.8 Sentiment classification,named-entity-recognition,4,46,0.92,267,0.9816176470588236,1,0.2,1,0
269,"A BCN model with a batch - normalized maxout network reached significantly lower validation accuracies in our experiments , although there maybe discrepancies between our implementation and that of .",Model,A.8 Sentiment classification,named-entity-recognition,4,47,0.94,268,0.9852941176470588,2,0.4,1,0
270,"To match the CoVe training setup , we only train on phrases that contain four or more tokens .",Model,A.8 Sentiment classification,named-entity-recognition,4,48,0.96,269,0.9889705882352942,3,0.6,1,0
271,We use 300 -d hidden states for the biLSTM and optimize the model parameters with Adam ( Kingma and using a learning rate of 0.0001 .,Model,A.8 Sentiment classification,named-entity-recognition,4,49,0.98,270,0.9926470588235294,4,0.8,1,0
272,"The trainable biLM layer weights are regularized by ? = 0.001 , and we add ELMo to both the input and output of the biLSTM ; the output ELMo vectors are computed with a second biLSTM and concatenated to the input .",Model,A.8 Sentiment classification,named-entity-recognition,4,50,1.0,271,0.9963235294117648,5,1.0,1,0
1,title,,,named-entity-recognition,5,0,0.0,0,0.0,0,0.0,1,0
2,Sentence - State LSTM for Text Representation,title,,named-entity-recognition,5,1,0.0,1,0.0047846889952153,1,0.0,1,1
3,abstract,,,named-entity-recognition,5,0,0.0,2,0.0095693779904306,0,0.0,1,0
4,Bi-directional,abstract,,named-entity-recognition,5,1,0.1666666666666666,3,0.0143540669856459,1,0.1666666666666666,1,0
5,LSTMs area powerful tool for text representation .,abstract,Bi-directional,named-entity-recognition,5,2,0.3333333333333333,4,0.0191387559808612,2,0.3333333333333333,1,0
6,"On the other hand , they have been shown to suffer various limitations due to their sequential nature .",abstract,Bi-directional,named-entity-recognition,5,3,0.5,5,0.0239234449760765,3,0.5,1,0
7,"We investigate an alternative LSTM structure for encoding text , which consists of a parallel state for each word .",abstract,Bi-directional,named-entity-recognition,5,4,0.6666666666666666,6,0.0287081339712918,4,0.6666666666666666,1,1
8,"Recurrent steps are used to perform local and global information exchange between words simultaneously , rather than incremental reading of a sequence of words .",abstract,Bi-directional,named-entity-recognition,5,5,0.8333333333333334,7,0.0334928229665071,5,0.8333333333333334,1,0
9,"Results on various classification and sequence labelling benchmarks show that the proposed model has strong representation power , giving highly competitive performances compared to stacked BiLSTM models with similar parameter numbers .",abstract,Bi-directional,named-entity-recognition,5,6,1.0,8,0.0382775119617224,6,1.0,1,0
10,Introduction,,,named-entity-recognition,5,0,0.0,9,0.0430622009569378,0,0.0,1,0
11,Neural models have become the dominant approach in the NLP literature .,Introduction,Introduction,named-entity-recognition,5,1,0.0476190476190476,10,0.0478468899521531,1,0.0476190476190476,1,0
12,"Compared to handcrafted indicator features , neural sentence representations are less sparse , and more flexible in encoding intricate syntactic and semantic information .",Introduction,Introduction,named-entity-recognition,5,2,0.0952380952380952,11,0.0526315789473684,2,0.0952380952380952,1,0
13,"Among various neural networks for encoding sentences , bi-directional LSTMs ( BiLSTM ) have been a dominant method , giving state - of - the - art results in language modelling , machine translation , syntactic parsing and question answering .",Introduction,Introduction,named-entity-recognition,5,3,0.1428571428571428,12,0.0574162679425837,3,0.1428571428571428,1,0
14,"Despite their success , BiLSTMs have been shown to suffer several limitations .",Introduction,Introduction,named-entity-recognition,5,4,0.1904761904761904,13,0.062200956937799,4,0.1904761904761904,1,0
15,"For example , their inherently sequential nature endows computation non-parallel within the same sentence , which can lead to a computational bottleneck , hindering their use in the in - dustry .",Introduction,Introduction,named-entity-recognition,5,5,0.238095238095238,14,0.0669856459330143,5,0.238095238095238,1,0
16,"In addition , local ngrams , which have been shown a highly useful source of contextual information for NLP , are not explicitly modelled .",Introduction,Introduction,named-entity-recognition,5,6,0.2857142857142857,15,0.0717703349282296,6,0.2857142857142857,1,0
17,"Finally , sequential information flow leads to relatively weaker power in capturing longrange dependencies , which results in lower performance in encoding longer sentences .",Introduction,Introduction,named-entity-recognition,5,7,0.3333333333333333,16,0.0765550239234449,7,0.3333333333333333,1,0
18,We investigate an alternative recurrent neural network structure for addressing these issues .,Introduction,Introduction,named-entity-recognition,5,8,0.3809523809523809,17,0.0813397129186602,8,0.3809523809523809,1,1
19,"As shown in , the main idea is to model the hidden states of all words simultaneously at each recurrent step , rather than one word at a time .",Introduction,Introduction,named-entity-recognition,5,9,0.4285714285714285,18,0.0861244019138756,9,0.4285714285714285,1,1
20,"In particular , we view the whole sentence as a single state , which consists of sub-states for individual words and an overall sentence - level state .",Introduction,Introduction,named-entity-recognition,5,10,0.4761904761904761,19,0.0909090909090909,10,0.4761904761904761,1,1
21,"To capture local and non-local contexts , states are updated recurrently by exchanging information between each other .",Introduction,Introduction,named-entity-recognition,5,11,0.5238095238095238,20,0.0956937799043062,11,0.5238095238095238,1,1
22,"Consequently , we refer to our model as sentence - state LSTM , or S - LSTM in short .",Introduction,Introduction,named-entity-recognition,5,12,0.5714285714285714,21,0.1004784688995215,12,0.5714285714285714,1,0
23,"Empirically , S - LSTM can give effective sentence encoding after 3 - 6 recurrent steps .",Introduction,Introduction,named-entity-recognition,5,13,0.6190476190476191,22,0.1052631578947368,13,0.6190476190476191,1,0
24,"In contrast , the number of recurrent steps necessary for BiLSTM scales with the size of the sentence .",Introduction,Introduction,named-entity-recognition,5,14,0.6666666666666666,23,0.1100478468899521,14,0.6666666666666666,1,0
25,"At each recurrent step , information exchange is conducted between consecutive words in the sentence , and between the sentence - level state and each word .",Introduction,Introduction,named-entity-recognition,5,15,0.7142857142857143,24,0.1148325358851674,15,0.7142857142857143,1,1
26,"In particular , each word receives information from its predecessor and successor simultaneously .",Introduction,Introduction,named-entity-recognition,5,16,0.7619047619047619,25,0.1196172248803827,16,0.7619047619047619,1,1
27,"From an initial state without information exchange , each word - level state can obtain 3 - gram , 5 - gram and 7 - gram information after 1 , 2 and 3 recurrent steps , respectively .",Introduction,Introduction,named-entity-recognition,5,17,0.8095238095238095,26,0.124401913875598,17,0.8095238095238095,1,0
28,"Being connected with every word , the sentence - level state vector serves to exchange non-local information with each word .",Introduction,Introduction,named-entity-recognition,5,18,0.8571428571428571,27,0.1291866028708134,18,0.8571428571428571,1,0
29,"In addition , it can also be used as a global sentence - level representation for classification tasks .",Introduction,Introduction,named-entity-recognition,5,19,0.9047619047619048,28,0.1339712918660287,19,0.9047619047619048,1,0
30,"Results on both classification and sequence labelling show that S - LSTM gives better accuracies compared to BiLSTM using the same number of parameters , while being faster .",Introduction,Introduction,named-entity-recognition,5,20,0.9523809523809524,29,0.138755980861244,20,0.9523809523809524,1,0
31,"We release our code and models at https://github.com/ leuchine /S - LSTM , which include all baselines and the final model .",Introduction,Introduction,named-entity-recognition,5,21,1.0,30,0.1435406698564593,21,1.0,1,1
32,Related Work,,,named-entity-recognition,5,0,0.0,31,0.1483253588516746,0,0.0,1,0
33,LSTM showed its early potentials in NLP when a neural machine translation system that leverages LSTM source encoding gave highly competitive results compared to the best SMT models .,Related Work,Related Work,named-entity-recognition,5,1,0.0476190476190476,32,0.1531100478468899,1,0.0476190476190476,0,0
34,"LSTM encoders have since been explored for other tasks , including syntactic parsing , text classification and machine reading .",Related Work,Related Work,named-entity-recognition,5,2,0.0952380952380952,33,0.1578947368421052,2,0.0952380952380952,0,0
35,Bidirectional extensions have become a standard configuration for achieving state - of - the - art accuracies among various tasks .,Related Work,Related Work,named-entity-recognition,5,3,0.1428571428571428,34,0.1626794258373205,3,0.1428571428571428,0,0
36,"S- LSTMs are similar to BiLSTMs in their recurrent bi-directional message flow between words , but different in the design of state transition .",Related Work,Related Work,named-entity-recognition,5,4,0.1904761904761904,35,0.1674641148325359,4,0.1904761904761904,0,0
37,"CNNs ) also allow better parallelisation compared to LSTMs for sentence encoding , thanks to parallelism among convolution filters .",Related Work,Related Work,named-entity-recognition,5,5,0.238095238095238,36,0.1722488038277512,5,0.238095238095238,0,0
38,"On the other hand , convolution features embody only fix - sized local ngram information , whereas sentence - level feature aggregation via pooling can lead to loss of information .",Related Work,Related Work,named-entity-recognition,5,6,0.2857142857142857,37,0.1770334928229665,6,0.2857142857142857,0,0
39,"In contrast , S - LSTM uses a global sentence - level node to assemble and back - distribute local information in the recurrent state transition process , suffering less information loss compared to pooling .",Related Work,Related Work,named-entity-recognition,5,7,0.3333333333333333,38,0.1818181818181818,7,0.3333333333333333,0,0
40,"Attention has recently been explored as a standalone method for sentence encoding , giving competitive results compared to Bi - LSTM encoders for neural machine translation .",Related Work,Related Work,named-entity-recognition,5,8,0.3809523809523809,39,0.1866028708133971,8,0.3809523809523809,0,0
41,"The attention mechanism allows parallelisation , and can play a similar role to the sentence - level state in S - LSTMs , which uses neural gates to integrate word - level information compared to hierarchical attention .",Related Work,Related Work,named-entity-recognition,5,9,0.4285714285714285,40,0.1913875598086124,9,0.4285714285714285,0,0
42,S - LSTM further allows local communication between neighbouring words .,Related Work,Related Work,named-entity-recognition,5,10,0.4761904761904761,41,0.1961722488038277,10,0.4761904761904761,0,0
43,Hierarchical stacking of CNN layers allows better interaction between non-local components in a sentence via incremental levels of abstraction .,Related Work,Related Work,named-entity-recognition,5,11,0.5238095238095238,42,0.200956937799043,11,0.5238095238095238,0,0
44,"S - LSTM is similar to hierarchical attention and stacked CNN in this respect , incrementally refining sentence representations .",Related Work,Related Work,named-entity-recognition,5,12,0.5714285714285714,43,0.2057416267942583,12,0.5714285714285714,0,0
45,"However , S - LSTM models hierarchical encoding of sentence structure as a recurrent state transition process .",Related Work,Related Work,named-entity-recognition,5,13,0.6190476190476191,44,0.2105263157894736,13,0.6190476190476191,0,0
46,"In nature , our work belongs to the family of LSTM sentence representations .",Related Work,Related Work,named-entity-recognition,5,14,0.6666666666666666,45,0.215311004784689,14,0.6666666666666666,0,0
47,S - LSTM is inspired by message passing over graphs ) .,Related Work,Related Work,named-entity-recognition,5,15,0.7142857142857143,46,0.2200956937799043,15,0.7142857142857143,0,0
48,Graph - structure neural models have been used for computer program verification and image object detection .,Related Work,Related Work,named-entity-recognition,5,16,0.7619047619047619,47,0.2248803827751196,16,0.7619047619047619,0,0
49,The closest previous work in NLP includes the use of convolutional neural networks and DAG LSTMs for modelling syntactic structures .,Related Work,Related Work,named-entity-recognition,5,17,0.8095238095238095,48,0.2296650717703349,17,0.8095238095238095,0,0
50,"Compared to our work , their motivations and network structures are highly different .",Related Work,Related Work,named-entity-recognition,5,18,0.8571428571428571,49,0.2344497607655502,18,0.8571428571428571,0,0
51,"In particular , the DAG LSTM of is a natural extension of tree LSTM , and is sequential rather than parallel in nature .",Related Work,Related Work,named-entity-recognition,5,19,0.9047619047619048,50,0.2392344497607655,19,0.9047619047619048,0,0
52,"To our knowledge , we are the first to investigate a graph RNN for encoding sentences , proposing parallel graph states for integrating word - level and sentence - level information .",Related Work,Related Work,named-entity-recognition,5,20,0.9523809523809524,51,0.2440191387559808,20,0.9523809523809524,0,0
53,"In this perspective , our contribution is similar to that of and in introducing a neural representation to the NLP literature .",Related Work,Related Work,named-entity-recognition,5,21,1.0,52,0.2488038277511961,21,1.0,0,0
54,Model,,,named-entity-recognition,5,0,0.0,53,0.2535885167464115,0,0.0,1,0
55,Time 4 ) generally results in faster plateauing .,Model,Model,named-entity-recognition,5,1,0.0588235294117647,54,0.2583732057416268,1,0.0588235294117647,1,0
56,"This can be be explained by the intuition that information exchange between distant nodes can be achieved using more recurrent steps under a smaller window size , as can be achieved using fewer steps under a larger window size .",Model,Model,named-entity-recognition,5,2,0.1176470588235294,55,0.2631578947368421,2,0.1176470588235294,1,0
57,"Considering efficiency , we choose a window size of 1 for the remaining experiments , setting the number of recurrent steps to 9 according to .",Model,Model,named-entity-recognition,5,3,0.1764705882352941,56,0.2679425837320574,3,0.1764705882352941,1,0
58,"S- LSTM vs BiLSTM : As shown in , BiLSTM gives significantly better accuracies compared to uni-directional LSTM 2 , with the training time per epoch growing from 67 seconds to 106 seconds .",Model,Model,named-entity-recognition,5,4,0.2352941176470588,57,0.2727272727272727,4,0.2352941176470588,1,0
59,"Stacking 2 layers of BiLSTM gives further improvements to development results , with a larger time of 207 seconds .",Model,Model,named-entity-recognition,5,5,0.2941176470588235,58,0.277511961722488,5,0.2941176470588235,1,0
60,3 layers of stacked BiLSTM does not further improve the results .,Model,Model,named-entity-recognition,5,6,0.3529411764705882,59,0.2822966507177033,6,0.3529411764705882,1,0
61,"In contrast , S - LSTM gives a development result of 82 . 64 % , which is significantly better compared to 2 - layer stacked BiLSTM , with a smaller number of model parameters and a shorter time of 65 seconds .",Model,Model,named-entity-recognition,5,7,0.4117647058823529,60,0.2870813397129186,7,0.4117647058823529,1,0
62,"We additionally make comparisons with stacked CNNs and hierarchical attention , shown in ( the CNN and Transformer rows ) , where N indicates the number of attention layers .",Model,Model,named-entity-recognition,5,8,0.4705882352941176,61,0.291866028708134,8,0.4705882352941176,1,0
63,"CNN is the most efficient among all models compared , with the smallest model size .",Model,Model,named-entity-recognition,5,9,0.5294117647058824,62,0.2966507177033493,9,0.5294117647058824,1,0
64,"On the other hand , a 3 - layer stacked CNN gives an accuracy of 81 . 46 % , which is also the lowest compared with BiLSTM , hierarchical attention and S - LSTM .",Model,Model,named-entity-recognition,5,10,0.5882352941176471,63,0.3014354066985646,10,0.5882352941176471,1,0
65,The best performance of hierarchical attention is between single - layer and two - layer BiLSTMs in terms of both accuracy and efficiency .,Model,Model,named-entity-recognition,5,11,0.6470588235294118,64,0.3062200956937799,11,0.6470588235294118,1,0
66,S - LSTM gives significantly better accuracies compared with both CNN and hierarchical attention .,Model,Model,named-entity-recognition,5,12,0.7058823529411765,65,0.3110047846889952,12,0.7058823529411765,1,0
67,Influence of external attention mechanism .,Model,Model,named-entity-recognition,5,13,0.7647058823529411,66,0.3157894736842105,13,0.7647058823529411,1,0
68,additionally shows the results of BiLSTM and S - LSTM when external attention is used as described in Section 3.3 .,Model,Model,named-entity-recognition,5,14,0.8235294117647058,67,0.3205741626794258,14,0.8235294117647058,1,0
69,"Attention leads to improved accuracies for both BiLSTM and S - LSTM in classification , with S - LSTM still outperforming BiLSTM significantly .",Model,Model,named-entity-recognition,5,15,0.8823529411764706,68,0.3253588516746411,15,0.8823529411764706,1,0
70,"The result suggests that external techniques such as attention can play orthogonal roles compared with internal recurrent structures , therefore benefiting both BiLSTMs and S - LSTMs .",Model,Model,named-entity-recognition,5,16,0.9411764705882352,69,0.3301435406698564,16,0.9411764705882352,1,0
71,Similar observations are found using external CRF layers for sequence labelling .,Model,Model,named-entity-recognition,5,17,1.0,70,0.3349282296650718,17,1.0,1,0
72,Baseline BiLSTM,,,named-entity-recognition,5,0,0.0,71,0.3397129186602871,0,0.0,1,0
73,"The baseline BiLSTM model consists of two LSTM components , which process the input in the forward left - to - right and the backward rightto - left directions , respectively .",Baseline BiLSTM,Baseline BiLSTM,named-entity-recognition,5,1,0.0136986301369863,72,0.3444976076555024,1,0.0172413793103448,1,0
74,"In each direction , the reading of input words is modelled as a recurrent process with a single hidden state .",Baseline BiLSTM,Baseline BiLSTM,named-entity-recognition,5,2,0.0273972602739726,73,0.3492822966507177,2,0.0344827586206896,1,0
75,"Given an initial value , the state changes its value recurrently , each time consuming an incoming word .",Baseline BiLSTM,Baseline BiLSTM,named-entity-recognition,5,3,0.0410958904109589,74,0.354066985645933,3,0.0517241379310344,1,0
76,Take the forward LSTM component for example .,Baseline BiLSTM,Baseline BiLSTM,named-entity-recognition,5,4,0.0547945205479452,75,0.3588516746411483,4,0.0689655172413793,1,0
77,"Denoting the initial state as ? ? h 0 , which is a model parameter , the recurrent state transition step for calculating ? ? h 1 , . . . , ? ? h n+1 is defined as follows :",Baseline BiLSTM,Baseline BiLSTM,named-entity-recognition,5,5,0.0684931506849315,76,0.3636363636363636,5,0.0862068965517241,1,0
78,"where x t denotes the word representation of wt ; it , o t , ft and u t represent the values of an input gate , an output gate , a forget gate and an actual input at time step t , respectively , which controls the information flow fora recurrent cell ? ?",Baseline BiLSTM,Baseline BiLSTM,named-entity-recognition,5,6,0.0821917808219178,77,0.3684210526315789,6,0.1034482758620689,1,0
79,ct and the state vector,Baseline BiLSTM,Baseline BiLSTM,named-entity-recognition,5,7,0.0958904109589041,78,0.3732057416267942,7,0.1206896551724138,1,0
80,The backward LSTM component follows the same recurrent state transition process as described in Eq 1 .,Baseline BiLSTM,Baseline BiLSTM,named-entity-recognition,5,8,0.1095890410958904,79,0.3779904306220095,8,0.1379310344827586,1,0
81,"Starting from an initial state h n + 1 , which is a model parameter , it reads the input x n ,",Baseline BiLSTM,Baseline BiLSTM,named-entity-recognition,5,9,0.1232876712328767,80,0.3827751196172249,9,0.1551724137931034,1,0
82,The BiLSTM model uses the concatenated value of ? ?,Baseline BiLSTM,Baseline BiLSTM,named-entity-recognition,5,10,0.136986301369863,81,0.3875598086124402,10,0.1724137931034483,1,0
83,ht and ? ?,Baseline BiLSTM,Baseline BiLSTM,named-entity-recognition,5,11,0.1506849315068493,82,0.3923444976076555,11,0.1896551724137931,1,0
84,ht as the hidden vector for wt :,Baseline BiLSTM,Baseline BiLSTM,named-entity-recognition,5,12,0.1643835616438356,83,0.3971291866028708,12,0.2068965517241379,1,0
85,A single hidden vector representation g of the whole input sentence can be obtained using the final state values of the two LSTM components :,Baseline BiLSTM,Baseline BiLSTM,named-entity-recognition,5,13,0.1780821917808219,84,0.4019138755980861,13,0.2241379310344827,1,0
86,Stacked BiLSTM,Baseline BiLSTM,,named-entity-recognition,5,14,0.1917808219178082,85,0.4066985645933014,14,0.2413793103448276,1,0
87,"Multiple layers of BiLTMs can be stacked for increased representation power , where the hidden vectors of a lower layer are used as inputs for an upper layer .",Baseline BiLSTM,Stacked BiLSTM,named-entity-recognition,5,15,0.2054794520547945,86,0.4114832535885167,15,0.2586206896551724,1,0
88,Different model parameters are used in each stacked BiLSTM layer .,Baseline BiLSTM,Stacked BiLSTM,named-entity-recognition,5,16,0.2191780821917808,87,0.416267942583732,16,0.2758620689655172,1,0
89,Sentence - State LSTM,Baseline BiLSTM,Stacked BiLSTM,named-entity-recognition,5,17,0.2328767123287671,88,0.4210526315789473,17,0.293103448275862,1,0
90,"Formally , an S - LSTM state at time step t can be denoted by :",Baseline BiLSTM,Stacked BiLSTM,named-entity-recognition,5,18,0.2465753424657534,89,0.4258373205741627,18,0.3103448275862069,1,0
91,which consists of a sub state ht i for each word w i and a sentence - level sub state gt .,Baseline BiLSTM,Stacked BiLSTM,named-entity-recognition,5,19,0.2602739726027397,90,0.430622009569378,19,0.3275862068965517,1,0
92,"S - LSTM uses a recurrent state transition process to model information exchange between sub states , which enriches state representations incrementally .",Baseline BiLSTM,Stacked BiLSTM,named-entity-recognition,5,20,0.273972602739726,91,0.4354066985645933,20,0.3448275862068966,1,0
93,"For the initial state H 0 , we set h 0",Baseline BiLSTM,Stacked BiLSTM,named-entity-recognition,5,21,0.2876712328767123,92,0.4401913875598086,21,0.3620689655172414,1,0
94,to ht i and from g t?1 tog t .,Baseline BiLSTM,Stacked BiLSTM,named-entity-recognition,5,22,0.3013698630136986,93,0.4449760765550239,22,0.3793103448275862,1,0
95,"We take an LSTM structure similar to the baseline BiLSTM for modelling state transition , using a recurrent cell ct i for each w i and a cell ct g for g.",Baseline BiLSTM,Stacked BiLSTM,named-entity-recognition,5,23,0.3150684931506849,94,0.4497607655502392,23,0.396551724137931,1,0
96,"As shown in , the value of each ht i is computed based on the values of",Baseline BiLSTM,Stacked BiLSTM,named-entity-recognition,5,24,0.3287671232876712,95,0.4545454545454545,24,0.4137931034482758,1,0
97,"i + 1 and g t?1 , together with their corresponding cell values :",Baseline BiLSTM,Stacked BiLSTM,named-entity-recognition,5,25,0.3424657534246575,96,0.4593301435406698,25,0.4310344827586206,1,0
98,where ?,Baseline BiLSTM,Stacked BiLSTM,named-entity-recognition,5,26,0.3561643835616438,97,0.4641148325358851,26,0.4482758620689655,1,0
99,"ti is the concatenation of hidden vectors of a context window , and l ti , rt i , ft i , st i and it i are gates that control information flow from ?",Baseline BiLSTM,Stacked BiLSTM,named-entity-recognition,5,27,0.3698630136986301,98,0.4688995215311005,27,0.4655172413793103,1,0
100,ti and xi to ct i .,Baseline BiLSTM,Stacked BiLSTM,named-entity-recognition,5,28,0.3835616438356164,99,0.4736842105263157,28,0.4827586206896552,1,0
101,"In particular , it i controls information from the input xi ; l ti , rt i , ft i and st i control information from the left context cell c t ?1 i ? 1 , the right context cell c t ?1 i + 1 , c t?1 i and the sentence context cell c t ? 1 g , respectively .",Baseline BiLSTM,Stacked BiLSTM,named-entity-recognition,5,29,0.3972602739726027,100,0.4784688995215311,29,0.5,1,0
102,"The values of it i , l ti , rt i , ft i and st i are normalised such that they sum to 1 .",Baseline BiLSTM,Stacked BiLSTM,named-entity-recognition,5,30,0.410958904109589,101,0.4832535885167464,30,0.5172413793103449,1,0
103,o ti is an output gate from the cell state ct i to the hidden state,Baseline BiLSTM,Stacked BiLSTM,named-entity-recognition,5,31,0.4246575342465753,102,0.4880382775119617,31,0.5344827586206896,1,0
104,The value of gt is computed based on the values,Baseline BiLSTM,Stacked BiLSTM,named-entity-recognition,5,32,0.4383561643835616,103,0.492822966507177,32,0.5517241379310345,1,0
105,". . , ft n+1 and ft g are gates controlling information from c t ? 1 0 , . . . , c t?1 n+1 and c t ? 1 g , respectively , which are normalised .",Baseline BiLSTM,Stacked BiLSTM,named-entity-recognition,5,33,0.4520547945205479,104,0.4976076555023923,33,0.5689655172413793,1,0
106,o t is an output gate from the recurrent cell ct g tog t .,Baseline BiLSTM,Stacked BiLSTM,named-entity-recognition,5,34,0.4657534246575342,105,0.5023923444976076,34,0.5862068965517241,1,0
107,"W x , U x and bx ( x ? {g , f , o} ) are model parameters .",Baseline BiLSTM,Stacked BiLSTM,named-entity-recognition,5,35,0.4794520547945205,106,0.507177033492823,35,0.603448275862069,1,0
108,Contrast with BiLSTM,Baseline BiLSTM,,named-entity-recognition,5,36,0.4931506849315068,107,0.5119617224880383,36,0.6206896551724138,1,0
109,The difference between S - LSTM and BiLSTM can be understood with respect to their recurrent states .,Baseline BiLSTM,Contrast with BiLSTM,named-entity-recognition,5,37,0.5068493150684932,108,0.5167464114832536,37,0.6379310344827587,1,0
110,"While BiL - STM uses only one state in each direction to represent the subsequence from the beginning to a certain word , S - LSTM uses a structural state to represent the full sentence , which consists of a sentence - level sub state and n + 2 word - level sub states , simultaneously .",Baseline BiLSTM,Contrast with BiLSTM,named-entity-recognition,5,38,0.5205479452054794,109,0.5215311004784688,38,0.6551724137931034,1,0
111,"Different from BiLSTMs , for which ht at different time steps are used to represent w 0 , . . . , w n + 1 , respectively , the word - level states ht i and sentence - level state gt of S - LSTMs directly correspond to the goal outputs hi and g , as introduced in the beginning of this section .",Baseline BiLSTM,Contrast with BiLSTM,named-entity-recognition,5,39,0.5342465753424658,110,0.5263157894736842,39,0.6724137931034483,1,0
112,"As t increases from 0 , ht i and gt are enriched with increasingly deeper context information .",Baseline BiLSTM,Contrast with BiLSTM,named-entity-recognition,5,40,0.547945205479452,111,0.5311004784688995,40,0.6896551724137931,1,0
113,"From the perspective of information flow , BiL - STM passes information from one end of the sentence to the other .",Baseline BiLSTM,Contrast with BiLSTM,named-entity-recognition,5,41,0.5616438356164384,112,0.5358851674641149,41,0.7068965517241379,1,0
114,"As a result , the number of time steps scales with the size of the input .",Baseline BiLSTM,Contrast with BiLSTM,named-entity-recognition,5,42,0.5753424657534246,113,0.5406698564593302,42,0.7241379310344828,1,0
115,"In contrast , S - LSTM allows bi-directional information flow at each word simultaneously , and additionally between the sentence - level state and every wordlevel state .",Baseline BiLSTM,Contrast with BiLSTM,named-entity-recognition,5,43,0.589041095890411,114,0.5454545454545454,43,0.7413793103448276,1,0
116,"At each step , each hi captures an increasing larger ngram context , while additionally communicating globally to all other h j via g.",Baseline BiLSTM,Contrast with BiLSTM,named-entity-recognition,5,44,0.6027397260273972,115,0.5502392344497608,44,0.7586206896551724,1,0
117,"The optimal number of recurrent steps is decided by the end - task performance , and does not necessarily scale with the sentence size .",Baseline BiLSTM,Contrast with BiLSTM,named-entity-recognition,5,45,0.6164383561643836,116,0.5550239234449761,45,0.7758620689655172,1,0
118,"As a result , S - LSTM can potentially be both more efficient and more accurate compared with BiLSTMs .",Baseline BiLSTM,Contrast with BiLSTM,named-entity-recognition,5,46,0.6301369863013698,117,0.5598086124401914,46,0.7931034482758621,1,0
119,Increasing window size .,Baseline BiLSTM,Contrast with BiLSTM,named-entity-recognition,5,47,0.6438356164383562,118,0.5645933014354066,47,0.8103448275862069,1,0
120,"By default S - LSTM exchanges information only between neighbouring words , which can be seen as adopting a 1 word window on each side .",Baseline BiLSTM,Contrast with BiLSTM,named-entity-recognition,5,48,0.6575342465753424,119,0.569377990430622,48,0.8275862068965517,1,0
121,"The window size can be extended to 2 , 3 or more words in order to allow more communication in a state transition , expediting information exchange .",Baseline BiLSTM,Contrast with BiLSTM,named-entity-recognition,5,49,0.6712328767123288,120,0.5741626794258373,49,0.8448275862068966,1,0
122,"To this end , we modify Eq 2 , integrating additional context words to ? ti , with extended gates and cells .",Baseline BiLSTM,Contrast with BiLSTM,named-entity-recognition,5,50,0.684931506849315,121,0.5789473684210527,50,0.8620689655172413,1,0
123,"For example , with a window size of 2 ,",Baseline BiLSTM,Contrast with BiLSTM,named-entity-recognition,5,51,0.6986301369863014,122,0.583732057416268,51,0.8793103448275862,1,0
124,We study the effectiveness of window size in our experiments .,Baseline BiLSTM,Contrast with BiLSTM,named-entity-recognition,5,52,0.7123287671232876,123,0.5885167464114832,52,0.896551724137931,1,0
125,Additional sentence - level nodes .,Baseline BiLSTM,Contrast with BiLSTM,named-entity-recognition,5,53,0.726027397260274,124,0.5933014354066986,53,0.913793103448276,1,0
126,By default S - LSTM uses one sentence - level node .,Baseline BiLSTM,Contrast with BiLSTM,named-entity-recognition,5,54,0.7397260273972602,125,0.5980861244019139,54,0.9310344827586208,1,0
127,"One way of enriching the parameter space is to add more sentence - level nodes , each communicating with word - level nodes in the same way as described by Eq 3 .",Baseline BiLSTM,Contrast with BiLSTM,named-entity-recognition,5,55,0.7534246575342466,126,0.6028708133971292,55,0.9482758620689656,1,0
128,"In addition , different sentence - level nodes can communicate with each other during state transition .",Baseline BiLSTM,Contrast with BiLSTM,named-entity-recognition,5,56,0.7671232876712328,127,0.6076555023923444,56,0.9655172413793104,1,0
129,"When one sentence - level node is used for classification outputs , the other sentencelevel node can serve as hidden memory units , or latent features .",Baseline BiLSTM,Contrast with BiLSTM,named-entity-recognition,5,57,0.7808219178082192,128,0.6124401913875598,57,0.9827586206896552,1,0
130,We study the effectiveness of multiple sentence - level nodes empirically .,Baseline BiLSTM,Contrast with BiLSTM,named-entity-recognition,5,58,0.7945205479452054,129,0.6172248803827751,58,1.0,1,0
131,Task settings,Baseline BiLSTM,,named-entity-recognition,5,59,0.8082191780821918,130,0.6220095693779905,0,0.0,1,0
132,"We consider two task settings , namely classification and sequence labelling .",Baseline BiLSTM,Task settings,named-entity-recognition,5,60,0.821917808219178,131,0.6267942583732058,1,0.0714285714285714,1,0
133,"For classification , g is fed to a softmax classification layer :",Baseline BiLSTM,Task settings,named-entity-recognition,5,61,0.8356164383561644,132,0.631578947368421,2,0.1428571428571428,1,0
134,where y is the probability distribution of output class labels and W c and b care model parameters .,Baseline BiLSTM,Task settings,named-entity-recognition,5,62,0.8493150684931506,133,0.6363636363636364,3,0.2142857142857142,1,0
135,"For sequence labelling , each hi can be used as feature representation fora corresponding word w i .",Baseline BiLSTM,Task settings,named-entity-recognition,5,63,0.863013698630137,134,0.6411483253588517,4,0.2857142857142857,1,0
136,External attention,Baseline BiLSTM,,named-entity-recognition,5,64,0.8767123287671232,135,0.645933014354067,5,0.3571428571428571,1,0
137,It has been shown that summation of hidden states using attention give better accuracies compared to using the end states of BiLSTMs .,Baseline BiLSTM,External attention,named-entity-recognition,5,65,0.8904109589041096,136,0.6507177033492823,6,0.4285714285714285,1,0
138,We study the influence of attention on both S - LSTM and BiLSTM for classification .,Baseline BiLSTM,External attention,named-entity-recognition,5,66,0.9041095890410958,137,0.6555023923444976,7,0.5,1,0
139,"In particular , additive attention ( Bahdanau",Baseline BiLSTM,External attention,named-entity-recognition,5,67,0.9178082191780822,138,0.6602870813397129,8,0.5714285714285714,1,0
140,"Here W ? , u and b ?",Baseline BiLSTM,External attention,named-entity-recognition,5,68,0.9315068493150684,139,0.6650717703349283,9,0.6428571428571429,1,0
141,are model parameters .,Baseline BiLSTM,External attention,named-entity-recognition,5,69,0.9452054794520548,140,0.6698564593301436,10,0.7142857142857143,1,0
142,External CRF,Baseline BiLSTM,,named-entity-recognition,5,70,0.958904109589041,141,0.6746411483253588,11,0.7857142857142857,1,0
143,"For sequential labelling , we use a CRF layer on top of the hidden vectors h 1 , h 2 , . . . , h n for calculating the conditional probabilities of label sequences :",Baseline BiLSTM,External CRF,named-entity-recognition,5,71,0.9726027397260274,142,0.6794258373205742,12,0.8571428571428571,1,0
144,are parameters specific to two consecutive labels y i ?1 and y i .,Baseline BiLSTM,External CRF,named-entity-recognition,5,72,0.9863013698630136,143,0.6842105263157895,13,0.9285714285714286,1,0
145,"For training , standard log - likelihood loss is used with L 2 regularization given a set of gold - standard instances .",Baseline BiLSTM,External CRF,named-entity-recognition,5,73,1.0,144,0.6889952153110048,14,1.0,1,0
146,Experiments,,,named-entity-recognition,5,0,0.0,145,0.69377990430622,0,0.0,1,0
147,We empirically compare S - LSTMs and BiLSTMs on different classification and sequence labelling tasks .,Experiments,Experiments,named-entity-recognition,5,1,0.5,146,0.6985645933014354,1,0.5,1,0
148,All experiments are conducted using a GeForce GTX 1080 GPU with 8 GB memory .,Experiments,Experiments,named-entity-recognition,5,2,1.0,147,0.7033492822966507,2,1.0,1,1
149,Development Experiments,,,named-entity-recognition,5,0,0.0,148,0.7081339712918661,0,0.0,1,0
150,We use the movie review development data to investigate different configurations of S - LSTMs and BiLSTMs .,Development Experiments,Development Experiments,named-entity-recognition,5,1,0.0666666666666666,149,0.7129186602870813,1,0.0666666666666666,1,0
151,"For S - LSTMs , the default configuration uses sand /s words for augmenting words Hyperparameters : shows the development results of various S - LSTM settings , where Time refers to training time per epoch .",Development Experiments,Development Experiments,named-entity-recognition,5,2,0.1333333333333333,150,0.7177033492822966,2,0.1333333333333333,1,0
152,"Without the sentence - level node , the accuracy of S - LSTM drops to 81.76 % , demonstrating the necessity of global information exchange .",Development Experiments,Development Experiments,named-entity-recognition,5,3,0.2,151,0.722488038277512,3,0.2,1,0
153,"Adding one additional sentence - level node as described in Section 3.2 does not lead to accuracy improvements , although the number of parameters and decoding time increase accordingly .",Development Experiments,Development Experiments,named-entity-recognition,5,4,0.2666666666666666,152,0.7272727272727273,4,0.2666666666666666,1,0
154,"As a result , we use only 1 sentence - level node for the remaining experiments .",Development Experiments,Development Experiments,named-entity-recognition,5,5,0.3333333333333333,153,0.7320574162679426,5,0.3333333333333333,1,0
155,"The accuracies of S - LSTM increases as the hidden layer size for each node increases from 100 to 300 , but does not further increase when the size increases beyond 300 .",Development Experiments,Development Experiments,named-entity-recognition,5,6,0.4,154,0.7368421052631579,6,0.4,1,0
156,We fix the hidden size to 300 accordingly .,Development Experiments,Development Experiments,named-entity-recognition,5,7,0.4666666666666667,155,0.7416267942583732,7,0.4666666666666667,1,0
157,"Without using sand /s , the performance of S - LSTM drops from 82. 64 % to 82.36 % , showing the effectiveness of having these additional nodes .",Development Experiments,Development Experiments,named-entity-recognition,5,8,0.5333333333333333,156,0.7464114832535885,8,0.5333333333333333,1,0
158,"Hyperparameters for BiLSTM models are also set according to the development data , which we omit here .",Development Experiments,Development Experiments,named-entity-recognition,5,9,0.6,157,0.7511961722488039,9,0.6,1,0
159,State transition .,Development Experiments,Development Experiments,named-entity-recognition,5,10,0.6666666666666666,158,0.7559808612440191,10,0.6666666666666666,1,0
160,"In , the number of recurrent state transition steps of S - LSTM is decided according to the best development performance .",Development Experiments,Development Experiments,named-entity-recognition,5,11,0.7333333333333333,159,0.7607655502392344,11,0.7333333333333333,1,0
161,draws the development accuracies of S - LSTMs with various window sizes against the number of recurrent steps .,Development Experiments,Development Experiments,named-entity-recognition,5,12,0.8,160,0.7655502392344498,12,0.8,1,0
162,"As can be seen from the figure , when the number of time steps increases from 1 to 11 , the accuracies generally increase , before reaching a maximum value .",Development Experiments,Development Experiments,named-entity-recognition,5,13,0.8666666666666667,161,0.7703349282296651,13,0.8666666666666667,1,0
163,This shows the effectiveness of recurrent information exchange in S - LSTM state transition .,Development Experiments,Development Experiments,named-entity-recognition,5,14,0.9333333333333332,162,0.7751196172248804,14,0.9333333333333332,1,0
164,"On the other hand , no significant differences are observed on the peak accuracies given by different window sizes , although a larger window size ( e.g.",Development Experiments,Development Experiments,named-entity-recognition,5,15,1.0,163,0.7799043062200957,15,1.0,1,0
165,Final Results for Classification,,,named-entity-recognition,5,0,0.0,164,0.784688995215311,0,0.0,1,1
166,"The final results on the movie review and rich text classification datasets are shown in Tables 4 and 5 , respectively .",Final Results for Classification,Final Results for Classification,named-entity-recognition,5,1,0.0263157894736842,165,0.7894736842105263,1,0.0833333333333333,1,0
167,"In addition to training time per epoch , test times are additionally reported .",Final Results for Classification,Final Results for Classification,named-entity-recognition,5,2,0.0526315789473684,166,0.7942583732057417,2,0.1666666666666666,1,0
168,We use the best settings on the movie review development dataset for both S - LSTMs and BiLSTMs .,Final Results for Classification,Final Results for Classification,named-entity-recognition,5,3,0.0789473684210526,167,0.7990430622009569,3,0.25,1,0
169,The step number for S - LSTMs is set to 9 .,Final Results for Classification,Final Results for Classification,named-entity-recognition,5,4,0.1052631578947368,168,0.8038277511961722,4,0.3333333333333333,1,0
170,"As shown in , the final results on the movie review dataset are consistent with the development results , where S - LSTM outperforms BiL - STM significantly , with a faster speed .",Final Results for Classification,Final Results for Classification,named-entity-recognition,5,5,0.131578947368421,169,0.8086124401913876,5,0.4166666666666667,1,1
171,Observations on CNN and hierarchical attention are consistent with the development results .,Final Results for Classification,Final Results for Classification,named-entity-recognition,5,6,0.1578947368421052,170,0.8133971291866029,6,0.5,1,0
172,S - LSTM also gives highly competitive results when compared with existing methods in the literature .,Final Results for Classification,Final Results for Classification,named-entity-recognition,5,7,0.1842105263157894,171,0.8181818181818182,7,0.5833333333333334,1,0
173,"As shown in , among the 16 datasets of , S - LSTM gives the best results on 12 , compared with BiLSTM and 2 layered BiL - STM models .",Final Results for Classification,Final Results for Classification,named-entity-recognition,5,8,0.2105263157894736,172,0.8229665071770335,8,0.6666666666666666,1,1
174,"The average accuracy of S - LSTM is 85.6 % , significantly higher compared with 84.9 % by 2 - layer stacked BiLSTM .",Final Results for Classification,Final Results for Classification,named-entity-recognition,5,9,0.2368421052631578,173,0.8277511961722488,9,0.75,1,1
175,"3 - layer stacked BiL - STM gives an average accuracy of 84. 57 % , which is lower compared to a 2 - layer stacked BiLSTM , with a training time per epoch of 423.6 seconds .",Final Results for Classification,Final Results for Classification,named-entity-recognition,5,10,0.2631578947368421,174,0.8325358851674641,10,0.8333333333333334,1,0
176,The relative speed advantage of S - LSTM over BiLSTM is larger on the 16 datasets as compared to the movie review test test .,Final Results for Classification,Final Results for Classification,named-entity-recognition,5,11,0.2894736842105263,175,0.8373205741626795,11,0.9166666666666666,1,0
177,This is because the average length of inputs is larger on the 16 datasets ( see Section 4.5 ) .,Final Results for Classification,Final Results for Classification,named-entity-recognition,5,12,0.3157894736842105,176,0.8421052631578947,12,1.0,1,0
178,Final Results for Sequence Labelling,Final Results for Classification,,named-entity-recognition,5,13,0.3421052631578947,177,0.84688995215311,0,0.0,1,0
179,Bi-directional,Final Results for Classification,,named-entity-recognition,5,14,0.3684210526315789,178,0.8516746411483254,1,0.0666666666666666,1,0
180,"RNN - CRF structures , and in particular BiLSTM - CRFs , have achieved the state of the art in the literature for sequence labelling tasks , including POS - tagging and NER .",Final Results for Classification,Bi-directional,named-entity-recognition,5,15,0.3947368421052631,179,0.8564593301435407,2,0.1333333333333333,1,0
181,"We compare S - LSTM - CRF with BiLSTM - CRF for sequence labelling , using the same settings as decided on the movie review development experiments for both BiLSTMs and S - LSTMs .",Final Results for Classification,Bi-directional,named-entity-recognition,5,16,0.4210526315789473,180,0.861244019138756,3,0.2,1,0
182,"For the latter , we decide the number of recurrent stepson the respective development sets for sequence labelling .",Final Results for Classification,Bi-directional,named-entity-recognition,5,17,0.4473684210526316,181,0.8660287081339713,4,0.2666666666666666,1,0
183,"The POS accuracies and NER F1 - scores against the number of recurrent steps are shown in ( a ) and ( b ) , respectively .",Final Results for Classification,Bi-directional,named-entity-recognition,5,18,0.4736842105263157,182,0.8708133971291866,5,0.3333333333333333,1,0
184,"For POS tagging , the best step number is set to 7 , with a development accuracy of 97.58 % .",Final Results for Classification,Bi-directional,named-entity-recognition,5,19,0.5,183,0.8755980861244019,6,0.4,1,0
185,"For NER , the step number is set to 9 , with a development F1 - score of 94.98 % .",Final Results for Classification,Bi-directional,named-entity-recognition,5,20,0.5263157894736842,184,0.8803827751196173,7,0.4666666666666667,1,0
186,As can be seen in with three layers of stacked LSTMs .,Final Results for Classification,Bi-directional,named-entity-recognition,5,21,0.5526315789473685,185,0.8851674641148325,8,0.5333333333333333,1,0
187,"For NER , S - LSTM gives an F1 - score of 91.57 % on the CoNLL test set , which is significantly better compared with BiLSTMs .",Final Results for Classification,Bi-directional,named-entity-recognition,5,22,0.5789473684210527,186,0.8899521531100478,9,0.6,1,1
188,Stacking more layers of BiLSTMs leads to slightly better F1 - scores compared with a single - layer BiL - STM .,Final Results for Classification,Bi-directional,named-entity-recognition,5,23,0.6052631578947368,187,0.8947368421052632,10,0.6666666666666666,1,0
189,"Our BiLSTM results are comparable to the results reported by and , who also use bidirectional RNN - CRF structures .",Final Results for Classification,Bi-directional,named-entity-recognition,5,24,0.631578947368421,188,0.8995215311004785,11,0.7333333333333333,1,0
190,"In contrast , S - LSTM gives the best reported results under the same settings .",Final Results for Classification,Bi-directional,named-entity-recognition,5,25,0.6578947368421053,189,0.9043062200956936,12,0.8,1,0
191,"In the second section of learning using additional language model objectives , obtaining an F-score of 86 . 26 % ; leverage character - level language models , obtaining an F- score of 91. 93 % , which is the current best result on the dataset .",Final Results for Classification,Bi-directional,named-entity-recognition,5,26,0.6842105263157895,190,0.9090909090909092,13,0.8666666666666667,1,0
192,All the three models are based on BiLSTM - CRF .,Final Results for Classification,Bi-directional,named-entity-recognition,5,27,0.7105263157894737,191,0.9138755980861244,14,0.9333333333333332,1,0
193,"On the other hand , these semi-supervised learning techniques are orthogonal to our work , and can potentially be used for S - LSTM also .",Final Results for Classification,Bi-directional,named-entity-recognition,5,28,0.7368421052631579,192,0.9186602870813396,15,1.0,1,0
194,Analysis,Final Results for Classification,,named-entity-recognition,5,29,0.7631578947368421,193,0.9234449760765552,0,0.0,1,0
195,"Figure 4 ( a ) and ( b ) show the accuracies against the sentence length on the movie review and CoNLL datasets , respectively , where test samples are binned in batches of 80 .",Final Results for Classification,Analysis,named-entity-recognition,5,30,0.7894736842105263,194,0.9282296650717704,1,0.1111111111111111,1,0
196,We find that the performances of both S - LSTM and BiLSTM decrease as the sentence length increases .,Final Results for Classification,Analysis,named-entity-recognition,5,31,0.8157894736842105,195,0.9330143540669856,2,0.2222222222222222,1,0
197,"On the other hand , S - LSTM demonstrates relatively better robustness compared to BiLSTMs .",Final Results for Classification,Analysis,named-entity-recognition,5,32,0.8421052631578947,196,0.937799043062201,3,0.3333333333333333,1,0
198,This confirms our intuition that a sentence - level node can facilitate better non-local communication .,Final Results for Classification,Analysis,named-entity-recognition,5,33,0.868421052631579,197,0.9425837320574164,4,0.4444444444444444,1,0
199,"these comparisons , we mix all training instances , order them by the size , and put them into 10 equal groups , the medium sentence lengths of which are shown .",Final Results for Classification,Analysis,named-entity-recognition,5,34,0.8947368421052632,198,0.9473684210526316,5,0.5555555555555556,1,0
200,"As can be seen from the figure , the speed advantage of S - LSTM is larger when the size of the input text increases , thanks to a fixed number of recurrent steps .",Final Results for Classification,Analysis,named-entity-recognition,5,35,0.9210526315789472,199,0.9521531100478468,6,0.6666666666666666,1,0
201,"Similar to hierarchical attention , there is a relative disadvantage of S - LSTM in comparison with BiLSTM , which is that the memory consumption is relatively larger .",Final Results for Classification,Analysis,named-entity-recognition,5,36,0.9473684210526316,200,0.9569377990430622,7,0.7777777777777778,1,0
202,"For example , over the movie review development set , the actual GPU memory consumption by S - LSTM , Bi LSTM , 2 - layer stacked BiLSTM and 4 - layer stacked BiLSTM are 252M , 89M , 146 M and 253M , respectively .",Final Results for Classification,Analysis,named-entity-recognition,5,37,0.9736842105263158,201,0.9617224880382776,8,0.8888888888888888,1,0
203,This is due to the fact that computation is performed in parallel by S - LSTM and hierarchical attention .,Final Results for Classification,Analysis,named-entity-recognition,5,38,1.0,202,0.9665071770334928,9,1.0,1,0
204,Conclusion,,,named-entity-recognition,5,0,0.0,203,0.971291866028708,0,0.0,1,0
205,"We have investigated S - LSTM , a recurrent neural network for encoding sentences , which offers richer contextual information exchange with more parallelism compared to BiLSTMs .",Conclusion,Conclusion,named-entity-recognition,5,1,0.2,204,0.9760765550239234,1,0.2,0,0
206,"Results on a range of classification and sequence labelling tasks show that S - LSTM outperforms BiLSTMs using the same number of parameters , demonstrating that S - LSTM can be a useful addition to the neural toolbox for encoding sentences .",Conclusion,Conclusion,named-entity-recognition,5,2,0.4,205,0.9808612440191388,2,0.4,0,0
207,"The structural nature in S - LSTM states allows straightforward extension to tree structures , resulting in highly parallelisable tree LSTMs .",Conclusion,Conclusion,named-entity-recognition,5,3,0.6,206,0.985645933014354,3,0.6,0,0
208,We leave such investigation to future work .,Conclusion,Conclusion,named-entity-recognition,5,4,0.8,207,0.9904306220095692,4,0.8,0,0
209,"Next directions also include the investigation of S - LSTM to more NLP tasks , such as machine translation .",Conclusion,Conclusion,named-entity-recognition,5,5,1.0,208,0.9952153110047848,5,1.0,0,0
1,title,,,named-entity-recognition,6,0,0.0,0,0.0,0,0.0,1,0
2,Robust Lexical Features for Improved Neural Network Named - Entity Recognition,title,title,named-entity-recognition,6,1,0.0,1,0.0047169811320754,1,0.0,1,1
3,abstract,,,named-entity-recognition,6,0,0.0,2,0.0094339622641509,0,0.0,1,0
4,Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features .,abstract,abstract,named-entity-recognition,6,1,0.1111111111111111,3,0.0141509433962264,1,0.1111111111111111,1,1
5,"While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers .",abstract,abstract,named-entity-recognition,6,2,0.2222222222222222,4,0.0188679245283018,2,0.2222222222222222,1,0
6,"In this work , we show that this is unfair : lexical features are actually quite useful .",abstract,abstract,named-entity-recognition,6,3,0.3333333333333333,5,0.0235849056603773,3,0.3333333333333333,1,0
7,We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia .,abstract,abstract,named-entity-recognition,6,4,0.4444444444444444,6,0.0283018867924528,4,0.4444444444444444,1,0
8,"From this , we compute - offline - a feature vector representing each word .",abstract,abstract,named-entity-recognition,6,5,0.5555555555555556,7,0.0330188679245283,5,0.5555555555555556,1,0
9,"When used with a vanilla recurrent neural network model , this representation yields substantial improvements .",abstract,abstract,named-entity-recognition,6,6,0.6666666666666666,8,0.0377358490566037,6,0.6666666666666666,1,0
10,"We establish anew state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset .",abstract,abstract,named-entity-recognition,6,7,0.7777777777777778,9,0.0424528301886792,7,0.7777777777777778,1,0
11,This work is licensed under a Creative Commons Attribution 4.0 International License .,abstract,abstract,named-entity-recognition,6,8,0.8888888888888888,10,0.0471698113207547,8,0.8888888888888888,1,0
12,License details :,abstract,abstract,named-entity-recognition,6,9,1.0,11,0.0518867924528301,9,1.0,1,0
13,Introduction,,,named-entity-recognition,6,0,0.0,12,0.0566037735849056,0,0.0,1,0
14,Named - Entity Recognition ( NER ) is the task of identifying textual mentions and classifying them into a predefined set of types .,Introduction,Introduction,named-entity-recognition,6,1,0.0263157894736842,13,0.0613207547169811,1,0.0555555555555555,1,1
15,"Various approaches have been proposed to tackle the task , from hand - crafted feature - based machine learning models like conditional random fields and perceptron , to deep neural models .",Introduction,Introduction,named-entity-recognition,6,2,0.0526315789473684,14,0.0660377358490566,2,0.1111111111111111,1,0
16,"Word representations , also known as word embeddings , area key element for multiple NLP tasks including NER .",Introduction,Introduction,named-entity-recognition,6,3,0.0789473684210526,15,0.070754716981132,3,0.1666666666666666,1,0
17,"Due to the small amount of named - entity annotated data , embeddings are used to extend , rather than replace , hand - crafted features in order to obtain state - of - the - art performance .",Introduction,Introduction,named-entity-recognition,6,4,0.1052631578947368,16,0.0754716981132075,4,0.2222222222222222,1,0
18,Recent studies have explored methods for supplying deep sequential taggers with complementary features to standard embeddings .,Introduction,Introduction,named-entity-recognition,6,5,0.131578947368421,17,0.080188679245283,5,0.2777777777777778,1,0
19,and tested special embeddings extracted from a neural language model ( LM ) trained on a large corpus .,Introduction,Introduction,named-entity-recognition,6,6,0.1578947368421052,18,0.0849056603773584,6,0.3333333333333333,1,0
20,LM embeddings capture context - dependent aspects of word meaning using future ( forward LM ) and previous ( backward LM ) context words .,Introduction,Introduction,named-entity-recognition,6,7,0.1842105263157894,19,0.0896226415094339,7,0.3888888888888889,1,0
21,"When this information is added to standard features , it leads to significant improvements in NER .",Introduction,Introduction,named-entity-recognition,6,8,0.2105263157894736,20,0.0943396226415094,8,0.4444444444444444,1,0
22,"Also , showed that external knowledge resources ( namely gazetteers ) are crucial to NER performance .",Introduction,Introduction,named-entity-recognition,6,9,0.2368421052631578,21,0.0990566037735849,9,0.5,1,0
23,Gazetteer features encode the presence of word n-grams in predefined lists of NEs .,Introduction,Introduction,named-entity-recognition,6,10,0.2631578947368421,22,0.1037735849056603,10,0.5555555555555556,1,0
24,"In this work , we discuss some of the limitations of gazetteer features and propose an alternative lexical representation which is trained offline and that can be added to any neural NER system .",Introduction,Introduction,named-entity-recognition,6,11,0.2894736842105263,23,0.1084905660377358,11,0.6111111111111112,1,1
25,"Ina nutshell , we embed words and entity types into a joint vector space by leveraging WiFiNE , a ressource which automatically annotates mentions in Wikipedia with 120 entity types .",Introduction,Introduction,named-entity-recognition,6,12,0.3157894736842105,24,0.1132075471698113,12,0.6666666666666666,1,1
26,"From this vector space , we compute for each word a 120 - dimensional vector , where each dimension encodes the similarity of the word with an entity type .",Introduction,Introduction,named-entity-recognition,6,13,0.3421052631578947,25,0.1179245283018868,13,0.7222222222222222,1,0
27,"We call this vector an LS representation , for Lexical Similarity .",Introduction,Introduction,named-entity-recognition,6,14,0.3684210526315789,26,0.1226415094339622,14,0.7777777777777778,1,0
28,"When included in a vanilla LSTM - CRF NER model , LS representations lead to significant gains .",Introduction,Introduction,named-entity-recognition,6,15,0.3947368421052631,27,0.1273584905660377,15,0.8333333333333334,1,0
29,"We establish anew state - of - the - art F 1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance on the over - studied In the rest of this paper , we motivate our work in Section 2 .",Introduction,Introduction,named-entity-recognition,6,16,0.4210526315789473,28,0.1320754716981132,16,0.8888888888888888,1,0
30,We describe how we compute LS vectors in Section 3 .,Introduction,Introduction,named-entity-recognition,6,17,0.4473684210526316,29,0.1367924528301886,17,0.9444444444444444,1,0
31,"We present our system in Section 4 and report results in Section 5 . In Section 6 , we discuss related works before concluding in Section 7 .",Introduction,Introduction,named-entity-recognition,6,18,0.4736842105263157,30,0.1415094339622641,18,1.0,1,0
32,Motivation,Introduction,,named-entity-recognition,6,19,0.5,31,0.1462264150943396,0,0.0,1,0
33,Gazetteers are lists of entities that are associated with specific NE categories .,Introduction,Motivation,named-entity-recognition,6,20,0.5263157894736842,32,0.1509433962264151,1,0.0526315789473684,1,0
34,"They are widely used as a feature source in NER , and have been successfully included in feature - based models .",Introduction,Motivation,named-entity-recognition,6,21,0.5526315789473685,33,0.1556603773584905,2,0.1052631578947368,1,0
35,"Typically , lists of entities are compiled from structured data sources such as DBpedia or Freebase .",Introduction,Motivation,named-entity-recognition,6,22,0.5789473684210527,34,0.160377358490566,3,0.1578947368421052,1,0
36,"The surface form of the title of a Wikipedia article , as well as aliases and redirects are mapped to an entity type using the object type attribute of the related DBpedia ( or Freebase ) page .",Introduction,Motivation,named-entity-recognition,6,23,0.6052631578947368,35,0.1650943396226415,4,0.2105263157894736,1,0
37,"use this methodology to compile 30 lists of fine - grained entity types extracted from Wikipedia , while Chiu and Nichols ( 2016 ) create 4 gazetteers that map to CoNLL categories ( PER , LOC , ORG and MISC ) .",Introduction,Motivation,named-entity-recognition,6,24,0.631578947368421,36,0.1698113207547169,5,0.2631578947368421,1,0
38,"Despite their importance , gazetteer - based features suffer from a number of limitations .",Introduction,Motivation,named-entity-recognition,6,25,0.6578947368421053,37,0.1745283018867924,6,0.3157894736842105,1,0
39,Binary representation .,Introduction,Motivation,named-entity-recognition,6,26,0.6842105263157895,38,0.1792452830188679,7,0.3684210526315789,1,0
40,Gazetteer features encode only the presence of an n-gram in each list and omit its relative frequency .,Introduction,Motivation,named-entity-recognition,6,27,0.7105263157894737,39,0.1839622641509433,8,0.4210526315789473,1,0
41,"For example , the word "" France "" can be used as a person , an organization , or a location , while it likely refers to the country most of the time .",Introduction,Motivation,named-entity-recognition,6,28,0.7368421052631579,40,0.1886792452830188,9,0.4736842105263157,1,0
42,Binary features can not capture this preference .,Introduction,Motivation,named-entity-recognition,6,29,0.7631578947368421,41,0.1933962264150943,10,0.5263157894736842,1,0
43,Generation .,Introduction,,named-entity-recognition,6,30,0.7894736842105263,42,0.1981132075471698,11,0.5789473684210527,1,0
44,"At test time , we need to match every n-gram ( up to the length of the longest lexicon entry ) in a sentence against entries in the lexicons , which is time consuming .",Introduction,Generation .,named-entity-recognition,6,31,0.8157894736842105,43,0.2028301886792453,12,0.631578947368421,1,0
45,"In their work , Chiu and Nichols ( 2016 ) use 4 lists that count over 2.3 M entries .",Introduction,Generation .,named-entity-recognition,6,32,0.8421052631578947,44,0.2075471698113207,13,0.6842105263157895,1,0
46,Non-entity words .,Introduction,Generation .,named-entity-recognition,6,33,0.868421052631579,45,0.2122641509433962,14,0.7368421052631579,1,0
47,"Gazetteer features do not capture signal from non-entity words , while earlier feature - based models strived to encode that some words ( or n-grams ) trigger specific entity types .",Introduction,Generation .,named-entity-recognition,6,34,0.8947368421052632,46,0.2169811320754717,15,0.7894736842105263,1,0
48,"For instance , words such as "" eat "" , "" directed "" or "" born "" are words that typically appear after a mention of type PER .",Introduction,Generation .,named-entity-recognition,6,35,0.9210526315789472,47,0.2216981132075471,16,0.8421052631578947,1,0
49,"To overcome those limitations , we propose an alternative approach where we embed annotations mined from Wikipedia into a vector space from which we compute a feature vector that represent words .",Introduction,Generation .,named-entity-recognition,6,36,0.9473684210526316,48,0.2264150943396226,17,0.8947368421052632,1,0
50,This vector compactly and efficiently encodes both gazetteer and lexical information .,Introduction,Generation .,named-entity-recognition,6,37,0.9736842105263158,49,0.2311320754716981,18,0.9473684210526316,1,0
51,"Note that attest time , we only have to feed our model with this feature vector , which is efficient .",Introduction,Generation .,named-entity-recognition,6,38,1.0,50,0.2358490566037736,19,1.0,1,0
52,Our Method,,,named-entity-recognition,6,0,0.0,51,0.240566037735849,0,0.0,1,0
53,Embedding Words and Entity Types,Our Method,,named-entity-recognition,6,1,0.0204081632653061,52,0.2452830188679245,0,0.0,1,0
54,Turning Wikipedia into a corpus of named - entities annotated with types is a task that received continuous attention over the years .,Our Method,Embedding Words and Entity Types,named-entity-recognition,6,2,0.0408163265306122,53,0.25,1,0.0344827586206896,1,0
55,It consists mainly in exploiting the hyperlink structure of Wikipedia in order to detect entity mentions .,Our Method,Embedding Words and Entity Types,named-entity-recognition,6,3,0.0612244897959183,54,0.2547169811320754,2,0.0689655172413793,1,0
56,"Then , structured data from a knowledge base ( for instance Freebase ) are used to map hyperlinks to entity types .",Our Method,Embedding Words and Entity Types,named-entity-recognition,6,4,0.0816326530612244,55,0.2594339622641509,3,0.1034482758620689,1,0
57,"Because the number of anchored strings in Wikipedia is no more than 3 % of the text tokens , proposed to augment Wikipedia articles with mentions unmarked in Wikipedia , thanks to a mix of heuristics that benefit the Wikipedia structure , as well as a coreference resolution system adapted specifically to Wikipedia .",Our Method,Embedding Words and Entity Types,named-entity-recognition,6,5,0.1020408163265306,56,0.2641509433962264,4,0.1379310344827586,1,0
58,"The authors applied their approach on English Wikipedia and produce coarse ( 4 classes ) and finegrained ( 120 labels ) named- entity annotations , leading to WiNER and WiFiNE .",Our Method,Embedding Words and Entity Types,named-entity-recognition,6,6,0.1224489795918367,57,0.2688679245283019,5,0.1724137931034483,1,0
59,"In this work , we adopt WiFiNE which is publicly available at http://rali.iro.umontreal.ca/rali/en/wifiner-wikipedia-for-et as our source of annotations .",Our Method,Embedding Words and Entity Types,named-entity-recognition,6,7,0.1428571428571428,58,0.2735849056603773,6,0.2068965517241379,1,0
60,Each entity mention is mapped ( via it s Freebase object type attribute ) to a pre-defined set of 120 entity types .,Our Method,Embedding Words and Entity Types,named-entity-recognition,6,8,0.1632653061224489,59,0.2783018867924528,7,0.2413793103448276,1,0
61,Types are stored in a 2 - level hierarchical structure ( e.g. / person and / person / musician ) .,Our Method,Embedding Words and Entity Types,named-entity-recognition,6,9,0.1836734693877551,60,0.2830188679245283,8,0.2758620689655172,1,0
62,"The corpus consist of 3.2 M Wikipedia articles , comprising 1.3G tokens that we annotated with 157.4 M named - entity mentions and their types .",Our Method,Embedding Words and Entity Types,named-entity-recognition,6,10,0.2040816326530612,61,0.2877358490566037,9,0.3103448275862069,1,0
63,We used this very large quantity of automatically annotated data for jointly embedding words and entity types into the same low - dimensional space .,Our Method,Embedding Words and Entity Types,named-entity-recognition,6,11,0.2244897959183673,62,0.2924528301886792,10,0.3448275862068966,1,0
64,The key idea consists in learning an embedding for each entity type using its surrounding words .,Our Method,Embedding Words and Entity Types,named-entity-recognition,6,12,0.2448979591836734,63,0.2971698113207547,11,0.3793103448275862,1,0
65,"For instance , the embedding for / product / software will be trained using context words that surround all entities that were ( automatically ) labelled as / product / software in Wikipedia .",Our Method,Embedding Words and Entity Types,named-entity-recognition,6,13,0.2653061224489796,64,0.3018867924528302,12,0.4137931034482758,1,0
66,"In practice , we found that simply concatenating a sentence ( v1 ) with its annotated version ( v 2 ) , as illustrated in , offers a simple but efficient way of combining words and entity types so that embeddings can make good use of them .",Our Method,Embedding Words and Entity Types,named-entity-recognition,6,14,0.2857142857142857,65,0.3066037735849056,13,0.4482758620689655,1,0
67,We use the FastText toolkit to learn the uncased embeddings for both words and entity types .,Our Method,Embedding Words and Entity Types,named-entity-recognition,6,15,0.3061224489795918,66,0.3113207547169811,14,0.4827586206896552,1,0
68,"We train a skipgram model to learn 100 - dimensional vectors with a minimum word frequency cutoff of 5 , and a window size of 5 .",Our Method,Embedding Words and Entity Types,named-entity-recognition,6,16,0.3265306122448979,67,0.3160377358490566,15,0.5172413793103449,1,0
69,This configuration ( recommended by the authors ) performs the best in the experiments described in Section 5 .,Our Method,Embedding Words and Entity Types,named-entity-recognition,6,17,0.3469387755102041,68,0.320754716981132,16,0.5517241379310345,1,0
70,"Since FastText learns representations of character ngrams , it has the ability to produce vectors for unknown words .",Our Method,Embedding Words and Entity Types,named-entity-recognition,6,18,0.3673469387755102,69,0.3254716981132075,17,0.5862068965517241,1,0
71,"For visualization proposes , we only plot single - word mentions that were annotated in WiFiNE with one of those 6 types .",Our Method,Embedding Words and Entity Types,named-entity-recognition,6,19,0.3877551020408163,70,0.330188679245283,18,0.6206896551724138,1,0
72,Words were randomly and proportionally sampled according to the frequency of each entity type .,Our Method,Embedding Words and Entity Types,named-entity-recognition,6,20,0.4081632653061224,71,0.3349056603773584,19,0.6551724137931034,1,0
73,"In addition , words have the color associated with the most frequent type they were annotated within WiFiNE .",Our Method,Embedding Words and Entity Types,named-entity-recognition,6,21,0.4285714285714285,72,0.3396226415094339,20,0.6896551724137931,1,0
74,We observe that mentions often annotated by a given type in our resource tend to cluster around this entity type .,Our Method,Embedding Words and Entity Types,named-entity-recognition,6,22,0.4489795918367347,73,0.3443396226415094,21,0.7241379310344828,1,0
75,"For instance , "" firefox "" is close to the type / product / software , while "" enzyme "" is close to the / biology entity type .",Our Method,Embedding Words and Entity Types,named-entity-recognition,6,23,0.4693877551020408,74,0.3490566037735849,22,0.7586206896551724,1,0
76,We also notice that words that are labelled with different types tend to appear between types they were annotated with .,Our Method,Embedding Words and Entity Types,named-entity-recognition,6,24,0.4897959183673469,75,0.3537735849056603,23,0.7931034482758621,1,0
77,"For instance , "" gpx2 "" , which is used both as a software and as a gene , has it s embedding in between / product / software and / biology .",Our Method,Embedding Words and Entity Types,named-entity-recognition,6,25,0.5102040816326531,76,0.3584905660377358,24,0.8275862068965517,1,0
78,"We inspected some of the words plotted in , and found that "" jrun "" and "" xp "" are incorrectly labelled as / product / weapon in WiFiNE .",Our Method,Embedding Words and Entity Types,named-entity-recognition,6,26,0.5306122448979592,77,0.3632075471698113,25,0.8620689655172413,1,0
79,"But since these words are seen in a software context , their embeddings are closer to the / product / software embedding than the / product / weapon one .",Our Method,Embedding Words and Entity Types,named-entity-recognition,6,27,0.5510204081632653,78,0.3679245283018867,26,0.896551724137931,1,0
80,"We feel this tolerance to noise is a desirable feature , one that hopefully allows a more efficient use of distant supervision .",Our Method,Embedding Words and Entity Types,named-entity-recognition,6,28,0.5714285714285714,79,0.3726415094339622,27,0.9310344827586208,1,0
81,"Last , we also observe the tendency of rare words to cluster around their entity type .",Our Method,Embedding Words and Entity Types,named-entity-recognition,6,29,0.5918367346938775,80,0.3773584905660377,28,0.9655172413793104,1,0
82,"For instance , "" iota "" and "" x.org "" are embedded near their respective types , despite the fact that they appear less than 30 times in the version of Wikipedia used to compile WiFiNE .",Our Method,Embedding Words and Entity Types,named-entity-recognition,6,30,0.6122448979591837,81,0.3820754716981132,29,1.0,1,0
83,LS Representation,Our Method,,named-entity-recognition,6,31,0.6326530612244898,82,0.3867924528301887,0,0.0,1,0
84,"This joint vector space only serves the purpose of associating to each word a LS representation , that is , a 120 - dimensional vector where the ith coefficient is a value in the [ ? 1 , + 1 ] interval , equal to the cosine similarity 1 between the word embedding and the embedding of the ith entity type ( we have 120 types ) .",Our Method,LS Representation,named-entity-recognition,6,32,0.6530612244897959,83,0.3915094339622642,1,0.0,1,0
85,Word,Our Method,,named-entity-recognition,6,33,0.673469387755102,84,0.3962264150943396,0,0.0,1,0
86,Entity .,Our Method,,named-entity-recognition,6,34,0.6938775510204082,85,0.4009433962264151,1,0.1,1,0
87,shows the topmost similar entity types for proper names ( left column ) and common words ( right column ) .,Our Method,Entity .,named-entity-recognition,6,35,0.7142857142857143,86,0.4056603773584906,2,0.2,1,0
88,We observe that ambiguous mentions ( those annotated with several types ) are adequately handled .,Our Method,Entity .,named-entity-recognition,6,36,0.7346938775510204,87,0.410377358490566,3,0.3,1,0
89,"For instance , the LS representation of the word "" hilton "" encodes that it more often refers to a hotel or a restaurant than to an actress .",Our Method,Entity .,named-entity-recognition,6,37,0.7551020408163265,88,0.4150943396226415,4,0.4,1,0
90,"Also , we observe that entity words that are either not or rarely annotated in WiFiNE are still adequately associated with their right type .",Our Method,Entity .,named-entity-recognition,6,38,0.7755102040816326,89,0.419811320754717,5,0.5,1,0
91,"For instance , "" dammstadt "" , which appears only 5 times in WiFiNE , and which refers to the Damm city in Germany , is most similar to / location / city and / location / railway .",Our Method,Entity .,named-entity-recognition,6,39,0.7959183673469388,90,0.4245283018867924,6,0.6,1,0
92,"Interestingly , this mention does not have its page in English Wikipedia .",Our Method,Entity .,named-entity-recognition,6,40,0.8163265306122449,91,0.4292452830188679,7,0.7,1,0
93,"Furthermore , we observe that non-entity context words have a strong similarity to types they precede or succeed .",Our Method,Entity .,named-entity-recognition,6,41,0.8367346938775511,92,0.4339622641509434,8,0.8,1,0
94,"For instance the verb "" directed "" is very close to / person / director , an entity type that usually precedes it , and to / art / film , that usually follows it .",Our Method,Entity .,named-entity-recognition,6,42,0.8571428571428571,93,0.4386792452830189,9,0.9,1,0
95,"Likewise , the preposition "" in "" is near / date and / location / city , which frequently follow "" in "" .",Our Method,Entity .,named-entity-recognition,6,43,0.8775510204081632,94,0.4433962264150943,10,1.0,1,0
96,Strength of the LS Representation,Our Method,,named-entity-recognition,6,44,0.8979591836734694,95,0.4481132075471698,0,0.0,1,0
97,"To summarize , we propose a compact lexical representation which is computed offline , therefore incurring no computation burden attest time",Our Method,Strength of the LS Representation,named-entity-recognition,6,45,0.9183673469387756,96,0.4528301886792453,1,0.2,1,0
98,"This representation encodes the preference of an entity - mention word fora given type , an information out of reach of binary gazetteer features .",Our Method,Strength of the LS Representation,named-entity-recognition,6,46,0.9387755102040816,97,0.4575471698113207,2,0.4,1,0
99,It also lends itself nicely to the inclusion of lexical features that have been successfully used in earlier feature - based systems .,Our Method,Strength of the LS Representation,named-entity-recognition,6,47,0.9591836734693876,98,0.4622641509433962,3,0.6,1,0
100,"Also , because entity types are well represented in WiFiNE , their embeddings are robust :",Our Method,Strength of the LS Representation,named-entity-recognition,6,48,0.979591836734694,99,0.4669811320754717,4,0.8,1,0
101,Our representation does accommodate unfrequent words and seems tolerant to the inherent noise of distant supervision .,Our Method,Strength of the LS Representation,named-entity-recognition,6,49,1.0,100,0.4716981132075472,5,1.0,1,0
102,Our NER System,,,named-entity-recognition,6,0,0.0,101,0.4764150943396226,0,0.0,1,0
103,"In order to test the efficiency of our lexical feature representation , we implemented a state - of - the - art NER system we now describe .",Our NER System,Our NER System,named-entity-recognition,6,1,0.5,102,0.4811320754716981,1,0.5,1,0
104,Bi-LSTM- CRF,Our NER System,Our NER System,named-entity-recognition,6,2,1.0,103,0.4858490566037736,2,1.0,1,0
105,Model,,,named-entity-recognition,6,0,0.0,104,0.490566037735849,0,0.0,1,0
106,"We adopt the popular Bi - LSTM - CRF architecture , a de facto baseline in many sequential tagging tasks .",Model,Model,named-entity-recognition,6,1,0.0526315789473684,105,0.4952830188679245,1,0.0,1,0
107,Features,Model,,named-entity-recognition,6,2,0.1052631578947368,106,0.5,0,0.0,1,0
108,"In addition to the LS vector , we incorporate publicly available pre-trained embeddings , as well as character - level , and capitalization features .",Model,Features,named-entity-recognition,6,3,0.1578947368421052,107,0.5047169811320755,1,0.5,1,0
109,Those features have been shown to be crucial for stateof - the - art performance .,Model,Features,named-entity-recognition,6,4,0.2105263157894736,108,0.5094339622641509,2,1.0,1,0
110,Word Embeddings,Model,,named-entity-recognition,6,5,0.2631578947368421,109,0.5141509433962265,0,0.0,1,0
111,"We experimented with several publicly available word embeddings , such as Senna , Word2 Vec , GloVe , and SSKIP .",Model,Word Embeddings,named-entity-recognition,6,6,0.3157894736842105,110,0.5188679245283019,1,0.2,1,0
112,We find that the latter performs the best in our experiments .,Model,Word Embeddings,named-entity-recognition,6,7,0.3684210526315789,111,0.5235849056603774,2,0.4,1,0
113,SSKIP embeddings are 100 - dimensional case sensitive vectors that where trained using a n-skip - gram model on 42B tokens .,Model,Word Embeddings,named-entity-recognition,6,8,0.4210526315789473,112,0.5283018867924528,3,0.6,1,0
114,"These embeddings were previously used by , who report good performance on CONLL , and state - of - the - art results on ONTONOTES respectively .",Model,Word Embeddings,named-entity-recognition,6,9,0.4736842105263157,113,0.5330188679245284,4,0.8,1,0
115,Note that these pre-trained embeddings are adjusted during training .,Model,Word Embeddings,named-entity-recognition,6,10,0.5263157894736842,114,0.5377358490566038,5,1.0,1,0
116,Character Embeddings,Model,,named-entity-recognition,6,11,0.5789473684210527,115,0.5424528301886793,0,0.0,1,0
117,"Following , we use a forward and a backward LSTM to derive a representation of each word from its characters ( right part of .",Model,Character Embeddings,named-entity-recognition,6,12,0.631578947368421,116,0.5471698113207547,1,0.5,1,0
118,"A character lookup table is randomly initialized , then trained at the same time as the Bi - LSTM model sketched in Section 4.1 .",Model,Character Embeddings,named-entity-recognition,6,13,0.6842105263157895,117,0.5518867924528302,2,1.0,1,0
119,Capitalization Features,Model,,named-entity-recognition,6,14,0.7368421052631579,118,0.5566037735849056,0,0.0,1,0
120,"Similarly to previous works , we use capitalization features for characterizing certain categories of capitalization patterns : all Upper , allLower , upperFirst , upperNotFirst , numeric or noAlphaNum .",Model,Capitalization Features,named-entity-recognition,6,15,0.7894736842105263,119,0.5613207547169812,1,0.5,1,0
121,"We define a random lookup table for these features , and learn its parameters during training .",Model,Capitalization Features,named-entity-recognition,6,16,0.8421052631578947,120,0.5660377358490566,2,1.0,1,0
122,LS Vectors,Model,,named-entity-recognition,6,17,0.8947368421052632,121,0.5707547169811321,0,0.0,1,0
123,"Contrarily to previous features , lexical vectors are computed offline and are not adjusted during training .",Model,LS Vectors,named-entity-recognition,6,18,0.9473684210526316,122,0.5754716981132075,1,0.5,1,0
124,"We found useful in practice to apply a MinMax scaler in the range [ ? 1 , + 1 ] to each LS vector we computed ; thus , [.. , 0.095 , .. , 0.20 , .. , 0.76 , .. ] becomes [.. , ? 1 , .. , ? 0.67 , .. , 1 , ..].",Model,LS Vectors,named-entity-recognition,6,19,1.0,123,0.5801886792452831,2,1.0,1,0
125,Experiments,,,named-entity-recognition,6,0,0.0,124,0.5849056603773585,0,0.0,1,0
126,Data and Evaluation,,,named-entity-recognition,6,0,0.0,125,0.589622641509434,0,0.0,1,0
127,We consider two well - established NER benchmarks :,Data and Evaluation,Data and Evaluation,named-entity-recognition,6,1,0.0909090909090909,126,0.5943396226415094,1,0.0909090909090909,1,0
128,CONLL-2003 and ONTONOTES 5.0 . provides an overview of the two datasets .,Data and Evaluation,Data and Evaluation,named-entity-recognition,6,2,0.1818181818181818,127,0.5990566037735849,2,0.1818181818181818,1,0
129,"As we can see , ONTONOTES is much larger .",Data and Evaluation,Data and Evaluation,named-entity-recognition,6,3,0.2727272727272727,128,0.6037735849056604,3,0.2727272727272727,1,0
130,"For both datasets , we convert the IOB encoding to BILOU , since found the latter to perform better .",Data and Evaluation,Data and Evaluation,named-entity-recognition,6,4,0.3636363636363636,129,0.6084905660377359,4,0.3636363636363636,1,0
131,"In keeping with others , we report mention - level F 1 score using the conlleval script 2 .",Data and Evaluation,Data and Evaluation,named-entity-recognition,6,5,0.4545454545454545,130,0.6132075471698113,5,0.4545454545454545,1,0
132,The ) is a well known collection of Reuters newswire articles that contains a large portion of sports news .,Data and Evaluation,Data and Evaluation,named-entity-recognition,6,6,0.5454545454545454,131,0.6179245283018868,6,0.5454545454545454,1,0
133,"It is annotated with four entity types : Person ( PER ) , Location ( LOC ) , Organization ( ORG ) and Miscellaneous ( MISC ) .",Data and Evaluation,Data and Evaluation,named-entity-recognition,6,7,0.6363636363636364,132,0.6226415094339622,7,0.6363636363636364,1,0
134,"The four entity types are fairly evenly distributed , and the train / dev / test datasets present a similar type distribution. , magazine ( 120 k ) , newswire ( 625 k ) , and web data ( 300 k ) .",Data and Evaluation,Data and Evaluation,named-entity-recognition,6,8,0.7272727272727273,133,0.6273584905660378,8,0.7272727272727273,1,0
135,"This dataset is annotated with 18 entity types , and is much larger than CONLL .",Data and Evaluation,Data and Evaluation,named-entity-recognition,6,9,0.8181818181818182,134,0.6320754716981132,9,0.8181818181818182,1,0
136,"Following previous researches , we use the official train / dev / test split of the CoNLL - 2012 shared task .",Data and Evaluation,Data and Evaluation,named-entity-recognition,6,10,0.9090909090909092,135,0.6367924528301887,10,0.9090909090909092,1,0
137,"Also , we exclude ( both during training and testing ) the New Testaments portion as it does not contain gold NE annotations .",Data and Evaluation,Data and Evaluation,named-entity-recognition,6,11,1.0,136,0.6415094339622641,11,1.0,1,0
138,Training and Implementation,,,named-entity-recognition,6,0,0.0,137,0.6462264150943396,0,0.0,1,0
139,Training is carried out by mini-batch stochastic gradient descent ( SGD ) with a momentum of 0.9 and a gradient clipping of 5.0 .,Training and Implementation,Training and Implementation,named-entity-recognition,6,1,0.03125,138,0.6509433962264151,1,0.0333333333333333,1,1
140,"The mini-batch is 10 for both datasets , and learning rates are 0.009 and 0.013 for CONLL and ONTONOTES respectively .",Training and Implementation,Training and Implementation,named-entity-recognition,6,2,0.0625,139,0.6556603773584906,2,0.0666666666666666,1,1
141,"More sophisticated optimization algorithms such as AdaDelta or Adam ( Kingma and Ba , 2014 ) converge faster , but none outperformed SGD with exponential learning rate decay in our experiments .",Training and Implementation,Training and Implementation,named-entity-recognition,6,3,0.09375,140,0.660377358490566,3,0.1,1,0
142,Our system uses a single Bi - LSTM layer at the word level whose hidden dimensions are set to 128 and 256 for CONLL and ONTONOTES respectively .,Training and Implementation,Training and Implementation,named-entity-recognition,6,4,0.125,141,0.6650943396226415,4,0.1333333333333333,1,0
143,"For both models , the character embedding size was set to 25 , and the hidden dimension of the forward and backward character LSTMs are set to 50 .",Training and Implementation,Training and Implementation,named-entity-recognition,6,5,0.15625,142,0.6698113207547169,5,0.1666666666666666,1,0
144,"To mitigate overfitting , we apply a dropout mask with a probability of 0.5 on the input and output vectors of the Bi - LSTM layer .",Training and Implementation,Training and Implementation,named-entity-recognition,6,6,0.1875,143,0.6745283018867925,6,0.2,1,0
145,"For both datasets , we set the dimension of capitalization embeddings to 25 and trained the models up to 50 epochs .",Training and Implementation,Training and Implementation,named-entity-recognition,6,7,0.21875,144,0.6792452830188679,7,0.2333333333333333,1,0
146,"We tuned the hyper - parameters by grid search , and used early stopping based on the performance on the development set .",Training and Implementation,Training and Implementation,named-entity-recognition,6,8,0.25,145,0.6839622641509434,8,0.2666666666666666,1,0
147,"We varied dropout . 65 ] ) , hidden units ) , capitalization ) and char ) embedding dimensions , learning rate ( [ 0.001 , 0.015 ] by step 0.002 ) , and optimization algorithms and fixed the other hyper - parameters .",Training and Implementation,Training and Implementation,named-entity-recognition,6,9,0.28125,146,0.6886792452830188,9,0.3,1,1
148,"We implemented our system using the Tensorflow library , and ran our models on a GeForce GTX TITAN Xp GPU .",Training and Implementation,Training and Implementation,named-entity-recognition,6,10,0.3125,147,0.6933962264150944,10,0.3333333333333333,1,1
149,Training requires about 2.5 hours for CONLL and 8 hours for ONTONOTES .,Training and Implementation,Training and Implementation,named-entity-recognition,6,11,0.34375,148,0.6981132075471698,11,0.3666666666666666,1,0
150,shows the development set performance of our final models on each dataset compared to the work of .,Training and Implementation,Training and Implementation,named-entity-recognition,6,12,0.375,149,0.7028301886792453,12,0.4,1,0
151,"The authors use an architecture similar to ours , but use a binary gazetteer feature set , while we use our LS representation .",Training and Implementation,Training and Implementation,named-entity-recognition,6,13,0.40625,150,0.7075471698113207,13,0.4333333333333333,1,0
152,"Since our systems involve random initialization , we report the mean as well as the standard deviation over five runs .",Training and Implementation,Training and Implementation,named-entity-recognition,6,14,0.4375,151,0.7122641509433962,14,0.4666666666666667,1,0
153,"The improvements yielded by our model on the CONLL dataset are significant although modest , while those observed on ONTONOTES are more substantial .",Training and Implementation,Training and Implementation,named-entity-recognition,6,15,0.46875,152,0.7169811320754716,15,0.5,1,0
154,We also observe a lower variance of our system over the 5 runs .,Training and Implementation,Training and Implementation,named-entity-recognition,6,16,0.5,153,0.7216981132075472,16,0.5333333333333333,1,0
155,"First , we observe that our model significantly outperforms models that use extensive sets of handcrafted features ) as well as the system of Standard deviation on the test set is reported in 2015 ) that uses NE and Entity Linking annotations to jointly optimize the performance on both tasks .",Training and Implementation,Training and Implementation,named-entity-recognition,6,17,0.53125,154,0.7264150943396226,17,0.5666666666666667,1,1
156,"Second , our model outperforms as well other NN models that only use standard word embeddings , which indicates that our lexical feature vector is complementary to standard word embeddings .",Training and Implementation,Training and Implementation,named-entity-recognition,6,18,0.5625,155,0.7311320754716981,18,0.6,1,1
157,"Third , our system matches state - of - the - art performances of models that use either more complex architectures or more elaborate features .",Training and Implementation,Training and Implementation,named-entity-recognition,6,19,0.59375,156,0.7358490566037735,19,0.6333333333333333,1,1
158,use three layers of stacked residual RNN ( Bi - LSTM ) with bias decoding .,Training and Implementation,Training and Implementation,named-entity-recognition,6,20,0.625,157,0.7405660377358491,20,0.6666666666666666,1,0
159,Our model is much simpler and faster .,Training and Implementation,Training and Implementation,named-entity-recognition,6,21,0.65625,158,0.7452830188679245,21,0.7,1,0
160,They report a performance of 90.43 when using an architecture similar to ours .,Training and Implementation,Training and Implementation,named-entity-recognition,6,22,0.6875,159,0.75,22,0.7333333333333333,1,0
161,The two systems that have slightly higher F 1 scores on the CONLL dataset both use embeddings obtained from a forward and a backward Language Model trained on the One Billion Word Benchmark .,Training and Implementation,Training and Implementation,named-entity-recognition,6,23,0.71875,160,0.7547169811320755,23,0.7666666666666667,1,0
162,"They report gains between 0.8 and 1.2 points by using such LM embeddings , which suggests that LS vectors are indeed efficient .",Training and Implementation,Training and Implementation,named-entity-recognition,6,24,0.75,161,0.7594339622641509,24,0.8,1,0
163,"Unfortunately , due to time and resource constraints , 4 we were notable to measure whether both features complement each other .",Training and Implementation,Training and Implementation,named-entity-recognition,6,25,0.78125,162,0.7641509433962265,25,0.8333333333333334,1,0
164,This is left for future investigations .,Training and Implementation,Training and Implementation,named-entity-recognition,6,26,0.8125,163,0.7688679245283019,26,0.8666666666666667,1,0
165,reports the F 1 score of our system compared to the performance reported by others on the ONTONOTES test set .,Training and Implementation,Training and Implementation,named-entity-recognition,6,27,0.84375,164,0.7735849056603774,27,0.9,1,0
166,"To the best of our knowledge , we surpass previously reported F 1 scores on this dataset .",Training and Implementation,Training and Implementation,named-entity-recognition,6,28,0.875,165,0.7783018867924528,28,0.9333333333333332,1,0
167,"In particular , our system significantly outperforms the Bi - LSTM - CNN - CRF models of ( Chiu and Nichols , 2016 ) and by an absolute gain of 1.68 and 0.96 points respectively .",Training and Implementation,Training and Implementation,named-entity-recognition,6,29,0.90625,166,0.7830188679245284,29,0.9666666666666668,1,1
168,"Less surprisingly , it surpasses systems with hand - crafted features , including that use gazetteers , and the system of which uses coreference annotation in ONTONOTES to jointly model NER , entity linking , and coreference resolution tasks .",Training and Implementation,Training and Implementation,named-entity-recognition,6,30,0.9375,167,0.7877358490566038,30,1.0,1,1
169,Results on the Development Set,Training and Implementation,,named-entity-recognition,6,31,0.96875,168,0.7924528301886793,0,0.0,1,0
170,CONLL,Training and Implementation,,named-entity-recognition,6,32,1.0,169,0.7971698113207547,0,0.0,1,0
171,Results on CONLL,,,named-entity-recognition,6,0,0.0,170,0.8018867924528302,0,0.0,1,0
172,Results on ONTONOTES,,,named-entity-recognition,6,0,0.0,171,0.8066037735849056,0,0.0,1,0
173,Model,,,named-entity-recognition,6,0,0.0,172,0.8113207547169812,0,0.0,1,0
174,We also observe that models that use both feature sets significantly outperform other configurations .,Model,Model,named-entity-recognition,6,1,0.25,173,0.8160377358490566,1,0.25,1,1
175,"To confirm that the gains came from our feature vector and not from increasing the number of hidden units , we tested several SSKIP models by increasing the LSTM hidden layer dimension so that number of parameters is the same as the model with LS vectors .",Model,Model,named-entity-recognition,6,2,0.5,174,0.8207547169811321,2,0.5,1,0
176,"We observed a degradation of performance on both datasets , mostly due to overfitting on the training set .",Model,Model,named-entity-recognition,6,3,0.75,175,0.8254716981132075,3,0.75,1,0
177,"From those results , we conclude that our lexical representation and the SSKIP one are complementary .",Model,Model,named-entity-recognition,6,4,1.0,176,0.8301886792452831,4,1.0,1,0
178,Ablation Results,,,named-entity-recognition,6,0,0.0,177,0.8349056603773585,0,0.0,1,0
179,"In this experiment , we directly compare the LS representation with the SSKIP word - embedding feature set .",Ablation Results,Ablation Results,named-entity-recognition,6,1,0.125,178,0.839622641509434,1,0.125,1,0
180,"In order to maintain a high level of performance , both character and capitalization features are used in all configurations .",Ablation Results,Ablation Results,named-entity-recognition,6,2,0.25,179,0.8443396226415094,2,0.25,1,0
181,"We want to point out that LS vectors are not adapted during training , contrarily to the SSKIP embeddings .",Ablation Results,Ablation Results,named-entity-recognition,6,3,0.375,180,0.8490566037735849,3,0.375,1,0
182,"Similarly to Section 5.3 , we report in , for each feature configuration , the average F 1 score as well as the standard deviation over five runs .",Ablation Results,Ablation Results,named-entity-recognition,6,4,0.5,181,0.8537735849056604,4,0.5,1,0
183,"We observe that on both CONLL and ONTONOTES , the SSKIP model outperforms our feature vector approach by 0.65 F1 points on average .",Ablation Results,Ablation Results,named-entity-recognition,6,5,0.625,182,0.8584905660377359,5,0.625,1,1
184,"The difference is not has high as we first expected , especially since the SSKIP model is adjusted during training , while our representation is not .",Ablation Results,Ablation Results,named-entity-recognition,6,6,0.75,183,0.8632075471698113,6,0.75,1,0
185,"Still , LS vectors seem to encode a large portion of the information needed to model the NER task .",Ablation Results,Ablation Results,named-entity-recognition,6,7,0.875,184,0.8679245283018868,7,0.875,1,0
186,"Also , it is worth mentioning that our embeddings are trained on 1.3B words compared to 42B for SSKIP .",Ablation Results,Ablation Results,named-entity-recognition,6,8,1.0,185,0.8726415094339622,8,1.0,1,0
187,Related Works,,,named-entity-recognition,6,0,0.0,186,0.8773584905660378,0,0.0,1,0
188,"Traditional approaches to NER , like CRF - based and Perceptron - based systems ( Ratinov and Roth , 2009 ) have dominated the field for over a decade .",Related Works,Related Works,named-entity-recognition,6,1,0.0588235294117647,187,0.8820754716981132,1,0.0588235294117647,0,0
189,They rely heavily on hand - engineered features and external resources such as gazetteers .,Related Works,Related Works,named-entity-recognition,6,2,0.1176470588235294,188,0.8867924528301887,2,0.1176470588235294,0,0
190,One major drawback of such an approach is its weak generalization power .,Related Works,Related Works,named-entity-recognition,6,3,0.1764705882352941,189,0.8915094339622641,3,0.1764705882352941,0,0
191,"Current state - of - the art systems use a combination of Convolutional Neural Networks ( CNNs ) , Bi - LSTMs , along with a CRF decoder .",Related Works,Related Works,named-entity-recognition,6,4,0.2352941176470588,190,0.8962264150943396,4,0.2352941176470588,0,0
192,"CNNs are used to encode character - level features ( prefix and suffix ) , while LSTM is used to encode word - level features .",Related Works,Related Works,named-entity-recognition,6,5,0.2941176470588235,191,0.9009433962264152,5,0.2941176470588235,0,0
193,"Finally , a CRF is placed on top of those models in order to decode the best tag sequence .",Related Works,Related Works,named-entity-recognition,6,6,0.3529411764705882,192,0.9056603773584906,6,0.3529411764705882,0,0
194,Pre-trained embeddings obtained by unsupervised learning are core features of those models .,Related Works,Related Works,named-entity-recognition,6,7,0.4117647058823529,193,0.910377358490566,7,0.4117647058823529,0,0
195,"In this work , we show that deep NN architectures can also benefit from lexical features , at least when encoded in the compact form we propose .",Related Works,Related Works,named-entity-recognition,6,8,0.4705882352941176,194,0.9150943396226416,8,0.4705882352941176,0,0
196,and propose an alternative approach different from ours .,Related Works,Related Works,named-entity-recognition,6,9,0.5294117647058824,195,0.9198113207547168,9,0.5294117647058824,0,0
197,They incorporate LM embeddings that were pre-trained on a large unlabelled corpus as features for NER .,Related Works,Related Works,named-entity-recognition,6,10,0.5882352941176471,196,0.9245283018867924,10,0.5882352941176471,0,0
198,These embeddings allow to generate a representation fora word depending on its context .,Related Works,Related Works,named-entity-recognition,6,11,0.6470588235294118,197,0.929245283018868,11,0.6470588235294118,0,0
199,"For instance , the LM embeddings of the word France in "" France is a developed country "" is different than that in "" Anatole France began his literary career "" .",Related Works,Related Works,named-entity-recognition,6,12,0.7058823529411765,198,0.9339622641509434,12,0.7058823529411765,0,0
200,Such embeddings are trained on very large amount of texts .,Related Works,Related Works,named-entity-recognition,6,13,0.7647058823529411,199,0.9386792452830188,13,0.7647058823529411,0,0
201,"Our feature set is crafted from distant supervision applied to Wikipedia , a much less time - consuming process which we showed to be nevertheless adapted to rare words .",Related Works,Related Works,named-entity-recognition,6,14,0.8235294117647058,200,0.9433962264150944,14,0.8235294117647058,0,0
202,Chiu and Nichols ( 2016 ) used gazetteer features in order to establish state - of - the - art performance on both CONLL and ONTONOTES .,Related Works,Related Works,named-entity-recognition,6,15,0.8823529411764706,201,0.9481132075471698,15,0.8823529411764706,0,0
203,They mined DBPedia in order to compile 4 lists of named - entities that contain over 2.3 M entries .,Related Works,Related Works,named-entity-recognition,6,16,0.9411764705882352,202,0.9528301886792452,16,0.9411764705882352,0,0
204,We show that LS representations outperform their gazetteer features .,Related Works,Related Works,named-entity-recognition,6,17,1.0,203,0.9575471698113208,17,1.0,0,0
205,Conclusion and Future Work,,,named-entity-recognition,6,0,0.0,204,0.9622641509433962,0,0.0,1,0
206,We have explored the idea of generating lexical features for NER out of Wikipedia data automatically annotated with fine - grained entity types .,Conclusion and Future Work,Conclusion and Future Work,named-entity-recognition,6,1,0.1428571428571428,205,0.9669811320754716,1,0.1428571428571428,0,0
207,"We used WiFiNE , a Wikipedia dump annotated with fine entity type mentions , for training a vector space that jointly embeds words and named - entities .",Conclusion and Future Work,Conclusion and Future Work,named-entity-recognition,6,2,0.2857142857142857,206,0.9716981132075472,2,0.2857142857142857,0,0
208,"This vector space is used to compute a 120 dimensional vector per word , which encodes the similarity of the word to each of the entity types .",Conclusion and Future Work,Conclusion and Future Work,named-entity-recognition,6,3,0.4285714285714285,207,0.9764150943396226,3,0.4285714285714285,0,0
209,"Our results show that our proposed lexical representation , even though it is not adjusted at training time , matches state - of - the - art results compared to more complex approaches on the well - studied CONLL dataset , and delivers anew state - of - the - art F 1 score of 87.95 on the more diversified ONTONOTES dataset .",Conclusion and Future Work,Conclusion and Future Work,named-entity-recognition,6,4,0.5714285714285714,208,0.981132075471698,4,0.5714285714285714,0,0
210,We further observe larger gains on collections with more unfrequent words .,Conclusion and Future Work,Conclusion and Future Work,named-entity-recognition,6,5,0.7142857142857143,209,0.9858490566037736,5,0.7142857142857143,0,0
211,"The source code and the data we used in this work are publicly available at http://rali.iro. umontreal.ca/rali/en/wikipedia-lex-sim , with the hope that other researchers will report gains , when using our lexical representation .",Conclusion and Future Work,Conclusion and Future Work,named-entity-recognition,6,6,0.8571428571428571,210,0.9905660377358492,6,0.8571428571428571,0,0
212,"As a future work , we want to investigate the usefulness of our LS feature representation on other NER tasks , including NER in tweets where out - of - vocabulary and low - frequency words represent a challenge ; as well as finer - grained NER which suffers from the lack of manually annotated training data .",Conclusion and Future Work,Conclusion and Future Work,named-entity-recognition,6,7,1.0,211,0.9952830188679244,7,1.0,0,0
1,title,,,named-entity-recognition,7,0,0.0,0,0.0,0,0.0,1,0
2,A Neural Transition - based Model for Nested Mention Recognition,title,title,named-entity-recognition,7,1,0.0,1,0.0063291139240506,1,0.0,1,1
3,abstract,,,named-entity-recognition,7,0,0.0,2,0.0126582278481012,0,0.0,1,0
4,It is common that entity mentions can contain other mentions recursively .,abstract,abstract,named-entity-recognition,7,1,0.1428571428571428,3,0.0189873417721519,1,0.1428571428571428,1,0
5,This paper introduces a scalable transition - based method to model the nested structure of mentions .,abstract,abstract,named-entity-recognition,7,2,0.2857142857142857,4,0.0253164556962025,2,0.2857142857142857,1,1
6,We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest .,abstract,abstract,named-entity-recognition,7,3,0.4285714285714285,5,0.0316455696202531,3,0.4285714285714285,1,0
7,Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length .,abstract,abstract,named-entity-recognition,7,4,0.5714285714285714,6,0.0379746835443038,4,0.5714285714285714,1,0
8,"Based on Stack - LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space , our system is further incorporated with a character - based component to capture letterlevel patterns .",abstract,abstract,named-entity-recognition,7,5,0.7142857142857143,7,0.0443037974683544,5,0.7142857142857143,1,0
9,"Our model achieves the stateof - the - art results on ACE datasets , showing its effectiveness in detecting nested mentions .",abstract,abstract,named-entity-recognition,7,6,0.8571428571428571,8,0.050632911392405,6,0.8571428571428571,1,0
10,1,abstract,abstract,named-entity-recognition,7,7,1.0,9,0.0569620253164556,7,1.0,1,0
11,Introduction,,,named-entity-recognition,7,0,0.0,10,0.0632911392405063,0,0.0,1,0
12,"There has been an increasing interest in named entity recognition or more generally recognizing entity mentions 2 ) that the nested hierarchical structure of entity mentions should betaken into account to better facilitate downstream tasks like question answering , relation extraction , event extraction , and coreference resolution .",Introduction,Introduction,named-entity-recognition,7,1,0.0588235294117647,11,0.0696202531645569,1,0.0588235294117647,1,1
13,"Practically , the mentions with nested structures frequently exist in news and biomedical documents .",Introduction,Introduction,named-entity-recognition,7,2,0.1176470588235294,12,0.0759493670886076,2,0.1176470588235294,1,0
14,"For example in Traditional sequence labeling models such as conditional random fields ( CRF ) do not allow hierarchical structures between segments , making them incapable to handle such problems .",Introduction,Introduction,named-entity-recognition,7,3,0.1764705882352941,13,0.0822784810126582,3,0.1764705882352941,1,0
15,presented a chart - based parsing approach where each sentence with nested mentions is mapped to a rooted constituent tree .,Introduction,Introduction,named-entity-recognition,7,4,0.2352941176470588,14,0.0886075949367088,4,0.2352941176470588,1,0
16,The issue of using a chart - based parser is its cubic time complexity in the number of words in the sentence .,Introduction,Introduction,named-entity-recognition,7,5,0.2941176470588235,15,0.0949367088607595,5,0.2941176470588235,1,0
17,"To achieve a scalable and effective solution for recognizing nested mentions , we design a transition - based system which is inspired by the recent success of employing transition - based methods for constituent parsing ) and named entity recognition , especially when they are paired with neural networks .",Introduction,Introduction,named-entity-recognition,7,6,0.3529411764705882,16,0.1012658227848101,6,0.3529411764705882,1,1
18,"Generally , each sentence with nested mentions is mapped to a forest where each outermost mention forms a tree consisting of its inner mentions .",Introduction,Introduction,named-entity-recognition,7,7,0.4117647058823529,17,0.1075949367088607,7,0.4117647058823529,1,1
19,Then our transition - based system learns to construct this forest through a sequence of shift - reduce actions .,Introduction,Introduction,named-entity-recognition,7,8,0.4705882352941176,18,0.1139240506329113,8,0.4705882352941176,1,1
20,shows an example of such a forest .,Introduction,Introduction,named-entity-recognition,7,9,0.5294117647058824,19,0.120253164556962,9,0.5294117647058824,1,0
21,"In contrast , the tree structure by further uses a root node to connect all tree elements .",Introduction,Introduction,named-entity-recognition,7,10,0.5882352941176471,20,0.1265822784810126,10,0.5882352941176471,1,0
22,Our forest representation eliminates the root node so that the number of actions required to construct it can be reduced significantly .,Introduction,Introduction,named-entity-recognition,7,11,0.6470588235294118,21,0.1329113924050632,11,0.6470588235294118,1,0
23,"Following , we employ Stack - LSTM to represent the system 's state , which consists of the states of input , stack and action history , in a continuous space incrementally .",Introduction,Introduction,named-entity-recognition,7,12,0.7058823529411765,22,0.1392405063291139,12,0.7058823529411765,1,1
24,The ( partially ) processed nested mentions in the stack are encoded with recursive neural networks where composition functions are used to capture dependencies between nested mentions .,Introduction,Introduction,named-entity-recognition,7,13,0.7647058823529411,23,0.1455696202531645,13,0.7647058823529411,1,1
25,"Based on the observation that letter - level patterns such as capitalization and prefix can be beneficial in detecting mentions , we incorporate a characterlevel LSTM to capture such morphological information .",Introduction,Introduction,named-entity-recognition,7,14,0.8235294117647058,24,0.1518987341772152,14,0.8235294117647058,1,1
26,"Meanwhile , this character - level component can also help deal with the out - of - vocabulary problem of neural models .",Introduction,Introduction,named-entity-recognition,7,15,0.8823529411764706,25,0.1582278481012658,15,0.8823529411764706,1,1
27,We conduct experiments in three standard datasets .,Introduction,Introduction,named-entity-recognition,7,16,0.9411764705882352,26,0.1645569620253164,16,0.9411764705882352,1,0
28,Our system achieves the state - of - the - art performance on ACE datasets and comparable performance in GENIA dataset .,Introduction,Introduction,named-entity-recognition,7,17,1.0,27,0.170886075949367,17,1.0,1,0
29,Related Work,,,named-entity-recognition,7,0,0.0,28,0.1772151898734177,0,0.0,1,0
30,Entity mention recognition with nested structures has been explored first with rule - based approaches where the authors first detected the innermost mentions and then relied on rule - based postprocessing methods to identify outer mentions .,Related Work,Related Work,named-entity-recognition,7,1,0.0714285714285714,29,0.1835443037974683,1,0.0714285714285714,0,0
31,proposed a structured multi-label model to represent overlapping segments in a sentence .,Related Work,Related Work,named-entity-recognition,7,2,0.1428571428571428,30,0.189873417721519,2,0.1428571428571428,0,0
32,but it came with a cubic time complexity in the number of words .,Related Work,Related Work,named-entity-recognition,7,3,0.2142857142857142,31,0.1962025316455696,3,0.2142857142857142,0,0
33,proposed several ways to combine multiple conditional random fields ( CRF ) for such tasks .,Related Work,Related Work,named-entity-recognition,7,4,0.2857142857142857,32,0.2025316455696202,4,0.2857142857142857,0,0
34,Their best results were obtained by cascading several CRF models in a specific order while each model is responsible for detecting mentions of a particular type .,Related Work,Related Work,named-entity-recognition,7,5,0.3571428571428571,33,0.2088607594936709,5,0.3571428571428571,0,0
35,"However , such an approach can not model nested mentions of the same type , which frequently appear .",Related Work,Related Work,named-entity-recognition,7,6,0.4285714285714285,34,0.2151898734177215,6,0.4285714285714285,0,0
36,and proposed new representations of mention hypergraph and mention separator to model overlapping mentions .,Related Work,Related Work,named-entity-recognition,7,7,0.5,35,0.2215189873417721,7,0.5,0,0
37,"However , the nested structure is not guaranteed in such approaches since overlapping structures additionally include the crossing structures 3 , which rarely exist in practice .",Related Work,Related Work,named-entity-recognition,7,8,0.5714285714285714,36,0.2278481012658227,8,0.5714285714285714,0,0
38,"Also , their representations did not model the dependencies between nested mentions explicitly , which may limit their performance .",Related Work,Related Work,named-entity-recognition,7,9,0.6428571428571429,37,0.2341772151898734,9,0.6428571428571429,0,0
39,"In contrast , the chart - based parsing method can capture the dependencies between nested mentions with composition rules which allow an outer entity to be influenced by its contained entities .",Related Work,Related Work,named-entity-recognition,7,10,0.7142857142857143,38,0.240506329113924,10,0.7142857142857143,0,0
40,"However , their cubic time complexity makes them not scalable to large datasets .",Related Work,Related Work,named-entity-recognition,7,11,0.7857142857142857,39,0.2468354430379747,11,0.7857142857142857,0,0
41,"As neural network based approaches are proven effective in entity or mention recognition , recent efforts focus on incorporating neural components for recognizing nested mentions .",Related Work,Related Work,named-entity-recognition,7,12,0.8571428571428571,40,0.2531645569620253,12,0.8571428571428571,0,0
42,"dynamically stacked multiple LSTM - CRF layers , detecting mentions in an inside - out manner until no outer entities are extracted .",Related Work,Related Work,named-entity-recognition,7,13,0.9285714285714286,41,0.2594936708860759,13,0.9285714285714286,0,0
43,used recurrent neural networks to extract features fora hypergraph which encodes all nested mentions based on the BILOU tagging scheme .,Related Work,Related Work,named-entity-recognition,7,14,1.0,42,0.2658227848101265,14,1.0,0,0
44,Model,,,named-entity-recognition,7,0,0.0,43,0.2721518987341772,0,0.0,1,0
45,"Specifically , given a sequence of words {x 0 , x 1 , . . . , x n } , the goal of our system is to output a set of mentions where nested structures are allowed .",Model,Model,named-entity-recognition,7,1,0.25,44,0.2784810126582278,1,0.0344827586206896,1,0
46,"We use the forest structure to model the nested mentions scattered in a sentence , as shown in .",Model,Model,named-entity-recognition,7,2,0.5,45,0.2848101265822785,2,0.0689655172413793,1,0
47,The mapping is straightforward : each outermost mention forms a tree where the mention is the root and its contained mentions correspond to constituents of the tree .,Model,Model,named-entity-recognition,7,3,0.75,46,0.2911392405063291,3,0.1034482758620689,1,0
48,4,Model,Model,named-entity-recognition,7,4,1.0,47,0.2974683544303797,4,0.1379310344827586,1,0
49,Shift - Reduce System,,,named-entity-recognition,7,0,0.0,48,0.3037974683544304,5,0.1724137931034483,1,0
50,"Our transition - based model is based on the shiftreduce parser for constituency parsing ( Watan - abe and Sumita , 2015 ) , which adopts .",Shift - Reduce System,Shift - Reduce System,named-entity-recognition,7,1,0.0161290322580645,49,0.310126582278481,6,0.2068965517241379,1,0
51,"Generally , our system employs a stack to store ( partially ) processed nested elements .",Shift - Reduce System,Shift - Reduce System,named-entity-recognition,7,2,0.032258064516129,50,0.3164556962025316,7,0.2413793103448276,1,0
52,"The system 's state is defined as [ S , i , A ] which denotes stack , buffer front index and action history respectively .",Shift - Reduce System,Shift - Reduce System,named-entity-recognition,7,3,0.0483870967741935,51,0.3227848101265823,8,0.2758620689655172,1,0
53,In each step .,Shift - Reduce System,Shift - Reduce System,named-entity-recognition,7,4,0.064516129032258,52,0.3291139240506329,9,0.3103448275862069,1,0
54,an action is applied to change the system 's state .,Shift - Reduce System,Shift - Reduce System,named-entity-recognition,7,5,0.0806451612903225,53,0.3354430379746835,10,0.3448275862068966,1,0
55,"Our system consists of three types of transition actions , which are also summarized in :",Shift - Reduce System,Shift - Reduce System,named-entity-recognition,7,6,0.0967741935483871,54,0.3417721518987341,11,0.3793103448275862,1,0
56,SHIFT pushes the next word from buffer to the stack .,Shift - Reduce System,Shift - Reduce System,named-entity-recognition,7,7,0.1129032258064516,55,0.3481012658227848,12,0.4137931034482758,1,0
57,REDUCE - X pops the top two items t 0 and t 1 from the tack and combines them as anew tree element { X ? t 0 t 1 } which is then pushed onto the stack .,Shift - Reduce System,Shift - Reduce System,named-entity-recognition,7,8,0.1290322580645161,56,0.3544303797468354,13,0.4482758620689655,1,0
58,UNARY - X pops the top item t 0 from the stack and constructs anew tree element { X ? t 0 } which is pushed back to the stack .,Shift - Reduce System,Shift - Reduce System,named-entity-recognition,7,9,0.1451612903225806,57,0.360759493670886,14,0.4827586206896552,1,0
59,"Since the shift - reduce system assumes unary and binary branching , we binarize the trees in each forest in a left - branching manner .",Shift - Reduce System,Shift - Reduce System,named-entity-recognition,7,10,0.1612903225806451,58,0.3670886075949367,15,0.5172413793103449,1,0
60,"For example , if three consecutive words A , B , C are annotated as Person , we convert it into a binary tree { P erson ?",Shift - Reduce System,Shift - Reduce System,named-entity-recognition,7,11,0.1774193548387097,59,0.3734177215189873,16,0.5517241379310345,1,0
61,{ P erson * ?,Shift - Reduce System,Shift - Reduce System,named-entity-recognition,7,12,0.1935483870967742,60,0.379746835443038,17,0.5862068965517241,1,0
62,"A , B} , C} where P erson * is a temporary label for P erson .",Shift - Reduce System,Shift - Reduce System,named-entity-recognition,7,13,0.2096774193548387,61,0.3860759493670886,18,0.6206896551724138,1,0
63,"Hence , the X in reduce - actions will also include such temporary labels .",Shift - Reduce System,Shift - Reduce System,named-entity-recognition,7,14,0.2258064516129032,62,0.3924050632911392,19,0.6551724137931034,1,0
64,"Note that since most words are not contained in any mention , they are only shifted to the stack and not involved in any reduce - or unary - actions .",Shift - Reduce System,Shift - Reduce System,named-entity-recognition,7,15,0.2419354838709677,63,0.3987341772151899,20,0.6896551724137931,1,0
65,An example sequence of transitions can be found in .,Shift - Reduce System,Shift - Reduce System,named-entity-recognition,7,16,0.2580645161290322,64,0.4050632911392405,21,0.7241379310344828,1,0
66,Our shift - reduce system is different from previous parsers in terms of the terminal state .,Shift - Reduce System,Shift - Reduce System,named-entity-recognition,7,17,0.2741935483870967,65,0.4113924050632911,22,0.7586206896551724,1,0
67,1 ) It does not require the terminal stack to be a rooted tree .,Shift - Reduce System,Shift - Reduce System,named-entity-recognition,7,18,0.2903225806451613,66,0.4177215189873418,23,0.7931034482758621,1,0
68,"Instead , the final stack should be a forest consisting of multiple nested elements with tree structures .",Shift - Reduce System,Shift - Reduce System,named-entity-recognition,7,19,0.3064516129032258,67,0.4240506329113924,24,0.8275862068965517,1,0
69,"2 ) To conveniently determine the ending of our transition process , we add an auxiliary symbol $ to each sentence .",Shift - Reduce System,Shift - Reduce System,named-entity-recognition,7,20,0.3225806451612903,68,0.430379746835443,25,0.8620689655172413,1,0
70,"Once it is pushed to the stack , it implies that all deductions of actual words are finished .",Shift - Reduce System,Shift - Reduce System,named-entity-recognition,7,21,0.3387096774193548,69,0.4367088607594936,26,0.896551724137931,1,0
71,Since we do not allow unary rules between labels like X1 ?,Shift - Reduce System,Shift - Reduce System,named-entity-recognition,7,22,0.3548387096774194,70,0.4430379746835443,27,0.9310344827586208,1,0
72,"X2 , the length of maximal action sequence is 3 n .",Shift - Reduce System,Shift - Reduce System,named-entity-recognition,7,23,0.3709677419354839,71,0.4493670886075949,28,0.9655172413793104,1,0
73,5,Shift - Reduce System,Shift - Reduce System,named-entity-recognition,7,24,0.3870967741935484,72,0.4556962025316455,29,1.0,1,0
74,Action Constraints,Shift - Reduce System,,named-entity-recognition,7,25,0.4032258064516129,73,0.4620253164556962,0,0.0,1,0
75,"To make sure that each action sequence is valid , we need to make some hard constraints on the ac - 5",Shift - Reduce System,Action Constraints,named-entity-recognition,7,26,0.4193548387096774,74,0.4683544303797468,1,0.0833333333333333,1,0
76,"In this case , each word is shifted ( n ) and involved in a unary action ( n ) .",Shift - Reduce System,Action Constraints,named-entity-recognition,7,27,0.4354838709677419,75,0.4746835443037974,2,0.1666666666666666,1,0
77,Then all elements are reduced to a single node ( n ? 1 ) .,Shift - Reduce System,Action Constraints,named-entity-recognition,7,28,0.4516129032258064,76,0.4810126582278481,3,0.25,1,0
78,The last action is to shift the symbol $. tion to take .,Shift - Reduce System,Action Constraints,named-entity-recognition,7,29,0.4677419354838709,77,0.4873417721518987,4,0.3333333333333333,1,0
79,"For example , reduce - action can only be conducted when there are at least two elements in the stack .",Shift - Reduce System,Action Constraints,named-entity-recognition,7,30,0.4838709677419355,78,0.4936708860759494,5,0.4166666666666667,1,0
80,Please seethe Appendix for the full list of restrictions .,Shift - Reduce System,Action Constraints,named-entity-recognition,7,31,0.5,79,0.5,6,0.5,1,0
81,"Formally , we use V(S , i , A ) to denote the valid actions given the parser state .",Shift - Reduce System,Action Constraints,named-entity-recognition,7,32,0.5161290322580645,80,0.5063291139240507,7,0.5833333333333334,1,0
82,Let us denote the feature vector for the parser state at time step k asp k .,Shift - Reduce System,Action Constraints,named-entity-recognition,7,33,0.532258064516129,81,0.5126582278481012,8,0.6666666666666666,1,0
83,The distribution of actions is computed as follows :,Shift - Reduce System,Action Constraints,named-entity-recognition,7,34,0.5483870967741935,82,0.5189873417721519,9,0.75,1,0
84,"( 1 ) where w z is a column weight vector for action z , and b z is a bias term .",Shift - Reduce System,Action Constraints,named-entity-recognition,7,35,0.5645161290322581,83,0.5253164556962026,10,0.8333333333333334,1,0
85,Neural Transition - based Model,Shift - Reduce System,Action Constraints,named-entity-recognition,7,36,0.5806451612903226,84,0.5316455696202531,11,0.9166666666666666,1,0
86,"We use neural networks to learn the representation of the parser state , which is pk in ( 1 ) .",Shift - Reduce System,Action Constraints,named-entity-recognition,7,37,0.5967741935483871,85,0.5379746835443038,12,1.0,1,0
87,Representation of Words,Shift - Reduce System,,named-entity-recognition,7,38,0.6129032258064516,86,0.5443037974683544,0,0.0,1,0
88,Words are represented by concatenating three vectors :,Shift - Reduce System,Representation of Words,named-entity-recognition,7,39,0.6290322580645161,87,0.5506329113924051,1,0.25,1,0
89,where e w i and e pi denote the embeddings for i - th word and it s POS tag respectively .,Shift - Reduce System,Representation of Words,named-entity-recognition,7,40,0.6451612903225806,88,0.5569620253164557,2,0.5,1,0
90,cw i denotes the representation learned by a character - level model using a bidirectional LSTM .,Shift - Reduce System,Representation of Words,named-entity-recognition,7,41,0.6612903225806451,89,0.5632911392405063,3,0.75,1,0
91,"Specifically , for character sequence s 0 , s 1 , . . . , s n in the i - th word , we use the last hidden states of forward and backward LSTM as the character - based representation of this word , as shown below :",Shift - Reduce System,Representation of Words,named-entity-recognition,7,42,0.6774193548387096,90,0.569620253164557,4,1.0,1,0
92,Representation of Parser States,Shift - Reduce System,,named-entity-recognition,7,43,0.6935483870967742,91,0.5759493670886076,0,0.0,1,0
93,"Generally , the buffer and action history are encoded using two vanilla LSTMs .",Shift - Reduce System,Representation of Parser States,named-entity-recognition,7,44,0.7096774193548387,92,0.5822784810126582,1,0.0416666666666666,1,0
94,"For the stack that involves popping out top elements , we use the Stack - LSTM to efficiently encode it .",Shift - Reduce System,Representation of Parser States,named-entity-recognition,7,45,0.7258064516129032,93,0.5886075949367089,2,0.0833333333333333,1,0
95,"Formally , if the unprocessed word sequence in the buffer is x i , x i +1 , . . . , x n and action history sequence is a 0 , a 1 , . . . , a k?1 , then we can compute buffer representation bk and action history representation a k at time step k as follows :",Shift - Reduce System,Representation of Parser States,named-entity-recognition,7,46,0.7419354838709677,94,0.5949367088607594,3,0.125,1,0
96,where each action is also mapped to a distributed representation ea .,Shift - Reduce System,Representation of Parser States,named-entity-recognition,7,47,0.7580645161290323,95,0.6012658227848101,4,0.1666666666666666,1,0
97,"6 For the state of the stack , we also use an LSTM to encode a sequence of tree elements .",Shift - Reduce System,Representation of Parser States,named-entity-recognition,7,48,0.7741935483870968,96,0.6075949367088608,5,0.2083333333333333,1,0
98,"However , the top elements of the stack are updated frequently .",Shift - Reduce System,Representation of Parser States,named-entity-recognition,7,49,0.7903225806451613,97,0.6139240506329114,6,0.25,1,0
99,Stack - LSTM provides an efficient implementation that incorporates a stackpointer .,Shift - Reduce System,Representation of Parser States,named-entity-recognition,7,50,0.8064516129032258,98,0.620253164556962,7,0.2916666666666667,1,0
100,7,Shift - Reduce System,Representation of Parser States,named-entity-recognition,7,51,0.8225806451612904,99,0.6265822784810127,8,0.3333333333333333,1,0
101,"Formally , the state of the stack bk at time step k is computed as :",Shift - Reduce System,Representation of Parser States,named-entity-recognition,7,52,0.8387096774193549,100,0.6329113924050633,9,0.375,1,0
102,"where ht i denotes the representation of the i - th tree element from the top , which can be computed recursively similar to Recursive Neural Network as follows :",Shift - Reduce System,Representation of Parser States,named-entity-recognition,7,53,0.8548387096774194,101,0.6392405063291139,10,0.4166666666666667,1,0
103,"where W u , l and W b , l denote the weight matrices for unary ( u ) and binary ( b ) composition with parent node being label ( l ) .",Shift - Reduce System,Representation of Parser States,named-entity-recognition,7,54,0.8709677419354839,102,0.6455696202531646,11,0.4583333333333333,1,0
104,Note that the composition function is distinct for each label l .,Shift - Reduce System,Representation of Parser States,named-entity-recognition,7,55,0.8870967741935484,103,0.6518987341772152,12,0.5,1,0
105,Recall that the leaf nodes of each tree element are raw words .,Shift - Reduce System,Representation of Parser States,named-entity-recognition,7,56,0.9032258064516128,104,0.6582278481012658,13,0.5416666666666666,1,0
106,"Instead of representing them with their original embeddings introduced in Section 3.3 , we found that 6 Note that LSTM b runs in a right - to - left order such that the output can represent the contextual information of x i.",Shift - Reduce System,Representation of Parser States,named-entity-recognition,7,57,0.9193548387096774,105,0.6645569620253164,14,0.5833333333333334,1,0
107,7,Shift - Reduce System,Representation of Parser States,named-entity-recognition,7,58,0.935483870967742,106,0.6708860759493671,15,0.625,1,0
108,Please refer to for details .,Shift - Reduce System,Representation of Parser States,named-entity-recognition,7,59,0.9516129032258064,107,0.6772151898734177,16,0.6666666666666666,1,0
109,concatenating the buffer state in ( 5 ) are beneficial during our initial experiments .,Shift - Reduce System,Representation of Parser States,named-entity-recognition,7,60,0.967741935483871,108,0.6835443037974683,17,0.7083333333333334,1,0
110,"Formally , when a word xi is shifted to the stack at time step k , it s representation is computed as :",Shift - Reduce System,Representation of Parser States,named-entity-recognition,7,61,0.9838709677419356,109,0.689873417721519,18,0.75,1,0
111,"Finally , the state of the system pk is the concatenation of the states of buffer , stack and action history :",Shift - Reduce System,Representation of Parser States,named-entity-recognition,7,62,1.0,110,0.6962025316455697,19,0.7916666666666666,1,0
112,Training,,,named-entity-recognition,7,0,0.0,111,0.7025316455696202,20,0.8333333333333334,1,0
113,We employ the greedy strategy to maximize the log -likelihood of the local action classifier in ( 1 ) .,Training,Training,named-entity-recognition,7,1,0.25,112,0.7088607594936709,21,0.875,1,0
114,"Specifically , let z ik denote the k - th action for the i - th sentence , the loss function with 2 norm is :",Training,Training,named-entity-recognition,7,2,0.5,113,0.7151898734177216,22,0.9166666666666666,1,0
115,where ?,Training,Training,named-entity-recognition,7,3,0.75,114,0.7215189873417721,23,0.9583333333333334,1,0
116,is the 2 coefficient .,Training,Training,named-entity-recognition,7,4,1.0,115,0.7278481012658228,24,1.0,1,0
117,Experiments,,,named-entity-recognition,7,0,0.0,116,0.7341772151898734,0,0.0,1,0
118,"We mainly evaluate our models on the standard ACE - 04 , , and GENIA datasets with the same splits used by previous research efforts .",Experiments,Experiments,named-entity-recognition,7,1,0.0769230769230769,117,0.740506329113924,1,0.25,1,0
119,"In ACE datasets , more than 40 % of the mentions form nested structures with some other mention .",Experiments,Experiments,named-entity-recognition,7,2,0.1538461538461538,118,0.7468354430379747,2,0.5,1,0
120,"In GENIA , this number is 18 % .",Experiments,Experiments,named-entity-recognition,7,3,0.2307692307692307,119,0.7531645569620253,3,0.75,1,0
121,Please see for the full statistics .,Experiments,Experiments,named-entity-recognition,7,4,0.3076923076923077,120,0.759493670886076,4,1.0,1,0
122,Setup,Experiments,,named-entity-recognition,7,5,0.3846153846153846,121,0.7658227848101266,0,0.0,1,0
123,Pre-trained embeddings,Experiments,Setup,named-entity-recognition,7,6,0.4615384615384615,122,0.7721518987341772,1,0.125,1,1
124,Glo Ve of dimension 100 are used to initialize the word vectors for all three datasets .,Experiments,Setup,named-entity-recognition,7,7,0.5384615384615384,123,0.7784810126582279,2,0.25,1,1
125,9 The embeddings of POS tags are initialized randomly with dimension 32 .,Experiments,Setup,named-entity-recognition,7,8,0.6153846153846154,124,0.7848101265822784,3,0.375,1,1
126,The model is trained using Adam and a gradient clipping of 3.0 .,Experiments,Setup,named-entity-recognition,7,9,0.6923076923076923,125,0.7911392405063291,4,0.5,1,1
127,Early stopping is used based on the performance of development sets .,Experiments,Setup,named-entity-recognition,7,10,0.7692307692307693,126,0.7974683544303798,5,0.625,1,0
128,Dropout is used after the input layer .,Experiments,Setup,named-entity-recognition,7,11,0.8461538461538461,127,0.8037974683544303,6,0.75,1,0
129,The 2 coefficient ?,Experiments,Setup,named-entity-recognition,7,12,0.9230769230769232,128,0.810126582278481,7,0.875,1,0
130,is also tuned during development process .,Experiments,Setup,named-entity-recognition,7,13,1.0,129,0.8164556962025317,8,1.0,1,0
131,Results,,,named-entity-recognition,7,0,0.0,130,0.8227848101265823,0,0.0,1,0
132,The main results are reported in .,Results,Results,named-entity-recognition,7,1,0.0666666666666666,131,0.8291139240506329,1,0.25,1,0
133,Our neural transition - based model achieves the best results in ACE datasets and comparable results in GENIA dataset in terms of F 1 measure .,Results,Results,named-entity-recognition,7,2,0.1333333333333333,132,0.8354430379746836,2,0.5,1,1
134,We hypothesize that the performance gain of our model compared with other methods is largely due to improved performance on the portions of nested mentions in our datasets .,Results,Results,named-entity-recognition,7,3,0.2,133,0.8417721518987342,3,0.75,1,0
135,"To verify this , we design an experiment to evaluate how well a system can recognize nested mentions .",Results,Results,named-entity-recognition,7,4,0.2666666666666666,134,0.8481012658227848,4,1.0,1,0
136,Handling Nested Mentions,Results,,named-entity-recognition,7,5,0.3333333333333333,135,0.8544303797468354,0,0.0,1,0
137,The idea is that we split the test data into two portions : sentences with and without nested mentions .,Results,Handling Nested Mentions,named-entity-recognition,7,6,0.4,136,0.8607594936708861,1,0.2,1,0
138,The results of GENIA are listed in .,Results,Handling Nested Mentions,named-entity-recognition,7,7,0.4666666666666667,137,0.8670886075949367,2,0.4,1,0
139,"We can observe that the margin of improvement is more significant in the portion of nested mentions , revealing our model 's effectiveness in handling nested mentions .",Results,Handling Nested Mentions,named-entity-recognition,7,8,0.5333333333333333,138,0.8734177215189873,3,0.6,1,1
140,This observation helps explain why our model achieves greater improvement in ACE than in GENIA in since the former has much more nested structures than the latter .,Results,Handling Nested Mentions,named-entity-recognition,7,9,0.6,139,0.879746835443038,4,0.8,1,0
141,"Moreover , performs better when it comes to non-nested mentions possibly due to the CRF they used , which globally normalizes each stacked layer .",Results,Handling Nested Mentions,named-entity-recognition,7,10,0.6666666666666666,140,0.8860759493670886,5,1.0,1,0
142,Decoding Speed,Results,,named-entity-recognition,7,11,0.7333333333333333,141,0.8924050632911392,0,0.0,1,0
143,"Note that and also feature linear - time complexity , but with a greater constant factor .",Results,Decoding Speed,named-entity-recognition,7,12,0.8,142,0.8987341772151899,1,0.25,1,0
144,"To compare the decoding speed , we re-implemented their model with the same platform ( PyTorch ) and run them on the same machine ( CPU : Intel i5 2.7 GHz ) .",Results,Decoding Speed,named-entity-recognition,7,13,0.8666666666666667,143,0.9050632911392406,2,0.5,1,0
145,"Our model turns out to be around 3 - 5 times faster than theirs , showing its scalability .",Results,Decoding Speed,named-entity-recognition,7,14,0.9333333333333332,144,0.9113924050632912,3,0.75,1,0
146,We also additionally tried using embeddings trained on PubMed for GENIA but the performance was comparable .,Results,Decoding Speed,named-entity-recognition,7,15,1.0,145,0.9177215189873418,4,1.0,1,0
147,Ablation Study,,,named-entity-recognition,7,0,0.0,146,0.9240506329113924,0,0.0,1,0
148,"To evaluate the contribution of neural components including pre-trained embeddings , the characterlevel LSTM and dropout layers , we test the performances of ablated models .",Ablation Study,Ablation Study,named-entity-recognition,7,1,0.3333333333333333,147,0.930379746835443,1,0.3333333333333333,1,1
149,The results are listed in .,Ablation Study,Ablation Study,named-entity-recognition,7,2,0.6666666666666666,148,0.9367088607594936,2,0.6666666666666666,1,0
150,"From the performance gap , we can conclude that these components contribute significantly to the effectiveness of our model in all three datasets .",Ablation Study,Ablation Study,named-entity-recognition,7,3,1.0,149,0.9430379746835444,3,1.0,1,1
151,Conclusion and Future Work,,,named-entity-recognition,7,0,0.0,150,0.9493670886075948,0,0.0,1,0
152,"In this paper , we present a transition - based model for nested mention recognition using a forest representation .",Conclusion and Future Work,Conclusion and Future Work,named-entity-recognition,7,1,0.1428571428571428,151,0.9556962025316456,1,0.1428571428571428,0,0
153,"Coupled with Stack - LSTM for representing the system 's state , our neural model can capture dependencies between nested mentions efficiently .",Conclusion and Future Work,Conclusion and Future Work,named-entity-recognition,7,2,0.2857142857142857,152,0.9620253164556962,2,0.2857142857142857,0,0
154,"Moreover , the character - based component helps capture letter - level patterns in words .",Conclusion and Future Work,Conclusion and Future Work,named-entity-recognition,7,3,0.4285714285714285,153,0.9683544303797468,3,0.4285714285714285,0,0
155,The system achieves the state - of - the - art performance in ACE datasets .,Conclusion and Future Work,Conclusion and Future Work,named-entity-recognition,7,4,0.5714285714285714,154,0.9746835443037974,4,0.5714285714285714,0,0
156,One potential drawback of the system is the greedy training and decoding .,Conclusion and Future Work,Conclusion and Future Work,named-entity-recognition,7,5,0.7142857142857143,155,0.981012658227848,5,0.7142857142857143,0,0
157,We believe that alternatives like beam search and training with exploration could further boost the performance .,Conclusion and Future Work,Conclusion and Future Work,named-entity-recognition,7,6,0.8571428571428571,156,0.9873417721518988,6,0.8571428571428571,0,0
158,Another direction that we plan to work on is to apply this model to recognizing overlapping and entities that involve discontinuous spans ( Muis and which frequently exist in the biomedical domain .,Conclusion and Future Work,Conclusion and Future Work,named-entity-recognition,7,7,1.0,157,0.9936708860759492,7,1.0,0,0
1,title,,,named-entity-recognition,8,0,0.0,0,0.0,0,0.0,1,0
2,BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding,title,title,named-entity-recognition,8,1,0.0,1,0.0025839793281653,1,0.0,1,1
3,abstract,,,named-entity-recognition,8,0,0.0,2,0.0051679586563307,0,0.0,1,0
4,"We introduce anew language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers .",abstract,abstract,named-entity-recognition,8,1,0.1111111111111111,3,0.0077519379844961,1,0.1111111111111111,1,1
5,"Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers .",abstract,abstract,named-entity-recognition,8,2,0.2222222222222222,4,0.0103359173126615,2,0.2222222222222222,1,0
6,"As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models fora wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications .",abstract,abstract,named-entity-recognition,8,3,0.3333333333333333,5,0.0129198966408268,3,0.3333333333333333,1,0
7,BERT is conceptually simple and empirically powerful .,abstract,abstract,named-entity-recognition,8,4,0.4444444444444444,6,0.0155038759689922,4,0.4444444444444444,1,0
8,"It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) .",abstract,abstract,named-entity-recognition,8,5,0.5555555555555556,7,0.0180878552971576,5,0.5555555555555556,1,0
9,Jeremy Howard and Sebastian Ruder . 2018 .,abstract,abstract,named-entity-recognition,8,6,0.6666666666666666,8,0.020671834625323,6,0.6666666666666666,1,0
10,Universal language model fine - tuning for text classification .,abstract,abstract,named-entity-recognition,8,7,0.7777777777777778,9,0.0232558139534883,7,0.7777777777777778,1,0
11,In ACL .,abstract,abstract,named-entity-recognition,8,8,0.8888888888888888,10,0.0258397932816537,8,0.8888888888888888,1,0
12,Association for Computational Linguistics .,abstract,,named-entity-recognition,8,9,1.0,11,0.0284237726098191,9,1.0,1,0
13,Introduction,,,named-entity-recognition,8,0,0.0,12,0.0310077519379844,0,0.0,1,0
14,Language model pre-training has been shown to be effective for improving many natural language processing tasks .,Introduction,Introduction,named-entity-recognition,8,1,0.0416666666666666,13,0.0335917312661498,1,0.0416666666666666,1,1
15,"These include sentence - level tasks such as natural language inference and paraphrasing , which aim to predict the relationships between sentences by analyzing them holistically , as well as token - level tasks such as named entity recognition and question answering , where models are required to produce fine - grained output at the token level .",Introduction,Introduction,named-entity-recognition,8,2,0.0833333333333333,14,0.0361757105943152,2,0.0833333333333333,1,0
16,There are two existing strategies for applying pre-trained language representations to downstream tasks : feature - based and fine - tuning .,Introduction,Introduction,named-entity-recognition,8,3,0.125,15,0.0387596899224806,3,0.125,1,0
17,"The feature - based approach , such as ELMo , uses task - specific architectures that include the pre-trained representations as additional features .",Introduction,Introduction,named-entity-recognition,8,4,0.1666666666666666,16,0.041343669250646,4,0.1666666666666666,1,0
18,"The fine - tuning approach , such as the Generative Pre-trained Transformer ( OpenAI GPT ) , introduces minimal task - specific parameters , and is trained on the downstream tasks by simply fine - tuning all pretrained parameters .",Introduction,Introduction,named-entity-recognition,8,5,0.2083333333333333,17,0.0439276485788113,5,0.2083333333333333,1,0
19,"The two approaches share the same objective function during pre-training , where they use unidirectional language models to learn general language representations .",Introduction,Introduction,named-entity-recognition,8,6,0.25,18,0.0465116279069767,6,0.25,1,0
20,"We argue that current techniques restrict the power of the pre-trained representations , especially for the fine - tuning approaches .",Introduction,Introduction,named-entity-recognition,8,7,0.2916666666666667,19,0.0490956072351421,7,0.2916666666666667,1,0
21,"The major limitation is that standard language models are unidirectional , and this limits the choice of architectures that can be used during pre-training .",Introduction,Introduction,named-entity-recognition,8,8,0.3333333333333333,20,0.0516795865633074,8,0.3333333333333333,1,0
22,"For example , in Open AI GPT , the authors use a left - toright architecture , where every token can only attend to previous tokens in the self - attention layers of the Transformer .",Introduction,Introduction,named-entity-recognition,8,9,0.375,21,0.0542635658914728,9,0.375,1,0
23,"Such restrictions are sub-optimal for sentence - level tasks , and could be very harmful when applying finetuning based approaches to token - level tasks such as question answering , where it is crucial to incorporate context from both directions .",Introduction,Introduction,named-entity-recognition,8,10,0.4166666666666667,22,0.0568475452196382,10,0.4166666666666667,1,0
24,"In this paper , we improve the fine - tuning based approaches by proposing BERT : Bidirectional Encoder Representations from Transformers .",Introduction,Introduction,named-entity-recognition,8,11,0.4583333333333333,23,0.0594315245478036,11,0.4583333333333333,1,1
25,"BERT alleviates the previously mentioned unidirectionality constraint by using a "" masked language model "" ( MLM ) pre-training objective , inspired by the Cloze task .",Introduction,Introduction,named-entity-recognition,8,12,0.5,24,0.0620155038759689,12,0.5,1,1
26,"The masked language model randomly masks some of the tokens from the input , and the objective is to predict the original vocabulary id of the masked word based only on its context .",Introduction,Introduction,named-entity-recognition,8,13,0.5416666666666666,25,0.0645994832041343,13,0.5416666666666666,1,1
27,"Unlike left - toright language model pre-training , the MLM objective enables the representation to fuse the left and the right context , which allows us to pretrain a deep bidirectional Transformer .",Introduction,Introduction,named-entity-recognition,8,14,0.5833333333333334,26,0.0671834625322997,14,0.5833333333333334,1,1
28,"In addition to the masked language model , we also use a "" next sentence prediction "" task that jointly pretrains text - pair representations .",Introduction,Introduction,named-entity-recognition,8,15,0.625,27,0.0697674418604651,15,0.625,1,1
29,The contributions of our paper are as follows :,Introduction,Introduction,named-entity-recognition,8,16,0.6666666666666666,28,0.0723514211886304,16,0.6666666666666666,1,0
30,We demonstrate the importance of bidirectional pre-training for language representations .,Introduction,Introduction,named-entity-recognition,8,17,0.7083333333333334,29,0.0749354005167958,17,0.7083333333333334,1,0
31,"Unlike , which uses unidirectional language models for pre-training , BERT uses masked language models to enable pretrained deep bidirectional representations .",Introduction,Introduction,named-entity-recognition,8,18,0.75,30,0.0775193798449612,18,0.75,1,0
32,"This is also in contrast to , which uses a shallow concatenation of independently trained left - to - right and right - to - left LMs .",Introduction,Introduction,named-entity-recognition,8,19,0.7916666666666666,31,0.0801033591731266,19,0.7916666666666666,1,0
33,We show that pre-trained representations reduce the need for many heavily - engineered taskspecific architectures .,Introduction,Introduction,named-entity-recognition,8,20,0.8333333333333334,32,0.082687338501292,20,0.8333333333333334,1,0
34,"BERT is the first finetuning based representation model that achieves state - of - the - art performance on a large suite of sentence - level and token - level tasks , outperforming many task - specific architectures .",Introduction,Introduction,named-entity-recognition,8,21,0.875,33,0.0852713178294573,21,0.875,1,0
35,BERT advances the state of the art for eleven NLP tasks .,Introduction,Introduction,named-entity-recognition,8,22,0.9166666666666666,34,0.0878552971576227,22,0.9166666666666666,1,0
36,The code and pre-trained models are available at https://github.com/,Introduction,Introduction,named-entity-recognition,8,23,0.9583333333333334,35,0.0904392764857881,23,0.9583333333333334,1,0
37,google-research/bert .,Introduction,Introduction,named-entity-recognition,8,24,1.0,36,0.0930232558139534,24,1.0,1,0
38,Related Work,,,named-entity-recognition,8,0,0.0,37,0.0956072351421188,0,0.0,1,0
39,"There is along history of pre-training general language representations , and we briefly review the most widely - used approaches in this section .",Related Work,Related Work,named-entity-recognition,8,1,0.0714285714285714,38,0.0981912144702842,1,0.0357142857142857,0,0
40,Unsupervised Feature - based Approaches,Related Work,Related Work,named-entity-recognition,8,2,0.1428571428571428,39,0.1007751937984496,2,0.0714285714285714,0,0
41,"Learning widely applicable representations of words has been an active area of research for decades , including non-neural and neural methods .",Related Work,Related Work,named-entity-recognition,8,3,0.2142857142857142,40,0.1033591731266149,3,0.1071428571428571,0,0
42,"Pre-trained word embeddings are an integral part of modern NLP systems , offering significant improvements over embeddings learned from scratch .",Related Work,Related Work,named-entity-recognition,8,4,0.2857142857142857,41,0.1059431524547803,4,0.1428571428571428,0,0
43,"To pretrain word embedding vectors , left - to - right language modeling objectives have been used , as well as objectives to discriminate correct from incorrect words in left and right context .",Related Work,Related Work,named-entity-recognition,8,5,0.3571428571428571,42,0.1085271317829457,5,0.1785714285714285,0,0
44,"These approaches have been generalized to coarser granularities , such as sentence embeddings or paragraph embeddings .",Related Work,Related Work,named-entity-recognition,8,6,0.4285714285714285,43,0.1111111111111111,6,0.2142857142857142,0,0
45,"To train sentence representations , prior work has used objectives to rank candidate next sentences , left - to - right generation of next sentence words given a representation of the previous sentence , or denoising autoencoder derived objectives .",Related Work,Related Work,named-entity-recognition,8,7,0.5,44,0.1136950904392764,7,0.25,0,0
46,ELMo and its predecessor generalize traditional word embedding research along a different dimension .,Related Work,Related Work,named-entity-recognition,8,8,0.5714285714285714,45,0.1162790697674418,8,0.2857142857142857,0,0
47,They extract context - sensitive features from a left - to - right and a right - to - left language model .,Related Work,Related Work,named-entity-recognition,8,9,0.6428571428571429,46,0.1188630490956072,9,0.3214285714285714,0,0
48,The contextual representation of each token is the concatenation of the left - to - right and right - to - left representations .,Related Work,Related Work,named-entity-recognition,8,10,0.7142857142857143,47,0.1214470284237726,10,0.3571428571428571,0,0
49,"When integrating contextual word embeddings with existing task - specific architectures , ELMo advances the state of the art for several major NLP benchmarks including question answering , sentiment analysis , and named entity recognition .",Related Work,Related Work,named-entity-recognition,8,11,0.7857142857142857,48,0.1240310077519379,11,0.3928571428571428,0,0
50,proposed learning contextual representations through a task to predict a single word from both left and right context using LSTMs .,Related Work,Related Work,named-entity-recognition,8,12,0.8571428571428571,49,0.1266149870801033,12,0.4285714285714285,0,0
51,"Similar to ELMo , their model is feature - based and not deeply bidirectional .",Related Work,Related Work,named-entity-recognition,8,13,0.9285714285714286,50,0.1291989664082687,13,0.4642857142857143,0,0
52,shows that the cloze task can be used to improve the robustness of text generation models .,Related Work,Related Work,named-entity-recognition,8,14,1.0,51,0.1317829457364341,14,0.5,0,0
53,Unsupervised Fine- tuning Approaches,,,named-entity-recognition,8,0,0.0,52,0.1343669250645995,15,0.5357142857142857,1,0
54,"As with the feature - based approaches , the first works in this direction only pre-trained word embedding parameters from unlabeled text .",Unsupervised Fine- tuning Approaches,Unsupervised Fine- tuning Approaches,named-entity-recognition,8,1,0.04,53,0.1369509043927648,16,0.5714285714285714,1,0
55,"More recently , sentence or document encoders which produce contextual token representations have been pre-trained from unlabeled text and fine - tuned fora supervised downstream task .",Unsupervised Fine- tuning Approaches,Unsupervised Fine- tuning Approaches,named-entity-recognition,8,2,0.08,54,0.1395348837209302,17,0.6071428571428571,1,0
56,The advantage of these approaches is that few parameters need to be learned from scratch .,Unsupervised Fine- tuning Approaches,Unsupervised Fine- tuning Approaches,named-entity-recognition,8,3,0.12,55,0.1421188630490956,18,0.6428571428571429,1,0
57,"At least partly due to this advantage , OpenAI achieved previously state - of - the - art results on many sentencelevel tasks from the GLUE benchmark .",Unsupervised Fine- tuning Approaches,Unsupervised Fine- tuning Approaches,named-entity-recognition,8,4,0.16,56,0.1447028423772609,19,0.6785714285714286,1,0
58,Left - to - right language model - BERT BERT E E 1 E ...,Unsupervised Fine- tuning Approaches,Unsupervised Fine- tuning Approaches,named-entity-recognition,8,5,0.2,57,0.1472868217054263,20,0.7142857142857143,1,0
59,CT 1 T ... E 1 E ...,Unsupervised Fine- tuning Approaches,Unsupervised Fine- tuning Approaches,named-entity-recognition,8,6,0.24,58,0.1498708010335917,21,0.75,1,0
60,CT 1 T ... :,Unsupervised Fine- tuning Approaches,Unsupervised Fine- tuning Approaches,named-entity-recognition,8,7,0.28,59,0.1524547803617571,22,0.7857142857142857,1,0
61,Overall pre-training and fine - tuning procedures for BERT .,Unsupervised Fine- tuning Approaches,Unsupervised Fine- tuning Approaches,named-entity-recognition,8,8,0.32,60,0.1550387596899224,23,0.8214285714285714,1,0
62,"Apart from output layers , the same architectures are used in both pre-training and fine - tuning .",Unsupervised Fine- tuning Approaches,Unsupervised Fine- tuning Approaches,named-entity-recognition,8,9,0.36,61,0.1576227390180878,24,0.8571428571428571,1,0
63,The same pre-trained model parameters are used to initialize models for different down - stream tasks .,Unsupervised Fine- tuning Approaches,Unsupervised Fine- tuning Approaches,named-entity-recognition,8,10,0.4,62,0.1602067183462532,25,0.8928571428571429,1,0
64,"During fine - tuning , all parameters are fine - tuned .",Unsupervised Fine- tuning Approaches,Unsupervised Fine- tuning Approaches,named-entity-recognition,8,11,0.44,63,0.1627906976744186,26,0.9285714285714286,1,0
65,"[ CLS ] is a special symbol added in front of every input example , and [ SEP ] is a special separator token ( e.g. separating questions / answers ) .",Unsupervised Fine- tuning Approaches,Unsupervised Fine- tuning Approaches,named-entity-recognition,8,12,0.48,64,0.165374677002584,27,0.9642857142857144,1,0
66,ing and auto - encoder objectives have been used for pre-training such models .,Unsupervised Fine- tuning Approaches,Unsupervised Fine- tuning Approaches,named-entity-recognition,8,13,0.52,65,0.1679586563307493,28,1.0,1,0
67,Transfer Learning from Supervised Data,Unsupervised Fine- tuning Approaches,,named-entity-recognition,8,14,0.56,66,0.1705426356589147,0,0.0,1,0
68,"There has also been work showing effective transfer from supervised tasks with large datasets , such as natural language inference and machine translation .",Unsupervised Fine- tuning Approaches,Transfer Learning from Supervised Data,named-entity-recognition,8,15,0.6,67,0.1731266149870801,1,0.5,1,0
69,"Computer vision research has also demonstrated the importance of transfer learning from large pre-trained models , where an effective recipe is to fine - tune models pre-trained with I ma - geNet .",Unsupervised Fine- tuning Approaches,Transfer Learning from Supervised Data,named-entity-recognition,8,16,0.64,68,0.1757105943152454,2,1.0,1,0
70,BERT,Unsupervised Fine- tuning Approaches,,named-entity-recognition,8,17,0.68,69,0.1782945736434108,0,0.0,1,0
71,We introduce BERT and its detailed implementation in this section .,Unsupervised Fine- tuning Approaches,BERT,named-entity-recognition,8,18,0.72,70,0.1808785529715762,1,0.0121951219512195,1,0
72,There are two steps in our framework : pre-training and fine - tuning .,Unsupervised Fine- tuning Approaches,BERT,named-entity-recognition,8,19,0.76,71,0.1834625322997416,2,0.024390243902439,1,0
73,"During pre-training , the model is trained on unlabeled data over different pre-training tasks .",Unsupervised Fine- tuning Approaches,BERT,named-entity-recognition,8,20,0.8,72,0.1860465116279069,3,0.0365853658536585,1,0
74,"For finetuning , the BERT model is first initialized with the pre-trained parameters , and all of the parameters are fine - tuned using labeled data from the downstream tasks .",Unsupervised Fine- tuning Approaches,BERT,named-entity-recognition,8,21,0.84,73,0.1886304909560723,4,0.048780487804878,1,0
75,"Each downstream task has separate fine - tuned models , even though they are initialized with the same pre-trained parameters .",Unsupervised Fine- tuning Approaches,BERT,named-entity-recognition,8,22,0.88,74,0.1912144702842377,5,0.0609756097560975,1,0
76,The question - answering example in will serve as a running example for this section .,Unsupervised Fine- tuning Approaches,BERT,named-entity-recognition,8,23,0.92,75,0.1937984496124031,6,0.073170731707317,1,0
77,A distinctive feature of BERT is its unified architecture across different tasks .,Unsupervised Fine- tuning Approaches,BERT,named-entity-recognition,8,24,0.96,76,0.1963824289405684,7,0.0853658536585365,1,0
78,There is mini-mal difference between the pre-trained architecture and the final downstream architecture .,Unsupervised Fine- tuning Approaches,BERT,named-entity-recognition,8,25,1.0,77,0.1989664082687338,8,0.0975609756097561,1,0
79,Model Architecture,,,named-entity-recognition,8,0,0.0,78,0.2015503875968992,9,0.1097560975609756,1,0
80,BERT 's model architecture is a multi - layer bidirectional Transformer encoder based on the original implementation and released in the tensor2 tensor library .,Model Architecture,Model Architecture,named-entity-recognition,8,1,0.0136986301369863,79,0.2041343669250646,10,0.1219512195121951,1,0
81,1,Model Architecture,Model Architecture,named-entity-recognition,8,2,0.0273972602739726,80,0.2067183462532299,11,0.1341463414634146,1,0
82,"Because the use of Transformers has become common and our implementation is almost identical to the original , we will omit an exhaustive background description of the model architecture and refer readers to as well as excellent guides such as "" The Annotated Transformer . """,Model Architecture,Model Architecture,named-entity-recognition,8,3,0.0410958904109589,81,0.2093023255813953,12,0.1463414634146341,1,0
83,2,Model Architecture,Model Architecture,named-entity-recognition,8,4,0.0547945205479452,82,0.2118863049095607,13,0.1585365853658536,1,0
84,"In this work , we denote the number of layers ( i.e. , Transformer blocks ) as L , the hidden size as H , and the number of self - attention heads as A .",Model Architecture,Model Architecture,named-entity-recognition,8,5,0.0684931506849315,83,0.2144702842377261,14,0.1707317073170731,1,0
85,"We primarily report results on two model sizes : BERT BASE ( L=12 , H = 768 , A = 12 , Total Param-eters=110M ) and BERT LARGE ( L=24 , H = 1024 , A = 16 , Total Parameters=340M ) .",Model Architecture,Model Architecture,named-entity-recognition,8,6,0.0821917808219178,84,0.2170542635658914,15,0.1829268292682926,1,0
86,BERT BASE was chosen to have the same model size as OpenAI GPT for comparison purposes .,Model Architecture,Model Architecture,named-entity-recognition,8,7,0.0958904109589041,85,0.2196382428940568,16,0.1951219512195122,1,0
87,"Critically , however , the BERT Transformer uses bidirectional self - attention , while the GPT Transformer uses constrained self - attention where every token can only attend to context to its left .",Model Architecture,Model Architecture,named-entity-recognition,8,8,0.1095890410958904,86,0.2222222222222222,17,0.2073170731707317,1,0
88,4,Model Architecture,Model Architecture,named-entity-recognition,8,9,0.1232876712328767,87,0.2248062015503876,18,0.2195121951219512,1,0
89,Input / Output Representations,Model Architecture,,named-entity-recognition,8,10,0.136986301369863,88,0.2273901808785529,19,0.2317073170731707,1,0
90,"To make BERT handle a variety of down - stream tasks , our input representation is able to unambiguously represent both a single sentence and a pair of sentences ( e.g. , Question , Answer ) in one token sequence .",Model Architecture,Input / Output Representations,named-entity-recognition,8,11,0.1506849315068493,89,0.2299741602067183,20,0.2439024390243902,1,0
91,"Throughout this work , a "" sentence "" can bean arbitrary span of contiguous text , rather than an actual linguistic sentence .",Model Architecture,Input / Output Representations,named-entity-recognition,8,12,0.1643835616438356,90,0.2325581395348837,21,0.2560975609756097,1,0
92,"A "" sequence "" refers to the input token sequence to BERT , which maybe a single sentence or two sentences packed together .",Model Architecture,Input / Output Representations,named-entity-recognition,8,13,0.1780821917808219,91,0.2351421188630491,22,0.2682926829268293,1,0
93,We use WordPiece embeddings,Model Architecture,Input / Output Representations,named-entity-recognition,8,14,0.1917808219178082,92,0.2377260981912144,23,0.2804878048780488,1,0
94,"( Wu et al. , 2016 ) with a 30,000 token vocabulary .",Model Architecture,Input / Output Representations,named-entity-recognition,8,15,0.2054794520547945,93,0.2403100775193798,24,0.2926829268292683,1,0
95,The first token of every sequence is always a special classification token ( [ CLS ] ) .,Model Architecture,Input / Output Representations,named-entity-recognition,8,16,0.2191780821917808,94,0.2428940568475452,25,0.3048780487804878,1,0
96,The final hidden state corresponding to this token is used as the aggregate sequence representation for classification tasks .,Model Architecture,Input / Output Representations,named-entity-recognition,8,17,0.2328767123287671,95,0.2454780361757106,26,0.3170731707317073,1,0
97,Sentence pairs are packed together into a single sequence .,Model Architecture,Input / Output Representations,named-entity-recognition,8,18,0.2465753424657534,96,0.2480620155038759,27,0.3292682926829268,1,0
98,We differentiate the sentences in two ways .,Model Architecture,Input / Output Representations,named-entity-recognition,8,19,0.2602739726027397,97,0.2506459948320413,28,0.3414634146341463,1,0
99,"First , we separate them with a special token ( [ SEP ] ) .",Model Architecture,Input / Output Representations,named-entity-recognition,8,20,0.273972602739726,98,0.2532299741602067,29,0.3536585365853658,1,0
100,"Second , we add a learned embedding to every token indicating whether it belongs to sentence A or sentence B .",Model Architecture,Input / Output Representations,named-entity-recognition,8,21,0.2876712328767123,99,0.2558139534883721,30,0.3658536585365853,1,0
101,"As shown in , we denote input embedding as E , the final hidden vector of the special [ CLS ] token as C ? R H , and the final hidden vector for the i th input token as",Model Architecture,Input / Output Representations,named-entity-recognition,8,22,0.3013698630136986,100,0.2583979328165374,31,0.3780487804878049,1,0
102,"For a given token , its input representation is constructed by summing the corresponding token , segment , and position embeddings .",Model Architecture,Input / Output Representations,named-entity-recognition,8,23,0.3150684931506849,101,0.2609819121447028,32,0.3902439024390244,1,0
103,"A visualization of this construction can be seen in . ( 2018 ) , we do not use traditional left - to - right or right - to - left language models to pre-train BERT .",Model Architecture,Input / Output Representations,named-entity-recognition,8,24,0.3287671232876712,102,0.2635658914728682,33,0.4024390243902439,1,0
104,"Instead , we pre-train BERT using two unsupervised tasks , described in this section .",Model Architecture,Input / Output Representations,named-entity-recognition,8,25,0.3424657534246575,103,0.2661498708010336,34,0.4146341463414634,1,0
105,This step is presented in the left part of .,Model Architecture,Input / Output Representations,named-entity-recognition,8,26,0.3561643835616438,104,0.268733850129199,35,0.4268292682926829,1,0
106,Task # 1 : Masked LM,Model Architecture,Input / Output Representations,named-entity-recognition,8,27,0.3698630136986301,105,0.2713178294573643,36,0.4390243902439024,1,0
107,"Intuitively , it is reasonable to believe that a deep bidirectional model is strictly more powerful than either a left - to - right model or the shallow concatenation of a left - toright and a right - to - left model .",Model Architecture,Input / Output Representations,named-entity-recognition,8,28,0.3835616438356164,106,0.2739018087855297,37,0.4512195121951219,1,0
108,"Unfortunately , standard conditional language models can only be trained left - to - right or right - to - left , since bidirectional conditioning would allow each word to indirectly "" see itself "" , and the model could trivially predict the target word in a multi - layered context .",Model Architecture,Input / Output Representations,named-entity-recognition,8,29,0.3972602739726027,107,0.276485788113695,38,0.4634146341463415,1,0
109,"former is often referred to as a "" Transformer encoder "" while the left - context - only version is referred to as a "" Transformer decoder "" since it can be used for text generation .",Model Architecture,Input / Output Representations,named-entity-recognition,8,30,0.410958904109589,108,0.2790697674418604,39,0.4756097560975609,1,0
110,"In order to train a deep bidirectional representation , we simply mask some percentage of the input tokens at random , and then predict those masked tokens .",Model Architecture,Input / Output Representations,named-entity-recognition,8,31,0.4246575342465753,109,0.2816537467700258,40,0.4878048780487805,1,0
111,"We refer to this procedure as a "" masked LM "" ( MLM ) , although it is often referred to as a Cloze task in the literature .",Model Architecture,Input / Output Representations,named-entity-recognition,8,32,0.4383561643835616,110,0.2842377260981912,41,0.5,1,0
112,"In this case , the final hidden vectors corresponding to the mask tokens are fed into an output softmax over the vocabulary , as in a standard LM .",Model Architecture,Input / Output Representations,named-entity-recognition,8,33,0.4520547945205479,111,0.2868217054263566,42,0.5121951219512195,1,0
113,"In all of our experiments , we mask 15 % of all WordPiece tokens in each sequence at random .",Model Architecture,Input / Output Representations,named-entity-recognition,8,34,0.4657534246575342,112,0.2894056847545219,43,0.524390243902439,1,0
114,"In contrast to denoising auto - encoders , we only predict the masked words rather than reconstructing the entire input .",Model Architecture,Input / Output Representations,named-entity-recognition,8,35,0.4794520547945205,113,0.2919896640826873,44,0.5365853658536586,1,0
115,"Although this allows us to obtain a bidirectional pre-trained model , a downside is that we are creating a mismatch between pre-training and fine - tuning , since the [ MASK ] token does not appear during fine - tuning .",Model Architecture,Input / Output Representations,named-entity-recognition,8,36,0.4931506849315068,114,0.2945736434108527,45,0.5487804878048781,1,0
116,"To mitigate this , we do not always replace "" masked "" words with the actual [ MASK ] token .",Model Architecture,Input / Output Representations,named-entity-recognition,8,37,0.5068493150684932,115,0.2971576227390181,46,0.5609756097560976,1,0
117,The training data generator chooses 15 % of the token positions at random for prediction .,Model Architecture,Input / Output Representations,named-entity-recognition,8,38,0.5205479452054794,116,0.2997416020671835,47,0.573170731707317,1,0
118,"If the i - th token is chosen , we replace the i - th token with ( 1 ) the [ MASK ] token 80 % of the time ( 2 ) a random token 10 % of the time ( 3 ) the unchanged i - th token 10 % of the time .",Model Architecture,Input / Output Representations,named-entity-recognition,8,39,0.5342465753424658,117,0.3023255813953488,48,0.5853658536585366,1,0
119,"Then ,",Model Architecture,,named-entity-recognition,8,40,0.547945205479452,118,0.3049095607235142,49,0.5975609756097561,1,0
120,Ti will be used to predict the original token with cross entropy loss .,Model Architecture,"Then ,",named-entity-recognition,8,41,0.5616438356164384,119,0.3074935400516795,50,0.6097560975609756,1,0
121,We compare variations of this procedure in Appendix C.2 .,Model Architecture,"Then ,",named-entity-recognition,8,42,0.5753424657534246,120,0.3100775193798449,51,0.6219512195121951,1,0
122,"Task # 2 : Next Sentence Prediction ( NSP ) Many important downstream tasks such as Question Answering ( QA ) and Natural Language Inference ( NLI ) are based on understanding the relationship between two sentences , which is not directly captured by language modeling .",Model Architecture,"Then ,",named-entity-recognition,8,43,0.589041095890411,121,0.3126614987080103,52,0.6341463414634146,1,0
123,"In order to train a model that understands sentence relationships , we pre-train fora binarized next sentence prediction task that can be trivially generated from any monolingual corpus .",Model Architecture,"Then ,",named-entity-recognition,8,44,0.6027397260273972,122,0.3152454780361757,53,0.6463414634146342,1,0
124,"Specifically , when choosing the sentences A and B for each pretraining example , 50 % of the time B is the actual next sentence that follows A ( labeled as IsNext ) , and 50 % of the time it is a random sentence from the corpus ( labeled as NotNext ) .",Model Architecture,"Then ,",named-entity-recognition,8,45,0.6164383561643836,123,0.3178294573643411,54,0.6585365853658537,1,0
125,"As we show in , C is used for next sentence prediction ( NSP ) .",Model Architecture,"Then ,",named-entity-recognition,8,46,0.6301369863013698,124,0.3204134366925064,55,0.6707317073170732,1,0
126,"5 Despite its simplicity , we demonstrate in Section 5.1 that pre-training towards this task is very beneficial to both QA and NLI .",Model Architecture,"Then ,",named-entity-recognition,8,47,0.6438356164383562,125,0.3229974160206718,56,0.6829268292682927,1,0
127,6,Model Architecture,"Then ,",named-entity-recognition,8,48,0.6575342465753424,126,0.3255813953488372,57,0.6951219512195121,1,0
128,5 The final model achieves 97 % - 98 % accuracy on NSP .,Model Architecture,"Then ,",named-entity-recognition,8,49,0.6712328767123288,127,0.3281653746770026,58,0.7073170731707317,1,0
129,"The vector C is not a meaningful sentence representation without fine - tuning , since it was trained with NSP .",Model Architecture,"Then ,",named-entity-recognition,8,50,0.684931506849315,128,0.330749354005168,59,0.7195121951219512,1,0
130,he likes play ##ing my dog is cute Input,Model Architecture,"Then ,",named-entity-recognition,8,51,0.6986301369863014,129,0.3333333333333333,60,0.7317073170731707,1,0
131,Position,Model Architecture,,named-entity-recognition,8,52,0.7123287671232876,130,0.3359173126614987,61,0.7439024390243902,1,0
132,Embeddings : BERT input representation .,Model Architecture,Position,named-entity-recognition,8,53,0.726027397260274,131,0.3385012919896641,62,0.7560975609756098,1,0
133,"The input embeddings are the sum of the token embeddings , the segmentation embeddings and the position embeddings .",Model Architecture,Position,named-entity-recognition,8,54,0.7397260273972602,132,0.3410852713178294,63,0.7682926829268293,1,0
134,The NSP task is closely related to representationlearning objectives used in Jernite et al. and Logeswaran and Lee ( 2018 ) .,Model Architecture,Position,named-entity-recognition,8,55,0.7534246575342466,133,0.3436692506459948,64,0.7804878048780488,1,0
135,"However , in prior work , only sentence embeddings are transferred to down - stream tasks , where BERT transfers all parameters to initialize end - task model parameters .",Model Architecture,Position,named-entity-recognition,8,56,0.7671232876712328,134,0.3462532299741602,65,0.7926829268292683,1,0
136,Pre-training data,Model Architecture,Position,named-entity-recognition,8,57,0.7808219178082192,135,0.3488372093023256,66,0.8048780487804879,1,0
137,The pre-training procedure largely follows the existing literature on language model pre-training .,Model Architecture,Position,named-entity-recognition,8,58,0.7945205479452054,136,0.3514211886304909,67,0.8170731707317073,1,0
138,"For the pre-training corpus we use the Books Corpus ( 800M words ) and English Wikipedia ( 2,500 M words ) .",Model Architecture,Position,named-entity-recognition,8,59,0.8082191780821918,137,0.3540051679586563,68,0.8292682926829268,1,0
139,"For Wikipedia we extract only the text passages and ignore lists , tables , and headers .",Model Architecture,Position,named-entity-recognition,8,60,0.821917808219178,138,0.3565891472868217,69,0.8414634146341463,1,0
140,It is critical to use a document - level corpus rather than a shuffled sentence - level corpus such as the Billion Word Benchmark in order to extract long contiguous sequences .,Model Architecture,Position,named-entity-recognition,8,61,0.8356164383561644,139,0.359173126614987,70,0.8536585365853658,1,0
141,Fine- tuning BERT,Model Architecture,Position,named-entity-recognition,8,62,0.8493150684931506,140,0.3617571059431524,71,0.8658536585365854,1,0
142,Fine- tuning is straightforward since the selfattention mechanism in the Transformer allows BERT to model many downstream tasks whether they involve single text or text pairs - by swapping out the appropriate inputs and outputs .,Model Architecture,Position,named-entity-recognition,8,63,0.863013698630137,141,0.3643410852713178,72,0.8780487804878049,1,0
143,"For applications involving text pairs , a common pattern is to independently encode text pairs before applying bidirectional cross attention , such as Parikh et al. ; Seo et al. ( 2017 ) .",Model Architecture,Position,named-entity-recognition,8,64,0.8767123287671232,142,0.3669250645994832,73,0.8902439024390244,1,0
144,"BERT instead uses the self - attention mechanism to unify these two stages , as encoding a concatenated text pair with self - attention effectively includes bidirectional cross attention between two sentences .",Model Architecture,Position,named-entity-recognition,8,65,0.8904109589041096,143,0.3695090439276486,74,0.902439024390244,1,0
145,"For each task , we simply plugin the taskspecific inputs and outputs into BERT and finetune all the parameters end - to - end .",Model Architecture,Position,named-entity-recognition,8,66,0.9041095890410958,144,0.3720930232558139,75,0.9146341463414634,1,0
146,"At the input , sentence A and sentence B from pre-training are analogous to ( 1 ) sentence pairs in paraphrasing , ( 2 ) hypothesis - premise pairs in entailment , ( 3 ) question - passage pairs in question answering , and ( 4 ) a degenerate text -?",Model Architecture,Position,named-entity-recognition,8,67,0.9178082191780822,145,0.3746770025839793,76,0.926829268292683,1,0
147,pair in text classification or sequence tagging .,Model Architecture,Position,named-entity-recognition,8,68,0.9315068493150684,146,0.3772609819121447,77,0.9390243902439024,1,0
148,"At the output , the token representations are fed into an output layer for tokenlevel tasks , such as sequence tagging or question answering , and the [ CLS ] representation is fed into an output layer for classification , such as entailment or sentiment analysis .",Model Architecture,Position,named-entity-recognition,8,69,0.9452054794520548,147,0.3798449612403101,78,0.951219512195122,1,0
149,"Compared to pre-training , fine - tuning is relatively inexpensive .",Model Architecture,Position,named-entity-recognition,8,70,0.958904109589041,148,0.3824289405684754,79,0.9634146341463414,1,0
150,"All of the results in the paper can be replicated in at most 1 hour on a single Cloud TPU , or a few hours on a GPU , starting from the exact same pre-trained model .",Model Architecture,Position,named-entity-recognition,8,71,0.9726027397260274,149,0.3850129198966408,80,0.975609756097561,1,0
151,We describe the task - specific details in the corresponding subsections of Section 4 .,Model Architecture,Position,named-entity-recognition,8,72,0.9863013698630136,150,0.3875968992248062,81,0.9878048780487804,1,0
152,More details can be found in Appendix A.5 .,Model Architecture,Position,named-entity-recognition,8,73,1.0,151,0.3901808785529715,82,1.0,1,0
153,Experiments,,,named-entity-recognition,8,0,0.0,152,0.3927648578811369,0,0.0,1,0
154,"In this section , we present BERT fine - tuning results on 11 NLP tasks .",Experiments,Experiments,named-entity-recognition,8,1,0.0142857142857142,153,0.3953488372093023,1,0.0,1,0
155,GLUE,Experiments,,named-entity-recognition,8,2,0.0285714285714285,154,0.3979328165374677,0,0.0,1,0
156,"The General Language Understanding Evaluation ( GLUE ) benchmark ( Wang et al. , 2018 a ) is a collection of diverse natural language understanding tasks .",Experiments,GLUE,named-entity-recognition,8,3,0.0428571428571428,155,0.4005167958656331,1,0.1428571428571428,1,1
157,Detailed descriptions of GLUE datasets are included in Appendix B.1 .,Experiments,GLUE,named-entity-recognition,8,4,0.0571428571428571,156,0.4031007751937984,2,0.2857142857142857,1,0
158,"To fine - tune on GLUE , we represent the input sequence ( for single sentence or sentence pairs ) as described in Section 3 , and use the final hidden vector C ?",Experiments,GLUE,named-entity-recognition,8,5,0.0714285714285714,157,0.4056847545219638,3,0.4285714285714285,1,0
159,R H corresponding to the first input token ( [ CLS ] ) as the aggregate representation .,Experiments,GLUE,named-entity-recognition,8,6,0.0857142857142857,158,0.4082687338501292,4,0.5714285714285714,1,0
160,The only new parameters introduced during fine - tuning are classification layer weights W ?,Experiments,GLUE,named-entity-recognition,8,7,0.1,159,0.4108527131782946,5,0.7142857142857143,1,0
161,"R KH , where K is the number of labels .",Experiments,GLUE,named-entity-recognition,8,8,0.1142857142857142,160,0.4134366925064599,6,0.8571428571428571,1,0
162,"We compute a standard classification loss with C and W , i.e. , log ( softmax ( CW T ) ) . :",Experiments,GLUE,named-entity-recognition,8,9,0.1285714285714285,161,0.4160206718346253,7,1.0,1,0
163,GLUE,Experiments,,named-entity-recognition,8,10,0.1428571428571428,162,0.4186046511627907,0,0.0,1,0
164,"Test results , scored by the evaluation server ( https://gluebenchmark.com/leaderboard ) .",Experiments,GLUE,named-entity-recognition,8,11,0.1571428571428571,163,0.421188630490956,1,0.0192307692307692,1,0
165,The number below each task denotes the number of training examples .,Experiments,GLUE,named-entity-recognition,8,12,0.1714285714285714,164,0.4237726098191214,2,0.0384615384615384,1,0
166,"The "" Average "" column is slightly different than the official GLUE score , since we exclude the problematic WNLI set .",Experiments,GLUE,named-entity-recognition,8,13,0.1857142857142857,165,0.4263565891472868,3,0.0576923076923076,1,0
167,"8 BERT and OpenAI GPT are singlemodel , single task .",Experiments,GLUE,named-entity-recognition,8,14,0.2,166,0.4289405684754522,4,0.0769230769230769,1,0
168,"F1 scores are reported for QQP and MRPC , Spearman correlations are reported for STS - B , and accuracy scores are reported for the other tasks .",Experiments,GLUE,named-entity-recognition,8,15,0.2142857142857142,167,0.4315245478036176,5,0.0961538461538461,1,0
169,We exclude entries that use BERT as one of their components .,Experiments,GLUE,named-entity-recognition,8,16,0.2285714285714285,168,0.4341085271317829,6,0.1153846153846153,1,0
170,We use a batch size of 32 and fine - tune for 3 epochs over the data for all GLUE tasks .,Experiments,GLUE,named-entity-recognition,8,17,0.2428571428571428,169,0.4366925064599483,7,0.1346153846153846,1,1
171,"For each task , we selected the best fine - tuning learning rate ( among 5 e - 5 , 4 e - 5 , 3 e - 5 , and 2 e - 5 ) on the Dev set .",Experiments,GLUE,named-entity-recognition,8,18,0.2571428571428571,170,0.4392764857881137,8,0.1538461538461538,1,1
172,"Additionally , for BERT LARGE we found that finetuning was sometimes unstable on small datasets , so we ran several random restarts and selected the best model on the Dev set .",Experiments,GLUE,named-entity-recognition,8,19,0.2714285714285714,171,0.4418604651162791,9,0.173076923076923,1,1
173,"With random restarts , we use the same pre-trained checkpoint but perform different fine - tuning data shuffling and classifier layer initialization .",Experiments,GLUE,named-entity-recognition,8,20,0.2857142857142857,172,0.4444444444444444,10,0.1923076923076923,1,0
174,Results are presented in .,Experiments,GLUE,named-entity-recognition,8,21,0.3,173,0.4470284237726098,11,0.2115384615384615,1,0
175,"Both BERT BASE and BERT LARGE outperform all systems on all tasks by a substantial margin , obtaining 4.5 % and 7.0 % respective average accuracy improvement over the prior state of the art .",Experiments,GLUE,named-entity-recognition,8,22,0.3142857142857143,174,0.4496124031007752,12,0.2307692307692307,1,1
176,Note that BERT BASE and OpenAI GPT are nearly identical in terms of model architecture apart from the attention masking .,Experiments,GLUE,named-entity-recognition,8,23,0.3285714285714285,175,0.4521963824289405,13,0.25,1,0
177,"For the largest and most widely reported GLUE task , MNLI , BERT obtains a 4.6 % absolute accuracy improvement .",Experiments,GLUE,named-entity-recognition,8,24,0.3428571428571428,176,0.4547803617571059,14,0.2692307692307692,1,0
178,"On the official GLUE leaderboard 10 , BERT LARGE obtains a score of 80.5 , compared to OpenAI GPT , which obtains 72.8 as of the date of writing .",Experiments,GLUE,named-entity-recognition,8,25,0.3571428571428571,177,0.4573643410852713,15,0.2884615384615384,1,0
179,"We find that BERT LARGE significantly outperforms BERT BASE across all tasks , especially those with very little training data .",Experiments,GLUE,named-entity-recognition,8,26,0.3714285714285714,178,0.4599483204134367,16,0.3076923076923077,1,1
180,The effect of model size is explored more thoroughly in Section 5.2 .,Experiments,GLUE,named-entity-recognition,8,27,0.3857142857142857,179,0.4625322997416021,17,0.3269230769230769,1,0
181,SQuAD v 1.1,Experiments,GLUE,named-entity-recognition,8,28,0.4,180,0.4651162790697674,18,0.3461538461538461,1,0
182,The Stanford Question Answering Dataset ( SQuAD v1.1 ) is a collection of 100 k crowdsourced question / answer pairs .,Experiments,GLUE,named-entity-recognition,8,29,0.4142857142857143,181,0.4677002583979328,19,0.3653846153846153,1,1
183,"Given a question and a passage from The GLUE data set distribution does not include the Test labels , and we only made a single GLUE evaluation server submission for each of BERTBASE and BERTLARGE .",Experiments,GLUE,named-entity-recognition,8,30,0.4285714285714285,182,0.4702842377260982,20,0.3846153846153846,1,0
184,10 https://gluebenchmark.com/leaderboard,Experiments,GLUE,named-entity-recognition,8,31,0.4428571428571428,183,0.4728682170542636,21,0.4038461538461538,1,0
185,"Wikipedia containing the answer , the task is to predict the answer text span in the passage .",Experiments,GLUE,named-entity-recognition,8,32,0.4571428571428571,184,0.4754521963824289,22,0.4230769230769231,1,0
186,"As shown in , in the question answering task , we represent the input question and passage as a single packed sequence , with the question using the A embedding and the passage using the B embedding .",Experiments,GLUE,named-entity-recognition,8,33,0.4714285714285714,185,0.4780361757105943,23,0.4423076923076923,1,0
187,We only introduce a start vector S ?,Experiments,GLUE,named-entity-recognition,8,34,0.4857142857142857,186,0.4806201550387597,24,0.4615384615384615,1,0
188,R H and an end vector E ?,Experiments,GLUE,named-entity-recognition,8,35,0.5,187,0.483204134366925,25,0.4807692307692308,1,0
189,R H during fine - tuning .,Experiments,GLUE,named-entity-recognition,8,36,0.5142857142857142,188,0.4857881136950904,26,0.5,1,0
190,The probability of word i being the start of the answer span is computed as a dot product between Ti and S followed by a softmax overall of the words in the paragraph : P i = e ST i j e ST j .,Experiments,GLUE,named-entity-recognition,8,37,0.5285714285714286,189,0.4883720930232558,27,0.5192307692307693,1,0
191,The analogous formula is used for the end of the answer span .,Experiments,GLUE,named-entity-recognition,8,38,0.5428571428571428,190,0.4909560723514212,28,0.5384615384615384,1,0
192,"The score of a candidate span from position i to position j is defined as ST i + ET j , and the maximum scoring span where j ?",Experiments,GLUE,named-entity-recognition,8,39,0.5571428571428572,191,0.4935400516795866,29,0.5576923076923077,1,0
193,i is used as a prediction .,Experiments,GLUE,named-entity-recognition,8,40,0.5714285714285714,192,0.4961240310077519,30,0.5769230769230769,1,0
194,The training objective is the sum of the log-likelihoods of the correct start and end positions .,Experiments,GLUE,named-entity-recognition,8,41,0.5857142857142857,193,0.4987080103359173,31,0.5961538461538461,1,0
195,We fine - tune for 3 epochs with a learning rate of 5 e - 5 and a batch size of 32 .,Experiments,GLUE,named-entity-recognition,8,42,0.6,194,0.5012919896640827,32,0.6153846153846154,1,1
196,shows top leaderboard entries as well as results from top published systems .,Experiments,GLUE,named-entity-recognition,8,43,0.6142857142857143,195,0.5038759689922481,33,0.6346153846153846,1,0
197,"The top results from the SQuAD leaderboard do not have up - to - date public system descriptions available , 11 and are allowed to use any public data when training their systems .",Experiments,GLUE,named-entity-recognition,8,44,0.6285714285714286,196,0.5064599483204134,34,0.6538461538461539,1,0
198,"We therefore use modest data augmentation in our system by first fine - tuning on TriviaQA ( Joshi et al. , 2017 ) befor fine - tuning on SQuAD .",Experiments,GLUE,named-entity-recognition,8,45,0.6428571428571429,197,0.5090439276485789,35,0.6730769230769231,1,0
199,Our best performing system outperforms the top leaderboard system by + 1.5 F1 in ensembling and + 1.3 F1 as a single system .,Experiments,GLUE,named-entity-recognition,8,46,0.6571428571428571,198,0.5116279069767442,36,0.6923076923076923,1,1
200,"In fact , our single BERT model outperforms the top ensemble system in terms of F1 score .",Experiments,GLUE,named-entity-recognition,8,47,0.6714285714285714,199,0.5142118863049095,37,0.7115384615384616,1,0
201,"Without Trivia QA fine - tuning data , we only lose 0.1 - 0.4 F1 , still outperforming all existing systems by a wide margin .",Experiments,GLUE,named-entity-recognition,8,48,0.6857142857142857,200,0.5167958656330749,38,0.7307692307692307,1,0
202,12,Experiments,GLUE,named-entity-recognition,8,49,0.7,201,0.5193798449612403,39,0.75,1,0
203,SQuAD v 2.0,Experiments,GLUE,named-entity-recognition,8,50,0.7142857142857143,202,0.5219638242894057,40,0.7692307692307693,1,0
204,"The SQuAD 2.0 task extends the SQuAD 1.1 problem definition by allowing for the possibility that no short answer exists in the provided paragraph , making the problem more realistic .",Experiments,GLUE,named-entity-recognition,8,51,0.7285714285714285,203,0.524547803617571,41,0.7884615384615384,1,0
205,We use a simple approach to extend the SQuAD v1.1 BERT model for this task .,Experiments,GLUE,named-entity-recognition,8,52,0.7428571428571429,204,0.5271317829457365,42,0.8076923076923077,1,0
206,We treat questions that do not have an answer as having an answer span with start and end at the [ CLS ] token .,Experiments,GLUE,named-entity-recognition,8,53,0.7571428571428571,205,0.5297157622739018,43,0.8269230769230769,1,0
207,The probability space for the start and end answer span positions is extended to include the position of the [ CLS ] token .,Experiments,GLUE,named-entity-recognition,8,54,0.7714285714285715,206,0.5322997416020672,44,0.8461538461538461,1,0
208,"For prediction , we compare the score of the no -answer span : s null = SC + EC to the score of the best non - null span The Trivia QA data we used consists of paragraphs from TriviaQA - Wiki formed of the first 400 tokens in documents , that contain at least one of the provided possible answers .",Experiments,GLUE,named-entity-recognition,8,55,0.7857142857142857,207,0.5348837209302325,45,0.8653846153846154,1,0
209,We predict a non-null answer when ?,Experiments,GLUE,named-entity-recognition,8,56,0.8,208,0.537467700258398,46,0.8846153846153846,1,0
210,"i , j > s null + ? , where the threshold ?",Experiments,GLUE,named-entity-recognition,8,57,0.8142857142857143,209,0.5400516795865633,47,0.903846153846154,1,0
211,is selected on the dev set to maximize F 1 .,Experiments,GLUE,named-entity-recognition,8,58,0.8285714285714286,210,0.5426356589147286,48,0.9230769230769232,1,0
212,We did not use Trivia QA data for this model .,Experiments,GLUE,named-entity-recognition,8,59,0.8428571428571429,211,0.5452196382428941,49,0.9423076923076924,1,0
213,We fine - tuned for 2 epochs with a learning rate of 5 e - 5 and a batch size of 48 .,Experiments,GLUE,named-entity-recognition,8,60,0.8571428571428571,212,0.5478036175710594,50,0.9615384615384616,1,1
214,"The results compared to prior leaderboard entries and top published work are shown in , excluding systems that use BERT as one of their components .",Experiments,GLUE,named-entity-recognition,8,61,0.8714285714285714,213,0.5503875968992248,51,0.9807692307692308,1,0
215,We observe a + 5.1 F1 improvement over the previous best system .,Experiments,GLUE,named-entity-recognition,8,62,0.8857142857142857,214,0.5529715762273901,52,1.0,1,1
216,SWAG,Experiments,,named-entity-recognition,8,63,0.9,215,0.5555555555555556,0,0.0,1,0
217,The Situations With Adversarial Generations ( SWAG ) dataset contains 113 k sentence - pair completion examples that evaluate grounded commonsense inference .,Experiments,SWAG,named-entity-recognition,8,64,0.9142857142857144,216,0.5581395348837209,1,0.1428571428571428,1,1
218,"Given a sentence , the task is to choose the most plausible continuation among four choices .",Experiments,SWAG,named-entity-recognition,8,65,0.9285714285714286,217,0.5607235142118863,2,0.2857142857142857,1,0
219,"When fine - tuning on the SWAG dataset , we construct four input sequences , each containing the concatenation of the given sentence ( sentence A ) and a possible continuation ( sentence B ) .",Experiments,SWAG,named-entity-recognition,8,66,0.9428571428571428,218,0.5633074935400517,3,0.4285714285714285,1,0
220,The only task - specific parameters introduced is a vector whose dot product with the [ CLS ] token representation C denotes a score for each choice which is normalized with a softmax layer .,Experiments,SWAG,named-entity-recognition,8,67,0.9571428571428572,219,0.5658914728682171,4,0.5714285714285714,1,0
221,We fine - tune the model for 3 epochs with a learning rate of 2 e - 5 and a batch size of 16 .,Experiments,SWAG,named-entity-recognition,8,68,0.9714285714285714,220,0.5684754521963824,5,0.7142857142857143,1,1
222,Results are presented in .,Experiments,SWAG,named-entity-recognition,8,69,0.9857142857142858,221,0.5710594315245479,6,0.8571428571428571,1,0
223,BERT LARGE outperforms the authors ' baseline ESIM + ELMo system by + 27.1 % and OpenAI GPT by 8.3 % .,Experiments,SWAG,named-entity-recognition,8,70,1.0,222,0.5736434108527132,7,1.0,1,1
224,Ablation Studies,,,named-entity-recognition,8,0,0.0,223,0.5762273901808785,0,0.0,1,0
225,"In this section , we perform ablation experiments over a number of facets of BERT in order to better understand their relative importance .",Ablation Studies,Ablation Studies,named-entity-recognition,8,1,0.1666666666666666,224,0.5788113695090439,1,0.1666666666666666,1,0
226,Additional : Ablation over the pre-training tasks using the BERT BASE architecture .,Ablation Studies,Ablation Studies,named-entity-recognition,8,2,0.3333333333333333,225,0.5813953488372093,2,0.3333333333333333,1,0
227,""" No NSP "" is trained without the next sentence prediction task .",Ablation Studies,Ablation Studies,named-entity-recognition,8,3,0.5,226,0.5839793281653747,3,0.5,1,0
228,""" LTR & No NSP "" is trained as a left - to - right LM without the next sentence prediction , like OpenAI GPT .",Ablation Studies,Ablation Studies,named-entity-recognition,8,4,0.6666666666666666,227,0.58656330749354,4,0.6666666666666666,1,0
229,""" + BiLSTM "" adds a randomly initialized BiLSTM on top of the "" LTR + No NSP "" model during fine - tuning .",Ablation Studies,Ablation Studies,named-entity-recognition,8,5,0.8333333333333334,228,0.5891472868217055,5,0.8333333333333334,1,0
230,ablation studies can be found in Appendix C.,Ablation Studies,Ablation Studies,named-entity-recognition,8,6,1.0,229,0.5917312661498708,6,1.0,1,0
231,Effect of Pre-training Tasks,,,named-entity-recognition,8,0,0.0,230,0.5943152454780362,0,0.0,1,0
232,"We demonstrate the importance of the deep bidirectionality of BERT by evaluating two pretraining objectives using exactly the same pretraining data , fine - tuning scheme , and hyperparameters as BERT BASE :",Effect of Pre-training Tasks,Effect of Pre-training Tasks,named-entity-recognition,8,1,0.0555555555555555,231,0.5968992248062015,1,0.0555555555555555,1,0
233,No NSP :,Effect of Pre-training Tasks,Effect of Pre-training Tasks,named-entity-recognition,8,2,0.1111111111111111,232,0.599483204134367,2,0.1111111111111111,1,0
234,"A bidirectional model which is trained using the "" masked LM "" ( MLM ) but without the "" next sentence prediction "" ( NSP ) task .",Effect of Pre-training Tasks,Effect of Pre-training Tasks,named-entity-recognition,8,3,0.1666666666666666,233,0.6020671834625323,3,0.1666666666666666,1,0
235,LTR & No NSP :,Effect of Pre-training Tasks,Effect of Pre-training Tasks,named-entity-recognition,8,4,0.2222222222222222,234,0.6046511627906976,4,0.2222222222222222,1,0
236,"A left - context - only model which is trained using a standard Left - to - Right ( LTR ) LM , rather than an MLM .",Effect of Pre-training Tasks,Effect of Pre-training Tasks,named-entity-recognition,8,5,0.2777777777777778,235,0.6072351421188631,5,0.2777777777777778,1,0
237,"The left - only constraint was also applied at fine - tuning , because removing it introduced a pre-train / fine - tune mismatch that degraded downstream performance .",Effect of Pre-training Tasks,Effect of Pre-training Tasks,named-entity-recognition,8,6,0.3333333333333333,236,0.6098191214470284,6,0.3333333333333333,1,0
238,"Additionally , this model was pre-trained without the NSP task .",Effect of Pre-training Tasks,Effect of Pre-training Tasks,named-entity-recognition,8,7,0.3888888888888889,237,0.6124031007751938,7,0.3888888888888889,1,0
239,"This is directly comparable to OpenAI GPT , but using our larger training dataset , our input representation , and our fine - tuning scheme .",Effect of Pre-training Tasks,Effect of Pre-training Tasks,named-entity-recognition,8,8,0.4444444444444444,238,0.6149870801033591,8,0.4444444444444444,1,0
240,We first examine the impact brought by the NSP task .,Effect of Pre-training Tasks,Effect of Pre-training Tasks,named-entity-recognition,8,9,0.5,239,0.6175710594315246,9,0.5,1,0
241,"In , we show that removing NSP hurts performance significantly on QNLI , MNLI , and SQu AD 1.1 .",Effect of Pre-training Tasks,Effect of Pre-training Tasks,named-entity-recognition,8,10,0.5555555555555556,240,0.6201550387596899,10,0.5555555555555556,1,0
242,"Next , we evaluate the impact of training bidirectional representations by comparing "" No NSP "" to "" LTR & No NSP "" .",Effect of Pre-training Tasks,Effect of Pre-training Tasks,named-entity-recognition,8,11,0.6111111111111112,241,0.6227390180878553,11,0.6111111111111112,1,0
243,"The LTR model performs worse than the MLM model on all tasks , with large drops on MRPC and SQuAD .",Effect of Pre-training Tasks,Effect of Pre-training Tasks,named-entity-recognition,8,12,0.6666666666666666,242,0.6253229974160207,12,0.6666666666666666,1,0
244,"For SQuAD it is intuitively clear that a LTR model will perform poorly at token predictions , since the token - level hidden states have no rightside context .",Effect of Pre-training Tasks,Effect of Pre-training Tasks,named-entity-recognition,8,13,0.7222222222222222,243,0.627906976744186,13,0.7222222222222222,1,0
245,"In order to make a good faith attempt at strengthening the LTR system , we added a randomly initialized BiLSTM on top .",Effect of Pre-training Tasks,Effect of Pre-training Tasks,named-entity-recognition,8,14,0.7777777777777778,244,0.6304909560723514,14,0.7777777777777778,1,0
246,"This does significantly improve results on SQuAD , but the results are still far worse than those of the pretrained bidirectional models .",Effect of Pre-training Tasks,Effect of Pre-training Tasks,named-entity-recognition,8,15,0.8333333333333334,245,0.6330749354005168,15,0.8333333333333334,1,0
247,The BiLSTM hurts performance on the GLUE tasks .,Effect of Pre-training Tasks,Effect of Pre-training Tasks,named-entity-recognition,8,16,0.8888888888888888,246,0.6356589147286822,16,0.8888888888888888,1,0
248,"We recognize that it would also be possible to train separate LTR and RTL models and represent each token as the concatenation of the two models , as ELMo does .",Effect of Pre-training Tasks,Effect of Pre-training Tasks,named-entity-recognition,8,17,0.9444444444444444,247,0.6382428940568475,17,0.9444444444444444,1,0
249,"However : ( a ) this is twice as expensive as a single bidirectional model ; ( b ) this is non-intuitive for tasks like QA , since the RTL model would not be able to condition the answer on the question ; ( c ) this it is strictly less powerful than a deep bidirectional model , since it can use both left and right context at every layer .",Effect of Pre-training Tasks,Effect of Pre-training Tasks,named-entity-recognition,8,18,1.0,248,0.6408268733850129,18,1.0,1,0
250,Effect of Model Size,,,named-entity-recognition,8,0,0.0,249,0.6434108527131783,0,0.0,1,0
251,"In this section , we explore the effect of model size on fine - tuning task accuracy .",Effect of Model Size,Effect of Model Size,named-entity-recognition,8,1,0.037037037037037,250,0.6459948320413437,1,0.037037037037037,1,0
252,"We trained a number of BERT models with a differing number of layers , hidden units , and attention heads , while otherwise using the same hyperparameters and training procedure as described previously .",Effect of Model Size,Effect of Model Size,named-entity-recognition,8,2,0.074074074074074,251,0.648578811369509,2,0.074074074074074,1,0
253,Results on selected GLUE tasks are shown in .,Effect of Model Size,Effect of Model Size,named-entity-recognition,8,3,0.1111111111111111,252,0.6511627906976745,3,0.1111111111111111,1,0
254,"In this table , we report the average Dev Set accuracy from 5 random restarts of fine - tuning .",Effect of Model Size,Effect of Model Size,named-entity-recognition,8,4,0.1481481481481481,253,0.6537467700258398,4,0.1481481481481481,1,0
255,"We can see that larger models lead to a strict accuracy improvement across all four datasets , even for MRPC which only has 3,600 labeled training examples , and is substantially different from the pre-training tasks .",Effect of Model Size,Effect of Model Size,named-entity-recognition,8,5,0.1851851851851851,254,0.6563307493540051,5,0.1851851851851851,1,0
256,It is also perhaps surprising that we are able to achieve such significant improvements on top of models which are already quite large relative to the existing literature .,Effect of Model Size,Effect of Model Size,named-entity-recognition,8,6,0.2222222222222222,255,0.6589147286821705,6,0.2222222222222222,1,0
257,"For example , the largest Transformer explored in is ( L=6 , H = 1024 , A = 16 ) with 100M parameters for the encoder , and the largest Transformer we have found in the literature is ( L=64 , H = 512 , A=2 ) with 235M parameters .",Effect of Model Size,Effect of Model Size,named-entity-recognition,8,7,0.2592592592592592,256,0.661498708010336,7,0.2592592592592592,1,0
258,"By contrast , BERT BASE contains 110M parameters and BERT LARGE contains 340M parameters .",Effect of Model Size,Effect of Model Size,named-entity-recognition,8,8,0.2962962962962963,257,0.6640826873385013,8,0.2962962962962963,1,0
259,"It has long been known that increasing the model size will lead to continual improvements on large - scale tasks such as machine translation and language modeling , which is demonstrated by the LM perplexity of held - out training data shown in .",Effect of Model Size,Effect of Model Size,named-entity-recognition,8,9,0.3333333333333333,258,0.6666666666666666,9,0.3333333333333333,1,0
260,"However , we believe that this is the first work to demonstrate convincingly that scaling to extreme model sizes also leads to large improvements on very small scale tasks , provided that the model has been sufficiently pre-trained .",Effect of Model Size,Effect of Model Size,named-entity-recognition,8,10,0.3703703703703703,259,0.6692506459948321,10,0.3703703703703703,1,1
261,"presented mixed results on the downstream task impact of increasing the pre-trained bi - LM size from two to four layers and mentioned in passing that increasing hidden dimension size from 200 to 600 helped , but increasing further to 1,000 did not bring further improvements .",Effect of Model Size,Effect of Model Size,named-entity-recognition,8,11,0.4074074074074074,260,0.6718346253229974,11,0.4074074074074074,1,0
262,"Both of these prior works used a featurebased approach - we hypothesize that when the model is fine - tuned directly on the downstream tasks and uses only a very small number of randomly initialized additional parameters , the taskspecific models can benefit from the larger , more expressive pre-trained representations even when downstream task data is very small .",Effect of Model Size,Effect of Model Size,named-entity-recognition,8,12,0.4444444444444444,261,0.6744186046511628,12,0.4444444444444444,1,0
263,Feature - based Approach with BERT,Effect of Model Size,Effect of Model Size,named-entity-recognition,8,13,0.4814814814814814,262,0.6770025839793282,13,0.4814814814814814,1,0
264,"All of the BERT results presented so far have used the fine - tuning approach , where a simple classification layer is added to the pre-trained model , and all parameters are jointly fine - tuned on a downstream task .",Effect of Model Size,Effect of Model Size,named-entity-recognition,8,14,0.5185185185185185,263,0.6795865633074936,14,0.5185185185185185,1,0
265,"However , the feature - based approach , where fixed features are extracted from the pretrained model , has certain advantages .",Effect of Model Size,Effect of Model Size,named-entity-recognition,8,15,0.5555555555555556,264,0.6821705426356589,15,0.5555555555555556,1,0
266,"First , not all tasks can be easily represented by a Transformer encoder architecture , and therefore require a task - specific model architecture to be added .",Effect of Model Size,Effect of Model Size,named-entity-recognition,8,16,0.5925925925925926,265,0.6847545219638242,16,0.5925925925925926,1,0
267,"Second , there are major computational benefits to pre-compute an expensive representation of the training data once and then run many experiments with cheaper models on top of this representation .",Effect of Model Size,Effect of Model Size,named-entity-recognition,8,17,0.6296296296296297,266,0.6873385012919897,17,0.6296296296296297,1,0
268,"In this section , we compare the two approaches by applying BERT to the CoNLL - 2003 Named Entity Recognition ( NER ) task .",Effect of Model Size,Effect of Model Size,named-entity-recognition,8,18,0.6666666666666666,267,0.689922480620155,18,0.6666666666666666,1,0
269,"In the input to BERT , we use a case - preserving WordPiece model , and we include the maximal document context provided by the data .",Effect of Model Size,Effect of Model Size,named-entity-recognition,8,19,0.7037037037037037,268,0.6925064599483204,19,0.7037037037037037,1,0
270,"Following standard practice , we formulate this as a tagging task but do not use a CRF layer in the output .",Effect of Model Size,Effect of Model Size,named-entity-recognition,8,20,0.7407407407407407,269,0.6950904392764858,20,0.7407407407407407,1,0
271,We use the representation of the first sub-token as the input to the token - level classifier over the NER label set .,Effect of Model Size,Effect of Model Size,named-entity-recognition,8,21,0.7777777777777778,270,0.6976744186046512,21,0.7777777777777778,1,0
272,"To ablate the fine - tuning approach , we apply the feature - based approach by extracting the activations from one or more layers without fine - tuning any parameters of BERT .",Effect of Model Size,Effect of Model Size,named-entity-recognition,8,22,0.8148148148148148,271,0.7002583979328165,22,0.8148148148148148,1,0
273,These contextual embeddings are used as input to a randomly initialized two - layer 768 - dimensional BiLSTM before the classification layer .,Effect of Model Size,Effect of Model Size,named-entity-recognition,8,23,0.8518518518518519,272,0.7028423772609819,23,0.8518518518518519,1,0
274,Results are presented in .,Effect of Model Size,Effect of Model Size,named-entity-recognition,8,24,0.8888888888888888,273,0.7054263565891473,24,0.8888888888888888,1,0
275,BERT LARGE performs competitively with state - of - the - art methods .,Effect of Model Size,Effect of Model Size,named-entity-recognition,8,25,0.925925925925926,274,0.7080103359173127,25,0.925925925925926,1,1
276,"The best performing method concatenates the token representations from the top four hidden layers of the pre-trained Transformer , which is only 0.3 F1 behind fine - tuning the entire model .",Effect of Model Size,Effect of Model Size,named-entity-recognition,8,26,0.9629629629629628,275,0.710594315245478,26,0.9629629629629628,1,0
277,This demonstrates that BERT is effective for both finetuning and feature - based approaches .,Effect of Model Size,Effect of Model Size,named-entity-recognition,8,27,1.0,276,0.7131782945736435,27,1.0,1,1
278,Conclusion,,,named-entity-recognition,8,0,0.0,277,0.7157622739018088,0,0.0,1,0
279,"Recent empirical improvements due to transfer learning with language models have demonstrated that rich , unsupervised pre-training is an integral part of many language understanding systems .",Conclusion,Conclusion,named-entity-recognition,8,1,0.0120481927710843,278,0.7183462532299741,1,0.1111111111111111,0,0
280,"In particular , these results enable even low - resource tasks to benefit from deep unidirectional architectures .",Conclusion,Conclusion,named-entity-recognition,8,2,0.0240963855421686,279,0.7209302325581395,2,0.2222222222222222,0,0
281,"Our major contribution is further generalizing these findings to deep bidirectional architectures , allowing the same pre-trained model to successfully tackle abroad set of NLP tasks .",Conclusion,Conclusion,named-entity-recognition,8,3,0.036144578313253,280,0.7235142118863049,3,0.3333333333333333,0,0
282,We organize the appendix into three sections :,Conclusion,Conclusion,named-entity-recognition,8,4,0.0481927710843373,281,0.7260981912144703,4,0.4444444444444444,0,0
283,Additional implementation details for BERT are presented in Appendix A ;,Conclusion,Conclusion,named-entity-recognition,8,5,0.0602409638554216,282,0.7286821705426356,5,0.5555555555555556,0,0
284,Additional details for our experiments are presented in Appendix B ; and,Conclusion,Conclusion,named-entity-recognition,8,6,0.072289156626506,283,0.7312661498708011,6,0.6666666666666666,0,0
285,Additional ablation studies are presented in Appendix C.,Conclusion,Conclusion,named-entity-recognition,8,7,0.0843373493975903,284,0.7338501291989664,7,0.7777777777777778,0,0
286,We present additional ablation studies for BERT including :,Conclusion,Conclusion,named-entity-recognition,8,8,0.0963855421686747,285,0.7364341085271318,8,0.8888888888888888,0,0
287,- Effect of Number of Training Steps ; and - Ablation for Different Masking Procedures .,Conclusion,Conclusion,named-entity-recognition,8,9,0.108433734939759,286,0.7390180878552972,9,1.0,0,0
288,A Additional Details for BERT,Conclusion,,named-entity-recognition,8,10,0.1204819277108433,287,0.7416020671834626,0,0.0,0,0
289,A.1 Illustration of the Pre-training Tasks,Conclusion,,named-entity-recognition,8,11,0.1325301204819277,288,0.7441860465116279,1,0.0136986301369863,0,0
290,We provide examples of the pre-training tasks in the following .,Conclusion,A.1 Illustration of the Pre-training Tasks,named-entity-recognition,8,12,0.144578313253012,289,0.7467700258397932,2,0.0273972602739726,0,0
291,"Masked LM and the Masking Procedure Assuming the unlabeled sentence is my dog is hairy , and during the random masking procedure we chose the 4 - th token ( which corresponding to hairy ) , our masking procedure can be further illustrated by The purpose of this is to bias the representation towards the actual observed word .",Conclusion,A.1 Illustration of the Pre-training Tasks,named-entity-recognition,8,13,0.1566265060240964,290,0.7493540051679587,3,0.0410958904109589,0,0
292,"The advantage of this procedure is that the Transformer encoder does not know which words it will be asked to predictor which have been replaced by random words , so it is forced to keep a distributional contextual representation of every input token .",Conclusion,A.1 Illustration of the Pre-training Tasks,named-entity-recognition,8,14,0.1686746987951807,291,0.751937984496124,4,0.0547945205479452,0,0
293,"Additionally , because random replacement only occurs for 1.5 % of all tokens ( i.e. , 10 % of 15 % ) , this does not seem to harm the model 's language understanding capability .",Conclusion,A.1 Illustration of the Pre-training Tasks,named-entity-recognition,8,15,0.180722891566265,292,0.7545219638242894,5,0.0684931506849315,0,0
294,"In Section C.2 , we evaluate the impact this procedure .",Conclusion,A.1 Illustration of the Pre-training Tasks,named-entity-recognition,8,16,0.1927710843373494,293,0.7571059431524548,6,0.0821917808219178,0,0
295,"Compared to standard langauge model training , the masked LM only make predictions on 15 % of tokens in each batch , which suggests that more pre-training steps maybe required for the model to converge .",Conclusion,A.1 Illustration of the Pre-training Tasks,named-entity-recognition,8,17,0.2048192771084337,294,0.7596899224806202,7,0.0958904109589041,0,0
296,"In Section C.1 we demonstrate that MLM does converge marginally slower than a leftto - right model ( which predicts every token ) , but the empirical improvements of the MLM model far outweigh the increased training cost .",Conclusion,A.1 Illustration of the Pre-training Tasks,named-entity-recognition,8,18,0.216867469879518,295,0.7622739018087855,8,0.1095890410958904,0,0
297,Next Sentence Prediction,Conclusion,,named-entity-recognition,8,19,0.2289156626506024,296,0.7648578811369509,9,0.1232876712328767,0,0
298,The next sentence prediction task can be illustrated in the following examples .,Conclusion,Next Sentence Prediction,named-entity-recognition,8,20,0.2409638554216867,297,0.7674418604651163,10,0.136986301369863,0,0
299,"To generate each training input sequence , we sample two spans of text from the corpus , which we refer to as "" sentences "" even though they are typically much longer than single sentences ( but can be shorter also ) .",Conclusion,Next Sentence Prediction,named-entity-recognition,8,21,0.253012048192771,298,0.7700258397932817,11,0.1506849315068493,0,0
300,The first sentence receives the A embedding and the second receives the B embedding .,Conclusion,Next Sentence Prediction,named-entity-recognition,8,22,0.2650602409638554,299,0.772609819121447,12,0.1643835616438356,0,0
301,"50 % of the time B is the actual next sentence that follows A and 50 % of the time it is a random sentence , which is done for the "" next sentence prediction "" task .",Conclusion,Next Sentence Prediction,named-entity-recognition,8,23,0.2771084337349397,300,0.7751937984496124,13,0.1780821917808219,0,0
302,They are sampled such that the combined length is ? 512 tokens .,Conclusion,Next Sentence Prediction,named-entity-recognition,8,24,0.2891566265060241,301,0.7777777777777778,14,0.1917808219178082,0,0
303,"The LM masking is applied after WordPiece tokenization with a uniform masking rate of 15 % , and no special consideration given to partial word pieces .",Conclusion,Next Sentence Prediction,named-entity-recognition,8,25,0.3012048192771084,302,0.7803617571059431,15,0.2054794520547945,0,0
304,"We train with batch size of 256 sequences ( 256 sequences * 512 tokens = 128,000 tokens / batch ) for 1,000,000 steps , which is approximately 40 epochs over the 3.3 billion word corpus .",Conclusion,Next Sentence Prediction,named-entity-recognition,8,26,0.3132530120481928,303,0.7829457364341085,16,0.2191780821917808,0,0
305,"We use Adam with learning rate of 1 e - 4 , ? 1 = 0.9 , ? 2 = 0.999 , L2 weight decay of 0.01 , learning rate warmup over the first 10,000 steps , and linear decay of the learning rate .",Conclusion,Next Sentence Prediction,named-entity-recognition,8,27,0.3253012048192771,304,0.7855297157622739,17,0.2328767123287671,0,0
306,We use a dropout probability of 0.1 on all layers .,Conclusion,Next Sentence Prediction,named-entity-recognition,8,28,0.3373493975903614,305,0.7881136950904393,18,0.2465753424657534,0,0
307,"We use a gelu activation rather than the standard relu , following OpenAI GPT .",Conclusion,Next Sentence Prediction,named-entity-recognition,8,29,0.3493975903614458,306,0.7906976744186046,19,0.2602739726027397,0,0
308,The training loss is the sum of the mean masked LM likelihood and the mean next sentence prediction likelihood .,Conclusion,Next Sentence Prediction,named-entity-recognition,8,30,0.3614457831325301,307,0.7932816537467701,20,0.273972602739726,0,0
309,Training of BERT BASE was performed on 4 Cloud TPUs in Pod configuration ( 16 TPU chips total ) .,Conclusion,Next Sentence Prediction,named-entity-recognition,8,31,0.3734939759036144,308,0.7958656330749354,21,0.2876712328767123,0,0
310,13 Training of BERT LARGE was performed on 16 Cloud TPUs ( 64 TPU chips total ) .,Conclusion,Next Sentence Prediction,named-entity-recognition,8,32,0.3855421686746988,309,0.7984496124031008,22,0.3013698630136986,0,0
311,Each pretraining took 4 days to complete .,Conclusion,Next Sentence Prediction,named-entity-recognition,8,33,0.3975903614457831,310,0.8010335917312662,23,0.3150684931506849,0,0
312,Longer sequences are disproportionately expensive because attention is quadratic to the sequence length .,Conclusion,Next Sentence Prediction,named-entity-recognition,8,34,0.4096385542168674,311,0.8036175710594315,24,0.3287671232876712,0,0
313,"To speedup pretraing in our experiments , we pre-train the model with sequence length of 128 for 90 % of the steps .",Conclusion,Next Sentence Prediction,named-entity-recognition,8,35,0.4216867469879518,312,0.8062015503875969,25,0.3424657534246575,0,0
314,"Then , we train the rest 10 % of the steps of sequence of 512 to learn the positional embeddings .",Conclusion,Next Sentence Prediction,named-entity-recognition,8,36,0.4337349397590361,313,0.8087855297157622,26,0.3561643835616438,0,0
315,A.3 Fine- tuning Procedure,Conclusion,,named-entity-recognition,8,37,0.4457831325301205,314,0.8113695090439277,27,0.3698630136986301,0,0
316,"For fine - tuning , most model hyperparameters are the same as in pre-training , with the exception of the batch size , learning rate , and number of training epochs .",Conclusion,A.3 Fine- tuning Procedure,named-entity-recognition,8,38,0.4578313253012048,315,0.813953488372093,28,0.3835616438356164,0,0
317,The dropout probability was always kept at 0.1 .,Conclusion,A.3 Fine- tuning Procedure,named-entity-recognition,8,39,0.4698795180722891,316,0.8165374677002584,29,0.3972602739726027,0,0
318,"The optimal hyperparameter values are task - specific , but we found the following range of possible values to work well across all tasks :",Conclusion,A.3 Fine- tuning Procedure,named-entity-recognition,8,40,0.4819277108433735,317,0.8191214470284238,30,0.410958904109589,0,0
319,"Batch size : 16 , 32",Conclusion,A.3 Fine- tuning Procedure,named-entity-recognition,8,41,0.4939759036144578,318,0.8217054263565892,31,0.4246575342465753,0,0
320,"Learning rate ( Adam ) : 5 e - 5 , 3 e - 5 , 2 e - 5 Number of epochs : 2 , 3 , 4",Conclusion,A.3 Fine- tuning Procedure,named-entity-recognition,8,42,0.5060240963855421,319,0.8242894056847545,32,0.4383561643835616,0,0
321,"We also observed that large data sets ( e.g. , 100 k + labeled training examples ) were far less sensitive to hyperparameter choice than small data sets .",Conclusion,A.3 Fine- tuning Procedure,named-entity-recognition,8,43,0.5180722891566265,320,0.8268733850129198,33,0.4520547945205479,0,0
322,"Fine - tuning is typically very fast , so it is reasonable to simply run an exhaustive search over the above parameters and choose the model that performs best on the development set .",Conclusion,A.3 Fine- tuning Procedure,named-entity-recognition,8,44,0.5301204819277109,321,0.8294573643410853,34,0.4657534246575342,0,0
323,"A.4 Comparison of BERT , ELMo , and",Conclusion,A.3 Fine- tuning Procedure,named-entity-recognition,8,45,0.5421686746987951,322,0.8320413436692506,35,0.4794520547945205,0,0
324,Open AI GPT,Conclusion,A.3 Fine- tuning Procedure,named-entity-recognition,8,46,0.5542168674698795,323,0.834625322997416,36,0.4931506849315068,0,0
325,"Here we studies the differences in recent popular representation learning models including ELMo , OpenAI GPT and BERT .",Conclusion,A.3 Fine- tuning Procedure,named-entity-recognition,8,47,0.5662650602409639,324,0.8372093023255814,37,0.5068493150684932,0,0
326,The comparisons between the model architectures are shown visually in .,Conclusion,A.3 Fine- tuning Procedure,named-entity-recognition,8,48,0.5783132530120482,325,0.8397932816537468,38,0.5205479452054794,0,0
327,"Note that in addition to the architecture differences , BERT and OpenAI GPT are finetuning approaches , while ELMo is a feature - based approach .",Conclusion,A.3 Fine- tuning Procedure,named-entity-recognition,8,49,0.5903614457831325,326,0.8423772609819121,39,0.5342465753424658,0,0
328,"The most comparable existing pre-training method to BERT is OpenAI GPT , which trains a left - to - right Transformer LM on a large text corpus .",Conclusion,A.3 Fine- tuning Procedure,named-entity-recognition,8,50,0.6024096385542169,327,0.8449612403100775,40,0.547945205479452,0,0
329,"In fact , many of the design decisions in BERT were intentionally made to make it as close to GPT as possible so that the two methods could be minimally compared .",Conclusion,A.3 Fine- tuning Procedure,named-entity-recognition,8,51,0.6144578313253012,328,0.8475452196382429,41,0.5616438356164384,0,0
330,"The core argument of this work is that the bi-directionality and the two pretraining tasks presented in Section 3.1 account for the majority of the empirical improvements , but we do note that there are several other differences between how BERT and GPT were trained :",Conclusion,A.3 Fine- tuning Procedure,named-entity-recognition,8,52,0.6265060240963856,329,0.8501291989664083,42,0.5753424657534246,0,0
331,"GPT is trained on the Books Corpus ( 800M words ) ; BERT is trained on the Books Corpus ( 800M words ) and Wikipedia ( 2,500 M words ) .",Conclusion,A.3 Fine- tuning Procedure,named-entity-recognition,8,53,0.6385542168674698,330,0.8527131782945736,43,0.589041095890411,0,0
332,"GPT was trained for 1 M steps with a batch size of 32,000 words ; BERT was trained for 1 M steps with a batch size of 128,000 words .",Conclusion,A.3 Fine- tuning Procedure,named-entity-recognition,8,54,0.6506024096385542,331,0.8552971576227391,44,0.6027397260273972,0,0
333,GPT used the same learning rate of 5 e - 5 for all fine - tuning experiments ; BERT chooses a task - specific fine - tuning learning rate which performs the best on the development set .,Conclusion,A.3 Fine- tuning Procedure,named-entity-recognition,8,55,0.6626506024096386,332,0.8578811369509044,45,0.6164383561643836,0,0
334,"To isolate the effect of these differences , we perform ablation experiments in Section 5.1 which demonstrate that the majority of the improvements are in fact coming from the two pre-training tasks and the bidirectionality they enable .",Conclusion,A.3 Fine- tuning Procedure,named-entity-recognition,8,56,0.6746987951807228,333,0.8604651162790697,46,0.6301369863013698,0,0
335,A.5 Illustrations of Fine - tuning on Different Tasks,Conclusion,,named-entity-recognition,8,57,0.6867469879518072,334,0.8630490956072352,47,0.6438356164383562,0,0
336,The illustration of fine - tuning BERT on different tasks can be seen in .,Conclusion,A.5 Illustrations of Fine - tuning on Different Tasks,named-entity-recognition,8,58,0.6987951807228916,335,0.8656330749354005,48,0.6575342465753424,0,0
337,"Our task - specific models are formed by incorporating BERT with one additional output layer , so a minimal number of parameters need to be learned from scratch .",Conclusion,A.5 Illustrations of Fine - tuning on Different Tasks,named-entity-recognition,8,59,0.7108433734939759,336,0.8682170542635659,49,0.6712328767123288,0,0
338,"Among the tasks , ( a ) and MNLI Multi - Genre Natural Language Inference is a large - scale , crowdsourced entailment classification task .",Conclusion,A.5 Illustrations of Fine - tuning on Different Tasks,named-entity-recognition,8,60,0.7228915662650602,337,0.8708010335917312,50,0.684931506849315,0,0
339,"Given a pair of sentences , the goal is to predict whether the second sentence is an entailment , contradiction , or neutral with respect to the first one .",Conclusion,A.5 Illustrations of Fine - tuning on Different Tasks,named-entity-recognition,8,61,0.7349397590361446,338,0.8733850129198967,51,0.6986301369863014,0,0
340,QQP,Conclusion,,named-entity-recognition,8,62,0.7469879518072289,339,0.875968992248062,52,0.7123287671232876,0,0
341,Quora Question,Conclusion,,named-entity-recognition,8,63,0.7590361445783133,340,0.8785529715762274,53,0.726027397260274,0,0
342,Pairs is a binary classification task where the goal is to determine if two questions asked on Quora are semantically equivalent .,Conclusion,Quora Question,named-entity-recognition,8,64,0.7710843373493976,341,0.8811369509043928,54,0.7397260273972602,0,0
343,BERT E [ CLS ],Conclusion,Quora Question,named-entity-recognition,8,65,0.7831325301204819,342,0.8837209302325582,55,0.7534246575342466,0,0
344,E 1 E ...,Conclusion,Quora Question,named-entity-recognition,8,66,0.7951807228915663,343,0.8863049095607235,56,0.7671232876712328,0,0
345,...,Conclusion,Quora Question,named-entity-recognition,8,67,0.8072289156626506,344,0.8888888888888888,57,0.7808219178082192,0,0
346,SST - 2,Conclusion,Quora Question,named-entity-recognition,8,68,0.8192771084337349,345,0.8914728682170543,58,0.7945205479452054,0,0
347,The Stanford Sentiment Treebank is a binary single - sentence classification task consisting of sentences extracted from movie reviews with human annotations of their sentiment .,Conclusion,Quora Question,named-entity-recognition,8,69,0.8313253012048193,346,0.8940568475452196,59,0.8082191780821918,0,0
348,CoLA,Conclusion,,named-entity-recognition,8,70,0.8433734939759037,347,0.896640826873385,60,0.821917808219178,0,0
349,"The Corpus of Linguistic Acceptability is a binary single - sentence classification task , where the goal is to predict whether an English sentence is linguistically "" acceptable "" or not .",Conclusion,CoLA,named-entity-recognition,8,71,0.8554216867469879,348,0.8992248062015504,61,0.8356164383561644,0,0
350,STS - B,Conclusion,CoLA,named-entity-recognition,8,72,0.8674698795180723,349,0.9018087855297158,62,0.8493150684931506,0,0
351,The Semantic Textual Similarity,Conclusion,,named-entity-recognition,8,73,0.8795180722891566,350,0.9043927648578812,63,0.863013698630137,0,0
352,Benchmark is a collection of sentence pairs drawn from news headlines and other sources .,Conclusion,The Semantic Textual Similarity,named-entity-recognition,8,74,0.891566265060241,351,0.9069767441860463,64,0.8767123287671232,0,0
353,They were annotated with a score from 1 to 5 denoting how similar the two sentences are in terms of semantic meaning .,Conclusion,The Semantic Textual Similarity,named-entity-recognition,8,75,0.9036144578313252,352,0.909560723514212,65,0.8904109589041096,0,0
354,MRPC,Conclusion,,named-entity-recognition,8,76,0.9156626506024096,353,0.9121447028423773,66,0.9041095890410958,0,0
355,"Microsoft Research Paraphrase Corpus consists of sentence pairs automatically extracted from online news sources , with human annotations for whether the sentences in the pair are semantically equivalent .",Conclusion,MRPC,named-entity-recognition,8,77,0.927710843373494,354,0.9147286821705426,67,0.9178082191780822,0,0
356,RTE,Conclusion,,named-entity-recognition,8,78,0.9397590361445785,355,0.917312661498708,68,0.9315068493150684,0,0
357,"Recognizing Textual Entailment is a binary entailment task similar to MNLI , but with much less training data ) .",Conclusion,RTE,named-entity-recognition,8,79,0.9518072289156626,356,0.9198966408268734,69,0.9452054794520548,0,0
358,14 WNLI Winograd NLI is a small natural language inference dataset .,Conclusion,RTE,named-entity-recognition,8,80,0.963855421686747,357,0.9224806201550388,70,0.958904109589041,0,0
359,"The GLUE webpage notes that there are issues with the construction of this dataset , 15 and every trained system that 's been submitted to GLUE has performed worse than the 65.1 baseline accuracy of predicting the majority class .",Conclusion,RTE,named-entity-recognition,8,81,0.9759036144578314,358,0.9250645994832042,71,0.9726027397260274,0,0
360,We therefore exclude this set to be fair to OpenAI GPT .,Conclusion,RTE,named-entity-recognition,8,82,0.9879518072289156,359,0.9276485788113696,72,0.9863013698630136,0,0
361,"For our GLUE submission , we always predicted the ma-jority class .",Conclusion,RTE,named-entity-recognition,8,83,1.0,360,0.9302325581395348,73,1.0,0,0
362,C Additional Ablation Studies,,,named-entity-recognition,8,0,0.0,361,0.9328165374677002,0,0.0,1,0
363,C.1 Effect of Number of Training Steps presents MNLI,C Additional Ablation Studies,,named-entity-recognition,8,1,0.04,362,0.9354005167958656,1,0.0909090909090909,1,0
364,Dev accuracy after finetuning from a checkpoint that has been pre-trained fork steps .,C Additional Ablation Studies,C.1 Effect of Number of Training Steps presents MNLI,named-entity-recognition,8,2,0.08,363,0.937984496124031,2,0.1818181818181818,1,0
365,This allows us to answer the following questions :,C Additional Ablation Studies,C.1 Effect of Number of Training Steps presents MNLI,named-entity-recognition,8,3,0.12,364,0.9405684754521964,3,0.2727272727272727,1,0
366,1 .,C Additional Ablation Studies,C.1 Effect of Number of Training Steps presents MNLI,named-entity-recognition,8,4,0.16,365,0.9431524547803618,4,0.3636363636363636,1,0
367,Question :,C Additional Ablation Studies,C.1 Effect of Number of Training Steps presents MNLI,named-entity-recognition,8,5,0.2,366,0.9457364341085271,5,0.4545454545454545,1,0
368,"Does BERT really need such a large amount of pre-training ( 128,000 words / batch * 1,000,000 steps ) to achieve high fine - tuning accuracy ?",C Additional Ablation Studies,C.1 Effect of Number of Training Steps presents MNLI,named-entity-recognition,8,6,0.24,367,0.9483204134366924,6,0.5454545454545454,1,0
369,"Answer : Yes , BERT BASE achieves almost 1.0 % additional accuracy on MNLI when trained on 1 M steps compared to 500 k steps .",C Additional Ablation Studies,C.1 Effect of Number of Training Steps presents MNLI,named-entity-recognition,8,7,0.28,368,0.9509043927648578,7,0.6363636363636364,1,0
370,2 . Question :,C Additional Ablation Studies,C.1 Effect of Number of Training Steps presents MNLI,named-entity-recognition,8,8,0.32,369,0.9534883720930232,8,0.7272727272727273,1,0
371,"Does MLM pre-training converge slower than LTR pre-training , since only 15 % of words are predicted in each batch rather than every word ?",C Additional Ablation Studies,C.1 Effect of Number of Training Steps presents MNLI,named-entity-recognition,8,9,0.36,370,0.9560723514211886,9,0.8181818181818182,1,0
372,Answer : The MLM model does converge slightly slower than the LTR model .,C Additional Ablation Studies,C.1 Effect of Number of Training Steps presents MNLI,named-entity-recognition,8,10,0.4,371,0.958656330749354,10,0.9090909090909092,1,0
373,"However , in terms of absolute accuracy the MLM model begins to outperform the LTR model almost immediately .",C Additional Ablation Studies,C.1 Effect of Number of Training Steps presents MNLI,named-entity-recognition,8,11,0.44,372,0.9612403100775194,11,1.0,1,0
374,C.2 Ablation for Different Masking Procedures,C Additional Ablation Studies,,named-entity-recognition,8,12,0.48,373,0.9638242894056848,0,0.0,1,0
375,"In Section 3.1 , we mention that BERT uses a mixed strategy for masking the target tokens when pre-training with the masked language model ( MLM ) objective .",C Additional Ablation Studies,C.2 Ablation for Different Masking Procedures,named-entity-recognition,8,13,0.52,374,0.96640826873385,1,0.0769230769230769,1,0
376,The following is an ablation study to evaluate the effect of different masking strategies .,C Additional Ablation Studies,C.2 Ablation for Different Masking Procedures,named-entity-recognition,8,14,0.56,375,0.9689922480620154,2,0.1538461538461538,1,0
377,"Note that the purpose of the masking strategies is to reduce the mismatch between pre-training and fine - tuning , as the [ MASK ] symbol never appears during the fine - tuning stage .",C Additional Ablation Studies,C.2 Ablation for Different Masking Procedures,named-entity-recognition,8,15,0.6,376,0.9715762273901808,3,0.2307692307692307,1,0
378,We report the Dev results for both MNLI and NER .,C Additional Ablation Studies,C.2 Ablation for Different Masking Procedures,named-entity-recognition,8,16,0.64,377,0.9741602067183462,4,0.3076923076923077,1,0
379,"For NER , we report both fine - tuning and feature - based approaches , as we expect the mismatch will be amplified for the feature - based approach as the model will not have the chance to adjust the representations .",C Additional Ablation Studies,C.2 Ablation for Different Masking Procedures,named-entity-recognition,8,17,0.68,378,0.9767441860465116,5,0.3846153846153846,1,0
380,The results are presented in .,C Additional Ablation Studies,C.2 Ablation for Different Masking Procedures,named-entity-recognition,8,18,0.72,379,0.979328165374677,6,0.4615384615384615,1,0
381,"In the table , MASK means that we replace the target token with the [ MASK ] symbol for MLM ; SAME means that we keep the target token as is ; RND means that we replace the target token with another random token .",C Additional Ablation Studies,C.2 Ablation for Different Masking Procedures,named-entity-recognition,8,19,0.76,380,0.9819121447028424,7,0.5384615384615384,1,0
382,"The numbers in the left part of the table represent the probabilities of the specific strategies used during MLM pre-training ( BERT uses 80 % , 10 % , 10 % ) .",C Additional Ablation Studies,C.2 Ablation for Different Masking Procedures,named-entity-recognition,8,20,0.8,381,0.9844961240310076,8,0.6153846153846154,1,0
383,The right part of the paper represents the Dev set results .,C Additional Ablation Studies,C.2 Ablation for Different Masking Procedures,named-entity-recognition,8,21,0.84,382,0.9870801033591732,9,0.6923076923076923,1,0
384,"For the feature - based approach , we concatenate the last 4 layers of BERT as the features , which was shown to be the best approach in Section 5.3 .",C Additional Ablation Studies,C.2 Ablation for Different Masking Procedures,named-entity-recognition,8,22,0.88,383,0.9896640826873384,10,0.7692307692307693,1,0
385,From the table it can be seen that fine - tuning is surprisingly robust to different masking strategies .,C Additional Ablation Studies,C.2 Ablation for Different Masking Procedures,named-entity-recognition,8,23,0.92,384,0.992248062015504,11,0.8461538461538461,1,0
386,"However , as expected , using only the MASK strategy was problematic when applying the featurebased approach to NER .",C Additional Ablation Studies,C.2 Ablation for Different Masking Procedures,named-entity-recognition,8,24,0.96,385,0.9948320413436692,12,0.9230769230769232,1,0
387,"Interestingly , using only the RND strategy performs much worse than our strategy as well .",C Additional Ablation Studies,C.2 Ablation for Different Masking Procedures,named-entity-recognition,8,25,1.0,386,0.9974160206718348,13,1.0,1,0
1,title,,,named-entity-recognition,9,0,0.0,0,0.0,0,0.0,1,0
2,Data and text mining BioBERT : a pre-trained biomedical language representation model for biomedical text mining,title,title,named-entity-recognition,9,1,0.0,1,0.0050251256281407,1,0.0,1,1
3,abstract,,,named-entity-recognition,9,0,0.0,2,0.0100502512562814,0,0.0,1,0
4,Motivation :,abstract,abstract,named-entity-recognition,9,1,0.0833333333333333,3,0.0150753768844221,1,0.0833333333333333,1,0
5,Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows .,abstract,abstract,named-entity-recognition,9,2,0.1666666666666666,4,0.0201005025125628,2,0.1666666666666666,1,0
6,"With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models .",abstract,abstract,named-entity-recognition,9,3,0.25,5,0.0251256281407035,3,0.25,1,1
7,"However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora .",abstract,abstract,named-entity-recognition,9,4,0.3333333333333333,6,0.0301507537688442,4,0.3333333333333333,1,1
8,"In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",abstract,abstract,named-entity-recognition,9,5,0.4166666666666667,7,0.0351758793969849,5,0.4166666666666667,1,1
9,Results :,abstract,abstract,named-entity-recognition,9,6,0.5,8,0.0402010050251256,6,0.5,1,0
10,"We introduce BioBERT ( Bidirectional Encoder Representations from Transformers for Biomedical Text Mining ) , which is a domain - specific language representation model pre-trained on large - scale biomedical corpora .",abstract,abstract,named-entity-recognition,9,7,0.5833333333333334,9,0.0452261306532663,7,0.5833333333333334,1,0
11,"With almost the same architecture across tasks , BioBERT largely outperforms BERT and previous state - of - the - art models in a variety of biomedical text mining tasks when pre-trained on biomedical corpora .",abstract,abstract,named-entity-recognition,9,8,0.6666666666666666,10,0.050251256281407,8,0.6666666666666666,1,0
12,"While BERT obtains performance comparable to that of previous state - of - the - art models , BioBERT significantly outperforms them on the following three representative biomedical text mining tasks : biomedical named entity recognition ( 0.62 % F1 score improvement ) , biomedical relation extraction ( 2.80 % F1 score improvement ) and biomedical question answering ( 12.24 % MRR improvement ) .",abstract,abstract,named-entity-recognition,9,9,0.75,11,0.0552763819095477,9,0.75,1,0
13,Our analysis results show that pre-training BERT on biomedical corpora helps it to understand complex biomedical texts .,abstract,abstract,named-entity-recognition,9,10,0.8333333333333334,12,0.0603015075376884,10,0.8333333333333334,1,0
14,Availability and implementation :,abstract,abstract,named-entity-recognition,9,11,0.9166666666666666,13,0.0653266331658291,11,0.9166666666666666,1,0
15,"We make the pre-trained weights of BioBERT freely available at https://github. com/naver/biobert-pretrained , and the source code for fine - tuning",abstract,abstract,named-entity-recognition,9,12,1.0,14,0.0703517587939698,12,1.0,1,1
16,Introduction,,,named-entity-recognition,9,0,0.0,15,0.0753768844221105,0,0.0,1,0
17,The volume of biomedical literature continues to rapidly increase .,Introduction,Introduction,named-entity-recognition,9,1,0.0625,16,0.0804020100502512,1,0.0625,1,0
18,"On average , more than 3000 new articles are published everyday in peer-reviewed journals , excluding pre-prints and technical reports such as clinical trial reports in various archives .",Introduction,Introduction,named-entity-recognition,9,2,0.125,17,0.0854271356783919,2,0.125,1,0
19,PubMed alone has a total of 29M articles as of January 2019 .,Introduction,Introduction,named-entity-recognition,9,3,0.1875,18,0.0904522613065326,3,0.1875,1,0
20,Reports containing valuable information about new discoveries and new insights are continuously added to the already overwhelming amount of literature .,Introduction,Introduction,named-entity-recognition,9,4,0.25,19,0.0954773869346733,4,0.25,1,0
21,"Consequently , there is increasingly more demand for accurate biomedical text mining tools for extracting information from the literature .",Introduction,Introduction,named-entity-recognition,9,5,0.3125,20,0.100502512562814,5,0.3125,1,0
22,Recent progress of biomedical text mining models was made possible by the advancements of deep learning techniques used in natural language processing ( NLP ) .,Introduction,Introduction,named-entity-recognition,9,6,0.375,21,0.1055276381909547,6,0.375,1,0
23,"For instance , Long Short - Term Memory ( LSTM ) and Conditional Random Field ( CRF ) have greatly improved performance in biomedical named entity recognition ( NER ) over the last few years .",Introduction,Introduction,named-entity-recognition,9,7,0.4375,22,0.1105527638190954,7,0.4375,1,0
24,Other deep learning based models have made improvements in biomedical text mining tasks such as relation extraction ( RE ) and question answering ( QA ) .,Introduction,Introduction,named-entity-recognition,9,8,0.5,23,0.1155778894472361,8,0.5,1,0
25,"However , directly applying state - of - the - art NLP methodologies to biomedical text mining has limitations .",Introduction,Introduction,named-entity-recognition,9,9,0.5625,24,0.1206030150753768,9,0.5625,1,0
26,"First , as recent word representation models such as Word2 Vec , ELMo and BERT are trained and tested mainly on datasets containing general domain texts ( e.g. Wikipedia ) , it is difficult to estimate their performance on datasets containing biomedical texts .",Introduction,Introduction,named-entity-recognition,9,10,0.625,25,0.1256281407035175,10,0.625,1,0
27,"Also , the word distributions of general and biomedical corpora are quite different , which can often be a problem for biomedical text mining models .",Introduction,Introduction,named-entity-recognition,9,11,0.6875,26,0.1306532663316583,11,0.6875,1,1
28,"As a result , recent models in biomedical text mining rely largely on adapted versions of word representations .",Introduction,Introduction,named-entity-recognition,9,12,0.75,27,0.135678391959799,12,0.75,1,0
29,"In this study , we hypothesize that current state - of - the - art word representation models such as BERT need to be trained on biomedical corpora to be effective in biomedical text mining tasks .",Introduction,Introduction,named-entity-recognition,9,13,0.8125,28,0.1407035175879397,13,0.8125,1,0
30,"Previously , Word2 Vec , which is one of the most widely known context independent word representation models , was trained on biomedical corpora which contain terms and expressions that are usually not included in a general domain corpus .",Introduction,Introduction,named-entity-recognition,9,14,0.875,29,0.1457286432160804,14,0.875,1,0
31,"While ELMo and BERT have proven the effectiveness of contextualized word representations , they can not obtain high performance on biomedical corpora because they are pre-trained on only general domain corpora .",Introduction,Introduction,named-entity-recognition,9,15,0.9375,30,0.1507537688442211,15,0.9375,1,0
32,"As BERT achieves very strong results on various NLP tasks while using almost the same structure across the tasks , adapting BERT for the biomedical domain could potentially benefit numerous biomedical NLP researches .",Introduction,Introduction,named-entity-recognition,9,16,1.0,31,0.1557788944723618,16,1.0,1,0
33,Approach,,,named-entity-recognition,9,0,0.0,32,0.1608040201005025,0,0.0,1,0
34,"In this article , we introduce BioBERT , which is a pre-trained language representation model for the biomedical domain .",Approach,Approach,named-entity-recognition,9,1,0.0769230769230769,33,0.1658291457286432,1,0.0769230769230769,1,1
35,The overall process of pre-training and fine - tuning BioBERT is illustrated in .,Approach,Approach,named-entity-recognition,9,2,0.1538461538461538,34,0.1708542713567839,2,0.1538461538461538,1,0
36,"First , we initialize BioBERT with weights from BERT , which was pretrained on general domain corpora ( English Wikipedia and Books Corpus ) .",Approach,Approach,named-entity-recognition,9,3,0.2307692307692307,35,0.1758793969849246,3,0.2307692307692307,1,1
37,"Then , BioBERT is pre-trained on biomedical domain corpora ( PubMed abstracts and PMC full - text articles ) .",Approach,Approach,named-entity-recognition,9,4,0.3076923076923077,36,0.1809045226130653,4,0.3076923076923077,1,1
38,"To show the effectiveness of our approach in biomedical text mining , BioBERT is fine - tuned and evaluated on three popular biomedical text mining tasks .",Approach,Approach,named-entity-recognition,9,5,0.3846153846153846,37,0.185929648241206,5,0.3846153846153846,1,1
39,"We test various pre-training strategies with different combinations and sizes of general domain corpora and biomedical corpora , and analyze the effect of each corpus on pre-training .",Approach,Approach,named-entity-recognition,9,6,0.4615384615384615,38,0.1909547738693467,6,0.4615384615384615,1,0
40,We also provide in - depth analyses of BERT and BioBERT to show the necessity of our pre-training strategies .,Approach,Approach,named-entity-recognition,9,7,0.5384615384615384,39,0.1959798994974874,7,0.5384615384615384,1,0
41,The contributions of our paper are as follows :,Approach,Approach,named-entity-recognition,9,8,0.6153846153846154,40,0.2010050251256281,8,0.6153846153846154,1,0
42,BioBERT is the first domain - specific BERT based model pretrained on biomedical corpora for 23 days on eight NVIDIA V100 GPUs .,Approach,Approach,named-entity-recognition,9,9,0.6923076923076923,41,0.2060301507537688,9,0.6923076923076923,1,0
43,We show that pre-training BERT on biomedical corpora largely improves its performance .,Approach,Approach,named-entity-recognition,9,10,0.7692307692307693,42,0.2110552763819095,10,0.7692307692307693,1,0
44,"BioBERT obtains higher F 1 scores in biomedical NER ( 0.62 ) and biomedical RE ( 2.80 ) , and a higher MRR score ( 12.24 ) in biomedical QA than the current state - of the - art models .",Approach,Approach,named-entity-recognition,9,11,0.8461538461538461,43,0.2160804020100502,11,0.8461538461538461,1,0
45,"Compared with most previous biomedical text mining models that are mainly focused on a single task such as NER or QA , our model BioBERT achieves state - of - the - art performance on various biomedical text mining tasks , while requiring only minimal architectural modifications .",Approach,Approach,named-entity-recognition,9,12,0.9230769230769232,44,0.2211055276381909,12,0.9230769230769232,1,0
46,"We make our pre-processed datasets , the pre-trained weights of BioBERT and the source code for fine - tuning BioBERT publicly available .",Approach,Approach,named-entity-recognition,9,13,1.0,45,0.2261306532663316,13,1.0,1,0
47,Materials and methods,,,named-entity-recognition,9,0,0.0,46,0.2311557788944723,0,0.0,1,0
48,BioBERT basically has the same structure as BERT .,Materials and methods,Materials and methods,named-entity-recognition,9,1,0.0204081632653061,47,0.236180904522613,1,0.0769230769230769,1,0
49,"We briefly discuss the recently proposed BERT , and then we describe in detail the pre-training and fine - tuning process of BioBERT .",Materials and methods,Materials and methods,named-entity-recognition,9,2,0.0408163265306122,48,0.2412060301507537,2,0.1538461538461538,1,0
50,BERT : bidirectional encoder representations from transformers,Materials and methods,Materials and methods,named-entity-recognition,9,3,0.0612244897959183,49,0.2462311557788944,3,0.2307692307692307,1,0
51,Learning word representations from a large amount of unannotated text is a long - established method .,Materials and methods,Materials and methods,named-entity-recognition,9,4,0.0816326530612244,50,0.2512562814070351,4,0.3076923076923077,1,0
52,"While previous models ( e.g. Word2 Vec , GloVe ) focused on learning context independent word representations , recent works have focused on learning context dependent word representations .",Materials and methods,Materials and methods,named-entity-recognition,9,5,0.1020408163265306,51,0.2562814070351759,5,0.3846153846153846,1,0
53,"For instance , ELMo uses a bidirectional language model , while uses machine translation to embed context information into word representations .",Materials and methods,Materials and methods,named-entity-recognition,9,6,0.1224489795918367,52,0.2613065326633166,6,0.4615384615384615,1,0
54,BERT is a contextualized word representation model that is based on a masked language model and pretrained using bidirectional transformers .,Materials and methods,Materials and methods,named-entity-recognition,9,7,0.1428571428571428,53,0.2663316582914573,7,0.5384615384615384,1,0
55,"Due to the nature of language modeling where future words can not be seen , previous language models were limited to a combination of two unidirectional language models ( i.e. left - to - right and right - toleft ) .",Materials and methods,Materials and methods,named-entity-recognition,9,8,0.1632653061224489,54,0.271356783919598,8,0.6153846153846154,1,0
56,"BERT uses a masked language model that predicts randomly masked words in a sequence , and hence can be used for learning bidirectional representations .",Materials and methods,Materials and methods,named-entity-recognition,9,9,0.1836734693877551,55,0.2763819095477386,9,0.6923076923076923,1,0
57,"Also , it obtains state - of - the - art performance on most NLP tasks , while requiring minimal task - specific architectural modification .",Materials and methods,Materials and methods,named-entity-recognition,9,10,0.2040816326530612,56,0.2814070351758794,10,0.7692307692307693,1,0
58,"According to the authors of BERT , incorporating information from bidirectional representations , rather than unidirectional representations , is crucial for representing words in natural language .",Materials and methods,Materials and methods,named-entity-recognition,9,11,0.2244897959183673,57,0.2864321608040201,11,0.8461538461538461,1,0
59,We hypothesize that such bidirectional representations are also critical in biomedical text mining as complex relationships between biomedical terms often exist in a biomedical corpus .,Materials and methods,Materials and methods,named-entity-recognition,9,12,0.2448979591836734,58,0.2914572864321608,12,0.9230769230769232,1,0
60,"Due to the space limitations , we refer readers to fora more detailed description of BERT .",Materials and methods,Materials and methods,named-entity-recognition,9,13,0.2653061224489796,59,0.2964824120603015,13,1.0,1,0
61,Pre-training BioBERT,Materials and methods,Materials and methods,named-entity-recognition,9,14,0.2857142857142857,60,0.3015075376884422,0,0.0,1,0
62,"As a general purpose language representation model , BERT was pretrained on English Wikipedia and Books Corpus .",Materials and methods,Materials and methods,named-entity-recognition,9,15,0.3061224489795918,61,0.3065326633165829,1,0.0769230769230769,1,0
63,"However , biomedical domain texts contain a considerable number of domain - specific .",Materials and methods,Materials and methods,named-entity-recognition,9,16,0.3265306122448979,62,0.3115577889447236,2,0.1538461538461538,1,0
64,"Overview of the pre-training and fine - tuning of BioBERT proper nouns ( e.g. BRCA1 , c.248T > C ) and terms ( e.g. transcriptional , antimicrobial ) , which are understood mostly by biomedical researchers .",Materials and methods,Materials and methods,named-entity-recognition,9,17,0.3469387755102041,63,0.3165829145728643,3,0.2307692307692307,1,0
65,"As a result , NLP models designed for general purpose language understanding often obtains poor performance in biomedical text mining tasks .",Materials and methods,Materials and methods,named-entity-recognition,9,18,0.3673469387755102,64,0.321608040201005,4,0.3076923076923077,1,0
66,"In this work , we pre-train BioBERT on PubMed abstracts ( PubMed ) and PubMed Central full - text articles ( PMC ) .",Materials and methods,Materials and methods,named-entity-recognition,9,19,0.3877551020408163,65,0.3266331658291457,5,0.3846153846153846,1,0
67,"The text corpora used for pre-training of BioBERT are listed in , and the tested combinations of text corpora are listed in .",Materials and methods,Materials and methods,named-entity-recognition,9,20,0.4081632653061224,66,0.3316582914572864,6,0.4615384615384615,1,0
68,"For computational efficiency , whenever the Wiki Books corpora were used for pre-training , we initialized BioBERT with the pre-trained BERT model provided by .",Materials and methods,Materials and methods,named-entity-recognition,9,21,0.4285714285714285,67,0.3366834170854271,7,0.5384615384615384,1,0
69,We define BioBERT as a language representation model whose pre-training corpora includes biomedical corpora ( e.g. BioBERT ( PubMed ) ) .,Materials and methods,Materials and methods,named-entity-recognition,9,22,0.4489795918367347,68,0.3417085427135678,8,0.6153846153846154,1,0
70,"For tokenization , BioBERT uses WordPiece tokenization , which mitigates the out - of - vocabulary issue .",Materials and methods,Materials and methods,named-entity-recognition,9,23,0.4693877551020408,69,0.3467336683417085,9,0.6923076923076923,1,0
71,"With WordPiece tokenization , any new words can be represented by frequent subwords ( e.g. Immunoglobulin >",Materials and methods,Materials and methods,named-entity-recognition,9,24,0.4897959183673469,70,0.3517587939698492,10,0.7692307692307693,1,0
72,I ##mm ##uno ##g ##lo # #bul # #in ) .,Materials and methods,Materials and methods,named-entity-recognition,9,25,0.5102040816326531,71,0.3567839195979899,11,0.8461538461538461,1,0
73,We found that using cased vocabulary ( not lowercasing ) results in slightly better performances in downstream tasks .,Materials and methods,Materials and methods,named-entity-recognition,9,26,0.5306122448979592,72,0.3618090452261306,12,0.9230769230769232,1,0
74,"Although we could have constructed new WordPiece vocabulary based on biomedical corpora , we used the original vocabulary of BERT BASE for the following reasons : ( i ) compatibility of BioBERT with BERT , which allows BERT pre-trained on general domain corpora to be re-used , and makes it easier to interchangeably use existing models based on BERT and BioBERT and ( ii ) any new words may still be represented and fine - tuned for the biomedical domain using the original WordPiece vocabulary of BERT .",Materials and methods,Materials and methods,named-entity-recognition,9,27,0.5510204081632653,73,0.3668341708542713,13,1.0,1,0
75,Fine-tuning BioBERT,Materials and methods,Materials and methods,named-entity-recognition,9,28,0.5714285714285714,74,0.371859296482412,0,0.0,1,0
76,"With minimal architectural modification , BioBERT can be applied to various downstream text mining tasks .",Materials and methods,Materials and methods,named-entity-recognition,9,29,0.5918367346938775,75,0.3768844221105528,1,0.0476190476190476,1,0
77,"We fine - tune BioBERT on the following three representative biomedical text mining tasks : NER , RE and QA .",Materials and methods,Materials and methods,named-entity-recognition,9,30,0.6122448979591837,76,0.3819095477386934,2,0.0952380952380952,1,0
78,"Named entity recognition is one of the most fundamental biomedical text mining tasks , which involves recognizing numerous domain - specific proper nouns in a biomedical corpus .",Materials and methods,Materials and methods,named-entity-recognition,9,31,0.6326530612244898,77,0.3869346733668342,3,0.1428571428571428,1,0
79,"While most previous works were built upon different combinations of LSTMs and CRFs , BERT has a simple architecture based on bidirectional transformers .",Materials and methods,Materials and methods,named-entity-recognition,9,32,0.6530612244897959,78,0.3919597989949748,4,0.1904761904761904,1,0
80,BERT uses a single output layer based on the representations from its last layer to compute only token level BIO2 probabilities .,Materials and methods,Materials and methods,named-entity-recognition,9,33,0.673469387755102,79,0.3969849246231156,5,0.238095238095238,1,0
81,"Note that while previous works in biomedical NER often used word embeddings trained on PubMed or PMC corpora , BioBERT directly learns WordPiece embeddings during pre-training and fine - tuning .",Materials and methods,Materials and methods,named-entity-recognition,9,34,0.6938775510204082,80,0.4020100502512563,6,0.2857142857142857,1,0
82,"For the evaluation metrics of NER , we used entity level precision , recall and F1 score .",Materials and methods,Materials and methods,named-entity-recognition,9,35,0.7142857142857143,81,0.4070351758793969,7,0.3333333333333333,1,0
83,Relation extraction is a task of classifying relations of named entities in a biomedical corpus .,Materials and methods,Materials and methods,named-entity-recognition,9,36,0.7346938775510204,82,0.4120603015075377,8,0.3809523809523809,1,0
84,"We utilized the sentence classifier of the original version of BERT , which uses a [ CLS ] token for the classification of relations .",Materials and methods,Materials and methods,named-entity-recognition,9,37,0.7551020408163265,83,0.4170854271356783,9,0.4285714285714285,1,0
85,Sentence classification is performed using a single output layer based on a [ CLS ] token representation from BERT .,Materials and methods,Materials and methods,named-entity-recognition,9,38,0.7755102040816326,84,0.4221105527638191,10,0.4761904761904761,1,0
86,We anonymized target named entities in a sentence using pre-defined tags such as @ GENE $ or @DISEASE $ .,Materials and methods,Materials and methods,named-entity-recognition,9,39,0.7959183673469388,85,0.4271356783919598,11,0.5238095238095238,1,0
87,"For instance , a sentence with two target entities ( gene and disease in this case ) is represented as "" Serine at position 986 of @GENE $ maybe an independent genetic predictor of angiographic @DISEASE $ . """,Materials and methods,Materials and methods,named-entity-recognition,9,40,0.8163265306122449,86,0.4321608040201005,12,0.5714285714285714,1,0
88,"The precision , recall and F 1 scores on the RE task are reported .",Materials and methods,Materials and methods,named-entity-recognition,9,41,0.8367346938775511,87,0.4371859296482412,13,0.6190476190476191,1,0
89,Question answering is a task of answering questions posed in natural language given related passages .,Materials and methods,Materials and methods,named-entity-recognition,9,42,0.8571428571428571,88,0.4422110552763819,14,0.6666666666666666,1,0
90,"To fine - tune BioBERT for QA , we used the same BERT architecture used for SQuAD .",Materials and methods,Materials and methods,named-entity-recognition,9,43,0.8775510204081632,89,0.4472361809045226,15,0.7142857142857143,1,0
91,We used the BioASQ factoid datasets because their format is similar to that of SQuAD .,Materials and methods,Materials and methods,named-entity-recognition,9,44,0.8979591836734694,90,0.4522613065326633,16,0.7619047619047619,1,0
92,Token level probabilities for the start / end location of answer phrases are computed using a single output layer .,Materials and methods,Materials and methods,named-entity-recognition,9,45,0.9183673469387756,91,0.457286432160804,17,0.8095238095238095,1,0
93,"However , we observed that about 30 % of the BioASQ factoid questions were unanswerable in an extractive QA setting as the exact answers did not appear in the given passages .",Materials and methods,Materials and methods,named-entity-recognition,9,46,0.9387755102040816,92,0.4623115577889447,18,0.8571428571428571,1,0
94,"Like , we excluded the samples with unanswerable questions from the training sets .",Materials and methods,Materials and methods,named-entity-recognition,9,47,0.9591836734693876,93,0.4673366834170854,19,0.9047619047619048,1,0
95,"Also , we used the same pre-training process of , which uses SQuAD , and it largely improved the performance of both BERT and BioBERT .",Materials and methods,Materials and methods,named-entity-recognition,9,48,0.979591836734694,94,0.4723618090452261,20,0.9523809523809524,1,0
96,"We used the following evaluation metrics from BioASQ : strict accuracy , lenient accuracy and mean reciprocal rank .",Materials and methods,Materials and methods,named-entity-recognition,9,49,1.0,95,0.4773869346733668,21,1.0,1,0
97,Results,,,named-entity-recognition,9,0,0.0,96,0.4824120603015075,0,0.0,1,0
98,Datasets,Results,,named-entity-recognition,9,1,0.05,97,0.4874371859296482,0,0.0,1,0
99,The statistics of biomedical NER datasets are listed in .,Results,Datasets,named-entity-recognition,9,2,0.1,98,0.4924623115577889,1,0.0526315789473684,1,0
100,"We used the pre-processed versions of all the NER datasets provided by except the 2010 i 2 b2 / VA , JNLPBA and Species - 800 datasets .",Results,Datasets,named-entity-recognition,9,3,0.15,99,0.4974874371859296,2,0.1052631578947368,1,0
101,The pre-processed NCBI Disease dataset has fewer annotations than the original dataset due to the removal of duplicate articles from its training set .,Results,Datasets,named-entity-recognition,9,4,0.2,100,0.5025125628140703,3,0.1578947368421052,1,0
102,We used the CoNLL format ( https :// github.com/spyysalo/standoff2conll ) for pre-processing the 2010 i 2b2 / VA and JNLPBA datasets .,Results,Datasets,named-entity-recognition,9,5,0.25,101,0.507537688442211,4,0.2105263157894736,1,0
103,The Species - 800 dataset was preprocessed and split based on the dataset of Pyysalo ( https://github. com/spyysalo/s800 ) .,Results,Datasets,named-entity-recognition,9,6,0.3,102,0.5125628140703518,5,0.2631578947368421,1,0
104,"We did not use alternate annotations for the BC2 GM dataset , and all NER evaluations are based on entity - level exact matches .",Results,Datasets,named-entity-recognition,9,7,0.35,103,0.5175879396984925,6,0.3157894736842105,1,0
105,"Note that although there are several other recently introduced high quality biomedical NER datasets , we use datasets that are frequently used by many biomedical NLP researchers , which makes it much easier to compare our work with theirs .",Results,Datasets,named-entity-recognition,9,8,0.4,104,0.5226130653266332,7,0.3684210526315789,1,0
106,The RE datasets contain gene - disease relations and protein - chemical relations ) .,Results,Datasets,named-entity-recognition,9,9,0.45,105,0.5276381909547738,8,0.4210526315789473,1,0
107,Pre-processed GAD and EU - ADR datasets are available with our provided codes .,Results,Datasets,named-entity-recognition,9,10,0.5,106,0.5326633165829145,9,0.4736842105263157,1,0
108,"For the CHEMPROT dataset , we used the same pre-processing procedure described in .",Results,Datasets,named-entity-recognition,9,11,0.55,107,0.5376884422110553,10,0.5263157894736842,1,0
109,"We used the BioASQ factoid datasets , which can be converted into the same format as the SQuAD dataset ) .",Results,Datasets,named-entity-recognition,9,12,0.6,108,0.542713567839196,11,0.5789473684210527,1,0
110,We used full abstracts ( PMIDs ) and related questions and answers provided by the BioASQ organizers .,Results,Datasets,named-entity-recognition,9,13,0.65,109,0.5477386934673367,12,0.631578947368421,1,0
111,We have made the pre-processed BioASQ datasets publicly available .,Results,Datasets,named-entity-recognition,9,14,0.7,110,0.5527638190954773,13,0.6842105263157895,1,0
112,"For all the datasets , we used the same dataset splits used in previous works ) fora fair evaluation ; however , the splits of LINAAEUS and Species - 800 could not be found from and maybe different .",Results,Datasets,named-entity-recognition,9,15,0.75,111,0.5577889447236181,14,0.7368421052631579,1,0
113,"Like previous work , we reported the performance of 10 - fold cross-validation on datasets that do not have separate test sets ( e.g. GAD , EU - ADR ) .",Results,Datasets,named-entity-recognition,9,16,0.8,112,0.5628140703517588,15,0.7894736842105263,1,0
114,We compare BERT and BioBERT with the current state - of - theart models and report their scores .,Results,Datasets,named-entity-recognition,9,17,0.85,113,0.5678391959798995,16,0.8421052631578947,1,0
115,Note that the state - of - the - art models each have a different architecture and training procedure .,Results,Datasets,named-entity-recognition,9,18,0.9,114,0.5728643216080402,17,0.8947368421052632,1,0
116,"For instance , the state - of - the - art model by trained on the JNLPBA dataset is based on multiple Bi - LSTM CRF models with character level CNNs , while the state - of - the - art model by trained on the LINNAEUS dataset uses a Bi - LSTM CRF model with character level LSTMs and is additionally trained on silver - standard datasets .",Results,Datasets,named-entity-recognition,9,19,0.95,115,0.5778894472361809,18,0.9473684210526316,1,0
117,"On the other hand , BERT and BioBERT have exactly the same structure , and use only the gold standard datasets and not any additional datasets .",Results,Datasets,named-entity-recognition,9,20,1.0,116,0.5829145728643216,19,1.0,1,0
118,Experimental setups,,,named-entity-recognition,9,0,0.0,117,0.5879396984924623,0,0.0,1,0
119,We used the BERT BASE model pre-trained on English Wikipedia and Books Corpus for 1 M steps .,Experimental setups,Experimental setups,named-entity-recognition,9,1,0.0625,118,0.592964824120603,1,0.0625,1,1
120,BioBERT v1.0 ( PubMed PMC ) is the version of BioBERT ( PubMed PMC ) trained for 470 K steps .,Experimental setups,Experimental setups,named-entity-recognition,9,2,0.125,119,0.5979899497487438,2,0.125,1,1
121,"When using both the PubMed and PMC corpora , we found that 200K and 270K pre-training steps were optimal for PubMed and PMC , respectively .",Experimental setups,Experimental setups,named-entity-recognition,9,3,0.1875,120,0.6030150753768844,3,0.1875,1,0
122,"We also used the ablated versions of BioBERT v1.0 , which were pre-trained on only PubMed for 200 K steps ( Bio BERT v1.0 ( PubMed ) ) and PMC for 270K steps ( Bio BERT v1.0 ( PMC ) ) .",Experimental setups,Experimental setups,named-entity-recognition,9,4,0.25,121,0.6080402010050251,4,0.25,1,0
123,"After our initial release of BioBERT v 1.0 , we pre-trained BioBERT on PubMed for 1 M steps , and we refer to this version as BioBERT v 1.1 ( PubMed ) .",Experimental setups,Experimental setups,named-entity-recognition,9,5,0.3125,122,0.6130653266331658,5,0.3125,1,0
124,Other hyper - parameters such as batch size and learning rate scheduling for pre-training BioBERT are the same as those for pre-training BERT unless stated otherwise .,Experimental setups,Experimental setups,named-entity-recognition,9,6,0.375,123,0.6180904522613065,6,0.375,1,0
125,"We pre-trained BioBERT using Naver Smart Machine Learning ( NSML ) , which is utilized for large - scale experiments that need to be run on several GPUs .",Experimental setups,Experimental setups,named-entity-recognition,9,7,0.4375,124,0.6231155778894473,7,0.4375,1,0
126,We used eight NVIDIA V100 ( 32GB ) GPUs for the pre-training .,Experimental setups,Experimental setups,named-entity-recognition,9,8,0.5,125,0.628140703517588,8,0.5,1,1
127,"The maximum sequence length was fixed to 512 and the mini-batch size was set to 192 , resulting in 98 304 words per iteration .",Experimental setups,Experimental setups,named-entity-recognition,9,9,0.5625,126,0.6331658291457286,9,0.5625,1,1
128,It takes more than 10 days to pre-train BioBERT v 1.0 ( PubMed PMC ) nearly 23 days for BioBERT v 1.1 ( PubMed ) in this setting .,Experimental setups,Experimental setups,named-entity-recognition,9,10,0.625,127,0.6381909547738693,10,0.625,1,0
129,"Despite our best efforts to use BERT LARGE , we used only BERT BASE due to the computational complexity of BERT LARGE .",Experimental setups,Experimental setups,named-entity-recognition,9,11,0.6875,128,0.6432160804020101,11,0.6875,1,0
130,We used a single NVIDIA Titan Xp ( 12GB ) GPU to fine - tune BioBERT on each task .,Experimental setups,Experimental setups,named-entity-recognition,9,12,0.75,129,0.6482412060301508,12,0.75,1,1
131,Note that the fine - tuning process is more computationally efficient than pre-training BioBERT .,Experimental setups,Experimental setups,named-entity-recognition,9,13,0.8125,130,0.6532663316582915,13,0.8125,1,0
132,"For finetuning , a batch size of 10 , 16 , 32 or 64 was selected , and a learning rate of 5e5 , 3e5 or 1e5 was selected .",Experimental setups,Experimental setups,named-entity-recognition,9,14,0.875,131,0.6582914572864321,14,0.875,1,0
133,Fine - tuning BioBERT on QA and RE tasks took less than an hour as the size of the training data is much smaller than that of the training data used by .,Experimental setups,Experimental setups,named-entity-recognition,9,15,0.9375,132,0.6633165829145728,15,0.9375,1,0
134,"On the other hand , it takes more than 20 epochs for BioBERT to reach its highest performance on the NER datasets .",Experimental setups,Experimental setups,named-entity-recognition,9,16,1.0,133,0.6683417085427136,16,1.0,1,0
135,Experimental results,,,named-entity-recognition,9,0,0.0,134,0.6733668341708543,0,0.0,1,0
136,The results of NER are shown in .,Experimental results,Experimental results,named-entity-recognition,9,1,0.032258064516129,135,0.678391959798995,1,0.0714285714285714,1,1
137,"First , we observe that BERT , which was pre-trained on only the general domain corpus is quite effective , but the micro averaged F 1 score of BERT was lower ( 2.01 lower ) than that of the state - of - the - art models .",Experimental results,Experimental results,named-entity-recognition,9,2,0.064516129032258,136,0.6834170854271356,2,0.1428571428571428,1,0
138,"On the other hand , BioBERT achieves higher scores than BERT on all the datasets .",Experimental results,Experimental results,named-entity-recognition,9,3,0.0967741935483871,137,0.6884422110552764,3,0.2142857142857142,1,1
139,"BioBERT outperformed the state - of - the - art models on six out of nine datasets , and BioBERT v 1.1 ( PubMed ) outperformed the state - of - the - art models by 0.62 in terms of micro averaged F1 score .",Experimental results,Experimental results,named-entity-recognition,9,4,0.1290322580645161,138,0.6934673366834171,4,0.2857142857142857,1,1
140,"The relatively low scores on the LINNAEUS dataset can be attributed to the following : ( i ) the lack of a silver - standard dataset for training previous state - of - the - art models and ( ii ) different training / test set splits used in previous work , which were unavailable .",Experimental results,Experimental results,named-entity-recognition,9,5,0.1612903225806451,139,0.6984924623115578,5,0.3571428571428571,1,0
141,The RE results of each model are shown in .,Experimental results,Experimental results,named-entity-recognition,9,6,0.1935483870967742,140,0.7035175879396985,6,0.4285714285714285,1,1
142,"BERT achieved better performance than the state - of - the - art model on the CHEMPROT dataset , which demonstrates its effectiveness in RE .",Experimental results,Experimental results,named-entity-recognition,9,7,0.2258064516129032,141,0.7085427135678392,7,0.5,1,0
143,"On average ( micro ) , BioBERT v1.0 ( PubMed ) obtained a higher F1 score ( 2.80 higher ) than the state - of - the - art models .",Experimental results,Experimental results,named-entity-recognition,9,8,0.2580645161290322,142,0.7135678391959799,8,0.5714285714285714,1,1
144,"Also , BioBERT achieved the highest F 1 scores on 2 out of 3 biomedical datasets .",Experimental results,Experimental results,named-entity-recognition,9,9,0.2903225806451613,143,0.7185929648241206,9,0.6428571428571429,1,1
145,The QA results are shown in .,Experimental results,Experimental results,named-entity-recognition,9,10,0.3225806451612903,144,0.7236180904522613,10,0.7142857142857143,1,0
146,We micro averaged the best scores of the state - of - the - art models from each batch .,Experimental results,Experimental results,named-entity-recognition,9,11,0.3548387096774194,145,0.7286432160804021,11,0.7857142857142857,1,0
147,BERT obtained a higher micro averaged MRR score ( 7.0 higher ) than the state - of - the - art models .,Experimental results,Experimental results,named-entity-recognition,9,12,0.3870967741935484,146,0.7336683417085427,12,0.8571428571428571,1,0
148,"All versions of BioBERT significantly outperformed BERT and the state - of - the - art models , and in particular , BioBERT v1.1 ( PubMed ) obtained a strict accuracy of 38.77 , a lenient accuracy of 53.81 and a mean reciprocal rank score of 44.77 , all of which were micro averaged .",Experimental results,Experimental results,named-entity-recognition,9,13,0.4193548387096774,147,0.7386934673366834,13,0.9285714285714286,1,0
149,"On all the biomedical QA datasets , BioBERT achieved new state - of - the - art performance in terms of MRR .",Experimental results,Experimental results,named-entity-recognition,9,14,0.4516129032258064,148,0.7437185929648241,14,1.0,1,1
150,Discussion,Experimental results,,named-entity-recognition,9,15,0.4838709677419355,149,0.7487437185929648,0,0.0,1,0
151,We used additional corpora of different sizes for pre-training and investigated their effect on performance .,Experimental results,Discussion,named-entity-recognition,9,16,0.5161290322580645,150,0.7537688442211056,1,0.0625,1,0
152,"For BioBERT v1.0 ( PubMed ) , we set the number of pre-training steps to 200K and varied the size of the PubMed corpus .",Experimental results,Discussion,named-entity-recognition,9,17,0.5483870967741935,151,0.7587939698492462,2,0.125,1,0
153,"shows that the performance of BioBERT v 1.0 ( PubMed ) on three NER datasets ( NCBI Disease , BC2GM , BC4CHEMD ) changes in relation to the size of the PubMed corpus .",Experimental results,Discussion,named-entity-recognition,9,18,0.5806451612903226,152,0.7638190954773869,3,0.1875,1,0
154,"Pre-training on 1 billion words is quite effective , and the performance on each dataset mostly improves until 4.5 billion words .",Experimental results,Discussion,named-entity-recognition,9,19,0.6129032258064516,153,0.7688442211055276,4,0.25,1,0
155,We also saved the pre-trained weights from BioBERT v 1.0 ( PubMed ) at different pre-training steps to measure how the number of pre-training steps affects its performance on fine - tuning tasks .,Experimental results,Discussion,named-entity-recognition,9,20,0.6451612903225806,154,0.7738693467336684,5,0.3125,1,0
156,shows the performance changes of BioBERT v 1.0 ( PubMed ) on the same three NER datasets in relation to the number of pre-training steps .,Experimental results,Discussion,named-entity-recognition,9,21,0.6774193548387096,155,0.7788944723618091,6,0.375,1,0
157,The results clearly show that the performance on each dataset improves as the number of pre-training steps increases .,Experimental results,Discussion,named-entity-recognition,9,22,0.7096774193548387,156,0.7839195979899497,7,0.4375,1,0
158,"Finally , shows the absolute performance improvements of BioBERT v 1.0 ( PubMed PMC ) over BERT on all 15 datasets .",Experimental results,Discussion,named-entity-recognition,9,23,0.7419354838709677,157,0.7889447236180904,8,0.5,1,0
159,"F1 scores were used for NER / RE , and MRR scores were used for QA .",Experimental results,Discussion,named-entity-recognition,9,24,0.7741935483870968,158,0.7939698492462312,9,0.5625,1,0
160,BioBERT significantly improves performance on most of the datasets .,Experimental results,Discussion,named-entity-recognition,9,25,0.8064516129032258,159,0.7989949748743719,10,0.625,1,0
161,"As shown in , we sampled predictions from BERT and BioBERT v 1.1 ( PubMed ) to seethe effect of pre-training on downstream tasks .",Experimental results,Discussion,named-entity-recognition,9,26,0.8387096774193549,160,0.8040201005025126,11,0.6875,1,0
162,BioBERT can recognize biomedical named entities that BERT can not and can find the exact boundaries of named Disease 19 665 BC5 CDR Disease 12 694 BC5CDR Drug / Chem. 15 411 BC4CHEMD Drug / Chem. 79 842 BC2 GM Gene / Protein 20 703 JNLPBA Gene / Protein 35 460 LINNAEUS Species 4077 Species - 800 Species 3708,Experimental results,Discussion,named-entity-recognition,9,27,0.8709677419354839,161,0.8090452261306532,12,0.75,1,0
163,"Note Note : For the CHEMPROT dataset , the number of relations in the training , validation and test sets was summed . 486 150 BioASQ 6 b-factoid 618 161",Experimental results,Discussion,named-entity-recognition,9,28,0.9032258064516128,162,0.8140703517587939,13,0.8125,1,0
164,entities .,Experimental results,Discussion,named-entity-recognition,9,29,0.935483870967742,163,0.8190954773869347,14,0.875,1,0
165,"While BERT often gives incorrect answers to simple biomedical questions , BioBERT provides correct answers to such questions .",Experimental results,Discussion,named-entity-recognition,9,30,0.967741935483871,164,0.8241206030150754,15,0.9375,1,0
166,"Also , BioBERT can provide longer named entities as answers .",Experimental results,Discussion,named-entity-recognition,9,31,1.0,165,0.8291457286432161,16,1.0,1,0
167,Conclusion,,,named-entity-recognition,9,0,0.0,166,0.8341708542713567,0,0.0,1,0
168,"In this article , we introduced BioBERT , which is a pre-trained language representation model for biomedical text mining .",Conclusion,Conclusion,named-entity-recognition,9,1,0.03125,167,0.8391959798994975,1,0.0714285714285714,0,0
169,We showed that pre-training BERT on biomedical corpora is crucial in applying it to the biomedical domain .,Conclusion,Conclusion,named-entity-recognition,9,2,0.0625,168,0.8442211055276382,2,0.1428571428571428,0,0
170,"Requiring minimal task - specific architectural modification , BioBERT outperforms previous models on biomedical text mining tasks such as NER , RE and QA .",Conclusion,Conclusion,named-entity-recognition,9,3,0.09375,169,0.8492462311557789,3,0.2142857142857142,0,0
171,"The pre-released version of BioBERT ( January 2019 ) has already been shown to be very effective in many biomedical text mining tasks such as NER for clinical notes , human phenotype - gene RE and clinical temporal RE .",Conclusion,Conclusion,named-entity-recognition,9,4,0.125,170,0.8542713567839196,4,0.2857142857142857,0,0
172,The following updated versions of BioBERT will be available to the bioNLP community : ( i ) BioBERT BASE and BioBERT LARGE trained on only PubMed abstracts without initialization from the existing BERT model and ( ii ) BioBERT BASE and BioBERT LARGE trained on domain - specific vocabulary based on WordPiece .,Conclusion,Conclusion,named-entity-recognition,9,5,0.15625,171,0.8592964824120602,5,0.3571428571428571,0,0
173,"Notes : Precision ( P ) , Recall ( R ) and F1 ( F ) scores on each dataset are reported .",Conclusion,Conclusion,named-entity-recognition,9,6,0.1875,172,0.864321608040201,6,0.4285714285714285,0,0
174,"The best scores are in bold , and the second best scores are underlined .",Conclusion,Conclusion,named-entity-recognition,9,7,0.21875,173,0.8693467336683417,7,0.5,0,0
175,"We list the scores of the state - of - the - art ( SOTA ) models on different datasets as follows : scores of Xu et al . Notes : Precision ( P ) , Recall ( R ) and F1 ( F ) scores on each dataset are reported .",Conclusion,Conclusion,named-entity-recognition,9,8,0.25,174,0.8743718592964824,8,0.5714285714285714,0,0
176,"The best scores are in bold , and the second best scores are underlined .",Conclusion,Conclusion,named-entity-recognition,9,9,0.28125,175,0.8793969849246231,9,0.6428571428571429,0,0
177,"The scores on GAD and EU - ADR were obtained from , and the scores on CHEMPROT were obtained from .",Conclusion,Conclusion,named-entity-recognition,9,10,0.3125,176,0.8844221105527639,10,0.7142857142857143,0,0
178,"Notes : Strict Accuracy ( S ) , Lenient Accuracy ( L ) and Mean Reciprocal Rank ( M ) scores on each dataset are reported .",Conclusion,Conclusion,named-entity-recognition,9,11,0.34375,177,0.8894472361809045,11,0.7857142857142857,0,0
179,"The best scores are in bold , and the second best scores are underlined .",Conclusion,Conclusion,named-entity-recognition,9,12,0.375,178,0.8944723618090452,12,0.8571428571428571,0,0
180,The best BioASQ 4 b / 5 b / 6 b scores were obtained from the BioASQ leaderboard ( http://participants-area.bioasq.org ) .,Conclusion,Conclusion,named-entity-recognition,9,13,0.40625,179,0.8994974874371859,13,0.9285714285714286,0,0
181,"BERT . . . a case of oral penicillin anaphylaxis is described , and the terminology . . .",Conclusion,Conclusion,named-entity-recognition,9,14,0.4375,180,0.9045226130653268,14,1.0,0,0
182,BioBERT,Conclusion,,named-entity-recognition,9,15,0.46875,181,0.9095477386934674,0,0.0,0,0
183,Corynebacterium minutissimum is the bacteria that leads to cutaneous eruptions of erythrasma . . .,Conclusion,BioBERT,named-entity-recognition,9,16,0.5,182,0.914572864321608,1,0.0,0,0
184,BC2GM,Conclusion,,named-entity-recognition,9,17,0.53125,183,0.9195979899497488,0,0.0,0,0
185,BERT,Conclusion,,named-entity-recognition,9,18,0.5625,184,0.9246231155778896,1,0.5,0,0
186,"Like the DMA , but unlike all other mammalian class II A genes , the zebrafish gene codes for two cysteine residues . . .",Conclusion,BERT,named-entity-recognition,9,19,0.59375,185,0.9296482412060302,2,1.0,0,0
187,QA,Conclusion,,named-entity-recognition,9,20,0.625,186,0.9346733668341708,0,0.0,0,0
188,BioASQ 6 b - factoid,Conclusion,QA,named-entity-recognition,9,21,0.65625,187,0.9396984924623116,1,0.0909090909090909,0,0
189,Q : Which type of urinary incontinence is diagnosed with the Q tip test ?,Conclusion,QA,named-entity-recognition,9,22,0.6875,188,0.9447236180904522,2,0.1818181818181818,0,0
190,BERT,Conclusion,,named-entity-recognition,9,23,0.71875,189,0.949748743718593,3,0.2727272727272727,0,0
191,A total of 25 women affected by clinical stress urinary incontinence ( SUI ) were enrolled .,Conclusion,BERT,named-entity-recognition,9,24,0.75,190,0.9547738693467336,4,0.3636363636363636,0,0
192,After undergoing ( . . .),Conclusion,BERT,named-entity-recognition,9,25,0.78125,191,0.9597989949748744,5,0.4545454545454545,0,0
193,"Q-tip test , . . .",Conclusion,BERT,named-entity-recognition,9,26,0.8125,192,0.964824120603015,6,0.5454545454545454,0,0
194,Q : Which bacteria causes erythrasma ?,Conclusion,BERT,named-entity-recognition,9,27,0.84375,193,0.9698492462311558,7,0.6363636363636364,0,0
195,BERT,Conclusion,,named-entity-recognition,9,28,0.875,194,0.9748743718592964,8,0.7272727272727273,0,0
196,Corynebacterium minutissimum is the bacteria that leads to cutaneous eruptions of erythrasma . . .,Conclusion,BERT,named-entity-recognition,9,29,0.90625,195,0.9798994974874372,9,0.8181818181818182,0,0
197,Note :,Conclusion,BERT,named-entity-recognition,9,30,0.9375,196,0.984924623115578,10,0.9090909090909092,0,0
198,Predicted named entities for NER and predicted answers for QA are in bold .,Conclusion,BERT,named-entity-recognition,9,31,0.96875,197,0.9899497487437184,11,1.0,0,0
199,Funding,Conclusion,,named-entity-recognition,9,32,1.0,198,0.9949748743718592,0,0.0,0,0
1,title,,,question-answering,0,0,0.0,0,0.0,0,0.0,1,0
2,Open Question Answering with Weakly Supervised Embedding Models,title,title,question-answering,0,1,0.0,1,0.003875968992248,1,0.0,1,1
3,abstract,,,question-answering,0,0,0.0,2,0.0077519379844961,0,0.0,1,0
4,Building computers able to answer questions on any subject is along standing goal of artificial intelligence .,abstract,abstract,question-answering,0,1,0.1428571428571428,3,0.0116279069767441,1,0.1428571428571428,1,1
5,Promising progress has recently been achieved by methods that learn to map questions to logical forms or database queries .,abstract,abstract,question-answering,0,2,0.2857142857142857,4,0.0155038759689922,2,0.2857142857142857,1,0
6,Such approaches can be effective but at the cost of either large amounts of human - labeled data or by defining lexicons and grammars tailored by practitioners .,abstract,abstract,question-answering,0,3,0.4285714285714285,5,0.0193798449612403,3,0.4285714285714285,1,0
7,"In this paper , we instead take the radical approach of learning to map questions to vectorial feature representations .",abstract,abstract,question-answering,0,4,0.5714285714285714,6,0.0232558139534883,4,0.5714285714285714,1,0
8,"By mapping answers into the same space one can query any knowledge base independent of its schema , without requiring any grammar or lexicon .",abstract,abstract,question-answering,0,5,0.7142857142857143,7,0.0271317829457364,5,0.7142857142857143,1,0
9,Our method is trained with anew optimization procedure combining stochastic gradient descent followed by a fine - tuning step using the weak supervision provided by blending automatically and collaboratively generated resources .,abstract,abstract,question-answering,0,6,0.8571428571428571,8,0.0310077519379844,6,0.8571428571428571,1,0
10,"We empirically demonstrate that our model can capture meaningful signals from its noisy supervision leading to major improvements over paralex , the only existing method able to be trained on similar weakly labeled data .",abstract,abstract,question-answering,0,7,1.0,9,0.0348837209302325,7,1.0,1,0
11,Introduction,,,question-answering,0,0,0.0,10,0.0387596899224806,0,0.0,1,0
12,"This paper addresses the challenging problem of open - domain question answering , which consists of building systems able to answer questions from any domain .",Introduction,Introduction,question-answering,0,1,0.0333333333333333,11,0.0426356589147286,1,0.0333333333333333,1,1
13,Any advance on this difficult topic would bring a huge leap forward in building new ways of accessing knowledge .,Introduction,Introduction,question-answering,0,2,0.0666666666666666,12,0.0465116279069767,2,0.0666666666666666,1,0
14,"An important development in this area has been the creation of large - scale Knowledge Bases ( KBs ) , such as Freebase and DBpedia which store huge amounts of general - purpose information .",Introduction,Introduction,question-answering,0,3,0.1,13,0.0503875968992248,3,0.1,1,0
15,"They are organized as databases of triples connecting pairs of entities by various relationships and of the form ( left entity , relationship , right entity ) .",Introduction,Introduction,question-answering,0,4,0.1333333333333333,14,0.0542635658914728,4,0.1333333333333333,1,0
16,Question answering is then defined as the task of retrieving the correct entity or set of entities from a KB given a query expressed as a question in natural language .,Introduction,Introduction,question-answering,0,5,0.1666666666666666,15,0.0581395348837209,5,0.1666666666666666,1,0
17,The use of KBs simplifies the problem by separating the issue of collecting and organizing information ( i.e. information extraction ) from the one of searching through it ( i.e. question answering or natural language interfacing ) .,Introduction,Introduction,question-answering,0,6,0.2,16,0.0620155038759689,6,0.2,1,0
18,"However , open question answering remains challenging because of the scale of these KBs ( billions of triples , millions of entities and relationships ) and of the difficulty for machines to interpret natural language .",Introduction,Introduction,question-answering,0,7,0.2333333333333333,17,0.065891472868217,7,0.2333333333333333,1,0
19,Recent progress has been made by tackling this problem with semantic parsers .,Introduction,Introduction,question-answering,0,8,0.2666666666666666,18,0.0697674418604651,8,0.2666666666666666,1,0
20,These methods convert questions into logical forms or database queries ( e.g. in SPARQL ) which are then subsequently used to query KBs for answers .,Introduction,Introduction,question-answering,0,9,0.3,19,0.0736434108527131,9,0.3,1,0
21,"Even if such systems have shown the ability to handle large - scale KBs , they require practitioners to hand - craft lexicons , grammars , and KB schema for the parsing to be effective .",Introduction,Introduction,question-answering,0,10,0.3333333333333333,20,0.0775193798449612,10,0.3333333333333333,1,0
22,"This nonnegligible human intervention might not be generic enough to conveniently scale up to new databases with other schema , broader vocabularies or other languages than English .",Introduction,Introduction,question-answering,0,11,0.3666666666666666,21,0.0813953488372093,11,0.3666666666666666,1,0
23,"In this paper , we instead take the approach of converting questions to ( uninterpretable ) vectorial representations which require no pre-defined grammars or lexicons and can query any KB independent of its schema .",Introduction,Introduction,question-answering,0,12,0.4,22,0.0852713178294573,12,0.4,1,1
24,"Following , we focus on answering simple factual questions on abroad range of topics , more specifically , those for which single KB triples stand for both the question and an answer ( of which there maybe many ) .",Introduction,Introduction,question-answering,0,13,0.4333333333333333,23,0.0891472868217054,13,0.4333333333333333,1,0
25,"For example , ( parrotfish.e , live - in.r , southern - water .e ) stands for",Introduction,Introduction,question-answering,0,14,0.4666666666666667,24,0.0930232558139534,14,0.4666666666666667,1,0
26,What is parrotfish 's habitat ?,Introduction,Introduction,question-answering,0,15,0.5,25,0.0968992248062015,15,0.5,1,0
27,"and southern - water.e and ( cantonese.e , be-major - language - in.r , hong - kong.e ) for",Introduction,Introduction,question-answering,0,16,0.5333333333333333,26,0.1007751937984496,16,0.5333333333333333,1,0
28,What is the main language of Hong - Kong ?,Introduction,Introduction,question-answering,0,17,0.5666666666666667,27,0.1046511627906976,17,0.5666666666666667,1,0
29,and cantonese.e.,Introduction,Introduction,question-answering,0,18,0.6,28,0.1085271317829457,18,0.6,1,0
30,"In this task , the main difficulties come from lexical variability rather than from complex syntax , having multiple answers per question , and the absence of a supervised training signal .",Introduction,Introduction,question-answering,0,19,0.6333333333333333,29,0.1124031007751938,19,0.6333333333333333,1,0
31,Our approach is based on learning low - dimensional vector embeddings of words and of KB triples so that representations of questions and corresponding answers end up being similar in the embedding space .,Introduction,Introduction,question-answering,0,20,0.6666666666666666,30,0.1162790697674418,20,0.6666666666666666,1,1
32,"Unfortunately , we do not have access to any human labeled ( query , answer ) supervision for this task .",Introduction,Introduction,question-answering,0,21,0.7,31,0.1201550387596899,21,0.7,1,0
33,"In order to avoid transferring the cost of manual intervention to the one of labeling large amounts of data , we make use of weak supervision .",Introduction,Introduction,question-answering,0,22,0.7333333333333333,32,0.1240310077519379,22,0.7333333333333333,1,1
34,We show empirically that our model is able to take advantage of noisy and indirect supervision by ( i ) automatically generating questions from KB triples and treating this as training data ; and ( ii ) supplementing this with a data set of questions collaboratively marked as paraphrases but with no associated answers .,Introduction,Introduction,question-answering,0,23,0.7666666666666667,33,0.127906976744186,23,0.7666666666666667,1,0
35,We end up learning meaningful vectorial representations for questions involving up to 800 k words and for triples of an mostly automatically created KB with 2.4 M entities and 600 k relationships .,Introduction,Introduction,question-answering,0,24,0.8,34,0.1317829457364341,24,0.8,1,1
36,Our method strongly outperforms previous results on the WikiAnswers + ReVerb evaluation data set introduced by .,Introduction,Introduction,question-answering,0,25,0.8333333333333334,35,0.1356589147286821,25,0.8333333333333334,1,0
37,"Even if the embeddings obtained after training are of good quality , the scale of the optimization problem makes it hard to control and to lead to convergence .",Introduction,Introduction,question-answering,0,26,0.8666666666666667,36,0.1395348837209302,26,0.8666666666666667,1,0
38,"Thus , we propose a method to fine - tune embedding - based models by carefully optimizing a matrix parameterizing the similarity used in the embedding space , leading to a consistent improvement in performance .",Introduction,Introduction,question-answering,0,27,0.9,37,0.1434108527131783,27,0.9,1,1
39,The rest of the paper is organized as follows .,Introduction,Introduction,question-answering,0,28,0.9333333333333332,38,0.1472868217054263,28,0.9333333333333332,1,0
40,Section 2 discusses some previous work and Section 3 introduces the problem of open question answering .,Introduction,Introduction,question-answering,0,29,0.9666666666666668,39,0.1511627906976744,29,0.9666666666666668,1,0
41,"Then , Section 4 presents our model and Section 5 our experimental results .",Introduction,Introduction,question-answering,0,30,1.0,40,0.1550387596899224,30,1.0,1,0
42,Related Work,,,question-answering,0,0,0.0,41,0.1589147286821705,0,0.0,1,0
43,"Large - scale question answering has along history , mostly initiated via the TREC tracks .",Related Work,Related Work,question-answering,0,1,0.03125,42,0.1627906976744186,1,0.0555555555555555,0,0
44,"The first successful systems transformed the questions into queries which were fed to web search engines , the answer being subsequently extracted from top returned pages or snippets .",Related Work,Related Work,question-answering,0,2,0.0625,43,0.1666666666666666,2,0.1111111111111111,0,0
45,Such approaches require significant engineering to hand - craft queries and then parse and search over results .,Related Work,Related Work,question-answering,0,3,0.09375,44,0.1705426356589147,3,0.1666666666666666,0,0
46,"The emergence of large - scale KBs , such as Freebase or DBpedia , changed the setting by transforming open question answering into a problem of querying a KB using natural language .",Related Work,Related Work,question-answering,0,4,0.125,45,0.1744186046511628,4,0.2222222222222222,0,0
47,"This is a challenging problem , which would require huge amount of labeled data to be tackled properly by purely supervised machine learning methods because of the great variability of language and of the large scale of KBs .",Related Work,Related Work,question-answering,0,5,0.15625,46,0.1782945736434108,5,0.2777777777777778,0,0
48,"The earliest methods for open question - answering with KBs , based on hand - written templates , were not robust enough to such variability over possibly evolving KBs ( addition / deletion of triples and entities ) .",Related Work,Related Work,question-answering,0,6,0.1875,47,0.1821705426356589,6,0.3333333333333333,0,0
49,The solution to gain more expressiveness via machine learning comes from distant or indirect supervision to circumvent the issue of labeled data .,Related Work,Related Work,question-answering,0,7,0.21875,48,0.1860465116279069,7,0.3888888888888889,0,0
50,Initial works attempting to learn to connect KBs and natural language with less supervision have actually been tackling the information extraction problem .,Related Work,Related Work,question-answering,0,8,0.25,49,0.189922480620155,8,0.4444444444444444,0,0
51,"Recently , new systems for learning question answering systems with few labeled data have been introduced based on semantic parsers .",Related Work,Related Work,question-answering,0,9,0.28125,50,0.1937984496124031,9,0.5,0,0
52,"Such works tend to require realistic amounts of manual intervention via labeled examples , but still need vast efforts to carefully design lexicons , grammars and the KB .",Related Work,Related Work,question-answering,0,10,0.3125,51,0.1976744186046511,10,0.5555555555555556,0,0
53,"In contrast , proposed a framework for open question answering requiring little human annotation .",Related Work,Related Work,question-answering,0,11,0.34375,52,0.2015503875968992,11,0.6111111111111112,0,0
54,"Their system , Paralex , answers questions with more limited semantics than those introduced in , but does so at a very large scale in an open - domain manner .",Related Work,Related Work,question-answering,0,12,0.375,53,0.2054263565891473,12,0.6666666666666666,0,0
55,It is trained using automatically and collaboratively generated data and using the KB ReVerb .,Related Work,Related Work,question-answering,0,13,0.40625,54,0.2093023255813953,13,0.7222222222222222,0,0
56,"In this work , we follow this trend by proposing an embedding - based model for question answering that is also trained under weak and cheap supervision .",Related Work,Related Work,question-answering,0,14,0.4375,55,0.2131782945736434,14,0.7777777777777778,0,0
57,Embedding - based models are getting more and more popular in natural language processing .,Related Work,Related Work,question-answering,0,15,0.46875,56,0.2170542635658914,15,0.8333333333333334,0,0
58,"Starting from the neural network language model of , these methods have now reached near state - of - the - art performance on many standard tasks while usually requiring less hand - crafted features .",Related Work,Related Work,question-answering,0,16,0.5,57,0.2209302325581395,16,0.8888888888888888,0,0
59,"Recently , some embedding models have been proposed to perform a connection between natural language and KBs for word - sense disambiguation and for information extraction .",Related Work,Related Work,question-answering,0,17,0.53125,58,0.2248062015503876,17,0.9444444444444444,0,0
60,"Our work builds on these approaches to instead learn to perform open question answering under weak supervision , which to our knowledge has not been attempted before .",Related Work,Related Work,question-answering,0,18,0.5625,59,0.2286821705426356,18,1.0,0,0
61,Open-domain Question Answering,Related Work,Related Work,question-answering,0,19,0.59375,60,0.2325581395348837,0,0.0,0,0
62,"In this paper , we follow the question answering framework of and use the same data .",Related Work,Related Work,question-answering,0,20,0.625,61,0.2364341085271318,1,0.5,0,0
63,"Hence , relatively little labeling or feature engineering has been used .",Related Work,Related Work,question-answering,0,21,0.65625,62,0.2403100775193798,2,1.0,0,0
64,Task Definition,Related Work,,question-answering,0,22,0.6875,63,0.2441860465116279,0,0.0,0,0
65,"Our work considers the task of question answering as in : given a question q , the corresponding answer is given by a triplet from a KB .",Related Work,Task Definition,question-answering,0,23,0.71875,64,0.2480620155038759,1,0.1,0,0
66,"This means that we consider questions for which a set of triples t provide an interpretation of the question and it s answer , such as : Here , we only give a singlet per question , but many can exist .",Related Work,Task Definition,question-answering,0,24,0.75,65,0.251937984496124,2,0.2,0,0
67,"In the remainder , the KB is denoted K and its set of entities and relationships is E .",Related Work,Task Definition,question-answering,0,25,0.78125,66,0.2558139534883721,3,0.3,0,0
68,The word vocabulary for questions is termed V. n v and n e are the sizes of V and E respectively .,Related Work,Task Definition,question-answering,0,26,0.8125,67,0.2596899224806202,4,0.4,0,0
69,"Our model consists in learning a function S ( ) , which can score questionanswer triple pairs ( q , t ) .",Related Work,Task Definition,question-answering,0,27,0.84375,68,0.2635658914728682,5,0.5,0,0
70,"Hence , finding the top - ranked answert ( q ) to a question q is directly carried out by : t",Related Work,Task Definition,question-answering,0,28,0.875,69,0.2674418604651162,6,0.6,0,0
71,"To handle multiple answer , we instead present the results as a ranked list , rather than taking the top prediction , and evaluate that instead .",Related Work,Task Definition,question-answering,0,29,0.90625,70,0.2713178294573643,7,0.7,0,0
72,Using the scoring function S ( ) allows to directly query the KB without needing to define an intermediate structured logical representation for questions as in semantic parsing systems .,Related Work,Task Definition,question-answering,0,30,0.9375,71,0.2751937984496124,8,0.8,0,0
73,"We aim at learning S ( ) , with no human - labeled supervised data in the form ( question , answer ) pairs , but only by indirect supervision , generated either automatically or collaboratively .",Related Work,Task Definition,question-answering,0,31,0.96875,72,0.2790697674418604,9,0.9,0,0
74,We detail in the rest of this section our process for creating training data .,Related Work,Task Definition,question-answering,0,32,1.0,73,0.2829457364341085,10,1.0,0,0
75,Training Data,,,question-answering,0,0,0.0,74,0.2868217054263566,0,0.0,1,0
76,"Our training data consists of two sources : an automatically created KB , Re - Verb , from which we generate questions and a set of pairs of questions collaboratively labeled as paraphrases from the website WikiAnswers .",Training Data,Training Data,question-answering,0,1,0.024390243902439,75,0.2906976744186046,1,0.1,1,0
77,Knowledge Base,Training Data,,question-answering,0,2,0.048780487804878,76,0.2945736434108527,2,0.2,1,0
78,The set of potential answers K is given by the KB ReVerb .,Training Data,Knowledge Base,question-answering,0,3,0.073170731707317,77,0.2984496124031007,3,0.3,1,0
79,"ReVerb is an open - source database composed of more than 14M triples , made of more than 2 M entities and 600 k relationships , which have been automatically extracted from the ClueWeb09 corpus .",Training Data,Knowledge Base,question-answering,0,4,0.0975609756097561,78,0.3023255813953488,4,0.4,1,0
80,"In the following , entities are denoted with a .e suffix and relationships with a .r suffix .",Training Data,Knowledge Base,question-answering,0,5,0.1219512195121951,79,0.3062015503875969,5,0.5,1,0
81,"ReVerb contains broad and general knowledge harvested with very little human intervention , which suits the realistically supervised setting .",Training Data,Knowledge Base,question-answering,0,6,0.1463414634146341,80,0.3100775193798449,6,0.6,1,0
82,"But , as a result , ReVerb is ambiguous and noisy with many useless triples and entities as well as numerous duplicates .",Training Data,Knowledge Base,question-answering,0,7,0.1707317073170731,81,0.313953488372093,7,0.7,1,0
83,"For instance , winston - churchill.e , churchill.e and even roosevelt - and - churchill.e are all distinct entities .. 2 presents some examples of triples : some make sense , some others are completely unclear or useless .",Training Data,Knowledge Base,question-answering,0,8,0.1951219512195122,82,0.3178294573643411,8,0.8,1,0
84,"In contrast to highly curated databases such Freebase , ReVerb has more noise but also many more relation types ( Freebase has around 20 k ) .",Training Data,Knowledge Base,question-answering,0,9,0.2195121951219512,83,0.3217054263565891,9,0.9,1,0
85,"So for some types of triple it has much better coverage , despite the larger size of Freebase ; for example Freebase does not cover verbs like afraid - of or suffer - from .",Training Data,Knowledge Base,question-answering,0,10,0.2439024390243902,84,0.3255813953488372,10,1.0,1,0
86,Questions Generation,Training Data,,question-answering,0,11,0.2682926829268293,85,0.3294573643410852,0,0.0,1,0
87,"We have no available data of questions q labeled with their answers , i.e. with the corresponding triples t ?",Training Data,Questions Generation,question-answering,0,12,0.2926829268292683,86,0.3333333333333333,1,0.0149253731343283,1,0
88,K .,Training Data,,question-answering,0,13,0.3170731707317073,87,0.3372093023255814,2,0.0298507462686567,1,0
89,"Following , we hence decided to create such question - triple pairs automatically .",Training Data,K .,question-answering,0,14,0.3414634146341463,88,0.3410852713178294,3,0.044776119402985,1,0
90,These pairs are generated using the 16 seed questions displayed in .,Training Data,K .,question-answering,0,15,0.3658536585365853,89,0.3449612403100775,4,0.0597014925373134,1,0
91,"At each round , we pick a triple at random and then generate randomly one of the seed questions .",Training Data,K .,question-answering,0,16,0.3902439024390244,90,0.3488372093023256,5,0.0746268656716417,1,0
92,"Note only triples with a *-in.r relation ( denoted r- in in ) can generate from the pattern where did er ? , for example , and similar for other constraints .",Training Data,K .,question-answering,0,17,0.4146341463414634,91,0.3527131782945736,6,0.0895522388059701,1,0
93,"Otherwise , the pattern is chosen randomly .",Training Data,,question-answering,0,18,0.4390243902439024,92,0.3565891472868217,7,0.1044776119402985,1,0
94,"Except for these exceptions , we used all 16 seed questions for all triples hence generating approximately 16 14M questions stored in a training set we denote D.",Training Data,"Otherwise , the pattern is chosen randomly .",question-answering,0,19,0.4634146341463415,93,0.3604651162790697,8,0.1194029850746268,1,0
95,The generated questions are imperfect and noisy and create a weak training signal .,Training Data,"Otherwise , the pattern is chosen randomly .",question-answering,0,20,0.4878048780487805,94,0.3643410852713178,9,0.1343283582089552,1,0
96,"Firstly , their syntactic structure is rather simplistic , and real questions as posed by humans ( such as in our actual test ) can look quite different to them .",Training Data,"Otherwise , the pattern is chosen randomly .",question-answering,0,21,0.5121951219512195,95,0.3682170542635659,10,0.1492537313432835,1,0
97,"Secondly , many generated questions do not correspond to semantically valid English sentences .",Training Data,"Otherwise , the pattern is chosen randomly .",question-answering,0,22,0.5365853658536586,96,0.3720930232558139,11,0.1641791044776119,1,0
98,"For instance , since the type of entities in ReVerb is unknown , a pattern like who does er ?",Training Data,"Otherwise , the pattern is chosen randomly .",question-answering,0,23,0.5609756097560976,97,0.375968992248062,12,0.1791044776119403,1,0
99,"can be chosen fora triple where the type of ? in ( ? , r , e ) is not a person , and similar for other types ( e.g. when ) .",Training Data,"Otherwise , the pattern is chosen randomly .",question-answering,0,24,0.5853658536585366,98,0.3798449612403101,13,0.1940298507462686,1,0
100,"Besides , for the strings representing entities and relationships in the questions , we simply used their names in ReVerb , replacingby spaces and stripping off .",Training Data,"Otherwise , the pattern is chosen randomly .",question-answering,0,25,0.6097560975609756,99,0.3837209302325581,14,0.208955223880597,1,0
101,Patterns for generating questions from ReVerb triples following .,Training Data,,question-answering,0,26,0.6341463414634146,100,0.3875968992248062,15,0.2238805970149253,1,0
102,"their suffixes , i.e. the string representing winston - churchill.e is simply winston churchill .",Training Data,Patterns for generating questions from ReVerb triples following .,question-answering,0,27,0.6585365853658537,101,0.3914728682170542,16,0.2388059701492537,1,0
103,"While this is often fine , this is also very limited and caused many incoherences in the data .",Training Data,Patterns for generating questions from ReVerb triples following .,question-answering,0,28,0.6829268292682927,102,0.3953488372093023,17,0.2537313432835821,1,0
104,"Generating questions with a richer KB than ReVerb , such as Freebase or DBpedia , would lead to better quality because typing and better lexicons could be used .",Training Data,Patterns for generating questions from ReVerb triples following .,question-answering,0,29,0.7073170731707317,103,0.3992248062015504,18,0.2686567164179104,1,0
105,"However , this would contradict one of our motivations which is to train a system with as little human intervention as possible ( and hence choosing ReVerb over hand - curated KBs ) .",Training Data,Patterns for generating questions from ReVerb triples following .,question-answering,0,30,0.7317073170731707,104,0.4031007751937984,19,0.2835820895522388,1,0
106,Paraphrases,Training Data,,question-answering,0,31,0.7560975609756098,105,0.4069767441860465,20,0.2985074626865671,1,0
107,The automatically generated examples are useful to connect KB triples and natural language .,Training Data,Paraphrases,question-answering,0,32,0.7804878048780488,106,0.4108527131782946,21,0.3134328358208955,1,0
108,"However , they do not allow fora satisfactory modeling of English language because of their poor wording .",Training Data,Paraphrases,question-answering,0,33,0.8048780487804879,107,0.4147286821705426,22,0.3283582089552239,1,0
109,"To overcome this issue , we again follow and supplement our training data with an indirect supervision signal made of pairs of question paraphrases collected from the WikiAnswers website .",Training Data,Paraphrases,question-answering,0,34,0.8292682926829268,108,0.4186046511627907,23,0.3432835820895522,1,0
110,"On WikiAnswers , users can tag pairs of questions as rephrasing of each other .",Training Data,Paraphrases,question-answering,0,35,0.8536585365853658,109,0.4224806201550387,24,0.3582089552238806,1,0
111,"harvested a set of 18 M of these question - paraphrase pairs , with 2.4 M distinct questions in the corpus .",Training Data,Paraphrases,question-answering,0,36,0.8780487804878049,110,0.4263565891472868,25,0.373134328358209,1,0
112,These pairs have been labeled collaboratively .,Training Data,,question-answering,0,37,0.902439024390244,111,0.4302325581395348,26,0.3880597014925373,1,0
113,This is cheap but also causes the data to be noisy .,Training Data,These pairs have been labeled collaboratively .,question-answering,0,38,0.926829268292683,112,0.4341085271317829,27,0.4029850746268656,1,0
114,"Hence , estimated that only 55 % of the pairs were actual paraphrases .",Training Data,These pairs have been labeled collaboratively .,question-answering,0,39,0.951219512195122,113,0.437984496124031,28,0.417910447761194,1,0
115,The set of paraphrases is denoted P in the following .,Training Data,These pairs have been labeled collaboratively .,question-answering,0,40,0.975609756097561,114,0.4418604651162791,29,0.4328358208955223,1,0
116,"By considering all words and tokens appearing in P and D , we end up with a size for the vocabulary V of more than 800k .",Training Data,These pairs have been labeled collaboratively .,question-answering,0,41,1.0,115,0.4457364341085271,30,0.4477611940298507,1,0
117,Embedding - based Model,,,question-answering,0,0,0.0,116,0.4496124031007752,31,0.4626865671641791,1,0
118,"Our model ends up learning vector embeddings of symbols , either for entities or relationships from ReVerb , or for each word of the vocabulary .",Embedding - based Model,Embedding - based Model,question-answering,0,1,0.5,117,0.4534883720930232,32,0.4776119402985074,1,0
119,Question - KB Triple Scoring,Embedding - based Model,Embedding - based Model,question-answering,0,2,1.0,118,0.4573643410852713,33,0.4925373134328358,1,0
120,Architecture,,,question-answering,0,0,0.0,119,0.4612403100775193,34,0.5074626865671642,1,0
121,"Our framework concerns the learning of a function S ( q , t ) , based on embeddings , that is designed to score the similarity of a question q and a triplet from K.",Architecture,Architecture,question-answering,0,1,0.0163934426229508,120,0.4651162790697674,35,0.5223880597014925,1,0
122,"Our scoring approach is inspired by previous work for labeling images with words , which we adapted , replacing images and labels by questions and triples .",Architecture,Architecture,question-answering,0,2,0.0327868852459016,121,0.4689922480620155,36,0.5373134328358209,1,0
123,"Intuitively , it consists of projecting questions , treated as a bag of words ( and possibly n-grams as well ) , on the one hand , and triples on the other hand , into a shared embedding space and then computing a similarity measure ( the dot product in this paper ) between both projections .",Architecture,Architecture,question-answering,0,3,0.0491803278688524,122,0.4728682170542636,37,0.5522388059701493,1,0
124,The scoring function is then :,Architecture,Architecture,question-answering,0,4,0.0655737704918032,123,0.4767441860465116,38,0.5671641791044776,1,0
125,"with f ( ) a function mapping words from questions into Contrary to previous work modeling KBs with embeddings ( e.g. ) , in our model , an entity does not have the same embedding when appearing in the lefthand or in the right - hand side of a triple .",Architecture,Architecture,question-answering,0,5,0.081967213114754,124,0.4806201550387597,39,0.582089552238806,1,0
126,"Since , g ( ) sums embeddings of all constituents of a triple , we need to use 2 embeddings per entity to encode for the fact that relationships in the KB are not symmetric and so that appearing as a left - hand or right - hand entity is different .",Architecture,Architecture,question-answering,0,6,0.0983606557377049,125,0.4844961240310077,40,0.5970149253731343,1,0
127,"This approach can be easily applied attest time to score any ( question , triple ) pairs .",Architecture,Architecture,question-answering,0,7,0.1147540983606557,126,0.4883720930232558,41,0.6119402985074627,1,0
128,"Given a question q , one can predict the corresponding answer ( a triple ) t ( q ) wit h:t ( q ) = arg max",Architecture,Architecture,question-answering,0,8,0.1311475409836065,127,0.4922480620155038,42,0.6268656716417911,1,0
129,Training by Ranking Previous work has shown that this kind of model can be conveniently trained using a ranking loss .,Architecture,Architecture,question-answering,0,9,0.1475409836065573,128,0.4961240310077519,43,0.6417910447761194,1,0
130,"Hence , given our data set D = { ( q i , ti ) , i = 1 , . . . , | D |} consisting of ( question , answer triple ) training pairs , one could learn the embeddings using constraints of the form :",Architecture,Architecture,question-answering,0,10,0.1639344262295081,129,0.5,44,0.6567164179104478,1,0
131,where 0.1 is the margin .,Architecture,Architecture,question-answering,0,11,0.180327868852459,130,0.5038759689922481,45,0.6716417910447762,1,0
132,"That is , we want the triple that labels a given question to be scored higher than other triples in K by a margin of 0.1 .",Architecture,Architecture,question-answering,0,12,0.1967213114754098,131,0.5077519379844961,46,0.6865671641791045,1,0
133,"We also enforce a constraint on the norms of the columns of V and W , i.e. ?",Architecture,Architecture,question-answering,0,13,0.2131147540983606,132,0.5116279069767442,47,0.7014925373134329,1,0
134,"i , | |v i | | 2 ? 1 and ? j , ||w j || 2 ?",Architecture,Architecture,question-answering,0,14,0.2295081967213114,133,0.5155038759689923,48,0.7164179104477612,1,0
135,1 .,Architecture,Architecture,question-answering,0,15,0.2459016393442623,134,0.5193798449612403,49,0.7313432835820896,1,0
136,"To train our model , we need positive and negative examples of ( q , t ) pairs .",Architecture,Architecture,question-answering,0,16,0.2622950819672131,135,0.5232558139534884,50,0.746268656716418,1,0
137,"However , D only contains positive samples , for which the triple actually corresponds to the question .",Architecture,Architecture,question-answering,0,17,0.2786885245901639,136,0.5271317829457365,51,0.7611940298507462,1,0
138,"Hence , during training , we use a procedure to corrupt triples .",Architecture,Architecture,question-answering,0,18,0.2950819672131147,137,0.5310077519379846,52,0.7761194029850746,1,0
139,"Given ( q , t ) ?",Architecture,Architecture,question-answering,0,19,0.3114754098360656,138,0.5348837209302325,53,0.7910447761194029,1,0
140,"D , we create a corrupted triplet with the following method : pick another random triplet tmp from K , and then , replace with 66 % chance each member oft ( left entity , relationship and right entity ) by the corresponding element int tmp .",Architecture,Architecture,question-answering,0,20,0.3278688524590163,139,0.5387596899224806,54,0.8059701492537313,1,0
141,"This heuristic creates negative triples t somewhat similar to their positive counterpart t , and is similar to schemes of previous work ( e.g. in ) .",Architecture,Architecture,question-answering,0,21,0.3442622950819672,140,0.5426356589147286,55,0.8208955223880597,1,0
142,"Training the embedding model is carried out by stochastic gradient descent ( SGD ) , updating W and V at each step .",Architecture,Architecture,question-answering,0,22,0.360655737704918,141,0.5465116279069767,56,0.835820895522388,1,0
143,At the start of training the parameters off ( ) and g ( ) ( the n v k word embeddings in V and then e k entities and rel .,Architecture,Architecture,question-answering,0,23,0.3770491803278688,142,0.5503875968992248,57,0.8507462686567164,1,0
144,"embeddings in W ) are initialized to random weights ( mean 0 , standard deviation 1 k ) .",Architecture,Architecture,question-answering,0,24,0.3934426229508196,143,0.5542635658914729,58,0.8656716417910447,1,0
145,"Then , we iterate the following steps to train them :",Architecture,Architecture,question-answering,0,25,0.4098360655737705,144,0.5581395348837209,59,0.8805970149253731,1,0
146,"1 . Sample a positive training pair ( q i , ti ) from D .",Architecture,Architecture,question-answering,0,26,0.4262295081967213,145,0.562015503875969,60,0.8955223880597015,1,0
147,2 .,Architecture,Architecture,question-answering,0,27,0.4426229508196721,146,0.5658914728682171,61,0.9104477611940298,1,0
148,Create a corrupted triplet i ensuring that ti = ti .,Architecture,Architecture,question-answering,0,28,0.4590163934426229,147,0.5697674418604651,62,0.9253731343283582,1,0
149,3 .,Architecture,Architecture,question-answering,0,29,0.4754098360655737,148,0.5736434108527132,63,0.9402985074626866,1,0
150,Make a stochastic gradient step to minimize 0.1 ? f ( q i ) g (t i ) +f ( q i ) g (t i ) + .,Architecture,Architecture,question-answering,0,30,0.4918032786885246,149,0.5775193798449613,64,0.9552238805970148,1,0
151,4 . Enforce the constraint that each embedding vector is normalized .,Architecture,Architecture,question-answering,0,31,0.5081967213114754,150,0.5813953488372093,65,0.9701492537313432,1,0
152,The learning rate of SGD is updated during the course of learning using adagrad .,Architecture,Architecture,question-answering,0,32,0.5245901639344263,151,0.5852713178294574,66,0.9850746268656716,1,0
153,x + is the positive part of x .,Architecture,Architecture,question-answering,0,33,0.5409836065573771,152,0.5891472868217055,67,1.0,1,0
154,Multitask Training with Paraphrases Pairs,Architecture,Architecture,question-answering,0,34,0.5573770491803278,153,0.5930232558139535,0,0.0,1,0
155,"We multitask the training of our model by training on pairs of paraphrases of questions ( q 1 , q 2 ) from P as well as training on the pseudolabeled data constructed in D .",Architecture,Architecture,question-answering,0,35,0.5737704918032787,154,0.5968992248062015,1,0.037037037037037,1,0
156,We use the same architecture simply replacing g ( ) by a copy off ( ) .,Architecture,Architecture,question-answering,0,36,0.5901639344262295,155,0.6007751937984496,2,0.074074074074074,1,0
157,This leads to the following function that scores the similarity between two questions :,Architecture,Architecture,question-answering,0,37,0.6065573770491803,156,0.6046511627906976,3,0.1111111111111111,1,0
158,"The matrix W containing embeddings of words is shared between Sand S prp , allowing it to encode information from examples from both D and P. Training of S prp is also conducted with SGD ( and adagrad ) as for S , but , in this case , negative examples are created by replacing one of the questions from the pair by another question chosen at random in P.",Architecture,Architecture,question-answering,0,38,0.6229508196721312,157,0.6085271317829457,4,0.1481481481481481,1,0
159,"During our experiments , W and V were learned by alternating training steps using Sand S prp , switching from one to another at each step .",Architecture,Architecture,question-answering,0,39,0.639344262295082,158,0.6124031007751938,5,0.1851851851851851,1,0
160,The initial learning rate was set to 0.1 and the dimension k of the embedding space to 64 .,Architecture,Architecture,question-answering,0,40,0.6557377049180327,159,0.6162790697674418,6,0.2222222222222222,1,0
161,Training ran for 1 day on a 16 core machine using hogwild .,Architecture,Architecture,question-answering,0,41,0.6721311475409836,160,0.6201550387596899,7,0.2592592592592592,1,0
162,Fine - tuning the Similarity between Embeddings,Architecture,,question-answering,0,42,0.6885245901639344,161,0.624031007751938,8,0.2962962962962963,1,0
163,"The scale of the problem forced us to keep our architecture simple : with n e ? 3.5M ( with 2 embeddings for each entity ) and n v ? 800 k , we have to learn around 4.3 M embeddings .",Architecture,Fine - tuning the Similarity between Embeddings,question-answering,0,43,0.7049180327868853,162,0.627906976744186,9,0.3333333333333333,1,0
164,"With an embedding space of dimension k = 64 , this leads to around 275M parameters to learn .",Architecture,Fine - tuning the Similarity between Embeddings,question-answering,0,44,0.7213114754098361,163,0.6317829457364341,10,0.3703703703703703,1,0
165,The training algorithm must also stay simple to scale on a training set of around 250 M of examples ( D and P combined ) ; SGD appears as the only viable option .,Architecture,Fine - tuning the Similarity between Embeddings,question-answering,0,45,0.7377049180327869,164,0.6356589147286822,11,0.4074074074074074,1,0
166,"SGD , combined with adagrad for adapting the learning rate on the course of training , is a powerful algorithm .",Architecture,Fine - tuning the Similarity between Embeddings,question-answering,0,46,0.7540983606557377,165,0.6395348837209303,12,0.4444444444444444,1,0
167,"However , the scale of the optimization problem makes it very hard to control and conduct properly until convergence .",Architecture,Fine - tuning the Similarity between Embeddings,question-answering,0,47,0.7704918032786885,166,0.6434108527131783,13,0.4814814814814814,1,0
168,"When SGD stops after a pre-defined number of epochs , we are almost certain that the problem is not fully solved and that some room for improvement remains : we observed that embeddings were able to often rank correct answers near the top of the candidates list , but not always in the first place .",Architecture,Fine - tuning the Similarity between Embeddings,question-answering,0,48,0.7868852459016393,167,0.6472868217054264,14,0.5185185185185185,1,0
169,"In this paper , we introduce away to fine - tune our embedding - based model so that correct answers might end up more often at the top of the list .",Architecture,Fine - tuning the Similarity between Embeddings,question-answering,0,49,0.8032786885245902,168,0.6511627906976745,15,0.5555555555555556,1,0
170,"Updating the embeddings involves working on too many parameters , but ultimately , these embeddings are meant to be used in a dot -product that computes the similarity between q and t.",Architecture,Fine - tuning the Similarity between Embeddings,question-answering,0,50,0.819672131147541,169,0.6550387596899225,16,0.5925925925925926,1,0
171,We propose to learn a matrix M ?,Architecture,Fine - tuning the Similarity between Embeddings,question-answering,0,51,0.8360655737704918,170,0.6589147286821705,17,0.6296296296296297,1,0
172,R kk parameterizing the similarity between words and triples embeddings .,Architecture,Fine - tuning the Similarity between Embeddings,question-answering,0,52,0.8524590163934426,171,0.6627906976744186,18,0.6666666666666666,1,0
173,The scoring function becomes :,Architecture,Fine - tuning the Similarity between Embeddings,question-answering,0,53,0.8688524590163934,172,0.6666666666666666,19,0.7037037037037037,1,0
174,M has only k 2 parameters and can be efficiently determined by solving the following convex problem ( fixing the embedding matrices W and V ) :,Architecture,Fine - tuning the Similarity between Embeddings,question-answering,0,54,0.8852459016393442,173,0.6705426356589147,20,0.7407407407407407,1,0
175,where X F is the Frobenius norm of X .,Architecture,Fine - tuning the Similarity between Embeddings,question-answering,0,55,0.9016393442622952,174,0.6744186046511628,21,0.7777777777777778,1,0
176,We solve this problem in a few minutes using L - BFGS on a subset of m = 10 M examples from D .,Architecture,Fine - tuning the Similarity between Embeddings,question-answering,0,56,0.918032786885246,175,0.6782945736434108,22,0.8148148148148148,1,0
177,We first use 4 M examples to train and 6M as validation set to determine the value of the regularization parameter ?.,Architecture,Fine - tuning the Similarity between Embeddings,question-answering,0,57,0.9344262295081968,176,0.6821705426356589,23,0.8518518518518519,1,0
178,"We then retrain the model on the whole 10M examples using the selected value , which happened to be ? = 1.7 10 ?5 .",Architecture,Fine - tuning the Similarity between Embeddings,question-answering,0,58,0.9508196721311476,177,0.686046511627907,24,0.8888888888888888,1,0
179,"This fine - tuning is related to learning anew metric in the embedding space , but since the resulting M is not symmetric , it does not define a dot-product .",Architecture,Fine - tuning the Similarity between Embeddings,question-answering,0,59,0.9672131147540984,178,0.689922480620155,25,0.925925925925926,1,0
180,"Still , M is close to a constant factor times identity ( as in the original score S ( ) ) .",Architecture,Fine - tuning the Similarity between Embeddings,question-answering,0,60,0.9836065573770492,179,0.6937984496124031,26,0.9629629629629628,1,0
181,"The fine - tuning does not deeply alter the ranking , but , as expected , allows fora slight change in the triples ranking , which ends in consistent improvement in performance , as we show in the experiments .",Architecture,Fine - tuning the Similarity between Embeddings,question-answering,0,61,1.0,180,0.6976744186046512,27,1.0,1,0
182,Experiments,,,question-answering,0,0,0.0,181,0.7015503875968992,0,0.0,1,0
183,Evaluation Protocols,,,question-answering,0,0,0.0,182,0.7054263565891473,0,0.0,1,0
184,We first detail the data and metrics which were chosen to assess the quality of our embedding model .,Evaluation Protocols,Evaluation Protocols,question-answering,0,1,0.0769230769230769,183,0.7093023255813954,1,0.0,1,0
185,Test Set,Evaluation Protocols,,question-answering,0,2,0.1538461538461538,184,0.7131782945736435,0,0.0,1,0
186,The data set WikiAnswers + ReVerb contains no labeled examples but some are needed for evaluating models .,Evaluation Protocols,Test Set,question-answering,0,3,0.2307692307692307,185,0.7170542635658915,1,0.1666666666666666,1,0
187,"We used the test set which has been created by in the following way : ( 1 ) they identified 37 questions from a heldout portion of WikiAnswers which were likely to have at least one answer in ReVerb , ( 2 ) they added all valid paraphrases of these questions to obtain a set of 691 questions , ( 3 ) they ran various versions of their paralex system on them to gather candidate triples ( for a total of 48 k ) , which they finally hand - labeled .",Evaluation Protocols,Test Set,question-answering,0,4,0.3076923076923077,186,0.7209302325581395,2,0.3333333333333333,1,0
188,Reranking,Evaluation Protocols,,question-answering,0,5,0.3846153846153846,187,0.7248062015503876,3,0.5,1,0
189,We first evaluated different versions of our model against the paralex system in a reranking setting .,Evaluation Protocols,Reranking,question-answering,0,6,0.4615384615384615,188,0.7286821705426356,4,0.6666666666666666,1,0
190,"For each question q from the WikiAn - swers + ReVerb test set , we take the provided candidate triples t and rerank them by sorting by the score S ( q , t ) or S ft ( q , t ) of our model , depending whether we use fine - tuning or not .",Evaluation Protocols,Reranking,question-answering,0,7,0.5384615384615384,189,0.7325581395348837,5,0.8333333333333334,1,0
191,"As in , we then compute the precision , recall and F1 -score of the highest ranked answer as well as the mean average precision ( MAP ) of the whole output , which measures the average precision overall levels of recall .",Evaluation Protocols,Reranking,question-answering,0,8,0.6153846153846154,190,0.7364341085271318,6,1.0,1,0
192,Full Ranking,Evaluation Protocols,,question-answering,0,9,0.6923076923076923,191,0.7403100775193798,0,0.0,1,0
193,"We hence decided to filter out some candidates before ranking by using a simple string matching strategy : after pos-tagging the question , we construct a set of candidate strings containing ( i ) all noun phrases that appear less than 1,000 .",Evaluation Protocols,Full Ranking,question-answering,0,10,0.7692307692307693,192,0.7441860465116279,1,0.25,1,0
194,Examples of nearest neighboring entities and relationships from REVERB for some words from our vocabulary .,Evaluation Protocols,Full Ranking,question-answering,0,11,0.8461538461538461,193,0.748062015503876,2,0.5,1,0
195,"The prefix L : , resp.",Evaluation Protocols,,question-answering,0,12,0.9230769230769232,194,0.751937984496124,3,0.75,1,0
196,"R : , indicates the embedding of an entity when appearing in left - hand side , resp. right - hand side , of triples .",Evaluation Protocols,"The prefix L : , resp.",question-answering,0,13,1.0,195,0.7558139534883721,4,1.0,1,0
197,Results,,,question-answering,0,0,0.0,196,0.7596899224806202,0,0.0,1,0
198,This section now discusses our empirical performance .,Results,,question-answering,0,1,0.0434782608695652,197,0.7635658914728682,1,0.05,1,0
199,Reranking and present the results of the reranking experiments .,Results,This section now discusses our empirical performance .,question-answering,0,2,0.0869565217391304,198,0.7674418604651163,2,0.1,1,0
200,"We compare various versions of our model against two versions of paralex , whose results were given in .",Results,This section now discusses our empirical performance .,question-answering,0,3,0.1304347826086956,199,0.7713178294573644,3,0.15,1,0
201,"First , we can see that multitasking with paraphrase data is essential since it improves F1 from 0.60 to 0.68 .",Results,This section now discusses our empirical performance .,question-answering,0,4,0.1739130434782608,200,0.7751937984496124,4,0.2,1,1
202,"Paraphrases allow for the embeddings to encode a richer connection between KB constituents and words , as well as between words themselves .",Results,This section now discusses our empirical performance .,question-answering,0,5,0.217391304347826,201,0.7790697674418605,5,0.25,1,0
203,"Note that the WikiAnswers data provides word alignment between paraphrases , which we did not use , unlike paralex .",Results,This section now discusses our empirical performance .,question-answering,0,6,0.2608695652173913,202,0.7829457364341085,6,0.3,1,0
204,"We also tried to use n-grams ( 2.5 M most frequent ) as well as the words to represent the question , but this did not bring any improvement , which might at first seem counter - intuitive .",Results,This section now discusses our empirical performance .,question-answering,0,7,0.3043478260869565,203,0.7868217054263565,7,0.35,1,0
205,"We believe this is due to two factors : it is hard to learn good embeddings for n-grams since their frequency is usually very low and ( 2 ) our automatically generated questions have a poor syntax and hence , many n-grams in this data set do not make sense .",Results,This section now discusses our empirical performance .,question-answering,0,8,0.3478260869565217,204,0.7906976744186046,8,0.4,1,0
206,"We actually conducted experiments with several variants of our model , which tried to take the word ordering into account ( e.g. with convolutions ) , and they all failed to outperform our best performance without word order , once again perhaps because the supervision is not clean enough to allow for such elaborated language modeling .",Results,This section now discusses our empirical performance .,question-answering,0,9,0.391304347826087,205,0.7945736434108527,9,0.45,1,0
207,Fine - tuning the embedding model is very beneficial to optimize the top of the list and grants a bump of 5 points of F1 : carefully tuning the similarity makes a clear difference .,Results,This section now discusses our empirical performance .,question-answering,0,10,0.4347826086956521,206,0.7984496124031008,10,0.5,1,1
208,"All versions of our system greatly outperform paralex : the fine - tuned model improves the F1 - score by almost 20 points and , according to , is better in precision for all levels of recall .",Results,This section now discusses our empirical performance .,question-answering,0,11,0.4782608695652174,207,0.8023255813953488,11,0.55,1,1
209,paralex works by starting with an initial lexicon mapping from the KB to language and then gradually increasing its coverage by iterating on the WikiAnswers + ReVerb data .,Results,This section now discusses our empirical performance .,question-answering,0,12,0.5217391304347826,208,0.8062015503875969,12,0.6,1,0
210,Most of its predictions come from automatically acquired templates and rules : this allows fora good precision but it is not flexible enough across language variations to grant a satisfying recall .,Results,This section now discusses our empirical performance .,question-answering,0,13,0.5652173913043478,209,0.810077519379845,13,0.65,1,0
211,Most of our improvement comes from a much better recall .,Results,This section now discusses our empirical performance .,question-answering,0,14,0.6086956521739131,210,0.813953488372093,14,0.7,1,0
212,"However , as we said earlier , this reranking setting is detrimental for paralex because paralex was evaluated on the task of reranking some of its own predictions .",Results,This section now discusses our empirical performance .,question-answering,0,15,0.6521739130434783,211,0.8178294573643411,15,0.75,1,0
213,"The results provided for paralex , while not corresponding to those of a full ranking among all triples from ReVerb ( it is still reranking among a subset of candidates ) , concerns an evaluation setting more complicated than for our model .",Results,This section now discusses our empirical performance .,question-answering,0,16,0.6956521739130435,212,0.8217054263565892,16,0.8,1,0
214,"Hence , we also display the results of a full ranking by our system in the following .",Results,This section now discusses our empirical performance .,question-answering,0,17,0.7391304347826086,213,0.8255813953488372,17,0.85,1,0
215,and display the results of our model to rank all 14 M triples from ReVerb .,Results,This section now discusses our empirical performance .,question-answering,0,18,0.782608695652174,214,0.8294573643410853,18,0.9,1,0
216,The performance of the plain models is not good ( F1 = 0.22 only for S ft ) because the ranking is degraded by too many candidates .,Results,This section now discusses our empirical performance .,question-answering,0,19,0.8260869565217391,215,0.8333333333333334,19,0.95,1,0
217,But most of these can be discarded beforehand .,Results,,question-answering,0,20,0.8695652173913043,216,0.8372093023255814,20,1.0,1,0
218,Word,Results,,question-answering,0,21,0.9130434782608696,217,0.8410852713178295,0,0.0,1,0
219,Closest entities or relationships from ReVerb in the embedding space get rid of get -rid - of.r be -get - rid - of.r rid-of.r can -get - rid - of.r will - get - rid - of.r should - get - rid - of.r have - to - get - rid - of.r want - to - get- rid- of.r will - not - get - rid-of.r,Results,Word,question-answering,0,22,0.9565217391304348,218,0.8449612403100775,1,0.0526315789473684,1,0
220,help- get-rid-of.r useful be-useful - for.r be-useful - in.r,Results,Word,question-answering,0,23,1.0,219,0.8488372093023255,2,0.1052631578947368,1,0
221,R:wide-range-of-application.e can-be-useful-for.r,,,question-answering,0,0,0.0,220,0.8527131782945736,3,0.1578947368421052,1,0
222,be-use-extensively-for. r be-not-very-useful-for.r,R:wide-range-of-application.e can-be-useful-for.r,R:wide-range-of-application.e can-be-useful-for.r,question-answering,0,1,0.0625,221,0.8565891472868217,4,0.2105263157894736,1,0
223,R:plex-or-technical-algorithm.e,R:wide-range-of-application.e can-be-useful-for.r,,question-answering,0,2,0.125,222,0.8604651162790697,5,0.2631578947368421,1,0
224,R:internal-and-external-use.e,R:wide-range-of-application.e can-be-useful-for.r,,question-answering,0,3,0.1875,223,0.8643410852713178,6,0.3157894736842105,1,0
225,R:authoring.e,R:wide-range-of-application.e can-be-useful-for.r,,question-answering,0,4,0.25,224,0.8682170542635659,7,0.3684210526315789,1,0
226,R:good- or- bad- purpose.e radiation R:radiation.e,R:wide-range-of-application.e can-be-useful-for.r,,question-answering,0,5,0.3125,225,0.872093023255814,8,0.4210526315789473,1,0
227,L:radiation.e,R:wide-range-of-application.e can-be-useful-for.r,,question-answering,0,6,0.375,226,0.875968992248062,9,0.4736842105263157,1,0
228,R:gamma-radiation.e,R:wide-range-of-application.e can-be-useful-for.r,,question-answering,0,7,0.4375,227,0.8798449612403101,10,0.5263157894736842,1,0
229,L:gamma- radiation.e,R:wide-range-of-application.e can-be-useful-for.r,,question-answering,0,8,0.5,228,0.8837209302325582,11,0.5789473684210527,1,0
230,L:x - ray.e L:gamma - ray .,R:wide-range-of-application.e can-be-useful-for.r,,question-answering,0,9,0.5625,229,0.8875968992248062,12,0.631578947368421,1,0
231,"As expected , string matching greatly improves results , both in precision and recall , and also significantly reduces evaluation time .",R:wide-range-of-application.e can-be-useful-for.r,L:x - ray.e L:gamma - ray .,question-answering,0,10,0.625,230,0.8914728682170543,13,0.6842105263157895,1,0
232,"The final F1 obtained by our fine - tuned model is even better then the result of paralex in reranking , which is pretty remarkable , because this time , this setting advantages it quite a lot .",R:wide-range-of-application.e can-be-useful-for.r,L:x - ray.e L:gamma - ray .,question-answering,0,11,0.6875,231,0.8953488372093024,14,0.7368421052631579,1,0
233,Embeddings displays some examples of nearest neighboring entities from ReVerb for some words from our vocabulary .,R:wide-range-of-application.e can-be-useful-for.r,L:x - ray.e L:gamma - ray .,question-answering,0,12,0.75,232,0.8992248062015504,15,0.7894736842105263,1,0
234,"As expected , we can see that verbs or adverbs tend to correspond to relationships while nouns refer to entities .",R:wide-range-of-application.e can-be-useful-for.r,L:x - ray.e L:gamma - ray .,question-answering,0,13,0.8125,233,0.9031007751937984,16,0.8421052631578947,1,0
235,"Interestingly , the model learns some synonymy and hyper / hyponymy .",R:wide-range-of-application.e can-be-useful-for.r,L:x - ray.e L:gamma - ray .,question-answering,0,14,0.875,234,0.9069767441860463,17,0.8947368421052632,1,0
236,"For instance , radiation is close to x - ray.e and iphone to smartphone .e.",R:wide-range-of-application.e can-be-useful-for.r,L:x - ray.e L:gamma - ray .,question-answering,0,15,0.9375,235,0.9108527131782944,18,0.9473684210526316,1,0
237,"This happens thanks to the multitasking with paraphrase data , since in our automatically generated ( q , t ) pairs , the words radiation and iphone are only used for entities with the strings radiation and iphone respectively in their names .",R:wide-range-of-application.e can-be-useful-for.r,L:x - ray.e L:gamma - ray .,question-answering,0,16,1.0,236,0.9147286821705426,19,1.0,1,0
238,Evaluation on WebQuestions,,,question-answering,0,0,0.0,237,0.9186046511627908,0,0.0,1,0
239,Our initial objective was to be able to perform open - domain question answering .,Evaluation on WebQuestions,Evaluation on WebQuestions,question-answering,0,1,0.0769230769230769,238,0.9224806201550388,1,0.0769230769230769,1,0
240,"In this last experimental section , we tend to evaluate how generic our learned system is .",Evaluation on WebQuestions,Evaluation on WebQuestions,question-answering,0,2,0.1538461538461538,239,0.9263565891472868,2,0.1538461538461538,1,0
241,"To this end , we propose to ask our model to answer questions coming from another dataset from the literature , but without retraining it with labeled data , just by directly using the parameters learned on WikiAnswers + ReVerb .",Evaluation on WebQuestions,Evaluation on WebQuestions,question-answering,0,3,0.2307692307692307,240,0.9302325581395348,3,0.2307692307692307,1,0
242,"We chose the data set WebQuestions , which consists of natural language questions matched with answers corresponding to entities of Freebase : in this case , no triple has to be returned , only a single entity .",Evaluation on WebQuestions,Evaluation on WebQuestions,question-answering,0,4,0.3076923076923077,241,0.9341085271317828,4,0.3076923076923077,1,0
243,"We used exact string matching to find the ReVerb entities corresponding to the Freebase answers from the test set of WebQuestions and obtained 1,538 questions labeled with ReVerb out of the original 2,034 .",Evaluation on WebQuestions,Evaluation on WebQuestions,question-answering,0,5,0.3846153846153846,242,0.937984496124031,5,0.3846153846153846,1,0
244,Results of different versions of our model are displayed in .,Evaluation on WebQuestions,Evaluation on WebQuestions,question-answering,0,6,0.4615384615384615,243,0.9418604651162792,6,0.4615384615384615,1,0
245,"For each test question , we record the rank of the first ReVerb triple containing the answer entity .",Evaluation on WebQuestions,Evaluation on WebQuestions,question-answering,0,7,0.5384615384615384,244,0.9457364341085271,7,0.5384615384615384,1,0
246,"Top - 1 and Top - 10 are computed on questions for which the system returned at least one answer ( around 1,000 questions using string matching ) , while F1 is computed for all questions .",Evaluation on WebQuestions,Evaluation on WebQuestions,question-answering,0,8,0.6153846153846154,245,0.9496124031007752,8,0.6153846153846154,1,0
247,"Of course , performance is not great and can not be directly compared with that of the best system reported in ( more than 0.30 of F1 ) .",Evaluation on WebQuestions,Evaluation on WebQuestions,question-answering,0,9,0.6923076923076923,246,0.9534883720930232,9,0.6923076923076923,1,0
248,"One of the main reasons is that most questions of WebQuestions , such as Who was vice - president after Kennedy died ? , should be represented by multiple triples , a setting for which our system has not been designed .",Evaluation on WebQuestions,Evaluation on WebQuestions,question-answering,0,10,0.7692307692307693,247,0.9573643410852714,10,0.7692307692307693,1,0
249,"Still , fora system trained with almost no manual annotation nor prior information on another dataset , with another - very noisy - KB , the results can be seen as particularly promising .",Evaluation on WebQuestions,Evaluation on WebQuestions,question-answering,0,11,0.8461538461538461,248,0.9612403100775194,11,0.8461538461538461,1,0
250,"Besides , evaluation is broad since , in ReVerb , most entities actually appear many times under different names as explained in Section 3 .",Evaluation on WebQuestions,Evaluation on WebQuestions,question-answering,0,12,0.9230769230769232,249,0.9651162790697676,12,0.9230769230769232,1,0
251,"Hence , there might be higher ranked answers but they are missed by our evaluation script .",Evaluation on WebQuestions,Evaluation on WebQuestions,question-answering,0,13,1.0,250,0.9689922480620154,13,1.0,1,0
252,Conclusion,,,question-answering,0,0,0.0,251,0.9728682170542636,0,0.0,1,0
253,This paper introduces anew framework for learning to perform open question answering with very little supervision .,Conclusion,Conclusion,question-answering,0,1,0.1666666666666666,252,0.9767441860465116,1,0.1666666666666666,0,0
254,"Using embeddings as its core , our approach can be successfully trained on imperfect labeled data and indirect supervision and significantly outperforms previous work for answering simple factual questions .",Conclusion,Conclusion,question-answering,0,2,0.3333333333333333,253,0.9806201550387597,2,0.3333333333333333,0,0
255,"Besides , we introduce anew way to fine - tune embedding models for cases where their optimization problem can not be completely solved .",Conclusion,Conclusion,question-answering,0,3,0.5,254,0.9844961240310076,3,0.5,0,0
256,"In spite of these promising results , some exciting challenges remain , especially in order to scale up this model to questions with more complex semantics .",Conclusion,Conclusion,question-answering,0,4,0.6666666666666666,255,0.9883720930232558,4,0.6666666666666666,0,0
257,"Due to the very low supervision signal , our work can only answer satisfactorily simple factual questions , and does not even take into account the word ordering when modeling them .",Conclusion,Conclusion,question-answering,0,5,0.8333333333333334,256,0.992248062015504,5,0.8333333333333334,0,0
258,"Further , much more work has to be carried out to encode the semantics of more complex questions into the embedding space .",Conclusion,Conclusion,question-answering,0,6,1.0,257,0.996124031007752,6,1.0,0,0
1,title,,,question-answering,1,0,0.0,0,0.0,0,0.0,1,0
2,Convolutional Neural Network Architectures for Matching Natural Language Sentences,title,,question-answering,1,1,0.0,1,0.005181347150259,1,0.0,1,1
3,abstract,,,question-answering,1,0,0.0,2,0.0103626943005181,0,0.0,1,0
4,"Semantic matching is of central importance to many natural language tasks [ 2,28 ] .",abstract,abstract,question-answering,1,1,0.1666666666666666,3,0.0155440414507772,1,0.1666666666666666,1,1
5,A successful matching algorithm needs to adequately model the internal structures of language objects and the interaction between them .,abstract,abstract,question-answering,1,2,0.3333333333333333,4,0.0207253886010362,2,0.3333333333333333,1,0
6,"As a step toward this goal , we propose convolutional neural network models for matching two sentences , by adapting the convolutional strategy in vision and speech .",abstract,abstract,question-answering,1,3,0.5,5,0.0259067357512953,3,0.5,1,0
7,"The proposed models not only nicely represent the hierarchical structures of sentences with their layerby - layer composition and pooling , but also capture the rich matching patterns at different levels .",abstract,abstract,question-answering,1,4,0.6666666666666666,6,0.0310880829015544,4,0.6666666666666666,1,0
8,"Our models are rather generic , requiring no prior knowledge on language , and can hence be applied to matching tasks of different nature and in different languages .",abstract,abstract,question-answering,1,5,0.8333333333333334,7,0.0362694300518134,5,0.8333333333333334,1,0
9,The empirical study on a variety of matching tasks demonstrates the efficacy of the proposed model on a variety of matching tasks and its superiority to competitor models .,abstract,abstract,question-answering,1,6,1.0,8,0.0414507772020725,6,1.0,1,0
10,Introduction,,,question-answering,1,0,0.0,9,0.0466321243523316,0,0.0,1,0
11,Matching two potentially heterogenous language objects is central to many natural language applications .,Introduction,Introduction,question-answering,1,1,0.0625,10,0.0518134715025906,1,0.0625,1,1
12,"It generalizes the conventional notion of similarity ( e.g. , in paraphrase identification ) or relevance ( e.g. , in information retrieval ) , since it aims to model the correspondence between "" linguistic objects "" of different nature at different levels of abstractions .",Introduction,Introduction,question-answering,1,2,0.125,11,0.0569948186528497,2,0.125,1,0
13,"Examples include top -k re-ranking in machine translation ( e.g. , comparing the meanings of a French sentence and an English sentence ) and dialogue ( e.g. , evaluating the appropriateness of a response to a given utterance ) .",Introduction,Introduction,question-answering,1,3,0.1875,12,0.0621761658031088,3,0.1875,1,0
14,"Natural language sentences have complicated structures , both sequential and hierarchical , that are essential for understanding them .",Introduction,Introduction,question-answering,1,4,0.25,13,0.0673575129533678,4,0.25,1,0
15,A successful sentence - matching algorithm therefore needs to capture not only the internal structures of sentences but also the rich patterns in their interactions .,Introduction,Introduction,question-answering,1,5,0.3125,14,0.0725388601036269,5,0.3125,1,1
16,"Towards this end , we propose deep neural network models , which adapt the convolutional strategy ( proven successful on image and speech ) to natural language .",Introduction,Introduction,question-answering,1,6,0.375,15,0.077720207253886,6,0.375,1,1
17,"To further explore the relation between representing sentences and matching them , we devise a novel model that can naturally host both the hierarchical composition for sentences and the simple - to - comprehensive fusion of matching patterns with the same convolutional architecture .",Introduction,Introduction,question-answering,1,7,0.4375,16,0.082901554404145,7,0.4375,1,1
18,"Our model is generic , requiring no prior knowledge of natural language ( e.g. , parse tree ) and putting essentially no constraints on the matching tasks .",Introduction,Introduction,question-answering,1,8,0.5,17,0.0880829015544041,8,0.5,1,0
19,This is part of our continuing effort 1 in understanding natural language objects and the matching between them .,Introduction,Introduction,question-answering,1,9,0.5625,18,0.0932642487046632,9,0.5625,1,0
20,Our main contributions can be summarized as follows .,Introduction,Introduction,question-answering,1,10,0.625,19,0.0984455958549222,10,0.625,1,0
21,"First , we devise novel deep convolutional network architectures that can naturally combine 1 ) the hierarchical sentence modeling through layer - by - layer composition and pooling , and 2 ) the capturing of the rich matching patterns at different levels of abstraction ; Second , we perform extensive empirical study on tasks with different scales and characteristics , and demonstrate the superior power of the proposed architectures over competitor methods .",Introduction,Introduction,question-answering,1,11,0.6875,20,0.1036269430051813,11,0.6875,1,0
22,Roadmap,Introduction,,question-answering,1,12,0.75,21,0.1088082901554404,12,0.75,1,0
23,"We start by introducing a convolution network in Section 2 as the basic architecture for sentence modeling , and how it is related to existing sentence models .",Introduction,Roadmap,question-answering,1,13,0.8125,22,0.1139896373056994,13,0.8125,1,0
24,"Based on that , in Section 3 , we propose two architectures for sentence matching , with a detailed discussion of their relation .",Introduction,Roadmap,question-answering,1,14,0.875,23,0.1191709844559585,14,0.875,1,0
25,"In Section 4 , we briefly discuss the learning of the proposed architectures .",Introduction,Roadmap,question-answering,1,15,0.9375,24,0.1243523316062176,15,0.9375,1,0
26,"Then in Section 5 , we report our empirical study , followed by a brief discussion of related work in Section 6 .",Introduction,Roadmap,question-answering,1,16,1.0,25,0.1295336787564766,16,1.0,1,0
27,Convolutional Sentence Model,,,question-answering,1,0,0.0,26,0.1347150259067357,0,0.0,1,0
28,We start with proposing anew convolutional architecture for modeling sentences .,Convolutional Sentence Model,Convolutional Sentence Model,question-answering,1,1,0.03125,27,0.1398963730569948,1,0.0555555555555555,1,0
29,"As illustrated in , it takes as input the embedding of words ( often trained beforehand with unsupervised methods ) in the sentence aligned sequentially , and summarize the meaning of a sentence through layers of convolution and pooling , until reaching a fixed length vectorial representation in the final layer .",Convolutional Sentence Model,Convolutional Sentence Model,question-answering,1,2,0.0625,28,0.1450777202072538,2,0.1111111111111111,1,0
30,"As inmost convolutional models , we use convolution units with a local "" receptive field "" and shared weights , but we design a large feature map to adequately model the rich structures in the composition of words .",Convolutional Sentence Model,Convolutional Sentence Model,question-answering,1,3,0.09375,29,0.1502590673575129,3,0.1666666666666666,1,0
31,Convolution,Convolutional Sentence Model,,question-answering,1,4,0.125,30,0.155440414507772,4,0.2222222222222222,1,0
32,"As shown in , the convolution in Layer - 1 operates on sliding windows of words ( width k 1 ) , and the convolutions in deeper layers are defined in a similar way .",Convolutional Sentence Model,Convolution,question-answering,1,5,0.15625,31,0.160621761658031,5,0.2777777777777778,1,0
33,"Generally , with sentence input x , the convolution unit for feature map of type -f ( among F of them ) on Layer - is",Convolutional Sentence Model,Convolution,question-answering,1,6,0.1875,32,0.1658031088082901,6,0.3333333333333333,1,0
34,and it s matrix form is z,Convolutional Sentence Model,Convolution,question-answering,1,7,0.21875,33,0.1709844559585492,7,0.3888888888888889,1,0
35,gives the output of feature map of type - f for location i in Layer - ;,Convolutional Sentence Model,Convolution,question-answering,1,8,0.25,34,0.1761658031088082,8,0.4444444444444444,1,0
36,"w ( , f ) is the parameters for f on Layer - , with matrix form",Convolutional Sentence Model,Convolution,question-answering,1,9,0.28125,35,0.1813471502590673,9,0.5,1,0
37,"? ( ) is the activation function ( e.g. , Sigmoid or Relu )",Convolutional Sentence Model,Convolution,question-answering,1,10,0.3125,36,0.1865284974093264,10,0.5555555555555556,1,0
38,?,Convolutional Sentence Model,Convolution,question-answering,1,11,0.34375,37,0.1917098445595854,11,0.6111111111111112,1,0
39,( ?1 ) i denotes the segment of Layer - ?,Convolutional Sentence Model,Convolution,question-answering,1,12,0.375,38,0.1968911917098445,12,0.6666666666666666,1,0
40,"1 for the convolution at location i , whil",Convolutional Sentence Model,Convolution,question-answering,1,13,0.40625,39,0.2020725388601036,13,0.7222222222222222,1,0
41,concatenates the vectors fork 1 ( width of sliding window ) words from sentence input x .,Convolutional Sentence Model,Convolution,question-answering,1,14,0.4375,40,0.2072538860103626,14,0.7777777777777778,1,0
42,Max - Pooling,Convolutional Sentence Model,,question-answering,1,15,0.46875,41,0.2124352331606217,15,0.8333333333333334,1,0
43,"We take a max - pooling in every two - unit window for every f , after each convolution",Convolutional Sentence Model,Max - Pooling,question-answering,1,16,0.5,42,0.2176165803108808,16,0.8888888888888888,1,0
44,The effects of pooling are two - fold :,Convolutional Sentence Model,Max - Pooling,question-answering,1,17,0.53125,43,0.2227979274611398,17,0.9444444444444444,1,0
45,"1 ) it shrinks the size of the representation by half , thus quickly absorbs the differences in length for sentence representation , and 2 ) it filters out undesirable composition of words ( see Section 2.1 for some analysis ) .",Convolutional Sentence Model,Max - Pooling,question-answering,1,18,0.5625,44,0.2279792746113989,18,1.0,1,0
46,Length Variability,Convolutional Sentence Model,,question-answering,1,19,0.59375,45,0.233160621761658,0,0.0,1,0
47,The variable length of sentences in a fairly broad range can be readily handled with the convolution and pooling strategy .,Convolutional Sentence Model,Length Variability,question-answering,1,20,0.625,46,0.238341968911917,1,0.0833333333333333,1,0
48,"More specifically , we put all - zero padding vectors after the last word of the sentence until the maximum length .",Convolutional Sentence Model,Length Variability,question-answering,1,21,0.65625,47,0.2435233160621761,2,0.1666666666666666,1,0
49,"To eliminate the boundary effect caused by the great variability of sentence lengths , we add to the convolutional unit agate which sets the output vectors to all - zeros if the input is all zeros .",Convolutional Sentence Model,Length Variability,question-answering,1,22,0.6875,48,0.2487046632124352,3,0.25,1,0
50,"For any given sentence input x , the output of type - f filter for location i in the th layer is given by",Convolutional Sentence Model,Length Variability,question-answering,1,23,0.71875,49,0.2538860103626943,4,0.3333333333333333,1,0
51,"( 2 ) where g ( v ) = 0 if all the elements in vector v equals 0 , otherwise g ( v ) = 1 . This gate , working with max - pooling and positive activation function ( e.g. , Sigmoid ) , keeps away the artifacts from padding in all layers .",Convolutional Sentence Model,Length Variability,question-answering,1,24,0.75,50,0.2590673575129533,5,0.4166666666666667,1,0
52,"Actually it creates a natural hierarchy of all - zero padding ( as illustrated in ) , consisting of nodes in the neural net that would not contribute in the forward process ( as in prediction ) and backward propagation ( as in learning ) .",Convolutional Sentence Model,Length Variability,question-answering,1,25,0.78125,51,0.2642487046632124,6,0.5,1,0
53,"The convolutional unit , when combined with max - pooling , can act as the compositional operator with local selection mechanism as in the recursive autoencoder .",Convolutional Sentence Model,Length Variability,question-answering,1,26,0.8125,52,0.2694300518134715,7,0.5833333333333334,1,0
54,"gives an example on what could happen on the first two layers with input sentence "" The cat sat on the mat "" .",Convolutional Sentence Model,Length Variability,question-answering,1,27,0.84375,53,0.2746113989637305,8,0.6666666666666666,1,0
55,"Just for illustration purpose , we present a dramatic choice of parameters ( by turning off some elements in W ) to make the convolution units focus on different segments within a 3 - word window .",Convolutional Sentence Model,Length Variability,question-answering,1,28,0.875,54,0.2797927461139896,9,0.75,1,0
56,"For example , some feature maps ( group 2 ) give compositions for "" the cat "" and "" cat sat "" , each being a vector .",Convolutional Sentence Model,Length Variability,question-answering,1,29,0.90625,55,0.2849740932642487,10,0.8333333333333334,1,0
57,"Different feature maps offer a variety of compositions , with confidence encoded in the values ( color coded in output of convolution layer in ) .",Convolutional Sentence Model,Length Variability,question-answering,1,30,0.9375,56,0.2901554404145077,11,0.9166666666666666,1,0
58,"The pooling then chooses , for each composition type , between two adjacent sliding windows , e.g. , between "" on the "" and "" the mat "" for feature maps group 2 from the rightmost two sliding windows .",Convolutional Sentence Model,Length Variability,question-answering,1,31,0.96875,57,0.2953367875647668,12,1.0,1,0
59,Some Analysis on the Convolutional Architecture,Convolutional Sentence Model,,question-answering,1,32,1.0,58,0.3005181347150259,0,0.0,1,0
60,Relation to Recursive Models,,,question-answering,1,0,0.0,59,0.3056994818652849,1,0.0769230769230769,1,0
61,"Our convolutional model differs from Recurrent Neural Network ( RNN , ) and Recursive Auto - Encoder ( RAE , ) in several important ways .",Relation to Recursive Models,Relation to Recursive Models,question-answering,1,1,0.0833333333333333,60,0.310880829015544,2,0.1538461538461538,1,0
62,"First , unlike RAE , it does not take a single path of word / phrase composition determined either by a separate gating function , an external parser , or just natural sequential order .",Relation to Recursive Models,Relation to Recursive Models,question-answering,1,2,0.1666666666666666,61,0.3160621761658031,3,0.2307692307692307,1,0
63,"Instead , it takes multiple choices of composition via a large feature map ( encoded in w ( , f ) for different f ) , and leaves the choices to the pooling afterwards to pick the more appropriate segments ( in every adjacent two ) for each composition .",Relation to Recursive Models,Relation to Recursive Models,question-answering,1,3,0.25,62,0.3212435233160621,4,0.3076923076923077,1,0
64,"With any window width k ? 3 , the type of composition would be much richer than that of RAE .",Relation to Recursive Models,Relation to Recursive Models,question-answering,1,4,0.3333333333333333,63,0.3264248704663212,5,0.3846153846153846,1,0
65,"Second , our convolutional model can take supervised training and tune the parameters fora specific task , a property vital to our supervised learning - to - match framework .",Relation to Recursive Models,Relation to Recursive Models,question-answering,1,5,0.4166666666666667,64,0.3316062176165803,6,0.4615384615384615,1,0
66,"However , unlike recursive models , the convolutional architecture has a fixed depth , which bounds the level of composition it could do .",Relation to Recursive Models,Relation to Recursive Models,question-answering,1,6,0.5,65,0.3367875647668393,7,0.5384615384615384,1,0
67,"For tasks like matching , this limitation can be largely compensated with a network afterwards that can take a "" global "" synthesis on the learned sentence representation .",Relation to Recursive Models,Relation to Recursive Models,question-answering,1,7,0.5833333333333334,66,0.3419689119170984,8,0.6153846153846154,1,0
68,"Relation to "" Shallow "" Convolutional Models",Relation to Recursive Models,,question-answering,1,8,0.6666666666666666,67,0.3471502590673575,9,0.6923076923076923,1,0
69,"The proposed convolutional sentence model takes simple architectures such as ( essentially the same convolutional architecture as SENNA ) , which consists of a convolution layer and a max - pooling over the entire sentence for each feature map .",Relation to Recursive Models,"Relation to "" Shallow "" Convolutional Models",question-answering,1,9,0.75,68,0.3523316062176165,10,0.7692307692307693,1,0
70,"This type of models , with local convolutions and a global pooling , essentially do a "" soft "" local template matching and is able to detect local features useful fora certain task .",Relation to Recursive Models,"Relation to "" Shallow "" Convolutional Models",question-answering,1,10,0.8333333333333334,69,0.3575129533678756,11,0.8461538461538461,1,0
71,"Since the sentencelevel sequential order is inevitably lost in the global pooling , the model is incapable of modeling more complicated structures .",Relation to Recursive Models,"Relation to "" Shallow "" Convolutional Models",question-answering,1,11,0.9166666666666666,70,0.3626943005181347,12,0.9230769230769232,1,0
72,It is not hard to see that our convolutional model degenerates to the SENNA - type architecture if we limit the number of layers to be two and set the pooling window infinitely large .,Relation to Recursive Models,"Relation to "" Shallow "" Convolutional Models",question-answering,1,12,1.0,71,0.3678756476683937,13,1.0,1,0
73,Convolutional Matching Models,,,question-answering,1,0,0.0,72,0.3730569948186528,0,0.0,1,0
74,"Based on the discussion in Section 2 , we propose two related convolutional architectures , namely ARC - I and ARC - II ) , for matching two sentences .",Convolutional Matching Models,Convolutional Matching Models,question-answering,1,1,0.03125,73,0.3782383419689119,1,0.0227272727272727,1,0
75,Architecture - I ( ARC - I ),Convolutional Matching Models,Convolutional Matching Models,question-answering,1,2,0.0625,74,0.3834196891191709,2,0.0454545454545454,1,0
76,"Architecture - I ( ARC - I ) , as illustrated in , takes a conventional approach :",Convolutional Matching Models,Convolutional Matching Models,question-answering,1,3,0.09375,75,0.38860103626943,3,0.0681818181818181,1,0
77,"It first finds the representation of each sentence , and then compares the representation for the two sentences with a multi - layer perceptron ( MLP ) .",Convolutional Matching Models,Convolutional Matching Models,question-answering,1,4,0.125,76,0.3937823834196891,4,0.0909090909090909,1,0
78,"It is essentially the Siamese architecture introduced in , which has been applied to different tasks as a nonlinear similarity function .",Convolutional Matching Models,Convolutional Matching Models,question-answering,1,5,0.15625,77,0.3989637305699481,5,0.1136363636363636,1,0
79,"Although ARC - I enjoys the flexibility brought by the convolutional sentence model , it suffers from a drawback inherited from the Siamese architecture : it defers the interaction between two sentences ( in the final MLP ) to until their individual representation matures ( in the convolution model ) , therefore runs at the risk of losing details ( e.g. , a city name ) important for the matching task in representing the sentences .",Convolutional Matching Models,Convolutional Matching Models,question-answering,1,6,0.1875,78,0.4041450777202072,6,0.1363636363636363,1,0
80,"In other words , in the forward phase ( prediction ) , the representation of each sentence is formed without knowledge of each other .",Convolutional Matching Models,Convolutional Matching Models,question-answering,1,7,0.21875,79,0.4093264248704663,7,0.1590909090909091,1,0
81,"This can not be adequately circumvented in backward phase ( learning ) , when the convolutional model learns to extract structures informative for matching on a population level .",Convolutional Matching Models,Convolutional Matching Models,question-answering,1,8,0.25,80,0.4145077720207253,8,0.1818181818181818,1,0
82,Architecture - II ( ARC - II ),Convolutional Matching Models,Convolutional Matching Models,question-answering,1,9,0.28125,81,0.4196891191709844,9,0.2045454545454545,1,0
83,"In view of the drawback of Architecture - I , we propose Architecture - II ( ARC - II ) that is built directly on the interaction space between two sentences .",Convolutional Matching Models,Convolutional Matching Models,question-answering,1,10,0.3125,82,0.4248704663212435,10,0.2272727272727272,1,0
84,"It has the desirable property of letting two sentences meet before their own high - level representations mature , while still retaining the space for the individual development of abstraction of each sentence .",Convolutional Matching Models,Convolutional Matching Models,question-answering,1,11,0.34375,83,0.4300518134715025,11,0.25,1,0
85,"Basically , in Layer - 1 , we take sliding windows on both sentences , and model all the possible combinations of them through "" one-dimensional "" ( 1D ) convolutions .",Convolutional Matching Models,Convolutional Matching Models,question-answering,1,12,0.375,84,0.4352331606217616,12,0.2727272727272727,1,0
86,"For segment ion S X and segment j on S Y , we have the feature map",Convolutional Matching Models,Convolutional Matching Models,question-answering,1,13,0.40625,85,0.4404145077720207,13,0.2954545454545454,1,0
87,where ?,Convolutional Matching Models,Convolutional Matching Models,question-answering,1,14,0.4375,86,0.4455958549222797,14,0.3181818181818182,1,0
88,"i , j ? R 2 k1 De simply concatenates the vectors for sentence segments for S X and S Y : z",Convolutional Matching Models,Convolutional Matching Models,question-answering,1,15,0.46875,87,0.4507772020725388,15,0.3409090909090909,1,0
89,.,Convolutional Matching Models,Convolutional Matching Models,question-answering,1,16,0.5,88,0.4559585492227979,16,0.3636363636363636,1,0
90,Clearly the 1D convolution preserves the location information about both segments .,Convolutional Matching Models,Convolutional Matching Models,question-answering,1,17,0.53125,89,0.4611398963730569,17,0.3863636363636363,1,0
91,"After that in Layer - 2 , it performs a 2D max - pooling in non-overlapping 2 2 windows ( illustrated in )",Convolutional Matching Models,Convolutional Matching Models,question-answering,1,18,0.5625,90,0.466321243523316,18,0.4090909090909091,1,0
92,"( 4 ) In Layer - 3 , we perform a 2D convolution on k 3 k 3 windows of output from Layer - 2 :",Convolutional Matching Models,Convolutional Matching Models,question-answering,1,19,0.59375,91,0.4715025906735751,19,0.4318181818181818,1,0
93,( 5 ),Convolutional Matching Models,Convolutional Matching Models,question-answering,1,20,0.625,92,0.4766839378238341,20,0.4545454545454545,1,0
94,"This could goon for more layers of 2D convolution and 2D max - pooling , analogous to that of convolutional architecture for image input .",Convolutional Matching Models,Convolutional Matching Models,question-answering,1,21,0.65625,93,0.4818652849740932,21,0.4772727272727273,1,0
95,The 2D - Convolution,Convolutional Matching Models,,question-answering,1,22,0.6875,94,0.4870466321243523,22,0.5,1,0
96,"After the first convolution , we obtain a low level representation of the interaction between the two sentences , and from then we obtain a high level representation z ( ) i , j which encodes the information from both sentences .",Convolutional Matching Models,The 2D - Convolution,question-answering,1,23,0.71875,95,0.4922279792746113,23,0.5227272727272727,1,0
97,The general two - dimensional convolution is formulated as z ( ),Convolutional Matching Models,The 2D - Convolution,question-answering,1,24,0.75,96,0.4974093264248704,24,0.5454545454545454,1,0
98,where ?,Convolutional Matching Models,The 2D - Convolution,question-answering,1,25,0.78125,97,0.5025906735751295,25,0.5681818181818182,1,0
99,concatenates the corresponding vectors from its 2 D receptive field in Layer -?1 .,Convolutional Matching Models,The 2D - Convolution,question-answering,1,26,0.8125,98,0.5077720207253886,26,0.5909090909090909,1,0
100,"This pooling has different mechanism as in the 1D case , for it selects not only among compositions on different segments but also among different local matchings .",Convolutional Matching Models,The 2D - Convolution,question-answering,1,27,0.84375,99,0.5129533678756477,27,0.6136363636363636,1,0
101,"This pooling strategy resembles the dynamic pooling in in a similarity learning context , but with two distinctions :",Convolutional Matching Models,The 2D - Convolution,question-answering,1,28,0.875,100,0.5181347150259067,28,0.6363636363636364,1,0
102,1 ) it happens on a fixed architecture and 2 ) it has much richer structure than just similarity .,Convolutional Matching Models,The 2D - Convolution,question-answering,1,29,0.90625,101,0.5233160621761658,29,0.6590909090909091,1,0
103,"contains information about the words in S X before those in z ( ) i + 1 , j , although they maybe generated with slightly different segments in S Y , due to the 2D pooling ( illustrated in .",Convolutional Matching Models,The 2D - Convolution,question-answering,1,30,0.9375,102,0.5284974093264249,30,0.6818181818181818,1,0
104,"The orders is however retained in a "" conditional "" sense .",Convolutional Matching Models,The 2D - Convolution,question-answering,1,31,0.96875,103,0.533678756476684,31,0.7045454545454546,1,0
105,"Our experiments show that when ARC - II is trained on the ( S X , S Y ,S Y ) triples whereS Y randomly shuffles the words in S Y , it consistently gains some ability of finding the correct S Y in the usual contrastive negative sampling setting , which however does not happen with ARC - I.",Convolutional Matching Models,The 2D - Convolution,question-answering,1,32,1.0,104,0.538860103626943,32,0.7272727272727273,1,0
106,Model Generality,,,question-answering,1,0,0.0,105,0.5440414507772021,33,0.75,1,0
107,It is not hard to show that ARC - II actually subsumes ARC - I as a special case .,Model Generality,Model Generality,question-answering,1,1,0.0909090909090909,106,0.5492227979274611,34,0.7727272727272727,1,0
108,"Indeed , in ARC - II if we choose ( by turning off some parameters in W ( , ) ) to keep the representations of the two sentences separated until the final MLP , ARC - II can actually act fully like ARC - I , as illustrated in .",Model Generality,Model Generality,question-answering,1,2,0.1818181818181818,107,0.5544041450777202,35,0.7954545454545454,1,0
109,"More specifically , if we let the feature maps in the first convolution layer to be either devoted to S X or devoted to S Y ( instead of taking both as in general case ) , the output of each segment - pair is naturally divided into two corresponding groups .",Model Generality,Model Generality,question-answering,1,3,0.2727272727272727,108,0.5595854922279793,36,0.8181818181818182,1,0
110,"As a result , the output for each filter f , denoted z",Model Generality,Model Generality,question-answering,1,4,0.3636363636363636,109,0.5647668393782384,37,0.8409090909090909,1,0
111,"( 1 , f ) 1:n , 1:n ( n is the number of sliding windows ) , will be of rank - one , possessing essentially the same information as the result of the first convolution layer in ARC - I .",Model Generality,Model Generality,question-answering,1,5,0.4545454545454545,110,0.5699481865284974,38,0.8636363636363636,1,0
112,"Clearly the 2D pooling that follows will reduce to 1 D pooling , with this separateness preserved .",Model Generality,Model Generality,question-answering,1,6,0.5454545454545454,111,0.5751295336787565,39,0.8863636363636364,1,0
113,"If we further limit the parameters in the second convolution units ( more specifically w ( 2 , f ) ) to those for S X and S Y , we can ensure the individual development of different levels of abstraction on each side , and fully recover the functionality of ARC - I .",Model Generality,Model Generality,question-answering,1,7,0.6363636363636364,112,0.5803108808290155,40,0.9090909090909092,1,0
114,"As suggested by the order - preserving property and the generality of ARC - II , this architecture offers not only the capability but also the inductive bias for the individual development of internal abstraction on each sentence , despite the fact that it is built on the interaction between two sentences .",Model Generality,Model Generality,question-answering,1,8,0.7272727272727273,113,0.5854922279792746,41,0.9318181818181818,1,0
115,"As a result , ARC - II can naturally blend two seemingly diverging processes :",Model Generality,Model Generality,question-answering,1,9,0.8181818181818182,114,0.5906735751295337,42,0.9545454545454546,1,0
116,"1 ) the successive composition within each sentence , and 2 ) the extraction and fusion of matching patterns between them , hence is powerful for matching linguistic objects with rich structures .",Model Generality,Model Generality,question-answering,1,10,0.9090909090909092,115,0.5958549222797928,43,0.9772727272727272,1,0
117,This intuition is verified by the superior performance of ARC - II in experiments ( Section 5 ) on different matching tasks .,Model Generality,Model Generality,question-answering,1,11,1.0,116,0.6010362694300518,44,1.0,1,0
118,Training,,,question-answering,1,0,0.0,117,0.6062176165803109,0,0.0,1,0
119,We employ a discriminative training strategy with a large margin objective .,Training,Training,question-answering,1,1,0.0555555555555555,118,0.6113989637305699,1,0.0555555555555555,1,0
120,"Suppose that we are given the following triples ( x , y + , y ? )",Training,Training,question-answering,1,2,0.1111111111111111,119,0.616580310880829,2,0.1111111111111111,1,0
121,"from the oracle , with x matched with y + better than with y ? .",Training,Training,question-answering,1,3,0.1666666666666666,120,0.6217616580310881,3,0.1666666666666666,1,0
122,We have the following ranking - based loss as objective :,Training,Training,question-answering,1,4,0.2222222222222222,121,0.6269430051813472,4,0.2222222222222222,1,0
123,"where s ( x , y) is predicted matching score for ( x , y ) , and ?",Training,Training,question-answering,1,5,0.2777777777777778,122,0.6321243523316062,5,0.2777777777777778,1,0
124,includes the parameters for convolution layers and those for the MLP .,Training,Training,question-answering,1,6,0.3333333333333333,123,0.6373056994818653,6,0.3333333333333333,1,0
125,The optimization is relatively straightforward for both architectures with the standard back - propagation .,Training,Training,question-answering,1,7,0.3888888888888889,124,0.6424870466321243,7,0.3888888888888889,1,0
126,The gating function ( see Section 2 ) can be easily adopted into the gradient by discounting the contribution from convolution units that have been turned off by the gating function .,Training,Training,question-answering,1,8,0.4444444444444444,125,0.6476683937823834,8,0.4444444444444444,1,0
127,"In other words , We use stochastic gradient descent for the optimization of models .",Training,Training,question-answering,1,9,0.5,126,0.6528497409326425,9,0.5,1,1
128,All the proposed models perform better with mini-batch ( 100 ? 200 in sizes ) which can be easily parallelized on single machine with multi-cores .,Training,Training,question-answering,1,10,0.5555555555555556,127,0.6580310880829016,10,0.5555555555555556,1,1
129,"For regularization , we find that for both architectures , early stopping is enough for models with medium size and large training sets ( with over 500K instances ) .",Training,Training,question-answering,1,11,0.6111111111111112,128,0.6632124352331606,11,0.6111111111111112,1,1
130,"For small datasets ( less than 10 k training instances ) however , we have to combine early stopping and dropout to deal with the serious overfitting problem .",Training,Training,question-answering,1,12,0.6666666666666666,129,0.6683937823834197,12,0.6666666666666666,1,0
131,"We use 50 - dimensional word embedding trained with the Word2 Vec : the embedding for English words ( Section 5.2 & 5.4 ) is learnt on Wikipedia ( ?1B words ) , while that for Chinese words ( Section 5.3 ) is learnt on Weibo data (? 300 M words ) .",Training,Training,question-answering,1,13,0.7222222222222222,130,0.6735751295336787,13,0.7222222222222222,1,1
132,"Our other experiments ( results omitted here ) suggest that fine - tuning the word embedding can further improve the performances of all models , at the cost of longer training .",Training,Training,question-answering,1,14,0.7777777777777778,131,0.6787564766839378,14,0.7777777777777778,1,0
133,We vary the maximum length of words for different tasks to cope with its longest sentence .,Training,Training,question-answering,1,15,0.8333333333333334,132,0.6839378238341969,15,0.8333333333333334,1,0
134,"We use 3 - word window throughout all experiments 2 , but test various numbers of feature maps ( typically from 200 to 500 ) , for optimal performance .",Training,Training,question-answering,1,16,0.8888888888888888,133,0.689119170984456,16,0.8888888888888888,1,1
135,"ARC - II models for all tasks have eight layers ( three for convolution , three for pooling , and two for MLP ) , while ARC - I performs better with less layers ( two for convolution , two for pooling , and two for MLP ) and more hidden nodes .",Training,Training,question-answering,1,17,0.9444444444444444,134,0.694300518134715,17,0.9444444444444444,1,0
136,"We use ReLu as the activation function for all of models ( convolution and MLP ) , which yields comparable or better results to sigmoid - like functions , but converges faster .",Training,Training,question-answering,1,18,1.0,135,0.6994818652849741,18,1.0,1,1
137,Experiments,,,question-answering,1,0,0.0,136,0.7046632124352331,0,0.0,1,0
138,"We report the performance of the proposed models on three matching tasks of different nature , and compare it with that of other competitor models .",Experiments,Experiments,question-answering,1,1,0.3333333333333333,137,0.7098445595854922,1,0.3333333333333333,1,0
139,"Among them , the first two tasks ( namely , Sentence Completion and Tweet - Response Matching ) are about matching of language objects of heterogenous natures , while the third one ( paraphrase identification ) is a natural example of matching homogeneous objects .",Experiments,Experiments,question-answering,1,2,0.6666666666666666,138,0.7150259067357513,2,0.6666666666666666,1,0
140,"Moreover , the three tasks involve two languages , different types of matching , and distinctive writing styles , proving the broad applicability of the proposed models .",Experiments,Experiments,question-answering,1,3,1.0,139,0.7202072538860104,3,1.0,1,0
141,Competitor Methods,,,question-answering,1,0,0.0,140,0.7253886010362695,0,0.0,1,0
142,WORDEMBED : We first represent each short - text as the sum of the embedding of the words it contains .,Competitor Methods,Competitor Methods,question-answering,1,1,0.1666666666666666,141,0.7305699481865285,1,0.0333333333333333,1,1
143,"The matching score of two short - texts are calculated with an MLP with the embedding of the two documents as input ; DEEPMATCH : We take the matching model in and train it on our datasets with 3 hidden layers and 1,000 hidden nodes in the first hidden layer ; URAE+ MLP :",Competitor Methods,Competitor Methods,question-answering,1,2,0.3333333333333333,142,0.7357512953367875,2,0.0666666666666666,1,1
144,"We use the Unfolding Recursive Autoencoder to get a 100 dimensional vector representation of each sentence , and put an MLP on the top as in WORDEMBED ; SENNA + MLP / SIM :",Competitor Methods,Competitor Methods,question-answering,1,3,0.5,143,0.7409326424870466,3,0.1,1,1
145,We use the SENNA - type sentence model for sentence representation ;,Competitor Methods,Competitor Methods,question-answering,1,4,0.6666666666666666,144,0.7461139896373057,4,0.1333333333333333,1,1
146,"SENMLP : We take the whole sentence as input ( with word embedding aligned sequentially ) , and use an MLP to obtain the score of coherence .",Competitor Methods,Competitor Methods,question-answering,1,5,0.8333333333333334,145,0.7512953367875648,5,0.1666666666666666,1,1
147,"All the competitor models are trained on the same training set as the proposed models , and we report the best test performance over different choices of models ( e.g. , the number and size of hidden layers in MLP ) .",Competitor Methods,Competitor Methods,question-answering,1,6,1.0,146,0.7564766839378239,6,0.2,1,0
148,Experiment I : Sentence,,,question-answering,1,0,0.0,147,0.7616580310880829,7,0.2333333333333333,1,0
149,Completion,Experiment I : Sentence,,question-answering,1,1,0.0303030303030303,148,0.7668393782383419,8,0.2666666666666666,1,0
150,This is an artificial task designed to elucidate how different matching models can capture the correspondence between two clauses within a sentence .,Experiment I : Sentence,Completion,question-answering,1,2,0.0606060606060606,149,0.772020725388601,9,0.3,1,0
151,"Basically , we take a sentence from Reuterswith two "" balanced "" clauses ( with 8 ? 28 words ) divided by one comma , and use the first clause as S X and the second as S Y .",Experiment I : Sentence,Completion,question-answering,1,3,0.0909090909090909,150,0.7772020725388601,10,0.3333333333333333,1,0
152,The task is then to recover the original second clause for any given first clause .,Experiment I : Sentence,Completion,question-answering,1,4,0.1212121212121212,151,0.7823834196891192,11,0.3666666666666666,1,0
153,The matching here is considered heterogeneous since the relation between the two is nonsymmetrical on both lexical and semantic levels .,Experiment I : Sentence,Completion,question-answering,1,5,0.1515151515151515,152,0.7875647668393783,12,0.4,1,0
154,"We deliberately make the task harder by using negative second clauses similar to the original ones 4 , both in training and testing .",Experiment I : Sentence,Completion,question-answering,1,6,0.1818181818181818,153,0.7927461139896373,13,0.4333333333333333,1,0
155,One representative example is given as follows :,Experiment I : Sentence,Completion,question-answering,1,7,0.2121212121212121,154,0.7979274611398963,14,0.4666666666666667,1,0
156,"All models are trained on 3 million triples ( from 600K positive pairs ) , and tested on 50K positive pairs , each accompanied by four negatives , with results shown in .",Experiment I : Sentence,Completion,question-answering,1,8,0.2424242424242424,155,0.8031088082901554,15,0.5,1,0
157,"The two proposed models get nearly half of the cases right 5 , with large margin over other sentence models and models without explicit sequence modeling .",Experiment I : Sentence,Completion,question-answering,1,9,0.2727272727272727,156,0.8082901554404145,16,0.5333333333333333,1,0
158,"ARC - II outperforms ARC - I significantly , showing the power of joint modeling of matching and sentence meaning .",Experiment I : Sentence,Completion,question-answering,1,10,0.303030303030303,157,0.8134715025906736,17,0.5666666666666667,1,0
159,"As another convolutional model , SENNA + MLP performs fairly well on this task , although still running behind the proposed convolutional architectures since it is too shallow to adequately model the sentence .",Experiment I : Sentence,Completion,question-answering,1,11,0.3333333333333333,158,0.8186528497409327,18,0.6,1,0
160,"It is a bit surprising that URAE comes last on this task , which might be caused by the facts that 1 ) the representation model ( including word - embedding ) is not trained on Reuters , and 2 ) the split - sentence setting hurts the parsing , which is vital to the quality of learned sentence representation ..",Experiment I : Sentence,Completion,question-answering,1,12,0.3636363636363636,159,0.8238341968911918,19,0.6333333333333333,1,0
161,"This task is slightly easier than Experiment I , with more training instances and purely random negatives .",Experiment I : Sentence,Completion,question-answering,1,13,0.3939393939393939,160,0.8290155440414507,20,0.6666666666666666,1,0
162,"It requires less about the grammatical rigor but more on detailed modeling of loose and local matching patterns ( e.g. , work - overtime ? rest ) .",Experiment I : Sentence,Completion,question-answering,1,14,0.4242424242424242,161,0.8341968911917098,21,0.7,1,0
163,"Again ARC - II beats other models with large margins , while two convolutional sentence models ARC - I and SENNA + MLP come next .",Experiment I : Sentence,Completion,question-answering,1,15,0.4545454545454545,162,0.8393782383419689,22,0.7333333333333333,1,0
164,Experiment II : Matching A Response to A Tweet,Experiment I : Sentence,,question-answering,1,16,0.4848484848484848,163,0.844559585492228,23,0.7666666666666667,1,0
165,Experiment III : Paraphrase Identification,Experiment I : Sentence,Experiment II : Matching A Response to A Tweet,question-answering,1,17,0.5151515151515151,164,0.8497409326424871,24,0.8,1,0
166,"Paraphrase identification aims to determine whether two sentences have the same meaning , a problem considered a touchstone of natural language understanding .",Experiment I : Sentence,Experiment II : Matching A Response to A Tweet,question-answering,1,18,0.5454545454545454,165,0.8549222797927462,25,0.8333333333333334,1,0
167,is included to test our methods on matching homogenous objects .,Experiment I : Sentence,Experiment II : Matching A Response to A Tweet,question-answering,1,19,0.5757575757575758,166,0.8601036269430051,26,0.8666666666666667,1,0
168,"Here we use the benchmark MSRP dataset , which contains 4,076 instances for training and 1,725 for test .",Experiment I : Sentence,Experiment II : Matching A Response to A Tweet,question-answering,1,20,0.6060606060606061,167,0.8652849740932642,27,0.9,1,0
169,We use all the training instances and report the test performance from early stopping .,Experiment I : Sentence,Experiment II : Matching A Response to A Tweet,question-answering,1,21,0.6363636363636364,168,0.8704663212435233,28,0.9333333333333332,1,0
170,"As stated earlier , our model is not specially tailored for modeling synonymy , and generally requires ? 100K instances to work favorably .",Experiment I : Sentence,Experiment II : Matching A Response to A Tweet,question-answering,1,22,0.6666666666666666,169,0.8756476683937824,29,0.9666666666666668,1,0
171,"Nevertheless , our generic matching models still manage to perform reasonably well , achieving an accuracy and F1 score close to the best performer in 2008 based on hand - crafted features , but still significantly lower than the state - of - the - art ( 76.8%/83.6 % ) , achieved with unfolding - RAE and other features designed for this task .",Experiment I : Sentence,Experiment II : Matching A Response to A Tweet,question-answering,1,23,0.696969696969697,170,0.8808290155440415,30,1.0,1,0
172,Discussions,Experiment I : Sentence,,question-answering,1,24,0.7272727272727273,171,0.8860103626943006,0,0.0,1,0
173,ARC - II outperforms others significantly when the training instances are relatively abundant ( as in Experiment I & II ) .,Experiment I : Sentence,Discussions,question-answering,1,25,0.7575757575757576,172,0.8911917098445595,1,0.1111111111111111,1,1
174,"It s superiority over ARC - I , however , is less salient when the sentences have deep grammatical structures and the matching relies lesson the local matching patterns , as in Experiment - I .",Experiment I : Sentence,Discussions,question-answering,1,26,0.7878787878787878,173,0.8963730569948186,2,0.2222222222222222,1,0
175,"This therefore raises the interesting question about how to balance the representation of matching and the representations of objects , and whether we can guide the learning process through something like curriculum learning .",Experiment I : Sentence,Discussions,question-answering,1,27,0.8181818181818182,174,0.9015544041450776,3,0.3333333333333333,1,0
176,"As another important observation , convolutional models ( ARC - I & II , SENNA + MLP ) perform favorably over bag - of - words models , indicating the importance of utilizing sequential structures in understanding and matching sentences .",Experiment I : Sentence,Discussions,question-answering,1,28,0.8484848484848485,175,0.9067357512953368,4,0.4444444444444444,1,1
177,"Quite interestingly , as shown by our other experiments , ARC - I and ARC - II trained purely with random negatives automatically gain some ability in telling whether the words in a given sentence are in right sequential order ( with around 60 % accuracy for both ) .",Experiment I : Sentence,Discussions,question-answering,1,29,0.8787878787878788,176,0.911917098445596,5,0.5555555555555556,1,1
178,It is therefore a bit surprising that an auxiliary task on identifying the correctness of word order in the response does not enhance the ability of the model on the original matching tasks .,Experiment I : Sentence,Discussions,question-answering,1,30,0.9090909090909092,177,0.917098445595855,6,0.6666666666666666,1,0
179,We noticed that simple sum of embedding learned via Word2 Vec yields reasonably good results on all three tasks .,Experiment I : Sentence,Discussions,question-answering,1,31,0.9393939393939394,178,0.922279792746114,7,0.7777777777777778,1,1
180,"We hypothesize that the Word2 Vec embedding is trained in such away that the vector summation can act as a simple composition , and hence retains a fair amount of meaning in the short text segment .",Experiment I : Sentence,Discussions,question-answering,1,32,0.9696969696969696,179,0.927461139896373,8,0.8888888888888888,1,0
181,This is in contrast with other bag - of - words models like DEEPMATCH .,Experiment I : Sentence,Discussions,question-answering,1,33,1.0,180,0.932642487046632,9,1.0,1,0
182,Related Work,,,question-answering,1,0,0.0,181,0.9378238341968912,0,0.0,1,0
183,"Matching structured objects rarely goes beyond estimating the similarity of objects in the same domain , with few exceptions like .",Related Work,Related Work,question-answering,1,1,0.125,182,0.9430051813471504,1,0.125,0,0
184,"When dealing with language objects , most methods still focus on seeking vectorial representations in a common latent space , and calculating the matching score with inner product .",Related Work,Related Work,question-answering,1,2,0.25,183,0.9481865284974094,2,0.25,0,0
185,"Few work has been done on building a deep architecture on the interaction space for texts - pairs , but it is largely based on a bag - of - words representation of text .",Related Work,Related Work,question-answering,1,3,0.375,184,0.9533678756476685,3,0.375,0,0
186,Our models are related to the long thread of work on sentence representation .,Related Work,Related Work,question-answering,1,4,0.5,185,0.9585492227979274,4,0.5,0,0
187,"Aside from the models with recursive nature ( as discussed in Section 2.1 ) , it is fairly common practice to use the sum of word - embedding to represent a short - text , mostly for classification .",Related Work,Related Work,question-answering,1,5,0.625,186,0.9637305699481864,5,0.625,0,0
188,There is very little work on convolutional modeling of language .,Related Work,Related Work,question-answering,1,6,0.75,187,0.9689119170984456,6,0.75,0,0
189,"In addition to , there is a very recent model on sentence representation with dynamic convolutional neural network .",Related Work,Related Work,question-answering,1,7,0.875,188,0.9740932642487048,7,0.875,0,0
190,"This work relies heavily on a carefully designed pooling strategy to handle the variable length of sentence with a relatively small feature map , tailored for classification problems with modest sizes .",Related Work,Related Work,question-answering,1,8,1.0,189,0.9792746113989638,8,1.0,0,0
191,Conclusion,,,question-answering,1,0,0.0,190,0.9844559585492229,0,0.0,1,0
192,"We propose deep convolutional architectures for matching natural language sentences , which can nicely combine the hierarchical modeling of individual sentences and the patterns of their matching .",Conclusion,Conclusion,question-answering,1,1,0.5,191,0.9896373056994818,1,0.5,0,0
193,Empirical study shows our models can outperform competitors on a variety of matching tasks .,Conclusion,Conclusion,question-answering,1,2,1.0,192,0.9948186528497408,2,1.0,0,0
1,title,,,question-answering,2,0,0.0,0,0.0,0,0.0,1,0
2,Large - scale Simple Question Answering with Memory Networks,title,,question-answering,2,1,0.0,1,0.0036630036630036,1,0.0,1,1
3,abstract,,,question-answering,2,0,0.0,2,0.0073260073260073,0,0.0,1,0
4,Training large - scale question answering systems is complicated because training sources usually cover a small portion of the range of possible questions .,abstract,abstract,question-answering,2,1,0.25,3,0.0109890109890109,1,0.25,1,1
5,"This paper studies the impact of multitask and transfer learning for simple question answering ; a setting for which the reasoning required to answer is quite easy , as long as one can retrieve the correct evidence given a question , which can be difficult in large - scale conditions .",abstract,abstract,question-answering,2,2,0.5,4,0.0146520146520146,2,0.5,1,1
6,"To this end , we introduce anew dataset of 100 k questions that we use in conjunction with existing benchmarks .",abstract,abstract,question-answering,2,3,0.75,5,0.0183150183150183,3,0.75,1,0
7,"We conduct our study within the framework of Memory Networks ( Weston et al. , 2015 ) because this perspective allows us to eventually scale up to more complex reasoning , and show that Memory Networks can be successfully trained to achieve excellent performance .",abstract,abstract,question-answering,2,4,1.0,6,0.0219780219780219,4,1.0,1,0
8,Introduction,,,question-answering,2,0,0.0,7,0.0256410256410256,0,0.0,1,0
9,"Open-domain Question Answering ( QA ) systems aim at providing the exact answer ( s ) to questions formulated in natural language , without restriction of domain .",Introduction,Introduction,question-answering,2,1,0.0066666666666666,8,0.0293040293040293,1,0.0416666666666666,1,0
10,"While there is along history of QA systems that search for textual documents or on the Web and extract answers from them ( see e.g. ) , recent progress has been made with the release of large Knowledge Bases ( KBs ) such as Freebase , which contain consolidated knowledge stored as atomic facts , and extracted from different sources , such as free text , tables in webpages or collaborative input .",Introduction,Introduction,question-answering,2,2,0.0133333333333333,9,0.0329670329670329,2,0.0833333333333333,1,0
11,Existing approaches for QA from KBs use learnable components to either transform the question into a structured KB query or learn to embed questions and facts in a low dimensional vector space and retrieve the answer by computing similarities in this embedding space .,Introduction,Introduction,question-answering,2,3,0.02,10,0.0366300366300366,3,0.125,1,0
12,"However , while most recent efforts have focused on designing systems with higher reasoning capabilities , that could jointly retrieve and use multiple facts to answer , the simpler problem of answering questions that refer to a single fact of the KB , which we call Simple Question Answering in this paper , is still far from solved .",Introduction,Introduction,question-answering,2,4,0.0266666666666666,11,0.0402930402930402,4,0.1666666666666666,1,0
13,"Hence , existing benchmarks are small ; they mostly cover the head of the distributions of facts , and are restricted in their question types and their syntactic and lexical variations .",Introduction,Introduction,question-answering,2,5,0.0333333333333333,12,0.0439560439560439,5,0.2083333333333333,1,0
14,"As such , it is still unknown how much the existing systems perform outside the range of the specific question templates of a few , small benchmark datasets , and it is also unknown whether learning on a single dataset transfers well on other ones , and whether such systems can learn from different training sources , which we believe is necessary to capture the whole range of possible questions .",Introduction,Introduction,question-answering,2,6,0.04,13,0.0476190476190476,6,0.25,1,0
15,"Besides , the actual need for reasoning , i.e. constructing the answer from more than a single fact from the KB , depends on the actual structure of the KB .",Introduction,Introduction,question-answering,2,7,0.0466666666666666,14,0.0512820512820512,7,0.2916666666666667,1,0
16,"As we shall see , for instance , a simple preprocessing of Freebase tremendously increases the coverage of simple QA in terms of possible questions that can be answered with a single fact , including list questions that expect more than a single answer .",Introduction,Introduction,question-answering,2,8,0.0533333333333333,15,0.0549450549450549,8,0.3333333333333333,1,0
17,"In fact , the task of simple QA itself might already cover a wide range of practical usages , if the KB is properly organized .",Introduction,Introduction,question-answering,2,9,0.06,16,0.0586080586080586,9,0.375,1,0
18,This paper presents two contributions .,Introduction,Introduction,question-answering,2,10,0.0666666666666666,17,0.0622710622710622,10,0.4166666666666667,1,0
19,"First , as an effort to study the coverage of existing systems and the possibility to train jointly on different data sources via multitasking , we collected the first large - scale dataset of questions and answers based on a KB , called SimpleQuestions .",Introduction,Introduction,question-answering,2,11,0.0733333333333333,18,0.0659340659340659,11,0.4583333333333333,1,1
20,"This dataset , which is presented in Section 2 , contains more than 100 k questions written by human anno-What American cartoonist is the creator of Andy Lippincott ?",Introduction,Introduction,question-answering,2,12,0.08,19,0.0695970695970696,12,0.5,1,0
21,Which forest is Fires Creek in ?,Introduction,Introduction,question-answering,2,13,0.0866666666666666,20,0.0732600732600732,13,0.5416666666666666,1,0
22,What is an active ingredient in childrens earache relief ?,Introduction,Introduction,question-answering,2,14,0.0933333333333333,21,0.0769230769230769,14,0.5833333333333334,1,0
23,"tators and associated to Freebase facts , while the largest existing benchmark , WebQuestions , contains less than 6 k questions created automatically using the Google suggest API .",Introduction,Introduction,question-answering,2,15,0.1,22,0.0805860805860805,15,0.625,1,0
24,"Second , in sections 3 and 4 , we present an embedding - based QA system developed under the framework of Memory Networks ( Mem NNs ) .",Introduction,Introduction,question-answering,2,16,0.1066666666666666,23,0.0842490842490842,16,0.6666666666666666,1,1
25,"Memory Networks are learning systems centered around a memory component that can be read and written to , with a particular focus on cases where the relationship between the input and response languages ( here natural language ) and the storage language ( here , the facts from KBs ) is performed by embedding all of them in the same vector space .",Introduction,Introduction,question-answering,2,17,0.1133333333333333,24,0.0879120879120879,17,0.7083333333333334,1,0
26,The setting of the simple QA corresponds to the elementary operation of performing a single lookup in the memory .,Introduction,Introduction,question-answering,2,18,0.12,25,0.0915750915750915,18,0.75,1,1
27,"While our model bares similarity with previous embedding models for QA , using the framework of MemNNs opens the perspective to more involved inference schemes in future work , since MemNNs were shown to perform well on complex reasoning toy QA tasks .",Introduction,Introduction,question-answering,2,19,0.1266666666666666,26,0.0952380952380952,19,0.7916666666666666,1,0
28,We discuss related work in Section 5 .,Introduction,Introduction,question-answering,2,20,0.1333333333333333,27,0.0989010989010989,20,0.8333333333333334,1,0
29,"We report experimental results in Section 6 , where we show that our model achieves excellent results on the benchmark WebQuestions .",Introduction,Introduction,question-answering,2,21,0.14,28,0.1025641025641025,21,0.875,1,0
30,We also show that it can learn from two different QA datasets to improve its performance on both .,Introduction,Introduction,question-answering,2,22,0.1466666666666666,29,0.1062271062271062,22,0.9166666666666666,1,0
31,We also present the first successful application of transfer learning for QA .,Introduction,Introduction,question-answering,2,23,0.1533333333333333,30,0.1098901098901098,23,0.9583333333333334,1,0
32,"Using the Reverb KB and QA datasets , we show that Reverb facts can be added to the memory and used to answer without retraining , and that MemNNs achieve better results than some systems designed on this dataset .",Introduction,Introduction,question-answering,2,24,0.16,31,0.1135531135531135,24,1.0,1,0
33,Simple Question Answering,Introduction,,question-answering,2,25,0.1666666666666666,32,0.1172161172161172,0,0.0,1,0
34,Knowledge Bases contain facts expressed as triples where subject and object are entities and relationship describes the type of ( directed ) link between these entities .,Introduction,Simple Question Answering,question-answering,2,26,0.1733333333333333,33,0.1208791208791208,1,0.1,1,0
35,"The simple QA prob - lem we address here consist in finding the answer to questions that can be rephrased as queries of the form , asking for all objects linked to subject by relationship .",Introduction,Simple Question Answering,question-answering,2,27,0.18,34,0.1245421245421245,2,0.2,1,0
36,The question,Introduction,,question-answering,2,28,0.1866666666666666,35,0.1282051282051282,3,0.3,1,0
37,"What do Jamaican people speak ? , for instance , could be rephrased as the Freebase query ( jamaica , language spoken , ? ) .",Introduction,The question,question-answering,2,29,0.1933333333333333,36,0.1318681318681318,4,0.4,1,0
38,"In other words , fetching a single fact from a KB is sufficient to answer correctly .",Introduction,The question,question-answering,2,30,0.2,37,0.1355311355311355,5,0.5,1,0
39,"The term simple QA refers to the simplicity of the reasoning process needed to answer questions , since it involves a single fact .",Introduction,The question,question-answering,2,31,0.2066666666666666,38,0.1391941391941392,6,0.6,1,0
40,"However , this does not mean that the QA problem is easy per se , since retrieving this single supporting fact can be very challenging as it involves to search over millions of alternatives given a query expressed in natural language .",Introduction,The question,question-answering,2,32,0.2133333333333333,39,0.1428571428571428,7,0.7,1,0
41,"shows that , with a KB with many types of relationships like",Introduction,The question,question-answering,2,33,0.22,40,0.1465201465201465,8,0.8,1,0
42,"Freebase , the range of questions that can be answered with a single fact is already very broad .",Introduction,The question,question-answering,2,34,0.2266666666666666,41,0.1501831501831501,9,0.9,1,0
43,"Besides , as we shall see , modiying slightly the structure of the KB can make some QA problems simpler by adding direct connections between entities and hence allow to bypass the need for more complex reasoning .",Introduction,The question,question-answering,2,35,0.2333333333333333,42,0.1538461538461538,10,1.0,1,0
44,Knowledge Bases,Introduction,,question-answering,2,36,0.24,43,0.1575091575091575,0,0.0,1,0
45,"We use the KB Freebase 1 as the basis of our QA system , our source of facts and answers .",Introduction,Knowledge Bases,question-answering,2,37,0.2466666666666666,44,0.1611721611721611,1,0.0769230769230769,1,0
46,All Freebase entities and relationships are typed and the lexicon for types and relationships is closed .,Introduction,Knowledge Bases,question-answering,2,38,0.2533333333333333,45,0.1648351648351648,2,0.1538461538461538,1,0
47,"Freebase data is collaboratively collected and curated , to ensure a high reliability of the facts .",Introduction,Knowledge Bases,question-answering,2,39,0.26,46,0.1684981684981685,3,0.2307692307692307,1,0
48,"Each entity has an internal identifier and a set of strings that are usually used to refer to that entity in text , termed aliases .",Introduction,Knowledge Bases,question-answering,2,40,0.2666666666666666,47,0.1721611721611721,4,0.3076923076923077,1,0
49,"We consider two extracts of Freebase , whose statistics are given in .",Introduction,Knowledge Bases,question-answering,2,41,0.2733333333333333,48,0.1758241758241758,5,0.3846153846153846,1,0
50,"FB2M , which was used in , contains about 2 M entities and 5 k relationships .",Introduction,Knowledge Bases,question-answering,2,42,0.28,49,0.1794871794871795,6,0.4615384615384615,1,0
51,"FB5M , is much larger with about 5 M entities and more than 7.5 k relationships .",Introduction,Knowledge Bases,question-answering,2,43,0.2866666666666667,50,0.1831501831501831,7,0.5384615384615384,1,0
52,"We also use the KB Reverb as a secondary source of facts to study how well a model trained to answer questions using Freebase facts could be used to answer using Reverb 's as well , without being trained on Reverb data .",Introduction,Knowledge Bases,question-answering,2,44,0.2933333333333333,51,0.1868131868131868,8,0.6153846153846154,1,0
53,This is a pure setting of transfer learning .,Introduction,Knowledge Bases,question-answering,2,45,0.3,52,0.1904761904761904,9,0.6923076923076923,1,0
54,Reverb is interesting for this experiment because it differs a lot from Freebase .,Introduction,Knowledge Bases,question-answering,2,46,0.3066666666666666,53,0.1941391941391941,10,0.7692307692307693,1,0
55,Its data was extracted automatically from text with minimal human intervention and is highly unstructured : entities are unique strings and the lexicon for relationships is open .,Introduction,Knowledge Bases,question-answering,2,47,0.3133333333333333,54,0.1978021978021978,11,0.8461538461538461,1,0
56,"This leads to many more relationships , but entities with multiple references are not deduplicated , ambiguous referents are not resolved , and the reliability of the stored facts is much lower than in Freebase .",Introduction,Knowledge Bases,question-answering,2,48,0.32,55,0.2014652014652014,12,0.9230769230769232,1,0
57,"We used the full extraction from , which contains 2 M entities and 600 k relationships .",Introduction,Knowledge Bases,question-answering,2,49,0.3266666666666666,56,0.2051282051282051,13,1.0,1,0
58,The SimpleQuestions dataset,Introduction,Knowledge Bases,question-answering,2,50,0.3333333333333333,57,0.2087912087912088,0,0.0,1,0
59,"Existing resources for QA such as WebQuestions are rather small ( few thousands questions ) and hence do not provide a very thorough coverage of the variety of questions that could be answered using a KB like Freebase , even in the context of simple QA .",Introduction,Knowledge Bases,question-answering,2,51,0.34,58,0.2124542124542124,1,0.05,1,0
60,"Hence , in this paper , we introduce anew dataset of much larger scale for the task of simple QA called SimpleQuestions .",Introduction,Knowledge Bases,question-answering,2,52,0.3466666666666667,59,0.2161172161172161,2,0.1,1,0
61,2,Introduction,Knowledge Bases,question-answering,2,53,0.3533333333333333,60,0.2197802197802197,3,0.15,1,0
62,"This dataset consists of a total of 108,442 questions written in natural language by human English - speaking annotators each paired with a corresponding fact from FB2M that provides the answer and explains it .",Introduction,Knowledge Bases,question-answering,2,54,0.36,61,0.2234432234432234,4,0.2,1,0
63,"We randomly shuffle these questions and use 70 % of them ( 75910 ) as training set , 10 % as validation set , and the remaining 20 % as test set .",Introduction,Knowledge Bases,question-answering,2,55,0.3666666666666666,62,0.2271062271062271,5,0.25,1,0
64,Examples of questions and facts are given in .,Introduction,Knowledge Bases,question-answering,2,56,0.3733333333333333,63,0.2307692307692307,6,0.3,1,0
65,We collected SimpleQuestions in two phases .,Introduction,Knowledge Bases,question-answering,2,57,0.38,64,0.2344322344322344,7,0.35,1,0
66,The first phase consisted of shortlisting the set of facts from Freebase to be annotated with questions .,Introduction,Knowledge Bases,question-answering,2,58,0.3866666666666666,65,0.238095238095238,8,0.4,1,0
67,We used FB2M as background KB and removed all facts with undefined relationship type i.e. containing the word freebase .,Introduction,Knowledge Bases,question-answering,2,59,0.3933333333333333,66,0.2417582417582417,9,0.45,1,0
68,"We also removed all facts for which the ( subject , relationship ) pair had more than a threshold number of objects .",Introduction,Knowledge Bases,question-answering,2,60,0.4,67,0.2454212454212454,10,0.5,1,0
69,This filtering step is crucial to remove facts,Introduction,Knowledge Bases,question-answering,2,61,0.4066666666666667,68,0.2490842490842491,11,0.55,1,0
70,"2 The dataset is available from http://fb.ai/babi. which would result in trivial uninformative questions , such as , Name a person who is an actor ?.",Introduction,Knowledge Bases,question-answering,2,62,0.4133333333333333,69,0.2527472527472527,12,0.6,1,0
71,The threshold was set to 10 .,Introduction,Knowledge Bases,question-answering,2,63,0.42,70,0.2564102564102564,13,0.65,1,0
72,"In the second phase , these selected facts were sampled and delivered to human annotators to generate questions from them .",Introduction,Knowledge Bases,question-answering,2,64,0.4266666666666667,71,0.2600732600732601,14,0.7,1,0
73,"For the sampling , each fact was associated with a probability which defined as a function of its relationship frequency in the KB : to favor variability , facts with relationship appearing more frequently were given lower probabilities .",Introduction,Knowledge Bases,question-answering,2,65,0.4333333333333333,72,0.2637362637362637,15,0.75,1,0
74,"For each sampled facts , annotators were shown the facts along with hyperlinks to freebase.com to provide some context while framing the question .",Introduction,Knowledge Bases,question-answering,2,66,0.44,73,0.2673992673992674,16,0.8,1,0
75,"Given this information , annotators were asked to phrase a question involving the subject and the relationship of the fact , with the answer being the object .",Introduction,Knowledge Bases,question-answering,2,67,0.4466666666666666,74,0.271062271062271,17,0.85,1,0
76,"The annotators were explicitly instructed to phrase the question differently as much as possible , if they encounter multiple facts with similar relationship .",Introduction,Knowledge Bases,question-answering,2,68,0.4533333333333333,75,0.2747252747252747,18,0.9,1,0
77,They were also given the option of skipping facts if they wish to do so .,Introduction,Knowledge Bases,question-answering,2,69,0.46,76,0.2783882783882784,19,0.95,1,0
78,This was very important to avoid the annotators to write a boilerplate questions when they had no background knowledge about some facts .,Introduction,Knowledge Bases,question-answering,2,70,0.4666666666666667,77,0.282051282051282,20,1.0,1,0
79,Memory Networks for Simple QA,Introduction,,question-answering,2,71,0.4733333333333333,78,0.2857142857142857,0,0.0,1,0
80,A Memory Network consists of a memory ( an indexed array of objects ) and a neural network that is trained to query it given some inputs ( usually questions ) .,Introduction,Memory Networks for Simple QA,question-answering,2,72,0.48,79,0.2893772893772894,1,0.0625,1,0
81,"It has four components : Input map ( I ) , Generalization ( G ) , Output map ( O ) and Response ( R ) which we detail below .",Introduction,Memory Networks for Simple QA,question-answering,2,73,0.4866666666666667,80,0.293040293040293,2,0.125,1,0
82,"But first , we describe the MemNNs workflow used to setup a model for simple QA .",Introduction,Memory Networks for Simple QA,question-answering,2,74,0.4933333333333333,81,0.2967032967032967,3,0.1875,1,0
83,This proceeds in three steps :,Introduction,Memory Networks for Simple QA,question-answering,2,75,0.5,82,0.3003663003663003,4,0.25,1,0
84,Storing Freebase : this first phase parses,Introduction,Memory Networks for Simple QA,question-answering,2,76,0.5066666666666667,83,0.304029304029304,5,0.3125,1,0
85,Freebase ( either FB2M or FB5M depending on the setting ) and stores it in memory .,Introduction,Memory Networks for Simple QA,question-answering,2,77,0.5133333333333333,84,0.3076923076923077,6,0.375,1,0
86,It uses the Input module to preprocess the data .,Introduction,Memory Networks for Simple QA,question-answering,2,78,0.52,85,0.3113553113553113,7,0.4375,1,0
87,2 .,Introduction,Memory Networks for Simple QA,question-answering,2,79,0.5266666666666666,86,0.315018315018315,8,0.5,1,0
88,Training : this second phase trains the Mem NN to answer question .,Introduction,Memory Networks for Simple QA,question-answering,2,80,0.5333333333333333,87,0.3186813186813186,9,0.5625,1,0
89,"This uses Input , Output and Response modules , the training concerns mainly the parameters of the embedding model at the core of the Output module .",Introduction,Memory Networks for Simple QA,question-answering,2,81,0.54,88,0.3223443223443223,10,0.625,1,0
90,Connecting Reverb : this third phase adds new facts coming from,Introduction,Memory Networks for Simple QA,question-answering,2,82,0.5466666666666666,89,0.326007326007326,11,0.6875,1,0
91,Reverb to the memory .,Introduction,Memory Networks for Simple QA,question-answering,2,83,0.5533333333333333,90,0.3296703296703296,12,0.75,1,0
92,This is done after training to test the ability of MemNNs to handle new facts without having to be re-trained .,Introduction,Memory Networks for Simple QA,question-answering,2,84,0.56,91,0.3333333333333333,13,0.8125,1,0
93,It uses the Input module to preprocess Reverb facts and the Generalization module to connect them to the facts already stored .,Introduction,Memory Networks for Simple QA,question-answering,2,85,0.5666666666666667,92,0.336996336996337,14,0.875,1,0
94,"After these three stages , the MemNN is ready to answer any question by running the I , O and R modules in turn .",Introduction,Memory Networks for Simple QA,question-answering,2,86,0.5733333333333334,93,0.3406593406593406,15,0.9375,1,0
95,We now detail the implementation of the four modules .,Introduction,Memory Networks for Simple QA,question-answering,2,87,0.58,94,0.3443223443223443,16,1.0,1,0
96,Input module,Introduction,,question-answering,2,88,0.5866666666666667,95,0.3479853479853479,0,0.0,1,0
97,This module preprocesses the three types of data that are input to the network :,Introduction,Input module,question-answering,2,89,0.5933333333333334,96,0.3516483516483517,1,0.5,1,0
98,"Freebase facts that are used to populate the memory , questions that the system need to answer , and Reverb facts that we use , in a second phase , to extend the memory .",Introduction,Input module,question-answering,2,90,0.6,97,0.3553113553113553,2,1.0,1,0
99,Preprocessing Freebase,Introduction,,question-answering,2,91,0.6066666666666667,98,0.358974358974359,0,0.0,1,0
100,"The Freebase data is initially stored as atomic facts involving single entities as subject and object , plus a relationship between them .",Introduction,Preprocessing Freebase,question-answering,2,92,0.6133333333333333,99,0.3626373626373626,1,0.05,1,0
101,"However , this storage needs to be adapted to the QA task in two aspects .",Introduction,Preprocessing Freebase,question-answering,2,93,0.62,100,0.3663003663003663,2,0.1,1,0
102,"First , in order to answer list questions , which expect more than one answer , we redefine a fact as being a triple containing a subject , a relationship , and the set of all objects linked to the subject by the relationship .",Introduction,Preprocessing Freebase,question-answering,2,94,0.6266666666666667,101,0.36996336996337,3,0.15,1,0
103,"This grouping process transforms atomic facts into grouped facts , which we simply refer to as facts in the following .",Introduction,Preprocessing Freebase,question-answering,2,95,0.6333333333333333,102,0.3736263736263736,4,0.2,1,0
104,"shows the impact of this grouping : on FB2M , this decreases the number of facts from 14 M to 11 M and , on FB5 M , from 22 M to 12M .",Introduction,Preprocessing Freebase,question-answering,2,96,0.64,103,0.3772893772893773,5,0.25,1,0
105,"Second , the underlying structure of Freebase is a hypergraph , in which more than two entities can be linked .",Introduction,Preprocessing Freebase,question-answering,2,97,0.6466666666666666,104,0.3809523809523809,6,0.3,1,0
106,For instance dates can be linked together with two entities to specify the time period over which the link was valid .,Introduction,Preprocessing Freebase,question-answering,2,98,0.6533333333333333,105,0.3846153846153846,7,0.35,1,0
107,"The underlying triple storage involves mediator nodes for each such fact , effectively making entities linked through paths of length 2 , instead of 1 .",Introduction,Preprocessing Freebase,question-answering,2,99,0.66,106,0.3882783882783883,8,0.4,1,0
108,"To obtain direct links between entities in such cases , we created a single fact for these facts by removing the intermediate node and using the second relationship as the relationship for the new condensed fact .",Introduction,Preprocessing Freebase,question-answering,2,100,0.6666666666666666,107,0.3919413919413919,9,0.45,1,0
109,"This step reduces the need for searching the answer outside the immediate neighborhood of the subject referred to in the question , widely increasing the scope of the simple QA task on Freebase .",Introduction,Preprocessing Freebase,question-answering,2,101,0.6733333333333333,108,0.3956043956043956,10,0.5,1,0
110,"On WebQuestions , a benchmark not primarily designed for simple QA , removing mediator nodes allows to jump from around 65 % to 86 % of questions that can be answered with a single fact .",Introduction,Preprocessing Freebase,question-answering,2,102,0.68,109,0.3992673992673993,11,0.55,1,0
111,Preprocessing Freebase facts,Introduction,,question-answering,2,103,0.6866666666666666,110,0.4029304029304029,12,0.6,1,0
112,"A fact with k objects y = ( s , r , {o 1 , ... , o k } ) is represented by a bag - of - symbol vector f ( y ) in RN S , where NS is the number of entities and relationships .",Introduction,Preprocessing Freebase facts,question-answering,2,104,0.6933333333333334,111,0.4065934065934066,13,0.65,1,0
113,Each dimension off ( y ) corresponds to a relationship or an entity ( independent of whether it appears as subject or object ) .,Introduction,Preprocessing Freebase facts,question-answering,2,105,0.7,112,0.4102564102564102,14,0.7,1,0
114,"The entries of the subject and of the relationship have value 1 , and the entries of the objects are set to 1 / k .",Introduction,Preprocessing Freebase facts,question-answering,2,106,0.7066666666666667,113,0.4139194139194139,15,0.75,1,0
115,All other entries are 0 .,Introduction,Preprocessing Freebase facts,question-answering,2,107,0.7133333333333334,114,0.4175824175824176,16,0.8,1,0
116,Preprocessing questions,Introduction,,question-answering,2,108,0.72,115,0.4212454212454212,17,0.85,1,0
117,A question q is mapped to a bag - of - ngrams representation g ( q ) of dimension RN V where NV is the size of the vocabulary .,Introduction,Preprocessing questions,question-answering,2,109,0.7266666666666667,116,0.4249084249084249,18,0.9,1,0
118,"The vocabulary contains all individual words that appear in the questions of our datasets , together with the aliases of Freebase entities , each alias being a single n-gram .",Introduction,Preprocessing questions,question-answering,2,110,0.7333333333333333,117,0.4285714285714285,19,0.95,1,0
119,"The entries of g ( q ) that correspond to words and n-grams of q are equal to 1 , all other ones are set to 0 .",Introduction,Preprocessing questions,question-answering,2,111,0.74,118,0.4322344322344322,20,1.0,1,0
120,Preprocessing Reverb facts,Introduction,,question-answering,2,112,0.7466666666666667,119,0.4358974358974359,0,0.0,1,0
121,"In our experiments with Reverb , each fact y = ( s , r , o) is represented as a vector h ( y ) ?",Introduction,Preprocessing Reverb facts,question-answering,2,113,0.7533333333333333,120,0.4395604395604395,1,0.25,1,0
122,RN S +N V .,Introduction,Preprocessing Reverb facts,question-answering,2,114,0.76,121,0.4432234432234432,2,0.5,1,0
123,"This vector is a bagof - symbol for the subject sand the object o , and a bag - of - words for the relationship r.",Introduction,Preprocessing Reverb facts,question-answering,2,115,0.7666666666666667,122,0.4468864468864469,3,0.75,1,0
124,"The exact composition of h is provided by the Generalization module , which we describe now .",Introduction,Preprocessing Reverb facts,question-answering,2,116,0.7733333333333333,123,0.4505494505494505,4,1.0,1,0
125,Generalization module,Introduction,,question-answering,2,117,0.78,124,0.4542124542124542,0,0.0,1,0
126,This module is responsible for adding new elements to the memory .,Introduction,Generalization module,question-answering,2,118,0.7866666666666666,125,0.4578754578754578,1,0.0909090909090909,1,0
127,"In our case , the memory has a multigraph structure where each node is a Freebase entity and labeled arcs in the multigraph are Freebase relationships : after their preprocessing , all Freebase facts are stored using this structure .",Introduction,Generalization module,question-answering,2,119,0.7933333333333333,126,0.4615384615384615,2,0.1818181818181818,1,0
128,"We also consider the case where new facts , with a different structure ( i.e. new kinds of relationship ) , are provided to the MemNNs by using Reverb .",Introduction,Generalization module,question-answering,2,120,0.8,127,0.4652014652014652,3,0.2727272727272727,1,0
129,"In this case , the generalization module is then used to connect Reverb facts to the Freebase - based memory structure , in order to make them usable and searchable by the MemNN .",Introduction,Generalization module,question-answering,2,121,0.8066666666666666,128,0.4688644688644688,4,0.3636363636363636,1,0
130,"To link the subject and the object of a Reverb fact to Freebase entities , we use precomputed entity links .",Introduction,Generalization module,question-answering,2,122,0.8133333333333334,129,0.4725274725274725,5,0.4545454545454545,1,0
131,"If such links do not give any result for an entity , we search for Freebase entities with at least one alias that matches the Reverb entity string .",Introduction,Generalization module,question-answering,2,123,0.82,130,0.4761904761904761,6,0.5454545454545454,1,0
132,These two processes allowed to match 17 % of Reverb entities to Freebase ones .,Introduction,Generalization module,question-answering,2,124,0.8266666666666667,131,0.4798534798534798,7,0.6363636363636364,1,0
133,"The remainder of entities were encoded using bag - of - words representation of their strings , since we had no other way of matching them to Freebase entities .",Introduction,Generalization module,question-answering,2,125,0.8333333333333334,132,0.4835164835164835,8,0.7272727272727273,1,0
134,All Reverb relationships were encoded using bag - of - words of their strings .,Introduction,Generalization module,question-answering,2,126,0.84,133,0.4871794871794871,9,0.8181818181818182,1,0
135,"Using this approximate process , we are able to store each Reverb fact as a bag - of - symbols ( words or Freebase entities ) all already seen by the MemNN during its training phase based on Freebase .",Introduction,Generalization module,question-answering,2,127,0.8466666666666667,134,0.4908424908424908,10,0.9090909090909092,1,0
136,We can then hope that what had been learned there could also be successfully used to query Reverb facts .,Introduction,Generalization module,question-answering,2,128,0.8533333333333334,135,0.4945054945054945,11,1.0,1,0
137,Output module,Introduction,,question-answering,2,129,0.86,136,0.4981684981684982,0,0.0,1,0
138,The output module performs the memory lookups given the input to return the supporting facts destined to eventually provide the answer given a question .,Introduction,Output module,question-answering,2,130,0.8666666666666667,137,0.5018315018315018,1,0.25,1,0
139,"In our case of simple QA , this module only returns a single supporting fact .",Introduction,Output module,question-answering,2,131,0.8733333333333333,138,0.5054945054945055,2,0.5,1,0
140,"To avoid scoring all the stored facts , we first perform an approximate entity linking step to generate a small set of candidate facts .",Introduction,Output module,question-answering,2,132,0.88,139,0.5091575091575091,3,0.75,1,0
141,The supporting fact is the candidate fact that is most similar to the question according to an embedding model .,Introduction,Output module,question-answering,2,133,0.8866666666666667,140,0.5128205128205128,4,1.0,1,0
142,Candidate generation,Introduction,,question-answering,2,134,0.8933333333333333,141,0.5164835164835165,0,0.0,1,0
143,"To generate candidate facts , we match n-grams of words of the question to aliases of Freebase entities and select a few matching entities .",Introduction,Candidate generation,question-answering,2,135,0.9,142,0.5201465201465202,1,0.0769230769230769,1,0
144,All facts having one of these entities as subject are scored in a second step .,Introduction,Candidate generation,question-answering,2,136,0.9066666666666666,143,0.5238095238095238,2,0.1538461538461538,1,0
145,"We first generate all possible n-grams from the question , removing those that contain an interrogative pronoun or 1 - grams that belong to a list of stopwords .",Introduction,Candidate generation,question-answering,2,137,0.9133333333333332,144,0.5274725274725275,3,0.2307692307692307,1,0
146,"We only keep the n-grams which are an alias of an entity , and then discard all n-grams that area subsequence of another n-gram , except if the longer n-gram only differs by in , of , for or the at the beginning .",Introduction,Candidate generation,question-answering,2,138,0.92,145,0.5311355311355311,4,0.3076923076923077,1,0
147,We finally keep the two entities with the most links in Freebase retrieved for each of the five longest matched n-grams .,Introduction,Candidate generation,question-answering,2,139,0.9266666666666666,146,0.5347985347985348,5,0.3846153846153846,1,0
148,Scoring Scoring is performed using an embedding model .,Introduction,Candidate generation,question-answering,2,140,0.9333333333333332,147,0.5384615384615384,6,0.4615384615384615,1,0
149,Given two embedding matrices WV ?,Introduction,Candidate generation,question-answering,2,141,0.94,148,0.5421245421245421,7,0.5384615384615384,1,0
150,R dN V and W S ?,Introduction,Candidate generation,question-answering,2,142,0.9466666666666668,149,0.5457875457875457,8,0.6153846153846154,1,0
151,"R dN S , which respectively contain , in columns , the d-dimensional embeddings of the words / n - grams of the vocabulary and the embeddings of the Freebase entities and relationships , the similarity between question q and a Freebase candidate fact y is computed as :",Introduction,Candidate generation,question-answering,2,143,0.9533333333333334,150,0.5494505494505495,9,0.6923076923076923,1,0
152,with cos ( ) the cosine similarity .,Introduction,Candidate generation,question-answering,2,144,0.96,151,0.5531135531135531,10,0.7692307692307693,1,0
153,"When scoring a fact y from Reverb , we use the same embeddings and build the matrix WV S ? R d( N V +N S ) , which contains the concatenation in columns of WV and W S , and also compute the cosine similarity :",Introduction,Candidate generation,question-answering,2,145,0.9666666666666668,152,0.5567765567765568,11,0.8461538461538461,1,0
154,"S RV B ( q , y ) = cos ( W V g ( q ) , WV S h ( y ) ) .",Introduction,Candidate generation,question-answering,2,146,0.9733333333333334,153,0.5604395604395604,12,0.9230769230769232,1,0
155,"The dimension dis a hyperparameter , and the embedding matrices WV and W S are the parameters learned with the training algorithm of Section 4 .",Introduction,Candidate generation,question-answering,2,147,0.98,154,0.5641025641025641,13,1.0,1,0
156,Response module,Introduction,,question-answering,2,148,0.9866666666666668,155,0.5677655677655677,0,0.0,1,0
157,"In Memory Networks , the Response module postprocesses the result of the Output module to compute the intended answer .",Introduction,Response module,question-answering,2,149,0.9933333333333332,156,0.5714285714285714,1,0.5,1,0
158,"In our case , it returns the set of objects of the selected supporting fact .",Introduction,Response module,question-answering,2,150,1.0,157,0.575091575091575,2,1.0,1,0
159,Training,,,question-answering,2,0,0.0,158,0.5787545787545788,0,0.0,1,0
160,This section details how we trained the scoring function of the Output module using a multitask training process on four different sources of data .,Training,Training,question-answering,2,1,0.125,159,0.5824175824175825,1,0.125,1,0
161,"First , in addition to the new SimpleQuestions dataset described in Section 2 , we also used We-bQuestions , a benchmark for QA introduced in : questions are labeled with answer strings from aliases of Freebase entities , and many questions expect multiple answers .",Training,Training,question-answering,2,2,0.25,160,0.5860805860805861,2,0.25,1,0
162,Table 3 details the statistics of both datasets .,Training,Training,question-answering,2,3,0.375,161,0.5897435897435898,3,0.375,1,0
163,"We also train on automatic questions generated from the KB , that is FB2M or FB5M depending on the setting , which are essential to learn embeddings for the entities not appearing in either WebQuestions or SimpleQuestions .",Training,Training,question-answering,2,4,0.5,162,0.5934065934065934,4,0.5,1,0
164,Statistics of FB2M or FB5M are given in ; we generated one training question per fact following the same process as that used in .,Training,Training,question-answering,2,5,0.625,163,0.5970695970695971,5,0.625,1,0
165,"Following previous work such as , we also use the indirect supervision signal of pairs of question paraphrases .",Training,Training,question-answering,2,6,0.75,164,0.6007326007326007,6,0.75,1,0
166,We used a subset of the large set of paraphrases extracted from WIKIANSWERS and introduced in .,Training,Training,question-answering,2,7,0.875,165,0.6043956043956044,7,0.875,1,0
167,Our Paraphrases dataset is made of 15M clusters containing 2 or more paraphrases each .,Training,Training,question-answering,2,8,1.0,166,0.608058608058608,8,1.0,1,0
168,Multitask training,,,question-answering,2,0,0.0,167,0.6117216117216118,0,0.0,1,0
169,"As in previous work on embedding models and Memory Networks , the embeddings are trained with a ranking criterion .",Multitask training,Multitask training,question-answering,2,1,0.0277777777777777,168,0.6153846153846154,1,0.0769230769230769,1,0
170,"For QA datasets the goal is that in the embedding space , a supporting fact is more similar to the question than any other non-supporting fact .",Multitask training,Multitask training,question-answering,2,2,0.0555555555555555,169,0.6190476190476191,2,0.1538461538461538,1,0
171,"For the paraphrase dataset , a question should be more similar to one of its paraphrases than to any another question .",Multitask training,Multitask training,question-answering,2,3,0.0833333333333333,170,0.6227106227106227,3,0.2307692307692307,1,0
172,The multitask learning of the embedding matrices WV and W S is performed by alternating stochastic gradient descent ( SGD ) steps over the loss function on the different datasets .,Multitask training,Multitask training,question-answering,2,4,0.1111111111111111,171,0.6263736263736264,4,0.3076923076923077,1,0
173,"For the QA datasets , given a question / supporting fact pair ( q , y) and a non-supporting fact y ? , we perform a step to minimize the loss function ? QA ( q , y , y ? ) = ? ? S QA ( q , y ) + S QA ( q , y ? ) + , where [. ] + is the positive part and ?",Multitask training,Multitask training,question-answering,2,5,0.1388888888888889,172,0.63003663003663,5,0.3846153846153846,1,0
174,is a margin hyperparameter .,Multitask training,Multitask training,question-answering,2,6,0.1666666666666666,173,0.6336996336996337,6,0.4615384615384615,1,0
175,"For the paraphrase dataset , the similarity score between two questions q and q ?",Multitask training,Multitask training,question-answering,2,7,0.1944444444444444,174,0.6373626373626373,7,0.5384615384615384,1,0
176,"is also the cosine between their embeddings , i.e. S QQ ( q , q ? ) = cos ( W V g ( q ) , WV g ( q ? ) ) , and given a paraphrase pair ( q , q ? ) and another question q ??",Multitask training,Multitask training,question-answering,2,8,0.2222222222222222,175,0.6410256410256411,8,0.6153846153846154,1,0
177,", the loss is :",Multitask training,Multitask training,question-answering,2,9,0.25,176,0.6446886446886447,9,0.6923076923076923,1,0
178,The embeddings ( i.e. the columns of WV and W S ) are projected onto the L 2 unit ball after each update .,Multitask training,Multitask training,question-answering,2,10,0.2777777777777778,177,0.6483516483516484,10,0.7692307692307693,1,0
179,"At each time step , a sample from the paraphrase dataset is drawn with probability 0.2 ( this probability is arbitrary ) .",Multitask training,Multitask training,question-answering,2,11,0.3055555555555556,178,0.652014652014652,11,0.8461538461538461,1,0
180,"Otherwise , a sample from one of the three QA datasets , chosen uniformly at random , is taken .",Multitask training,Multitask training,question-answering,2,12,0.3333333333333333,179,0.6556776556776557,12,0.9230769230769232,1,0
181,We use the WARP loss,Multitask training,Multitask training,question-answering,2,13,0.3611111111111111,180,0.6593406593406593,13,1.0,1,0
182,Distant supervision,Multitask training,,question-answering,2,14,0.3888888888888889,181,0.663003663003663,0,0.0,1,0
183,"Unlike for SimpleQuestions or the synthetic QA data generated from Freebase , for WebQuestions only answer strings are provided for questions : the supporting facts are unknown .",Multitask training,Distant supervision,question-answering,2,15,0.4166666666666667,182,0.6666666666666666,1,0.1,1,0
184,"In order to generate the supervision , we use the candidate fact generation algorithm of Section 3.3 .",Multitask training,Distant supervision,question-answering,2,16,0.4444444444444444,183,0.6703296703296703,2,0.2,1,0
185,"For each candidate fact , the aliases of its objects are compared to the set of provided answer strings .",Multitask training,Distant supervision,question-answering,2,17,0.4722222222222222,184,0.673992673992674,3,0.3,1,0
186,The fact ( s ) which can generate the maximum number of answer strings from their objects ' aliases are then kept .,Multitask training,Distant supervision,question-answering,2,18,0.5,185,0.6776556776556777,4,0.4,1,0
187,"If multiple facts are obtained for the same question , the ones with the minimal number of objects are considered as supervision facts .",Multitask training,Distant supervision,question-answering,2,19,0.5277777777777778,186,0.6813186813186813,5,0.5,1,0
188,This last selection avoids favoring irrelevant relationships that would be kept only because they point to many objects but would not be specific enough .,Multitask training,Distant supervision,question-answering,2,20,0.5555555555555556,187,0.684981684981685,6,0.6,1,0
189,"If no answer string could be found from the objects of the initial candidates , the question is discarded from the training set .",Multitask training,Distant supervision,question-answering,2,21,0.5833333333333334,188,0.6886446886446886,7,0.7,1,0
190,Future work should investigate the process of weak supervised training of MemNNs recently introduced in that allows to train them without any supervision coming from the supporting facts . :,Multitask training,Distant supervision,question-answering,2,22,0.6111111111111112,189,0.6923076923076923,8,0.8,1,0
191,Training and evaluation datasets .,Multitask training,Distant supervision,question-answering,2,23,0.6388888888888888,190,0.6959706959706959,9,0.9,1,0
192,Questions automatically generated from the KB and paraphrases can also be used in training .,Multitask training,Distant supervision,question-answering,2,24,0.6666666666666666,191,0.6996336996336996,10,1.0,1,0
193,WebQuestions SimpleQuestions,Multitask training,Distant supervision,question-answering,2,25,0.6944444444444444,192,0.7032967032967034,0,0.0,1,0
194,Reverb,Multitask training,,question-answering,2,26,0.7222222222222222,193,0.706959706959707,1,0.0,1,0
195,Generating negative examples,Multitask training,Reverb,question-answering,2,27,0.75,194,0.7106227106227107,0,0.0,1,0
196,"As in , learning is performed with gradient descent , so that negative examples ( non - supporting facts or non-paraphrases ) are generated according to a randomized policy during training .",Multitask training,Reverb,question-answering,2,28,0.7777777777777778,195,0.7142857142857143,1,0.1111111111111111,1,0
197,"For paraphrases , given a pair ( q , q ? ) , a nonparaphrase pair is generated as ( q , q ?? )",Multitask training,Reverb,question-answering,2,29,0.8055555555555556,196,0.717948717948718,2,0.2222222222222222,1,0
198,where q ??,Multitask training,Reverb,question-answering,2,30,0.8333333333333334,197,0.7216117216117216,3,0.3333333333333333,1,0
199,"is a random question of the dataset , not belonging to the cluser of q .",Multitask training,Reverb,question-answering,2,31,0.8611111111111112,198,0.7252747252747253,4,0.4444444444444444,1,0
200,"For question / supporting fact pairs , we use two policies .",Multitask training,Reverb,question-answering,2,32,0.8888888888888888,199,0.7289377289377289,5,0.5555555555555556,1,0
201,"The default policy to obtain a non-supporting fact is to corrupt the answer fact by exchanging it s subject , its relationship or its object ( s ) with that of another fact chosen uniformly at random from the KB .",Multitask training,Reverb,question-answering,2,33,0.9166666666666666,200,0.7326007326007326,6,0.6666666666666666,1,0
202,"In this policy , the element of the fact to corrupt is chosen randomly , with a small probability ( 0.3 ) of corrupting more than one element of the answer fact .",Multitask training,Reverb,question-answering,2,34,0.9444444444444444,201,0.7362637362637363,7,0.7777777777777778,1,0
203,"The second policy we propose , called candidates as negatives , is to take as non-supporting fact a randomly chosen fact from the set of candidate facts .",Multitask training,Reverb,question-answering,2,35,0.9722222222222222,202,0.73992673992674,8,0.8888888888888888,1,0
204,"While the first policy is standard in learning embeddings , the second one is more original , and , as we see in the experiments , gives slightly better performance .",Multitask training,Reverb,question-answering,2,36,1.0,203,0.7435897435897436,9,1.0,1,0
205,Related Work,,,question-answering,2,0,0.0,204,0.7472527472527473,0,0.0,1,0
206,"The first approaches to open - domain QA were search engine - based systems , where keywords extracted from the question are sent to a search engine , and the answer is extracted from the top results .",Related Work,Related Work,question-answering,2,1,0.1111111111111111,205,0.7509157509157509,1,0.1111111111111111,0,0
207,"This method has been adapted to KB - based QA , and obtained competitive results with respect to semantic parsing and embedding - based approaches .",Related Work,Related Work,question-answering,2,2,0.2222222222222222,206,0.7545787545787546,2,0.2222222222222222,0,0
208,Semantic parsing approaches perform a functional parse of the sentence that can be interpreted as a KB query .,Related Work,Related Work,question-answering,2,3,0.3333333333333333,207,0.7582417582417582,3,0.3333333333333333,0,0
209,"Even though these approaches are difficult to train at scale because of the complexity of their inference , their advantage is to provide a deep interpretation of the question .",Related Work,Related Work,question-answering,2,4,0.4444444444444444,208,0.7619047619047619,4,0.4444444444444444,0,0
210,"Some of these approaches require little to no question - answer pairs , relying on simple rules to tranform the semantic interpretation to a KB query .",Related Work,Related Work,question-answering,2,5,0.5555555555555556,209,0.7655677655677655,5,0.5555555555555556,0,0
211,"Like our work , embedding - based methods for QA can be seen as simple MemNNs .",Related Work,Related Work,question-answering,2,6,0.6666666666666666,210,0.7692307692307693,6,0.6666666666666666,0,0
212,"The algorithms of use an approach similar to ours but are based on Reverb rather than Freebase , and relied purely on bag - of - word for both questions and facts .",Related Work,Related Work,question-answering,2,7,0.7777777777777778,211,0.7728937728937729,7,0.7777777777777778,0,0
213,"The approach of ( Yang et al. , 2014 ) uses a different representation of questions , in which recognized entities are replaced by an entity token , and a different training data using entity mentions from WIKIPEDIA .",Related Work,Related Work,question-answering,2,8,0.8888888888888888,212,0.7765567765567766,8,0.8888888888888888,0,0
214,"Our model is closest to the one presented in , which is discussed in more details in the experiments .",Related Work,Related Work,question-answering,2,9,1.0,213,0.7802197802197802,9,1.0,0,0
215,Experiments,,,question-answering,2,0,0.0,214,0.7838827838827839,0,0.0,1,0
216,This section provides an extensive evaluation of our MemNNs implementation against state - of the - art QA methods as well as an empirical study of the impact of using multiple training sources on the prediction performance .,Experiments,Experiments,question-answering,2,1,0.5,215,0.7875457875457875,1,0.5,1,0
217,"details the dimensions of the test sets of WebQuestions , SimpleQuestions and Reverb which we used for evaluation .",Experiments,Experiments,question-answering,2,2,1.0,216,0.7912087912087912,2,1.0,1,0
218,Evaluation and baselines,,,question-answering,2,0,0.0,217,0.7948717948717948,0,0.0,1,0
219,"On WebQuestions , we evaluate against previous results on this benchmark in terms of F1 - score as defined in , which is the average , overall test questions , of the F1 - score of the sets of predicted answers .",Evaluation and baselines,Evaluation and baselines,question-answering,2,1,0.1428571428571428,218,0.7985347985347986,1,0.1428571428571428,1,0
220,"Since no previous result was published on SimpleQuestions , we only compare different versions of MemNNs .",Evaluation and baselines,Evaluation and baselines,question-answering,2,2,0.2857142857142857,219,0.8021978021978022,2,0.2857142857142857,1,0
221,"SimpleQuestions questions are labeled with their entire Freebase fact , so we evaluate in terms of path - level accuracy , in which a prediction is correct if the subject and the relationship were correctly retrieved by the system .",Evaluation and baselines,Evaluation and baselines,question-answering,2,3,0.4285714285714285,220,0.8058608058608059,3,0.4285714285714285,1,0
222,"The Reverb test set , based on the KB of the same name and introduced in is used for evaluation only .",Evaluation and baselines,Evaluation and baselines,question-answering,2,4,0.5714285714285714,221,0.8095238095238095,4,0.5714285714285714,1,0
223,It contains 691 questions .,Evaluation and baselines,Evaluation and baselines,question-answering,2,5,0.7142857142857143,222,0.8131868131868132,5,0.7142857142857143,1,0
224,"We consider the task of re-ranking a small set of candidate answers , which are Reverb facts and are labeled as corrector incorrect .",Evaluation and baselines,Evaluation and baselines,question-answering,2,6,0.8571428571428571,223,0.8168498168498168,6,0.8571428571428571,1,0
225,"We compare our approach to the original system , to and to the original MemNNs , in terms of accuracy , which is the percentage of questions for which the top - ranked candidate fact is correct .",Evaluation and baselines,Evaluation and baselines,question-answering,2,7,1.0,224,0.8205128205128205,7,1.0,1,0
226,Experimental setup,,,question-answering,2,0,0.0,225,0.8241758241758241,0,0.0,1,0
227,All models were trained with at least the dataset made of synthetic questions created from the KB .,Experimental setup,Experimental setup,question-answering,2,1,0.0833333333333333,226,0.8278388278388278,1,0.0833333333333333,1,0
228,"The hyperparameters were chosen to maximize the F1-score on WebQuestions validation set , independently of the testing dataset .",Experimental setup,Experimental setup,question-answering,2,2,0.1666666666666666,227,0.8315018315018315,2,0.1666666666666666,1,0
229,"The embedding dimension and the learning rate were chosen among { 64 , 128 , 256 } and { 1 , 0.1 , ... , 1.0e ? 4 } respectively , and the margin ?",Experimental setup,Experimental setup,question-answering,2,3,0.25,228,0.8351648351648352,3,0.25,1,1
230,was set to 0.1 .,Experimental setup,Experimental setup,question-answering,2,4,0.3333333333333333,229,0.8388278388278388,4,0.3333333333333333,1,1
231,"For each configuration of hyperparameters , the F1score on the validation set was computed regularly during learning to perform early stopping .",Experimental setup,Experimental setup,question-answering,2,5,0.4166666666666667,230,0.8424908424908425,5,0.4166666666666667,1,0
232,We tested additional configurations for our algorithm .,Experimental setup,Experimental setup,question-answering,2,6,0.5,231,0.8461538461538461,6,0.5,1,0
233,"First , in the Candidates as Negatives setting ( negative facts are sampled from the candidate set , see Section 4 ) , abbreviated CANDS AS NEGS , the experimental protocol is the same as in the default setting but the embeddings are initialized with the best configuration of the default setup .",Experimental setup,Experimental setup,question-answering,2,7,0.5833333333333334,232,0.8498168498168498,7,0.5833333333333334,1,0
234,"Second , our model shares some similarities with an approach studied in , in which the authors noticed important gains using a subgraph representation of answers .",Experimental setup,Experimental setup,question-answering,2,8,0.6666666666666666,233,0.8534798534798534,8,0.6666666666666666,1,0
235,"For completeness , we also added such a subgraph representation of objects .",Experimental setup,Experimental setup,question-answering,2,9,0.75,234,0.8571428571428571,9,0.75,1,0
236,"In that setting , called Subgraph , each object o of a fact is itself represented as a bag - of - entities that encodes the immediate neighborhood of o .",Experimental setup,Experimental setup,question-answering,2,10,0.8333333333333334,235,0.8608058608058609,10,0.8333333333333334,1,0
237,This Subgraph model is trained similarly as our main approach and only the results of a post -hoc ensemble combination of the two models ( where the scores are added ) are presented .,Experimental setup,Experimental setup,question-answering,2,11,0.9166666666666666,236,0.8644688644688645,11,0.9166666666666666,1,0
238,We also report the results obtained by an ensemble of the 5 best models on validation ( subgraph excepted ) ; this is denoted 5 models .,Experimental setup,Experimental setup,question-answering,2,12,1.0,237,0.8681318681318682,12,1.0,1,0
239,Results,,,question-answering,2,0,0.0,238,0.8717948717948718,0,0.0,1,0
240,Comparative results,,,question-answering,2,0,0.0,239,0.8754578754578755,0,0.0,1,0
241,The results of the comparative experiments are given in .,Comparative results,Comparative results,question-answering,2,1,0.0357142857142857,240,0.8791208791208791,1,0.3333333333333333,1,0
242,"On the main benchmark WebQuestions , our best results use all data sources , the bigger extract from Freebase and the CANDS AS NEGS setting .",Comparative results,Comparative results,question-answering,2,2,0.0714285714285714,241,0.8827838827838828,2,0.6666666666666666,1,1
243,"The two ensembles achieve excellent results , with F1 -",Comparative results,Comparative results,question-answering,2,3,0.1071428571428571,242,0.8864468864468864,3,1.0,1,0
244,WebQuestions SimpleQuestions,Comparative results,Comparative results,question-answering,2,4,0.1428571428571428,243,0.8901098901098901,0,0.0,1,0
245,Reverb F1-SCORE ( % ) ACCURACY ( % ) ACCURACY ( % ) BASELINES Random guess 1.9 4.9 35 31.3 n / a n / a n / a n / a 54 29.7 n / a 73 ) - using path 35.3 n/ a n / a ) - using path + subgraph 39.2 n / a n / a 39.9 n/ a n/ a 41.3 n/ a n / a of SimpleQuestions questions .,Comparative results,Comparative results,question-answering,2,5,0.1785714285714285,244,0.8937728937728938,1,0.0909090909090909,1,0
246,"This shows that MemNNs are effective at re-ranking the candidates , but also that simple QA is still not solved .",Comparative results,Comparative results,question-answering,2,6,0.2142857142857142,245,0.8974358974358975,2,0.1818181818181818,1,0
247,Our approach bares similarity to ) - using path .,Comparative results,Comparative results,question-answering,2,7,0.25,246,0.9010989010989012,3,0.2727272727272727,1,0
248,"They use FB2M , and so their result ( 35.3 % F1 - score on WebQuestions ) should be compared to our 36.2 % .",Comparative results,Comparative results,question-answering,2,8,0.2857142857142857,247,0.9047619047619048,4,0.3636363636363636,1,0
249,"The models are slightly different in that they replace the entity string with the subject entity in the question representation and that we use the cosine similarity instead of the dot product , which gave consistent improvements .",Comparative results,Comparative results,question-answering,2,9,0.3214285714285714,248,0.9084249084249084,5,0.4545454545454545,1,0
250,"Still , the major differences come from how we use Freebase .",Comparative results,Comparative results,question-answering,2,10,0.3571428571428571,249,0.912087912087912,6,0.5454545454545454,1,0
251,"First , the removal of the mediator nodes allows us to restrict ourselves to single supporting facts , while they search in paths of length 2 with a heuristic to select the paths to follow ( otherwise , inference is too costly ) , which makes our inference simpler and more efficient .",Comparative results,Comparative results,question-answering,2,11,0.3928571428571428,250,0.9157509157509156,7,0.6363636363636364,1,0
252,"Second , using grouped facts , we integrate multiple answers during learning ( through the distant supervision ) , while they use a grouping heuristic attest time .",Comparative results,Comparative results,question-answering,2,12,0.4285714285714285,251,0.9194139194139194,8,0.7272727272727273,1,0
253,Grouping facts also allows us to scale much better and to train on FB5M .,Comparative results,Comparative results,question-answering,2,13,0.4642857142857143,252,0.9230769230769232,9,0.8181818181818182,1,0
254,"On WebQuestions , not specifically designed as a simple QA dataset , 86 % of the questions can now be answered with a single supporting fact , and performance increases significantly ( from 36.2 % to 41.0 % F1-score ) .",Comparative results,Comparative results,question-answering,2,14,0.5,253,0.9267399267399268,10,0.9090909090909092,1,0
255,"Using the bigger FB5M as KB does not change performance on SimpleQuestions because it was based on FB2M , but the results show that our model is robust to the addition of more entities than necessary .",Comparative results,Comparative results,question-answering,2,15,0.5357142857142857,254,0.9304029304029304,11,1.0,1,0
256,Transfer learning on Reverb,Comparative results,,question-answering,2,16,0.5714285714285714,255,0.934065934065934,0,0.0,1,0
257,"In this set of experiments , all Reverb facts are added to the memory , without any retraining , and we test our ability to rerank answers on the companion QA set .",Comparative results,Transfer learning on Reverb,question-answering,2,17,0.6071428571428571,256,0.9377289377289376,1,0.0833333333333333,1,1
258,"Thus , ( last column ) presents the result of our model without training on Reverb against methods specifically developed on that dataset .",Comparative results,Transfer learning on Reverb,question-answering,2,18,0.6428571428571429,257,0.9413919413919414,2,0.1666666666666666,1,0
259,"Our best results are 67 % accuracy ( and 68 % for the ensemble of 5 models ) , which are better than the 54 % of the original paper and close to the stateof - the - art 73 % of .",Comparative results,Transfer learning on Reverb,question-answering,2,19,0.6785714285714286,258,0.945054945054945,3,0.25,1,1
260,These results show that the Memory Network approach can integrate and use new entities and links .,Comparative results,Transfer learning on Reverb,question-answering,2,20,0.7142857142857143,259,0.9487179487179488,4,0.3333333333333333,1,0
261,presents the results on the three datasets when our model is trained with different data sources .,Comparative results,Transfer learning on Reverb,question-answering,2,21,0.75,260,0.9523809523809524,5,0.4166666666666667,1,0
262,"We first notice that models trained on a single QA dataset perform poorly on the other datasets ( e.g. 46.6 % accuracy on SimpleQuestions for the model trained on WebQuestions only ) , which shows that the performance on We-bQuestions does not necessarily guarantee high coverage for simple QA .",Comparative results,Transfer learning on Reverb,question-answering,2,22,0.7857142857142857,261,0.956043956043956,6,0.5,1,1
263,"On the other hand , training on both datasets only improves performance ; in particular , the model is able to capture all question patterns of the two datasets ; there is no "" negative interaction "" .",Comparative results,Transfer learning on Reverb,question-answering,2,23,0.8214285714285714,262,0.9597069597069596,7,0.5833333333333334,1,1
264,Importance of data sources,Comparative results,Transfer learning on Reverb,question-answering,2,24,0.8571428571428571,263,0.9633699633699634,8,0.6666666666666666,1,0
265,The bottom half of,Comparative results,Transfer learning on Reverb,question-answering,2,25,0.8928571428571429,264,0.967032967032967,9,0.75,1,0
266,"While paraphrases do not seem to help much on WebQuestions and SimpleQuestions , except when training only with synthetic questions , they have a dramatic impact on the performance on Reverb .",Comparative results,Transfer learning on Reverb,question-answering,2,26,0.9285714285714286,265,0.9706959706959708,10,0.8333333333333334,1,0
267,"This is because WebQuestions and SimpleQuestions questions follow simple patterns and are well formed , while Reverb questions have more syntactic and lexical variability .",Comparative results,Transfer learning on Reverb,question-answering,2,27,0.9642857142857144,266,0.9743589743589745,11,0.9166666666666666,1,0
268,"Thus , paraphrases are important to avoid overfitting on specific question patterns of the training sets .",Comparative results,Transfer learning on Reverb,question-answering,2,28,1.0,267,0.978021978021978,12,1.0,1,0
269,Conclusion,,,question-answering,2,0,0.0,268,0.9816849816849816,0,0.0,1,0
270,This paper presents an implementation of MemNNs for the task of large - scale simple QA .,Conclusion,Conclusion,question-answering,2,1,0.25,269,0.9853479853479854,1,0.25,0,0
271,"Our results demonstrate that , if properly trained , MemNNs are able to handle natural language and a very large memory ( millions of entries ) , and hence can reach state - of - the - art on the popular benchmark WebQuestions .",Conclusion,Conclusion,question-answering,2,2,0.5,270,0.989010989010989,2,0.5,0,0
272,"We want to emphasize that many of our findings , especially those regarding how to format the KB , do not only concern MemNNs but potentially any QA system .",Conclusion,Conclusion,question-answering,2,3,0.75,271,0.9926739926739928,3,0.75,0,0
273,"This paper also introduced the new dataset SimpleQuestions , which , with 100 k examples , is one order of magnitude bigger than WebQuestions : we hope that it will foster interesting new research in QA , simple or not .",Conclusion,Conclusion,question-answering,2,4,1.0,272,0.9963369963369964,4,1.0,0,0
1,title,,,question-answering,3,0,0.0,0,0.0,0,0.0,1,0
2,Sentence Similarity Learning by Lexical Decomposition and Composition,title,,question-answering,3,1,0.0,1,0.0039370078740157,1,0.0,1,1
3,abstract,,,question-answering,3,0,0.0,2,0.0078740157480314,0,0.0,1,0
4,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences .",abstract,abstract,question-answering,3,1,0.02,3,0.0118110236220472,1,0.02,1,1
5,"In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences .",abstract,abstract,question-answering,3,2,0.04,4,0.0157480314960629,2,0.04,1,0
6,"The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence .",abstract,abstract,question-answering,3,3,0.06,5,0.0196850393700787,3,0.06,1,0
7,"Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector .",abstract,abstract,question-answering,3,4,0.08,6,0.0236220472440944,4,0.08,1,0
8,"After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components .",abstract,abstract,question-answering,3,5,0.1,7,0.0275590551181102,5,0.1,1,0
9,"Finally , a similarity score is estimated over the composed feature vectors .",abstract,abstract,question-answering,3,6,0.12,8,0.0314960629921259,6,0.12,1,0
10,"Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task .",abstract,abstract,question-answering,3,7,0.14,9,0.0354330708661417,7,0.14,1,0
11,Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences .,abstract,abstract,question-answering,3,8,0.16,10,0.0393700787401574,8,0.16,1,0
12,It plays an important role fora variety of tasks in both NLP and IR communities .,abstract,abstract,question-answering,3,9,0.18,11,0.0433070866141732,9,0.18,1,0
13,"For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) .",abstract,abstract,question-answering,3,10,0.2,12,0.0472440944881889,10,0.2,1,0
14,"For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",abstract,abstract,question-answering,3,11,0.22,13,0.0511811023622047,11,0.22,1,0
15,"However , sentence similarity learning has following challenges :",abstract,abstract,question-answering,3,12,0.24,14,0.0551181102362204,12,0.24,1,0
16,1 .,abstract,abstract,question-answering,3,13,0.26,15,0.0590551181102362,13,0.26,1,0
17,There is a lexical gap between semantically equivalent sentences .,abstract,abstract,question-answering,3,14,0.28,16,0.0629921259842519,14,0.28,1,0
18,"Take the E 1 and E 2 in Table 1 for example , they have the similar meaning but with different lexicons .",abstract,abstract,question-answering,3,15,0.3,17,0.0669291338582677,15,0.3,1,0
19,"2 . Semantic similarity should be measured at different levels of granularity ( word - level , phrase - level and syntax - level ) .",abstract,abstract,question-answering,3,16,0.32,18,0.0708661417322834,16,0.32,1,0
20,"E.g. , "" not related "" in E 2 is an indivisible phrase when matching with "" irrelevant "" in E 1 ( shown in square brackets ) .",abstract,abstract,question-answering,3,17,0.34,19,0.0748031496062992,17,0.34,1,0
21,3 .,abstract,abstract,question-answering,3,18,0.36,20,0.0787401574803149,18,0.36,1,0
22,"The dissimilarity ( shown in angle brackets ) between two sentences is also a significant clue ( Qiu et al. , 2006 ) .",abstract,abstract,question-answering,3,19,0.38,21,0.0826771653543307,19,0.38,1,0
23,"For example , by judging the dissimilar parts , we can easily identify that E 3 and E 5 share the similar meaning "" The study is about salmon "" , because "" sockeye "" belongs to the salmon family , and "" flounder "" does not .",abstract,abstract,question-answering,3,20,0.4,22,0.0866141732283464,20,0.4,1,0
24,"Whereas the meaning of E 4 is quite different from E 3 , which emphasizes "" The study is about red ( a special kind of ) salmon "" , because both "" sockeye "" and "" coho "" are in the salmon family .",abstract,abstract,question-answering,3,21,0.42,23,0.0905511811023622,21,0.42,1,0
25,How we can extract and utilize those information becomes another challenge .,abstract,abstract,question-answering,3,22,0.44,24,0.0944881889763779,22,0.44,1,0
26,"In order to handle the above challenges , researchers have been working on sentence similarity algorithms fora longtime .",abstract,abstract,question-answering,3,23,0.46,25,0.0984251968503937,23,0.46,1,0
27,"To bridge the lexical gap ( challenge 1 ) , some word similarity metrics were proposed to match different but semantically related words .",abstract,abstract,question-answering,3,24,0.48,26,0.1023622047244094,24,0.48,1,0
28,"Examples include knowledge - based metrics ( Resnik , 1995 ) and corpus - based metrics ( Jiang and Conrath , 1997 ; Yin and Schtze , 2015 ; He et al. , 2015 ) .",abstract,abstract,question-answering,3,25,0.5,27,0.1062992125984252,25,0.5,1,0
29,"To measure sentence similarity from various granularities ( challenge 2 ) , researchers have explored features extracted from n-grams , continuous phrases , discontinuous phrases , and parse trees ( Yin and Schtze , 2015 ; He et al. , 2015 ; Heilman and Smith , 2010 ) .",abstract,abstract,question-answering,3,26,0.52,28,0.1102362204724409,26,0.52,1,0
30,The third challenge did not get much,abstract,abstract,question-answering,3,27,0.54,29,0.1141732283464567,27,0.54,1,0
31,narrative,abstract,abstract,question-answering,3,28,0.56,30,0.1181102362204724,28,0.56,1,0
32,E1,abstract,,question-answering,3,29,0.58,31,0.1220472440944882,29,0.58,1,0
33,The research is to sockeye .,abstract,E1,question-answering,3,30,0.6,32,0.1259842519685039,30,0.6,1,0
34,E2,abstract,,question-answering,3,31,0.62,33,0.1299212598425196,31,0.62,1,0
35,The study is [ not related ] to salmon .,abstract,E2,question-answering,3,32,0.64,34,0.1338582677165354,32,0.64,1,0
36,E3,abstract,,question-answering,3,33,0.66,35,0.1377952755905512,33,0.66,1,0
37,The research is relevant to salmon .,abstract,E3,question-answering,3,34,0.68,36,0.1417322834645669,34,0.68,1,0
38,E4,abstract,,question-answering,3,35,0.7,37,0.1456692913385826,35,0.7,1,0
39,"The study is relevant to sockeye , instead of coho .",abstract,E4,question-answering,3,36,0.72,38,0.1496062992125984,36,0.72,1,0
40,E5,abstract,,question-answering,3,37,0.74,39,0.1535433070866141,37,0.74,1,0
41,"The study is relevant to sockeye , rather than flounder .:",abstract,E5,question-answering,3,38,0.76,40,0.1574803149606299,38,0.76,1,0
42,"Examples for sentence similarity learning , where sockeye means "" red salmon "" , and coho means "" silver salmon "" .",abstract,E5,question-answering,3,39,0.78,41,0.1614173228346456,39,0.78,1,0
43,""" coho "" and "" sockeye "" are in the salmon family , while "" flounder "" is not .",abstract,E5,question-answering,3,40,0.8,42,0.1653543307086614,40,0.8,1,0
44,"attention in the past , the only related work of explored the dissimilarity between sentences in a pair for paraphrase identification task , but they require human annotations in order to train a classifier , and their performance is still below the state of the art .",abstract,E5,question-answering,3,41,0.82,43,0.1692913385826771,41,0.82,1,0
45,"In this paper , we propose a novel model to tackle all these challenges jointly by decomposing and composing lexical semantics over sentences .",abstract,E5,question-answering,3,42,0.84,44,0.1732283464566929,42,0.84,1,1
46,"Given a sentence pair , the model represents each word as a low -dimensional vector ( challenge 1 ) , and calculates a semantic matching vector for each word based on all words in the other sentence ( challenge 2 ) .",abstract,E5,question-answering,3,43,0.86,45,0.1771653543307086,43,0.86,1,1
47,"Then based on the semantic matching vector , each word vector is decomposed into two components : a similar component and a dissimilar component ( challenge 3 ) .",abstract,E5,question-answering,3,44,0.88,46,0.1811023622047244,44,0.88,1,1
48,"We use similar components of all the words to represent the similar parts of the sentence pair , and dissimilar components of every word to model the dissimilar parts explicitly .",abstract,E5,question-answering,3,45,0.9,47,0.1850393700787401,45,0.9,1,1
49,"After this , a two - channel CNN operation is performed to compose the similar and dissimilar components into a feature vector ( challenge 2 and 3 ) .",abstract,E5,question-answering,3,46,0.92,48,0.1889763779527559,46,0.92,1,1
50,"Finally , the composed feature vector is utilized to predict the sentence similarity .",abstract,E5,question-answering,3,47,0.94,49,0.1929133858267716,47,0.94,1,1
51,"Experimental results on two tasks show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task .",abstract,E5,question-answering,3,48,0.96,50,0.1968503937007874,48,0.96,1,0
52,"In following parts , we start with a brief overview of our model ( Section 2 ) , followed by the details of our end - to - end implementation ( Section 3 ) .",abstract,E5,question-answering,3,49,0.98,51,0.2007874015748031,49,0.98,1,0
53,Then we evaluate our model on answer sentence selection and paraphrase identifications tasks ( Section 4 ) .,abstract,E5,question-answering,3,50,1.0,52,0.2047244094488189,50,1.0,1,0
54,Model Overview,,,question-answering,3,0,0.0,53,0.2086614173228346,0,0.0,1,0
55,"In this section , we propose a sentence similarity learning model to tackle all three challenges ( mentioned in Section 1 ) .",Model Overview,Model Overview,question-answering,3,1,0.0101010101010101,54,0.2125984251968504,1,0.0277777777777777,1,0
56,"To deal with the first challenge , we represent each word as a distributed vector , so that we can calculate similarities for formally different but semantically related words .",Model Overview,Model Overview,question-answering,3,2,0.0202020202020202,55,0.2165354330708661,2,0.0555555555555555,1,0
57,"To tackle the second challenge , we assume that each word can be semantically matched by several words in the other sentence , and we calculate a semantic matching vector for each word vector based on all the word vectors in the other side .",Model Overview,Model Overview,question-answering,3,3,0.0303030303030303,56,0.2204724409448819,3,0.0833333333333333,1,0
58,"To cope with the third challenge , we assume that each semantic unit ( word ) can be partially matched , and can be decomposed into a similar component and a dissimilar component based on its semantic matching vector .",Model Overview,Model Overview,question-answering,3,4,0.0404040404040404,57,0.2244094488188976,4,0.1111111111111111,1,0
59,shows an overview of our sentence similarity model .,Model Overview,Model Overview,question-answering,3,5,0.0505050505050505,58,0.2283464566929134,5,0.1388888888888889,1,0
60,"Given a pair of sentences Sand T , our task is to calculate a similarity score sim ( S , T ) in following steps :",Model Overview,Model Overview,question-answering,3,6,0.0606060606060606,59,0.2322834645669291,6,0.1666666666666666,1,0
61,Word Representation .,Model Overview,,question-answering,3,7,0.0707070707070707,60,0.2362204724409448,7,0.1944444444444444,1,0
62,"Word embedding of is an effective way to handle the lexical gap challenge in the sentence similarity task , as it represents each word with a distributed vector , and words appearing in similar contexts tend to have similar meanings .",Model Overview,Word Representation .,question-answering,3,8,0.0808080808080808,61,0.2401574803149606,8,0.2222222222222222,1,0
63,"With those pre-trained embeddings , we transform Sand T into sentence matrixes S = [ s 1 , ... , s i , ... , s m ] and T = [t 1 , ... , t j , ... , tn ] , where s i and t j are d-dimension vectors of the corresponding words , and m and n are sentence length of Sand T respectively .",Model Overview,Word Representation .,question-answering,3,9,0.0909090909090909,62,0.2440944881889764,9,0.25,1,0
64,Semantic Matching .,Model Overview,,question-answering,3,10,0.101010101010101,63,0.2480314960629921,10,0.2777777777777778,1,0
65,"In order to judge the similarity between two sentences , we need to check whether each semantic unit in one sentence is covered by the other sentence , or vice versa .",Model Overview,Semantic Matching .,question-answering,3,11,0.1111111111111111,64,0.2519685039370078,11,0.3055555555555556,1,0
66,"For example , in , to check whether E 2 is a paraphrase of E 1 , we need to know the single word "" irrelevant "" in E 1 is matched or covered by the phrase "" not related "" in E 2 .",Model Overview,Semantic Matching .,question-answering,3,12,0.1212121212121212,65,0.2559055118110236,12,0.3333333333333333,1,0
67,"In our model , we treat each word as a primitive semantic unit , and calculate a semantic matching vector ?",Model Overview,Semantic Matching .,question-answering,3,13,0.1313131313131313,66,0.2598425196850393,13,0.3611111111111111,1,0
68,i for each word s i by composing part or full word vectors in the other sentence T .,Model Overview,Semantic Matching .,question-answering,3,14,0.1414141414141414,67,0.2637795275590551,14,0.3888888888888889,1,0
69,"In this way , we can match a word s i to a word or phrase in T . Similarly , for the reverse direction , we also calculate all semantic matching vectorst j in T .",Model Overview,Semantic Matching .,question-answering,3,15,0.1515151515151515,68,0.2677165354330709,15,0.4166666666666667,1,0
70,We explore different f match functions later in Section 3 .,Model Overview,Semantic Matching .,question-answering,3,16,0.1616161616161616,69,0.2716535433070866,16,0.4444444444444444,1,0
71,Decomposition .,Model Overview,,question-answering,3,17,0.1717171717171717,70,0.2755905511811024,17,0.4722222222222222,1,0
72,"After the semantic matching phase , we have the semantic matching vectors of ?",Model Overview,Decomposition .,question-answering,3,18,0.1818181818181818,71,0.2795275590551181,18,0.5,1,0
73,i and t j .,Model Overview,Decomposition .,question-answering,3,19,0.1919191919191919,72,0.2834645669291338,19,0.5277777777777778,1,0
74,We interpret ?,Model Overview,Decomposition .,question-answering,3,20,0.202020202020202,73,0.2874015748031496,20,0.5555555555555556,1,0
75,i ( ort j ) as a semantic coverage of word s i ( or t j ) by the sentence T ( or S ) .,Model Overview,Decomposition .,question-answering,3,21,0.2121212121212121,74,0.2913385826771653,21,0.5833333333333334,1,0
76,"However , it is not necessary that all the semantics of s i ( or t j ) are fully covered by ?",Model Overview,Decomposition .,question-answering,3,22,0.2222222222222222,75,0.2952755905511811,22,0.6111111111111112,1,0
77,i ( ort j ) .,Model Overview,Decomposition .,question-answering,3,23,0.2323232323232323,76,0.2992125984251969,23,0.6388888888888888,1,0
78,"Take the E 1 and E 2 in for example , the word "" sockeye "" in E 1 is only partially matched by the word "" salmon "" ( the similar part ) in E 2 , as the full meaning of "" sockeye "" is "" red salmon "" ( the semantic meaning of "" red "" is the dissimilar part ) .",Model Overview,Decomposition .,question-answering,3,24,0.2424242424242424,77,0.3031496062992126,24,0.6666666666666666,1,0
79,"Motivated by this phenomenon , our model further decomposes word s i ( or t j ) , based on its semantic matching vector ?",Model Overview,Decomposition .,question-answering,3,25,0.2525252525252525,78,0.3070866141732283,25,0.6944444444444444,1,0
80,"i ( ort j ) , into two components : similar component s + i ( or t + j ) and dissimilar component s ? i ( or t ? j ) .",Model Overview,Decomposition .,question-answering,3,26,0.2626262626262626,79,0.3110236220472441,26,0.7222222222222222,1,0
81,"Formally , we define the decomposition function as :",Model Overview,Decomposition .,question-answering,3,27,0.2727272727272727,80,0.3149606299212598,27,0.75,1,0
82,", our goal in this step is how to utilize those information .",Model Overview,Decomposition .,question-answering,3,28,0.2828282828282828,81,0.3188976377952756,28,0.7777777777777778,1,0
83,"Besides the suggestion from that the significance of the dissimilar parts alone between two sentences has a great effect of their similarity , we also think that the dissimilar and similar components have strong connections .",Model Overview,Decomposition .,question-answering,3,29,0.2929292929292929,82,0.3228346456692913,29,0.8055555555555556,1,0
84,"For example , in , if we only look at the dissimilar or similar part alone , it is hard to judge which one between E 4 and E 5 is more similar to E 3 .",Model Overview,Decomposition .,question-answering,3,30,0.303030303030303,83,0.326771653543307,30,0.8333333333333334,1,0
85,"We can easily identify that E 5 is more similar to E 3 , when we consider both the similar and dissimilar parts .",Model Overview,Decomposition .,question-answering,3,31,0.3131313131313131,84,0.3307086614173228,31,0.8611111111111112,1,0
86,"Therefore , our model composes the similar component matrix and dissimilar component matrix into a feature vector S ( or T ) with the composition function :",Model Overview,Decomposition .,question-answering,3,32,0.3232323232323232,85,0.3346456692913386,32,0.8888888888888888,1,0
87,Similarity assessing .,Model Overview,Decomposition .,question-answering,3,33,0.3333333333333333,86,0.3385826771653543,33,0.9166666666666666,1,0
88,"In the final stage , we concatenate the two feature vectors ( Sand T ) and predict the final similarity score :",Model Overview,Decomposition .,question-answering,3,34,0.3434343434343434,87,0.3425196850393701,34,0.9444444444444444,1,0
89,3 An End - to - End Implementation Section 2 gives us a glance of our model .,Model Overview,Decomposition .,question-answering,3,35,0.3535353535353535,88,0.3464566929133858,35,0.9722222222222222,1,0
90,"In this section , we describe details of each phase .",Model Overview,Decomposition .,question-answering,3,36,0.3636363636363636,89,0.3503937007874015,36,1.0,1,0
91,Semantic Matching Functions,Model Overview,,question-answering,3,37,0.3737373737373737,90,0.3543307086614173,0,0.0,1,0
92,This subsection describes our specifications for the semantic matching function f match in Eq.,Model Overview,Semantic Matching Functions,question-answering,3,38,0.3838383838383838,91,0.358267716535433,1,0.0666666666666666,1,0
93,( 1 ) .,Model Overview,Semantic Matching Functions,question-answering,3,39,0.3939393939393939,92,0.3622047244094488,2,0.1333333333333333,1,0
94,The goal off match is to generate a semantic matching vector ?,Model Overview,Semantic Matching Functions,question-answering,3,40,0.404040404040404,93,0.3661417322834646,3,0.2,1,0
95,i for s i by composing the vectors from T .,Model Overview,Semantic Matching Functions,question-answering,3,41,0.4141414141414141,94,0.3700787401574803,4,0.2666666666666666,1,0
96,"For a sentence pair Sand T , we first calculate a similarity matrix A mn , where each element a i , j ?",Model Overview,Semantic Matching Functions,question-answering,3,42,0.4242424242424242,95,0.374015748031496,5,0.3333333333333333,1,0
97,A mn computes the cosine similarity between words s i and t j as,Model Overview,Semantic Matching Functions,question-answering,3,43,0.4343434343434343,96,0.3779527559055118,6,0.4,1,0
98,"Then , we define three different semantic matching functions over A mn :",Model Overview,Semantic Matching Functions,question-answering,3,44,0.4444444444444444,97,0.3818897637795275,7,0.4666666666666667,1,0
99,"where k = argmax j a i , j .",Model Overview,Semantic Matching Functions,question-answering,3,45,0.4545454545454545,98,0.3858267716535433,8,0.5333333333333333,1,0
100,The idea of the global function is to consider all word vectors t j in T .,Model Overview,Semantic Matching Functions,question-answering,3,46,0.4646464646464646,99,0.389763779527559,9,0.6,1,0
101,A semantic matching vector ?,Model Overview,Semantic Matching Functions,question-answering,3,47,0.4747474747474747,100,0.3937007874015748,10,0.6666666666666666,1,0
102,"i is a weighted sum vector of all words t j in T , where each weight is the normalized word similarity a i , j .",Model Overview,Semantic Matching Functions,question-answering,3,48,0.4848484848484848,101,0.3976377952755905,11,0.7333333333333333,1,0
103,The max function moves to the other extreme .,Model Overview,Semantic Matching Functions,question-answering,3,49,0.494949494949495,102,0.4015748031496063,12,0.8,1,0
104,It generates the semantic matching vector by selecting the most similar word vector t k from T .,Model Overview,Semantic Matching Functions,question-answering,3,50,0.5050505050505051,103,0.405511811023622,13,0.8666666666666667,1,0
105,"The local -w function takes a compromise between global and max , where w indicates the size of the window to consider centered at k ( the most similar word position ) .",Model Overview,Semantic Matching Functions,question-answering,3,51,0.5151515151515151,104,0.4094488188976378,14,0.9333333333333332,1,0
106,So the semantic matching vector is a weighted average vector from t k?w tot k+w .,Model Overview,Semantic Matching Functions,question-answering,3,52,0.5252525252525253,105,0.4133858267716535,15,1.0,1,0
107,Decomposition Functions,Model Overview,,question-answering,3,53,0.5353535353535354,106,0.4173228346456692,0,0.0,1,0
108,This subsection describes the implementations for the decomposition function f decomp in Eq.,Model Overview,Decomposition Functions,question-answering,3,54,0.5454545454545454,107,0.421259842519685,1,0.0333333333333333,1,0
109,( 2 ) .,Model Overview,Decomposition Functions,question-answering,3,55,0.5555555555555556,108,0.4251968503937008,2,0.0666666666666666,1,0
110,The intention off decomp is to decompose a word vector s j based on its semantic matching vector ?,Model Overview,Decomposition Functions,question-answering,3,56,0.5656565656565656,109,0.4291338582677165,3,0.1,1,0
111,j into a similar component s + i and a dissimilar component s ?,Model Overview,Decomposition Functions,question-answering,3,57,0.5757575757575758,110,0.4330708661417323,4,0.1333333333333333,1,0
112,"i , where s +",Model Overview,Decomposition Functions,question-answering,3,58,0.5858585858585859,111,0.437007874015748,5,0.1666666666666666,1,0
113,i indicates the semantics of s i covered by ?,Model Overview,Decomposition Functions,question-answering,3,59,0.5959595959595959,112,0.4409448818897638,6,0.2,1,0
114,i and s ?,Model Overview,Decomposition Functions,question-answering,3,60,0.6060606060606061,113,0.4448818897637795,7,0.2333333333333333,1,0
115,i indicates the uncovered part .,Model Overview,Decomposition Functions,question-answering,3,61,0.6161616161616161,114,0.4488188976377952,8,0.2666666666666666,1,0
116,"We implement three types of decomposition function : rigid , linear and orthogonal .",Model Overview,Decomposition Functions,question-answering,3,62,0.6262626262626263,115,0.452755905511811,9,0.3,1,0
117,The rigid decomposition only adapts to the max version off match .,Model Overview,Decomposition Functions,question-answering,3,63,0.6363636363636364,116,0.4566929133858268,10,0.3333333333333333,1,0
118,"First , it detects whether there is an exactly matched word in the other sentence , or s i equal to ?",Model Overview,Decomposition Functions,question-answering,3,64,0.6464646464646465,117,0.4606299212598425,11,0.3666666666666666,1,0
119,i .,Model Overview,Decomposition Functions,question-answering,3,65,0.6565656565656566,118,0.4645669291338583,12,0.4,1,0
120,"If yes , the vector s i is dispatched to the similar component s + i , and the dissimilar component is assigned with a zero vector",Model Overview,Decomposition Functions,question-answering,3,66,0.6666666666666666,119,0.468503937007874,13,0.4333333333333333,1,0
121,0 .,Model Overview,Decomposition Functions,question-answering,3,67,0.6767676767676768,120,0.4724409448818897,14,0.4666666666666667,1,0
122,"Otherwise , the vector s i is assigned to the dissimilar component s ?",Model Overview,Decomposition Functions,question-answering,3,68,0.6868686868686869,121,0.4763779527559055,15,0.5,1,0
123,i .,Model Overview,Decomposition Functions,question-answering,3,69,0.696969696969697,122,0.4803149606299212,16,0.5333333333333333,1,0
124,Eq. gives the formal definition :,Model Overview,Decomposition Functions,question-answering,3,70,0.7070707070707071,123,0.484251968503937,17,0.5666666666666667,1,0
125,The motivation for the linear decomposition is that the more similar between s i and ?,Model Overview,Decomposition Functions,question-answering,3,71,0.7171717171717171,124,0.4881889763779528,18,0.6,1,0
126,"i , the higher proportion of s i should be assigned to the similar component .",Model Overview,Decomposition Functions,question-answering,3,72,0.7272727272727273,125,0.4921259842519685,19,0.6333333333333333,1,0
127,"First , we calculate the cosine similarity ?",Model Overview,Decomposition Functions,question-answering,3,73,0.7373737373737373,126,0.4960629921259842,20,0.6666666666666666,1,0
128,between s i and ?,Model Overview,Decomposition Functions,question-answering,3,74,0.7474747474747475,127,0.5,21,0.7,1,0
129,i .,Model Overview,Decomposition Functions,question-answering,3,75,0.7575757575757576,128,0.5039370078740157,22,0.7333333333333333,1,0
130,"Then , we decompose s i linearly based on ?.",Model Overview,Decomposition Functions,question-answering,3,76,0.7676767676767676,129,0.5078740157480315,23,0.7666666666666667,1,0
131,Eq. gives the corresponding definition :,Model Overview,Decomposition Functions,question-answering,3,77,0.7777777777777778,130,0.5118110236220472,24,0.8,1,0
132,The orthogonal decomposition is to decompose a vector in the geometric space .,Model Overview,Decomposition Functions,question-answering,3,78,0.7878787878787878,131,0.515748031496063,25,0.8333333333333334,1,0
133,Based on the semantic matching vector ?,Model Overview,Decomposition Functions,question-answering,3,79,0.797979797979798,132,0.5196850393700787,26,0.8666666666666667,1,0
134,"i , our model decomposes s i into a parallel component and a perpendicular component .",Model Overview,Decomposition Functions,question-answering,3,80,0.8080808080808081,133,0.5236220472440944,27,0.9,1,0
135,"Then , the parallel component is viewed as the similar component s + i , and perpendicular component is taken as the dissimilar component s ?",Model Overview,Decomposition Functions,question-answering,3,81,0.8181818181818182,134,0.5275590551181102,28,0.9333333333333332,1,0
136,i .,Model Overview,Decomposition Functions,question-answering,3,82,0.8282828282828283,135,0.531496062992126,29,0.9666666666666668,1,0
137,Eq. gives the concrete definitions .,Model Overview,Decomposition Functions,question-answering,3,83,0.8383838383838383,136,0.5354330708661418,30,1.0,1,0
138,Composition Functions,Model Overview,,question-answering,3,84,0.8484848484848485,137,0.5393700787401575,0,0.0,1,0
139,The aim of composition function f comp in Eq. is to extract features from both the similar component matrix and the dissimilar component matrix .,Model Overview,Composition Functions,question-answering,3,85,0.8585858585858586,138,0.5433070866141733,1,0.0833333333333333,1,0
140,We also want to acquire similarities and dissimilarities of various granularity during the composition phase .,Model Overview,Composition Functions,question-answering,3,86,0.8686868686868687,139,0.547244094488189,2,0.1666666666666666,1,0
141,"Inspired from Kim , we utilize a two - channel convolutional neural networks ( CNN ) and design filters based on various order of n-grams , e.g. , unigram , bigram and trigram .",Model Overview,Composition Functions,question-answering,3,87,0.8787878787878788,140,0.5511811023622047,3,0.25,1,0
142,The CNN model involves two sequential operations : convolution and max - pooling .,Model Overview,Composition Functions,question-answering,3,88,0.8888888888888888,141,0.5551181102362205,4,0.3333333333333333,1,0
143,"For the convolution operation , we define a list of filters {w o }.",Model Overview,Composition Functions,question-answering,3,89,0.898989898989899,142,0.5590551181102362,5,0.4166666666666667,1,0
144,"The shape of each filter is d h , where dis the dimension of word vectors and h is the window size .",Model Overview,Composition Functions,question-answering,3,90,0.9090909090909092,143,0.562992125984252,6,0.5,1,0
145,"Each filter is applied to two patches ( a window size h of vectors ) from both similar and dissimilar channels , and generates a feature .",Model Overview,Composition Functions,question-answering,3,91,0.9191919191919192,144,0.5669291338582677,7,0.5833333333333334,1,0
146,Eq. ( 10 ) expresses this process .,Model Overview,Composition Functions,question-answering,3,92,0.9292929292929292,145,0.5708661417322834,8,0.6666666666666666,1,0
147,"To deal with variable feature size , we perform a max - pooling operation over co by selecting the maximum value co = max co .",Model Overview,Composition Functions,question-answering,3,93,0.9393939393939394,146,0.5748031496062992,9,0.75,1,0
148,"Therefore , after these two operations , each filter generates only one feature .",Model Overview,Composition Functions,question-answering,3,94,0.9494949494949496,147,0.5787401574803149,10,0.8333333333333334,1,0
149,We define several filters by varying the window size and the initial values .,Model Overview,Composition Functions,question-answering,3,95,0.9595959595959596,148,0.5826771653543307,11,0.9166666666666666,1,0
150,"Eventually , a vector of features is captured by composing the two component matrixes , and the feature dimension is equal to the number of filters .",Model Overview,Composition Functions,question-answering,3,96,0.9696969696969696,149,0.5866141732283464,12,1.0,1,0
151,Similarity Assessment Function,Model Overview,,question-answering,3,97,0.9797979797979798,150,0.5905511811023622,0,0.0,1,0
152,The similarity assessment function f sim in Eq. ( 4 ) predicts a similarity score by taking two feature vectors as input .,Model Overview,Similarity Assessment Function,question-answering,3,98,0.98989898989899,151,0.594488188976378,1,0.5,1,0
153,"We employ a linear function to sum up all the features and apply a sigmoid function to constrain the similarity within the range [ 0 , 1 ] .",Model Overview,Similarity Assessment Function,question-answering,3,99,1.0,152,0.5984251968503937,2,1.0,1,0
154,Training,,,question-answering,3,0,0.0,153,0.6023622047244095,0,0.0,1,0
155,We train our sentence similariy model by maximizing the likelihood on a training set .,Training,Training,question-answering,3,1,0.2,154,0.6062992125984252,1,0.2,1,0
156,"Each training instance in the training set is represented as a triple ( S i , Ti , Li ) , where Si and Ti area pair of sentences , and Li ? { 0 , 1 } indicates the similarity between them .",Training,Training,question-answering,3,2,0.4,155,0.610236220472441,2,0.4,1,0
157,"We assign Li = 1 if Ti is a paraphrase of Si for the paraphrase identification task , or Ti is a correct answer for Si for the answer sentence selection task .",Training,Training,question-answering,3,3,0.6,156,0.6141732283464567,3,0.6,1,0
158,"Otherwise , we assign Li = 0 .",Training,Training,question-answering,3,4,0.8,157,0.6181102362204725,4,0.8,1,0
159,We implement the mathematical expressions with Theano and use Adam for optimization .,Training,Training,question-answering,3,5,1.0,158,0.6220472440944882,5,1.0,1,0
160,Experiment,,,question-answering,3,0,0.0,159,0.6259842519685039,0,0.0,1,0
161,Experimental Setting,,,question-answering,3,0,0.0,160,0.6299212598425197,0,0.0,1,0
162,We evaluate our model on two tasks : answer sentence selection and paraphrase identification .,Experimental Setting,Experimental Setting,question-answering,3,1,0.1111111111111111,161,0.6338582677165354,1,0.1111111111111111,1,0
163,"The answer sentence selection task is to rank a list of candidate answers based on their similarities to a question sentence , and the performance is measured by mean average precision ( MAP ) and mean reciprocal rank ( MRR ) .",Experimental Setting,Experimental Setting,question-answering,3,2,0.2222222222222222,162,0.6377952755905512,2,0.2222222222222222,1,0
164,We experiment on two datasets : QASent and Wiki QA .,Experimental Setting,Experimental Setting,question-answering,3,3,0.3333333333333333,163,0.6417322834645669,3,0.3333333333333333,1,0
165,"The statistics of the two datasets can be found in , where QASent was created from the TREC QA track , and WikiQA ( Yang et al. , 2015 ) is constructed from real queries of Bing and Wikipedia .",Experimental Setting,Experimental Setting,question-answering,3,4,0.4444444444444444,164,0.6456692913385826,4,0.4444444444444444,1,0
166,The paraphrase identification task is to detect whether two sentences are paraphrases based on the similarity between them .,Experimental Setting,Experimental Setting,question-answering,3,5,0.5555555555555556,165,0.6496062992125984,5,0.5555555555555556,1,0
167,The metrics include the accuracy and the positive class F 1 score .,Experimental Setting,Experimental Setting,question-answering,3,6,0.6666666666666666,166,0.6535433070866141,6,0.6666666666666666,1,0
168,"We experiment on the Microsoft Research Paraphrase corpus ( MSRP ) , which includes 2753 true and 1323 false instances in the training set , and 1147 true and 578 false instances in the test set .",Experimental Setting,Experimental Setting,question-answering,3,7,0.7777777777777778,167,0.65748031496063,7,0.7777777777777778,1,0
169,We build a development set by randomly selecting 100 true and 100 false instances from the training set .,Experimental Setting,Experimental Setting,question-answering,3,8,0.8888888888888888,168,0.6614173228346457,8,0.8888888888888888,1,0
170,"In all experiments , we set the size of word vector dimension as d = 300 , and pre-train the vectors with the word2 vec toolkit on the English Gigaword ( LDC2011T07 ) .",Experimental Setting,Experimental Setting,question-answering,3,9,1.0,169,0.6653543307086615,9,1.0,1,0
171,Model Properties,,,question-answering,3,0,0.0,170,0.6692913385826772,0,0.0,1,0
172,"There are several alternative options in our model , e.g. , the semantic matching functions , the decomposition operations , and the filter types .",Model Properties,Model Properties,question-answering,3,1,0.0144927536231884,171,0.6732283464566929,1,0.0144927536231884,1,0
173,The choice of these options may affect the final performance .,Model Properties,Model Properties,question-answering,3,2,0.0289855072463768,172,0.6771653543307087,2,0.0289855072463768,1,0
174,"In this subsection , we present some experiments to demonstrate the properties of our model , and find a good configuration that we use to evaluate our final model .",Model Properties,Model Properties,question-answering,3,3,0.0434782608695652,173,0.6811023622047244,3,0.0434782608695652,1,0
175,All the experiments in this subsection were performed on the QASent dataset and evaluated on the development set .,Model Properties,Model Properties,question-answering,3,4,0.0579710144927536,174,0.6850393700787402,4,0.0579710144927536,1,0
176,"First , we evaluated the effectiveness of various semantic matching functions .",Model Properties,Model Properties,question-answering,3,5,0.072463768115942,175,0.6889763779527559,5,0.072463768115942,1,0
177,"We switched the semantic matching functions among { max , global , local - l} , where l ? { 1 , 2 , 3 , 4 } , and fixed the other options as : the linear decomposition , the filter types including {unigram , bigram , trigram } , and 500 filters for each type .",Model Properties,Model Properties,question-answering,3,6,0.0869565217391304,176,0.6929133858267716,6,0.0869565217391304,1,1
178,presents the results .,Model Properties,Model Properties,question-answering,3,7,0.1014492753623188,177,0.6968503937007874,7,0.1014492753623188,1,0
179,We found that the max function worked better than the global function on both MAP and MRR .,Model Properties,Model Properties,question-answering,3,8,0.1159420289855072,178,0.7007874015748031,8,0.1159420289855072,1,1
180,"By increasing the window size , the local -l function acquired progressive improvements when the window size is smaller than 4 .",Model Properties,Model Properties,question-answering,3,9,0.1304347826086956,179,0.7047244094488189,9,0.1304347826086956,1,0
181,"But after we enlarged the window size to 4 , the performance dropped .",Model Properties,Model Properties,question-answering,3,10,0.144927536231884,180,0.7086614173228346,10,0.144927536231884,1,0
182,"The local - 3 function worked better than the max function in term of the MAP , and also got a comparable MRR .",Model Properties,Model Properties,question-answering,3,11,0.1594202898550724,181,0.7125984251968503,11,0.1594202898550724,1,0
183,"Therefore , we use the local - 3 function in the following experiments .",Model Properties,Model Properties,question-answering,3,12,0.1739130434782608,182,0.7165354330708661,12,0.1739130434782608,1,0
184,"Second , we studied the effect of various decomposition operations .",Model Properties,Model Properties,question-answering,3,13,0.1884057971014492,183,0.7204724409448819,13,0.1884057971014492,1,0
185,"We varied the decomposition operation among { rigid , linear , orthogonal } , and kept the other options unchanged .",Model Properties,Model Properties,question-answering,3,14,0.2028985507246377,184,0.7244094488188977,14,0.2028985507246377,1,1
186,shows the performance .,Model Properties,Model Properties,question-answering,3,15,0.217391304347826,185,0.7283464566929134,15,0.217391304347826,1,0
187,We found that the rigid operation got the worst result .,Model Properties,Model Properties,question-answering,3,16,0.2318840579710145,186,0.7322834645669292,16,0.2318840579710145,1,0
188,"This is reasonable , because the rigid operation decomposes word vectors by exactly matching words .",Model Properties,Model Properties,question-answering,3,17,0.2463768115942029,187,0.7362204724409449,17,0.2463768115942029,1,0
189,"The orthogonal operation got a similar MAP as the linear operation , and it worked better in term of MRR .",Model Properties,Model Properties,question-answering,3,18,0.2608695652173913,188,0.7401574803149606,18,0.2608695652173913,1,0
190,"Therefore , we choose the orthogonal operation in the following experiments .",Model Properties,Model Properties,question-answering,3,19,0.2753623188405797,189,0.7440944881889764,19,0.2753623188405797,1,0
191,"Third , we tested the influence of various filter types .",Model Properties,Model Properties,question-answering,3,20,0.2898550724637681,190,0.7480314960629921,20,0.2898550724637681,1,1
192,"We constructed 5 groups of filters : win - 1 contains only the unigram filters , win - 2 contains both unigram and bigram filters , win - 3 contains all the filters in win - 2 plus trigram filters , win - 4 extends filters in win - 3 with 4 - gram filters , and win - 5 adds 5 - gram filters into win - 4 .",Model Properties,Model Properties,question-answering,3,21,0.3043478260869565,191,0.7519685039370079,21,0.3043478260869565,1,1
193,We generate 500 filters for each filter type ( with different initial values ) .,Model Properties,Model Properties,question-answering,3,22,0.3188405797101449,192,0.7559055118110236,22,0.3188405797101449,1,0
194,Experimental results are shown in .,Model Properties,Model Properties,question-answering,3,23,0.3333333333333333,193,0.7598425196850394,23,0.3333333333333333,1,0
195,"At the beginning , adding higher - order ngram filters was helpful for the performance .",Model Properties,Model Properties,question-answering,3,24,0.3478260869565217,194,0.7637795275590551,24,0.3478260869565217,1,0
196,"The performance reached to the peak , when we used the win - 3 filters .",Model Properties,Model Properties,question-answering,3,25,0.3623188405797101,195,0.7677165354330708,25,0.3623188405797101,1,0
197,"After that , adding more complex filters decreased the performance .",Model Properties,Model Properties,question-answering,3,26,0.3768115942028985,196,0.7716535433070866,26,0.3768115942028985,1,0
198,"Therefore , the trigram is the best granularity for our model .",Model Properties,Model Properties,question-answering,3,27,0.391304347826087,197,0.7755905511811023,27,0.391304347826087,1,0
199,"In the following experiments , we utilize filter types in win - 3 .",Model Properties,Model Properties,question-answering,3,28,0.4057971014492754,198,0.7795275590551181,28,0.4057971014492754,1,0
200,Comparing with State - of - the - art Models,Model Properties,Model Properties,question-answering,3,29,0.4202898550724637,199,0.7834645669291339,29,0.4202898550724637,1,0
201,"In this subsection , we evaluated our model on the test sets of QASent , WikiQA and MSRP .",Model Properties,Model Properties,question-answering,3,30,0.4347826086956521,200,0.7874015748031497,30,0.4347826086956521,1,0
202,QASent dataset .,Model Properties,Model Properties,question-answering,3,31,0.4492753623188406,201,0.7913385826771654,31,0.4492753623188406,1,0
203,"presents the performances of the state - of - the - art systems and our model , where the performances were evaluated with the standard trec eval - 8.1 script",Model Properties,Model Properties,question-answering,3,32,0.463768115942029,202,0.7952755905511811,32,0.463768115942029,1,0
204,1 .,Model Properties,Model Properties,question-answering,3,33,0.4782608695652174,203,0.7992125984251969,33,0.4782608695652174,1,0
205,"Given a pair of sentences , Severyn and Moschitti ( 2015 ) employed a CNN model to compose each sentence into a vector separately , and joined the two sentence vectors to compute the sentence similarity .",Model Properties,Model Properties,question-answering,3,34,0.4927536231884058,204,0.8031496062992126,34,0.4927536231884058,1,0
206,"Because only the sentencelevel granularity was used , the performance is much lower ( the second row of ) .",Model Properties,Model Properties,question-answering,3,35,0.5072463768115942,205,0.8070866141732284,35,0.5072463768115942,1,0
207,"After adding some word overlap features between the two sentences , the performance was improved significantly ( the third row of ) .",Model Properties,Model Properties,question-answering,3,36,0.5217391304347826,206,0.8110236220472441,36,0.5217391304347826,1,0
208,"Therefore , the lower - level granularity is an indispensable factor fora good performance .",Model Properties,Model Properties,question-answering,3,37,0.5362318840579711,207,0.8149606299212598,37,0.5362318840579711,1,0
209,"conducted word alignment fora sentence pair based on word vectors , and measured the sentence similarity based on a couple of word alignment features .",Model Properties,Model Properties,question-answering,3,38,0.5507246376811594,208,0.8188976377952756,38,0.5507246376811594,1,0
210,"They got a slightly better performance ( the fourth row of ) , which indicates that the vector representation for words is helpful to bridging the lexical gap problem .",Model Properties,Model Properties,question-answering,3,39,0.5652173913043478,209,0.8228346456692913,39,0.5652173913043478,1,0
211,"dos introduced the attention mechanism into the CNN model , and learnt sentence representation by considering the influence of the other sentence .",Model Properties,Model Properties,question-answering,3,40,0.5797101449275363,210,0.8267716535433071,40,0.5797101449275363,1,0
212,They got better performance than all the other previous work .,Model Properties,Model Properties,question-answering,3,41,0.5942028985507246,211,0.8307086614173228,41,0.5942028985507246,1,0
213,Our model makes use of all these useful factors and also considers the dissimilarities of a sentence pair .,Model Properties,Model Properties,question-answering,3,42,0.6086956521739131,212,0.8346456692913385,42,0.6086956521739131,1,0
214,"We can see that our model ( the last row of ) got the best MAP among all previous work , and a comparable MRR than dos .",Model Properties,Model Properties,question-answering,3,43,0.6231884057971014,213,0.8385826771653543,43,0.6231884057971014,1,1
215,Wiki QA dataset .,Model Properties,Model Properties,question-answering,3,44,0.6376811594202898,214,0.84251968503937,44,0.6376811594202898,1,0
216,presents the results of our model and several state - of - the - art models .,Model Properties,Model Properties,question-answering,3,45,0.6521739130434783,215,0.8464566929133859,45,0.6521739130434783,1,0
217,constructed the dataset and reimplemented several baseline models .,Model Properties,Model Properties,question-answering,3,46,0.6666666666666666,216,0.8503937007874016,46,0.6666666666666666,1,0
218,The best performance ( shown at the second row of ) was acquired by a bigram CNN model combining with the word overlap features .,Model Properties,Model Properties,question-answering,3,47,0.6811594202898551,217,0.8543307086614174,47,0.6811594202898551,1,0
219,models the sentence similarity by enriching LSTMs with a latent stochastic attention mechanism .,Model Properties,Model Properties,question-answering,3,48,0.6956521739130435,218,0.8582677165354331,48,0.6956521739130435,1,0
220,The corresponding performance is given at the fourth row of .,Model Properties,Model Properties,question-answering,3,49,0.7101449275362319,219,0.8622047244094488,49,0.7101449275362319,1,0
221,"introduced the attention mechanism into the CNN model , and captured the best performance ( the fifth row of ) .",Model Properties,Model Properties,question-answering,3,50,0.7246376811594203,220,0.8661417322834646,50,0.7246376811594203,1,0
222,The semantic matching phase in our model is similar to the attention mechanism .,Model Properties,Model Properties,question-answering,3,51,0.7391304347826086,221,0.8700787401574803,51,0.7391304347826086,1,0
223,"But different from the previous models , our model utilizes both the similarity and dissimilarity simultaneously .",Model Properties,Model Properties,question-answering,3,52,0.7536231884057971,222,0.8740157480314961,52,0.7536231884057971,1,0
224,The last row of shows that our model is more effective than the other models .,Model Properties,Model Properties,question-answering,3,53,0.7681159420289855,223,0.8779527559055118,53,0.7681159420289855,1,1
225,MSRP dataset .,Model Properties,Model Properties,question-answering,3,54,0.782608695652174,224,0.8818897637795275,54,0.782608695652174,1,0
226,granularity and modeled interaction features at each level fora pair of sentences .,Model Properties,Model Properties,question-answering,3,55,0.7971014492753623,225,0.8858267716535433,55,0.7971014492753623,1,0
227,They obtained their best performance by pretraining the model on a language modeling task ( the 3rd row of ) .,Model Properties,Model Properties,question-answering,3,56,0.8115942028985508,226,0.889763779527559,56,0.8115942028985508,1,0
228,"However , their model heavily depends on the pretraining strategy .",Model Properties,Model Properties,question-answering,3,57,0.8260869565217391,227,0.8937007874015748,57,0.8260869565217391,1,0
229,"Without pretraining , they got a much worse performance ( the second row of ) .",Model Properties,Model Properties,question-answering,3,58,0.8405797101449275,228,0.8976377952755905,58,0.8405797101449275,1,0
230,proposed a similar model to .,Model Properties,Model Properties,question-answering,3,59,0.855072463768116,229,0.9015748031496064,59,0.855072463768116,1,0
231,"Similarly , they also used a CNN model to extract features at multiple levels of granularity .",Model Properties,Model Properties,question-answering,3,60,0.8695652173913043,230,0.905511811023622,60,0.8695652173913043,1,0
232,"Differently , they utilized some extra annotated resources , e.g. , embeddings from part - of - speech ( POS ) tags and PARAGRAM vectors trained from the Paraphrase Database .",Model Properties,Model Properties,question-answering,3,61,0.8840579710144928,231,0.9094488188976378,61,0.8840579710144928,1,0
233,Their model outperformed without the need of pretraining ( the sixth row of ) .,Model Properties,Model Properties,question-answering,3,62,0.8985507246376812,232,0.9133858267716536,62,0.8985507246376812,1,0
234,"However , the performance was reduced after removing the extra resources ( the fourth and fifth rows of ) .",Model Properties,Model Properties,question-answering,3,63,0.9130434782608696,233,0.9173228346456692,63,0.9130434782608696,1,0
235,applied their attention - based CNN model on this dataset .,Model Properties,Model Properties,question-answering,3,64,0.927536231884058,234,0.9212598425196852,64,0.927536231884058,1,0
236,"By adding a couple of sparse features and using a layerwise training strategy , they got a pretty good performance .",Model Properties,Model Properties,question-answering,3,65,0.9420289855072465,235,0.9251968503937008,65,0.9420289855072465,1,0
237,"Comparing to these neural network based models , our model obtained a comparable performance ( the last row of ) without using any sparse features , extra annotated resources and specific training strategies .",Model Properties,Model Properties,question-answering,3,66,0.9565217391304348,236,0.9291338582677166,66,0.9565217391304348,1,1
238,"However , the best performance so far on this dataset is obtained by .",Model Properties,Model Properties,question-answering,3,67,0.9710144927536232,237,0.9330708661417324,67,0.9710144927536232,1,0
239,"In their model , they just utilized several hand - crafted features in a Support Vector Machine ( SVM ) model .",Model Properties,Model Properties,question-answering,3,68,0.9855072463768116,238,0.937007874015748,68,0.9855072463768116,1,0
240,"Therefore , the deep learning methods still have along way to go for this task .",Model Properties,Model Properties,question-answering,3,69,1.0,239,0.9409448818897638,69,1.0,1,0
241,Related Work,,,question-answering,3,0,0.0,240,0.9448818897637796,0,0.0,1,0
242,The semantic matching functions in subsection 3.1 are inspired from the attention - based neural machine translation .,Related Work,Related Work,question-answering,3,1,0.1428571428571428,241,0.9488188976377953,1,0.1428571428571428,0,0
243,"However , most of the previous work using the attention mechanism in only LSTM models .",Related Work,Related Work,question-answering,3,2,0.2857142857142857,242,0.952755905511811,2,0.2857142857142857,0,0
244,Whereas our model introduces the attention mechanism into the CNN model .,Related Work,Related Work,question-answering,3,3,0.4285714285714285,243,0.9566929133858268,3,0.4285714285714285,0,0
245,A similar work is the attention - based CNN model proposed by .,Related Work,Related Work,question-answering,3,4,0.5714285714285714,244,0.9606299212598424,4,0.5714285714285714,0,0
246,"They first build an attention matrix fora sentence pair , and then directly take the attention matrix as anew channel of the CNN model .",Related Work,Related Work,question-answering,3,5,0.7142857142857143,245,0.9645669291338582,5,0.7142857142857143,0,0
247,"Differently , our model uses the attention matrix ( or similarity matrix ) to decompose the original sentence matrix into a similar component matrix and a dissimilar component matrix , and then feeds these two matrixes into a two - channel CNN model .",Related Work,Related Work,question-answering,3,6,0.8571428571428571,246,0.968503937007874,6,0.8571428571428571,0,0
248,The model can then focus much on the interactions between similar and dissimilar parts of a sentence pair .,Related Work,Related Work,question-answering,3,7,1.0,247,0.9724409448818898,7,1.0,0,0
249,Conclusion,,,question-answering,3,0,0.0,248,0.9763779527559056,0,0.0,1,0
250,"In this work , we proposed a model to assess sentence similarity by decomposing and composing lexical semantics .",Conclusion,Conclusion,question-answering,3,1,0.2,249,0.9803149606299212,1,0.2,0,0
251,"To bridge the lexical gap problem , our model represents each word with its context vector .",Conclusion,Conclusion,question-answering,3,2,0.4,250,0.984251968503937,2,0.4,0,0
252,"To extract features from both the similarity and dissimilarity of a sentence pair , we designed several methods to decompose the word vector into a similar component and a dissimilar component .",Conclusion,Conclusion,question-answering,3,3,0.6,251,0.9881889763779528,3,0.6,0,0
253,"To extract features at multiple levels of granularity , we employed a two - channel CNN model and equipped it with multiple types of ngram filters .",Conclusion,Conclusion,question-answering,3,4,0.8,252,0.9921259842519684,4,0.8,0,0
254,Experimental results show that our model is quite effective on both the answer sentence selection task and the paraphrase identification task .,Conclusion,Conclusion,question-answering,3,5,1.0,253,0.9960629921259844,5,1.0,0,0
1,title,,,question-answering,4,0,0.0,0,0.0,0,0.0,1,0
2,A Parallel - Hierarchical Model for Machine Comprehension on Sparse Data,title,title,question-answering,4,1,0.0,1,0.0034364261168384,1,0.0,1,1
3,abstract,,,question-answering,4,0,0.0,2,0.0068728522336769,0,0.0,1,0
4,Understanding unstructured text is a major goal within natural language processing .,abstract,abstract,question-answering,4,1,0.125,3,0.0103092783505154,1,0.125,1,1
5,Comprehension tests pose questions based on short text passages to evaluate such understanding .,abstract,abstract,question-answering,4,2,0.25,4,0.0137457044673539,2,0.25,1,0
6,"In this work , we investigate machine comprehension on the challenging MCTest benchmark .",abstract,abstract,question-answering,4,3,0.375,5,0.0171821305841924,3,0.375,1,1
7,"Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features .",abstract,abstract,question-answering,4,4,0.5,6,0.0206185567010309,4,0.5,1,0
8,"We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy .",abstract,abstract,question-answering,4,5,0.625,7,0.0240549828178694,5,0.625,1,0
9,"The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set .",abstract,abstract,question-answering,4,6,0.75,8,0.0274914089347079,6,0.75,1,0
10,Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text .,abstract,abstract,question-answering,4,7,0.875,9,0.0309278350515463,7,0.875,1,0
11,"When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets anew state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",abstract,abstract,question-answering,4,8,1.0,10,0.0343642611683848,8,1.0,1,0
12,Introduction,,,question-answering,4,0,0.0,11,0.0378006872852233,0,0.0,1,0
13,"Humans learn in a variety of ways - by communication with each other , and by study , the reading of text .",Introduction,Introduction,question-answering,4,1,0.0217391304347826,12,0.0412371134020618,1,0.0277777777777777,1,0
14,"Comprehension of unstructured text by machines , at a near- human level , is a major goal for natural language processing .",Introduction,Introduction,question-answering,4,2,0.0434782608695652,13,0.0446735395189003,2,0.0555555555555555,1,1
15,It has garnered significant attention from the machine learning research community in recent years .,Introduction,Introduction,question-answering,4,3,0.0652173913043478,14,0.0481099656357388,3,0.0833333333333333,1,0
16,Machine comprehension ( MC ) is evaluated by posing a set of questions based on a text passage ( akin to the reading tests we all took in school ) .,Introduction,Introduction,question-answering,4,4,0.0869565217391304,15,0.0515463917525773,4,0.1111111111111111,1,1
17,"Such tests are objectively gradable and can be used to assess a range of abilities , from basic understanding to causal reasoning to inference .",Introduction,Introduction,question-answering,4,5,0.108695652173913,16,0.0549828178694158,5,0.1388888888888889,1,0
18,"Given a text passage and a question about its content , a system is tested on its ability to determine the correct answer .",Introduction,Introduction,question-answering,4,6,0.1304347826086956,17,0.0584192439862542,6,0.1666666666666666,1,0
19,"In this work , we focus on MCTest , a complex but data - limited comprehension benchmark , whose multiple - choice questions require not only extraction but also inference and limited reasoning .",Introduction,Introduction,question-answering,4,7,0.1521739130434782,18,0.0618556701030927,7,0.1944444444444444,1,0
20,"Inference and reasoning are important human skills that apply broadly , beyond language .",Introduction,Introduction,question-answering,4,8,0.1739130434782608,19,0.0652920962199312,8,0.2222222222222222,1,0
21,We present a parallel - hierarchical approach to machine comprehension designed to work well in a data - limited setting .,Introduction,Introduction,question-answering,4,9,0.1956521739130435,20,0.0687285223367697,9,0.25,1,0
22,"There are many use-cases in which comprehension over limited data would be handy : for example , user manuals , internal documentation , legal contracts , and soon .",Introduction,Introduction,question-answering,4,10,0.217391304347826,21,0.0721649484536082,10,0.2777777777777778,1,0
23,"Moreover , work towards more efficient learning from any quantity of data is important in its own right , for bringing machines more inline with the way humans learn .",Introduction,Introduction,question-answering,4,11,0.2391304347826087,22,0.0756013745704467,11,0.3055555555555556,1,0
24,"Typically , artificial neural networks require numerous parameters to capture complex patterns , and the more parameters , the more training data is required to tune them .",Introduction,Introduction,question-answering,4,12,0.2608695652173913,23,0.0790378006872852,12,0.3333333333333333,1,0
25,"Likewise , deep models learn to extract their own features , but this is a data - intensive process .",Introduction,Introduction,question-answering,4,13,0.2826086956521739,24,0.0824742268041237,13,0.3611111111111111,1,0
26,Our model learns to comprehend at a high level even when data is sparse .,Introduction,Introduction,question-answering,4,14,0.3043478260869565,25,0.0859106529209622,14,0.3888888888888889,1,0
27,The key to our model is that it compares the question and answer candidates to the text using several distinct perspectives .,Introduction,Introduction,question-answering,4,15,0.3260869565217391,26,0.0893470790378006,15,0.4166666666666667,1,1
28,We refer to a question combined with one of its answer candidates as a hypothesis ( to be detailed below ) .,Introduction,Introduction,question-answering,4,16,0.3478260869565217,27,0.0927835051546391,16,0.4444444444444444,1,0
29,"The semantic perspective compares the hypothesis to sentences in the text viewed as single , self - contained thoughts ; these are represented using a sum and transformation of word embedding vectors , similarly to in .",Introduction,Introduction,question-answering,4,17,0.3695652173913043,28,0.0962199312714776,17,0.4722222222222222,1,1
30,"The word - by - word perspective focuses on similarity matches between individual words from hypothesis and text , at various scales .",Introduction,Introduction,question-answering,4,18,0.391304347826087,29,0.0996563573883161,18,0.5,1,1
31,"As in the semantic perspective , we consider matches over complete sentences .",Introduction,Introduction,question-answering,4,19,0.4130434782608695,30,0.1030927835051546,19,0.5277777777777778,1,0
32,"We also use a sliding window acting on a subsentential scale ( inspired by the work of ) , which implicitly considers the linear distance between matched words .",Introduction,Introduction,question-answering,4,20,0.4347826086956521,31,0.1065292096219931,20,0.5555555555555556,1,1
33,"Finally , this word - level sliding window operates on two different views of text sentences : the sequential view , where words appear in their natural order , and the dependency view , where words are reordered based on a linearization of the sentence 's dependency graph .",Introduction,Introduction,question-answering,4,21,0.4565217391304347,32,0.1099656357388316,21,0.5833333333333334,1,1
34,Words are represented throughout by embedding vectors .,Introduction,Introduction,question-answering,4,22,0.4782608695652174,33,0.1134020618556701,22,0.6111111111111112,1,0
35,These distinct perspectives naturally form a hierarchy that we depict in .,Introduction,Introduction,question-answering,4,23,0.5,34,0.1168384879725085,23,0.6388888888888888,1,0
36,"Language is hierarchical , so it makes sense that comprehension relies on hierarchical levels of understanding .",Introduction,Introduction,question-answering,4,24,0.5217391304347826,35,0.120274914089347,24,0.6666666666666666,1,0
37,The perspectives of our model can be considered a type of feature .,Introduction,Introduction,question-answering,4,25,0.5434782608695652,36,0.1237113402061855,25,0.6944444444444444,1,0
38,"However , they are implemented by parametric differentiable functions .",Introduction,Introduction,question-answering,4,26,0.5652173913043478,37,0.127147766323024,26,0.7222222222222222,1,0
39,"This is in contrast to most previous efforts on MCTest , whose numerous hand - engineered features can not be trained .",Introduction,Introduction,question-answering,4,27,0.5869565217391305,38,0.1305841924398625,27,0.75,1,0
40,"Our model , significantly , can be trained end - to - end with backpropagation .",Introduction,Introduction,question-answering,4,28,0.6086956521739131,39,0.134020618556701,28,0.7777777777777778,1,0
41,"To facilitate learning with limited data , we also develop a unique training scheme .",Introduction,Introduction,question-answering,4,29,0.6304347826086957,40,0.1374570446735395,29,0.8055555555555556,1,0
42,We initialize the model 's neural networks to perform specific heuristic functions that yield decent ( thought not impressive ) performance on the dataset .,Introduction,Introduction,question-answering,4,30,0.6521739130434783,41,0.140893470790378,30,0.8333333333333334,1,0
43,"Thus , the training scheme gives the model a safe , reasonable baseline from which to start learning .",Introduction,Introduction,question-answering,4,31,0.6739130434782609,42,0.1443298969072164,31,0.8611111111111112,1,0
44,We call this technique training wheels .,Introduction,Introduction,question-answering,4,32,0.6956521739130435,43,0.1477663230240549,32,0.8888888888888888,1,0
45,Computational models that comprehend ( insofar as they perform well on MC datasets ) have developed contemporaneously in several research groups .,Introduction,Introduction,question-answering,4,33,0.717391304347826,44,0.1512027491408934,33,0.9166666666666666,1,0
46,"Models designed specifically for MCTest include those of , and more recently , , and .",Introduction,Introduction,question-answering,4,34,0.7391304347826086,45,0.1546391752577319,34,0.9444444444444444,1,0
47,"In experiments , our Parallel - Hierarchical model achieves state - of - the - art accuracy on MCTest , outperforming these existing methods .",Introduction,Introduction,question-answering,4,35,0.7608695652173914,46,0.1580756013745704,35,0.9722222222222222,1,0
48,"Below we describe related work , the mathematical details of our model , and our experiments , then analyze our results .",Introduction,Introduction,question-answering,4,36,0.782608695652174,47,0.1615120274914089,36,1.0,1,0
49,The Problem,Introduction,,question-answering,4,37,0.8043478260869565,48,0.1649484536082474,0,0.0,1,0
50,"In this section we borrow from , who laid out the MC problem nicely .",Introduction,The Problem,question-answering,4,38,0.8260869565217391,49,0.1683848797250859,1,0.1111111111111111,1,0
51,Machine comprehension requires machines to answer questions based on unstructured text .,Introduction,The Problem,question-answering,4,39,0.8478260869565217,50,0.1718213058419244,2,0.2222222222222222,1,0
52,This can be viewed as selecting the best answer from a set of candidates .,Introduction,The Problem,question-answering,4,40,0.8695652173913043,51,0.1752577319587628,3,0.3333333333333333,1,0
53,"In the multiple - choice case , candidate answers are predefined , but candidate answers may also be undefined yet restricted ( e.g. , to yes , no , or any noun phrase in the text ) .",Introduction,The Problem,question-answering,4,41,0.8913043478260869,52,0.1786941580756013,4,0.4444444444444444,1,0
54,"For each question q , let T be the unstructured text and A = {a i } the set of candidate answers to q .",Introduction,The Problem,question-answering,4,42,0.9130434782608696,53,0.1821305841924398,5,0.5555555555555556,1,0
55,The machine comprehension task reduces to selecting the answer that has the highest evidence given T .,Introduction,The Problem,question-answering,4,43,0.9347826086956522,54,0.1855670103092783,6,0.6666666666666666,1,0
56,"As in , we combine an answer and a question into a hypothesis , hi = f ( q , a i ) .",Introduction,The Problem,question-answering,4,44,0.9565217391304348,55,0.1890034364261168,7,0.7777777777777778,1,0
57,"To facilitate comparisons of the text with the hypotheses , we also breakdown the passage into sentences t j , T = {t j }.",Introduction,The Problem,question-answering,4,45,0.9782608695652174,56,0.1924398625429553,8,0.8888888888888888,1,0
58,"In our setting , q , a i , and t j each represent a sequence of embedding vectors , one for each word and punctuation mark in the respective item .",Introduction,The Problem,question-answering,4,46,1.0,57,0.1958762886597938,9,1.0,1,0
59,Related Work,,,question-answering,4,0,0.0,58,0.1993127147766323,0,0.0,1,0
60,Machine comprehension is currently a hot topic within the machine learning community .,Related Work,Related Work,question-answering,4,1,0.0434782608695652,59,0.2027491408934708,1,0.0434782608695652,0,0
61,"In this section we will focus on the best - performing models applied specifically to MCTest , since it is somewhat unique among MC datasets ( see Section 5 ) .",Related Work,Related Work,question-answering,4,2,0.0869565217391304,60,0.2061855670103092,2,0.0869565217391304,0,0
62,"Generally , models can be divided into two categories : those that use fixed , engineered features , and neural models .",Related Work,Related Work,question-answering,4,3,0.1304347826086956,61,0.2096219931271477,3,0.1304347826086956,0,0
63,The bulk of the work on MCTest falls into the former category .,Related Work,Related Work,question-answering,4,4,0.1739130434782608,62,0.2130584192439862,4,0.1739130434782608,0,0
64,"Manually engineered features often require significant effort on the part of a designer , and / or various auxiliary tools to extract them , and they can not be modified by training .",Related Work,Related Work,question-answering,4,5,0.217391304347826,63,0.2164948453608247,5,0.217391304347826,0,0
65,"On the other hand , neural models can be trained end - to - end and typically harness only a single feature : vectorrepresentations of words .",Related Work,Related Work,question-answering,4,6,0.2608695652173913,64,0.2199312714776632,6,0.2608695652173913,0,0
66,Word embeddings are fed into a complex and possibly deep neural network which processes and compares text to question and answer .,Related Work,Related Work,question-answering,4,7,0.3043478260869565,65,0.2233676975945017,7,0.3043478260869565,0,0
67,"Among deep models , mechanisms of attention and working memory are common , as in and .",Related Work,Related Work,question-answering,4,8,0.3478260869565217,66,0.2268041237113402,8,0.3478260869565217,0,0
68,"3.1 Feature - engineering models treated MCTest as a structured prediction problem , searching fora latent answerentailing structure connecting question , answer , and text .",Related Work,Related Work,question-answering,4,9,0.391304347826087,67,0.2302405498281786,9,0.391304347826087,0,0
69,This structure corresponds to the best latent alignment of a hypothesis with appropriate snippets of the text .,Related Work,Related Work,question-answering,4,10,0.4347826086956521,68,0.2336769759450171,10,0.4347826086956521,0,0
70,The process of ( latently ) selecting text snippets is related to the attention mechanisms typically used in deep networks designed for MC and machine translation .,Related Work,Related Work,question-answering,4,11,0.4782608695652174,69,0.2371134020618556,11,0.4782608695652174,0,0
71,The model uses event and entity coreference links across sentences along with a host of other features .,Related Work,Related Work,question-answering,4,12,0.5217391304347826,70,0.2405498281786941,12,0.5217391304347826,0,0
72,These include specifically trained word vectors for synonymy ; antonymy and class - inclusion relations from external database sources ; dependencies and semantic role labels .,Related Work,Related Work,question-answering,4,13,0.5652173913043478,71,0.2439862542955326,13,0.5652173913043478,0,0
73,"The model is trained using a latent structural SVM extended to a multitask setting , so that questions are first classified using a pretrained top - level classifier .",Related Work,Related Work,question-answering,4,14,0.6086956521739131,72,0.2474226804123711,14,0.6086956521739131,0,0
74,This enables the system to use different processing strategies for different question categories .,Related Work,Related Work,question-answering,4,15,0.6521739130434783,73,0.2508591065292096,15,0.6521739130434783,0,0
75,The model also combines question and answer into a well - formed statement using the rules of Cucerzan and Agichtein ( 2005 ) .,Related Work,Related Work,question-answering,4,16,0.6956521739130435,74,0.2542955326460481,16,0.6956521739130435,0,0
76,"Our model is simpler than that of in terms of the features it takes in , the training procedure ( stochastic gradient descent vs. alternating minimization ) , question classification ( we use none ) , and question - answer combination ( simple concatenation or mean vs. a set of rules ) .",Related Work,Related Work,question-answering,4,17,0.7391304347826086,75,0.2577319587628865,17,0.7391304347826086,0,0
77,"augmented the baseline feature set from with features for syntax , frame semantics , coreference chains , and word embeddings .",Related Work,Related Work,question-answering,4,18,0.782608695652174,76,0.2611683848797251,18,0.782608695652174,0,0
78,They combined features using a linear latent - variable classifier trained to minimize a max - margin loss function .,Related Work,Related Work,question-answering,4,19,0.8260869565217391,77,0.2646048109965636,19,0.8260869565217391,0,0
79,"As in , questions and answers are combined using a set of manually written rules .",Related Work,Related Work,question-answering,4,20,0.8695652173913043,78,0.268041237113402,20,0.8695652173913043,0,0
80,"The method of achieved the previous state of the art , but has significant complexity in terms of the feature set .",Related Work,Related Work,question-answering,4,21,0.9130434782608696,79,0.2714776632302405,21,0.9130434782608696,0,0
81,"Space does not permit a full description of all models in this category , but see also and .",Related Work,Related Work,question-answering,4,22,0.9565217391304348,80,0.274914089347079,22,0.9565217391304348,0,0
82,"Despite its relative lack of features , the Parallel - Hierarchical model improves upon the featureengineered state of the art for MCTest by a small amount ( about 1 % absolute ) as detailed in Section 5 .",Related Work,Related Work,question-answering,4,23,1.0,81,0.2783505154639175,23,1.0,0,0
83,Neural models,,,question-answering,4,0,0.0,82,0.281786941580756,0,0.0,1,0
84,"Neural models have , to date , performed relatively poorly on MCTest .",Neural models,Neural models,question-answering,4,1,0.009009009009009,83,0.2852233676975945,1,0.0588235294117647,1,0
85,This is because the dataset is sparse and complex .,Neural models,Neural models,question-answering,4,2,0.018018018018018,84,0.2886597938144329,2,0.1176470588235294,1,0
86,investigated deep - learning approaches concurrently with the present work .,Neural models,Neural models,question-answering,4,3,0.027027027027027,85,0.2920962199312715,3,0.1764705882352941,1,0
87,"They measured the performance of the Attentive Reader and the Neural Reasoner , both deep , end - to - end recurrent models with attention mechanisms , and also developed an attention - based convolutional network , the HABCNN .",Neural models,Neural models,question-answering,4,4,0.036036036036036,86,0.2955326460481099,4,0.2352941176470588,1,0
88,"Their network operates on a hierarchy similar to our own , providing further evidence of the promise of hierarchical perspectives .",Neural models,Neural models,question-answering,4,5,0.045045045045045,87,0.2989690721649484,5,0.2941176470588235,1,0
89,"Specifically , the HABCNN processes text at the sentence level and the snippet level , where the latter combines adjacent sentences ( as we do through an n-gram input ) .",Neural models,Neural models,question-answering,4,6,0.054054054054054,88,0.3024054982817869,6,0.3529411764705882,1,0
90,Embedding vectors for the question and the answer candidates are combined and encoded by a convolutional network .,Neural models,Neural models,question-answering,4,7,0.063063063063063,89,0.3058419243986254,7,0.4117647058823529,1,0
91,"This encoding modulates attention over sentence and snippet encodings , followed by maxpooling to determine the best matches between question , answer , and text .",Neural models,Neural models,question-answering,4,8,0.072072072072072,90,0.3092783505154639,8,0.4705882352941176,1,0
92,"As in the present work , matching scores are given by cosine similarity .",Neural models,Neural models,question-answering,4,9,0.081081081081081,91,0.3127147766323024,9,0.5294117647058824,1,0
93,The HABCNN also makes use of a question classifier .,Neural models,Neural models,question-answering,4,10,0.09009009009009,92,0.3161512027491409,10,0.5882352941176471,1,0
94,"Despite the shared concepts between the HABCNN and our approach , the Parallel - Hierarchical model performs significantly better on MCTest ( more than 15 % absolute ) as detailed in Section 5 .",Neural models,Neural models,question-answering,4,11,0.0990990990990991,93,0.3195876288659793,11,0.6470588235294118,1,0
95,Other neural models tested in fare even worse .,Neural models,Neural models,question-answering,4,12,0.1081081081081081,94,0.3230240549828179,12,0.7058823529411765,1,0
96,The Parallel - Hierarchical Model,Neural models,,question-answering,4,13,0.1171171171171171,95,0.3264604810996563,13,0.7647058823529411,1,0
97,Let us now define our machine comprehension model in full .,Neural models,The Parallel - Hierarchical Model,question-answering,4,14,0.1261261261261261,96,0.3298969072164948,14,0.8235294117647058,1,0
98,"We first describe each of the perspectives separately , then describe how they are combined .",Neural models,The Parallel - Hierarchical Model,question-answering,4,15,0.1351351351351351,97,0.3333333333333333,15,0.8823529411764706,1,0
99,"Below , we use subscripts to index elements of sequences , like word vectors , and superscripts to indicate whether elements come from the text , question , or answer .",Neural models,The Parallel - Hierarchical Model,question-answering,4,16,0.1441441441441441,98,0.3367697594501718,16,0.9411764705882352,1,0
100,"In particular , we use the subscripts k , m , n , p to index sequences from the text , question , answer , and hypothesis , respectively , and superscripts t , q , a , h. We depict the model schematically in .",Neural models,The Parallel - Hierarchical Model,question-answering,4,17,0.1531531531531531,99,0.3402061855670103,17,1.0,1,0
101,Semantic Perspective,Neural models,,question-answering,4,18,0.1621621621621621,100,0.3436426116838488,0,0.0,1,0
102,The semantic perspective is similar to the Memory Networks approach for embedding inputs into memory space .,Neural models,Semantic Perspective,question-answering,4,19,0.1711711711711711,101,0.3470790378006873,1,0.0434782608695652,1,0
103,Each sen - tence of the text is a sequence of d-dimensional word vectors :,Neural models,Semantic Perspective,question-answering,4,20,0.1801801801801801,102,0.3505154639175257,2,0.0869565217391304,1,0
104,"The semantic vector st is computed by embedding the word vectors into a D-dimensional space using a two - layer network that implements weighted sum followed by an affine tranformation and a nonlinearity ; i.e. ,",Neural models,Semantic Perspective,question-answering,4,21,0.1891891891891892,103,0.3539518900343643,3,0.1304347826086956,1,0
105,"The matrix At ? R Dd , the bias vector b t A ? RD , and for f we use the leaky ReLU function .",Neural models,Semantic Perspective,question-answering,4,22,0.1981981981981982,104,0.3573883161512027,4,0.1739130434782608,1,0
106,The scalar ?,Neural models,Semantic Perspective,question-answering,4,23,0.2072072072072072,105,0.3608247422680412,5,0.217391304347826,1,0
107,k is a trainable weight associated to each word in the vocabulary .,Neural models,Semantic Perspective,question-answering,4,24,0.2162162162162162,106,0.3642611683848797,6,0.2608695652173913,1,0
108,These scalar weights implement a kind of exogenous or bottomup attention that depends only on the input stimulus .,Neural models,Semantic Perspective,question-answering,4,25,0.2252252252252252,107,0.3676975945017182,7,0.3043478260869565,1,0
109,"They can , for example , learn to perform the function of stopword lists in a soft , trainable way , to nullify the contribution of unimportant filler words .",Neural models,Semantic Perspective,question-answering,4,26,0.2342342342342342,108,0.3711340206185567,8,0.3478260869565217,1,0
110,"The semantic representation of a hypothesis is formed analogously , except that we combine the question word vectors q m and answer word vectors an as a single sequence {h p } = {q m , an }.",Neural models,Semantic Perspective,question-answering,4,27,0.2432432432432432,109,0.3745704467353952,9,0.391304347826087,1,0
111,"For semantic vector sh of the hypothesis , we use a unique transformation matrix A h ?",Neural models,Semantic Perspective,question-answering,4,28,0.2522522522522522,110,0.3780068728522336,10,0.4347826086956521,1,0
112,R Dd and bias vector b h A ?,Neural models,Semantic Perspective,question-answering,4,29,0.2612612612612612,111,0.3814432989690721,11,0.4782608695652174,1,0
113,RD .,Neural models,Semantic Perspective,question-answering,4,30,0.2702702702702703,112,0.3848797250859107,12,0.5217391304347826,1,0
114,These transformations map a text sentence and a hypothesis into a common space where they can be compared .,Neural models,Semantic Perspective,question-answering,4,31,0.2792792792792792,113,0.3883161512027491,13,0.5652173913043478,1,0
115,"We compute the semantic match be-tween text sentence and hypothesis using the cosine similarity , M sem = cos ( s t , sh ) .",Neural models,Semantic Perspective,question-answering,4,32,0.2882882882882883,114,0.3917525773195876,14,0.6086956521739131,1,0
116,( 2 ),Neural models,Semantic Perspective,question-answering,4,33,0.2972972972972973,115,0.3951890034364261,15,0.6521739130434783,1,0
117,Word - by - Word Perspective,Neural models,,question-answering,4,34,0.3063063063063063,116,0.3986254295532646,16,0.6956521739130435,1,0
118,"The first step in building the word - by - word perspective is to transform word vectors from a text sentence , question , and answer through respective neural functions .",Neural models,Word - by - Word Perspective,question-answering,4,35,0.3153153153153153,117,0.4020618556701031,17,0.7391304347826086,1,0
119,"For the text ,",Neural models,Word - by - Word Perspective,question-answering,4,36,0.3243243243243243,118,0.4054982817869416,18,0.782608695652174,1,0
120,?,Neural models,Word - by - Word Perspective,question-answering,4,37,0.3333333333333333,119,0.40893470790378,19,0.8260869565217391,1,0
121,RD and f is again the leaky ReLU .,Neural models,Word - by - Word Perspective,question-answering,4,38,0.3423423423423423,120,0.4123711340206185,20,0.8695652173913043,1,0
122,We transform the question and the answer toq m and n analogously using distinct matrices and bias vectors .,Neural models,Word - by - Word Perspective,question-answering,4,39,0.3513513513513513,121,0.415807560137457,21,0.9130434782608696,1,0
123,"In contrast with the semantic perspective , we keep the question and answer candidates separate in the wordby - word perspective .",Neural models,Word - by - Word Perspective,question-answering,4,40,0.3603603603603603,122,0.4192439862542955,22,0.9565217391304348,1,0
124,"This is because matches to answer words are inherently more important than matches to question words , and we want our model to learn to use this property .",Neural models,Word - by - Word Perspective,question-answering,4,41,0.3693693693693693,123,0.422680412371134,23,1.0,1,0
125,Sentential,Neural models,,question-answering,4,42,0.3783783783783784,124,0.4261168384879725,0,0.0,1,0
126,"Inspired by the work of in paraphrase detection , we compute matches between hypotheses and text sentences at the word level .",Neural models,Sentential,question-answering,4,43,0.3873873873873873,125,0.4295532646048109,1,0.1,1,0
127,This computation uses the cosine similarity as before :,Neural models,Sentential,question-answering,4,44,0.3963963963963964,126,0.4329896907216495,2,0.2,1,0
128,"ca kn = cos (t k , n ) .",Neural models,Sentential,question-answering,4,45,0.4054054054054054,127,0.436426116838488,3,0.3,1,0
129,The word - by - word match between a text sentence and question is determined by taking the maximum over k ( finding the text word that best matches each question word ) and then taking a weighted mean over m ( finding the average match over the full question ) :,Neural models,Sentential,question-answering,4,46,0.4144144144144144,128,0.4398625429553264,4,0.4,1,0
130,"Here , ?",Neural models,Sentential,question-answering,4,47,0.4234234234234234,129,0.4432989690721649,5,0.5,1,0
131,m is the word weight for the question word and Z normalizes these weights to sum to one over the question .,Neural models,Sentential,question-answering,4,48,0.4324324324324324,130,0.4467353951890034,6,0.6,1,0
132,"We define the match between a sentence and answer candidate , M a , analogously .",Neural models,Sentential,question-answering,4,49,0.4414414414414414,131,0.4501718213058419,7,0.7,1,0
133,"Finally , we combine the matches to question and answer according to",Neural models,Sentential,question-answering,4,50,0.4504504504504504,132,0.4536082474226804,8,0.8,1,0
134,Here the ?,Neural models,Sentential,question-answering,4,51,0.4594594594594595,133,0.4570446735395189,9,0.9,1,0
135,are trainable parameters that control the relative importance of the terms .,Neural models,Sentential,question-answering,4,52,0.4684684684684684,134,0.4604810996563573,10,1.0,1,0
136,Sequential Sliding Window,Neural models,,question-answering,4,53,0.4774774774774775,135,0.4639175257731959,0,0.0,1,0
137,The sequential sliding window is related to the original MCTest baseline by .,Neural models,Sequential Sliding Window,question-answering,4,54,0.4864864864864865,136,0.4673539518900343,1,0.1,1,0
138,"Our sliding window decays from its focus word according to a Gaussian distribution , which we extend by assigning a trainable weight to each location in the window .",Neural models,Sequential Sliding Window,question-answering,4,55,0.4954954954954955,137,0.4707903780068728,2,0.2,1,0
139,This modification enables the window to use information about the distance between word matches ; the original baseline used distance information through a predefined function .,Neural models,Sequential Sliding Window,question-answering,4,56,0.5045045045045045,138,0.4742268041237113,3,0.3,1,0
140,"The sliding window scans over the words of the text as one continuous sequence , without sentence breaks .",Neural models,Sequential Sliding Window,question-answering,4,57,0.5135135135135135,139,0.4776632302405498,4,0.4,1,0
141,"Each window is treated like a sentence in the previous subsection , but we include a location - based weight ?( k ) .",Neural models,Sequential Sliding Window,question-answering,4,58,0.5225225225225225,140,0.4810996563573883,5,0.5,1,0
142,"This weight is based on a word 's position in the window , which , given a window , depends on its global position k.",Neural models,Sequential Sliding Window,question-answering,4,59,0.5315315315315315,141,0.4845360824742268,6,0.6,1,0
143,The cosine similarity is adapted as,Neural models,Sequential Sliding Window,question-answering,4,60,0.5405405405405406,142,0.4879725085910653,7,0.7,1,0
144,for the question and analogously for the answer .,Neural models,Sequential Sliding Window,question-answering,4,61,0.5495495495495496,143,0.4914089347079037,8,0.8,1,0
145,We initialize the location weights with a Gaussian and fine - tune them during training .,Neural models,Sequential Sliding Window,question-answering,4,62,0.5585585585585585,144,0.4948453608247423,9,0.9,1,0
146,"The final matching score , denoted as M sws , is computed as in and with sq km replacing c q km .",Neural models,Sequential Sliding Window,question-answering,4,63,0.5675675675675675,145,0.4982817869415807,10,1.0,1,0
147,Dependency Sliding Window,Neural models,,question-answering,4,64,0.5765765765765766,146,0.5017182130584192,0,0.0,1,0
148,"The dependency sliding window operates identically to the linear sliding window , but on a different view of the text passage .",Neural models,Dependency Sliding Window,question-answering,4,65,0.5855855855855856,147,0.5051546391752577,1,0.0256410256410256,1,0
149,The output of this component is M swd and is formed analogously to M sws .,Neural models,Dependency Sliding Window,question-answering,4,66,0.5945945945945946,148,0.5085910652920962,2,0.0512820512820512,1,0
150,The dependency perspective uses the Stanford Dependency Parser as an auxiliary tool .,Neural models,Dependency Sliding Window,question-answering,4,67,0.6036036036036037,149,0.5120274914089347,3,0.0769230769230769,1,0
151,"Thus , the dependency graph can be considered a fixed feature .",Neural models,Dependency Sliding Window,question-answering,4,68,0.6126126126126126,150,0.5154639175257731,4,0.1025641025641025,1,0
152,"Moreover , linearization of the dependency graph , because it relies on an eigendecomposition , is not differentiable .",Neural models,Dependency Sliding Window,question-answering,4,69,0.6216216216216216,151,0.5189003436426117,5,0.1282051282051282,1,0
153,"However , we handle the linearization in data preprocessing so that the model sees only reordered word - vector inputs .",Neural models,Dependency Sliding Window,question-answering,4,70,0.6306306306306306,152,0.5223367697594502,6,0.1538461538461538,1,0
154,"Specifically , we run the Stanford Dependency Parser on each text sentence to build a dependency graph .",Neural models,Dependency Sliding Window,question-answering,4,71,0.6396396396396397,153,0.5257731958762887,7,0.1794871794871795,1,0
155,"This graph has n w vertices , one for each word in the sentence .",Neural models,Dependency Sliding Window,question-answering,4,72,0.6486486486486487,154,0.5292096219931272,8,0.2051282051282051,1,0
156,From the dependency graph we form the Laplacian matrix L ?,Neural models,Dependency Sliding Window,question-answering,4,73,0.6576576576576577,155,0.5326460481099656,9,0.2307692307692307,1,0
157,R nwnw and determine its eigenvectors .,Neural models,Dependency Sliding Window,question-answering,4,74,0.6666666666666666,156,0.5360824742268041,10,0.2564102564102564,1,0
158,The second eigenvector u 2 of the Laplacian is known as the Fiedler vector .,Neural models,Dependency Sliding Window,question-answering,4,75,0.6756756756756757,157,0.5395189003436426,11,0.282051282051282,1,0
159,It is the solution to the minimization,Neural models,Dependency Sliding Window,question-answering,4,76,0.6846846846846847,158,0.5429553264604811,12,0.3076923076923077,1,0
160,"where vi are the vertices of the graph , and ?",Neural models,Dependency Sliding Window,question-answering,4,77,0.6936936936936937,159,0.5463917525773195,13,0.3333333333333333,1,0
161,ij is the weight of the edge from vertex i to vertex j.,Neural models,Dependency Sliding Window,question-answering,4,78,0.7027027027027027,160,0.5498281786941581,14,0.358974358974359,1,0
162,"The Fiedler vector maps a weighted graph onto a line such that connected nodes stay close , modulated by the connection weights .",Neural models,Dependency Sliding Window,question-answering,4,79,0.7117117117117117,161,0.5532646048109966,15,0.3846153846153846,1,0
163,1,Neural models,Dependency Sliding Window,question-answering,4,80,0.7207207207207207,162,0.5567010309278351,16,0.4102564102564102,1,0
164,This enables us to reorder the words of a sentence based on their proximity in the dependency graph .,Neural models,Dependency Sliding Window,question-answering,4,81,0.7297297297297297,163,0.5601374570446735,17,0.4358974358974359,1,0
165,The reordering of the words is given by the ordered index set I = arg sort ( u 2 ) .,Neural models,Dependency Sliding Window,question-answering,4,82,0.7387387387387387,164,0.563573883161512,18,0.4615384615384615,1,0
166,"To give an example of how this works , consider the following sentence from MCTest and its dependency - based reordering :",Neural models,Dependency Sliding Window,question-answering,4,83,0.7477477477477478,165,0.5670103092783505,19,0.4871794871794871,1,0
167,"Jenny , Mrs. Mustard 's helper , called the police .",Neural models,Dependency Sliding Window,question-answering,4,84,0.7567567567567568,166,0.570446735395189,20,0.5128205128205128,1,0
168,"the police , called Jenny helper , Mrs. 's Mustard .",Neural models,Dependency Sliding Window,question-answering,4,85,0.7657657657657657,167,0.5738831615120275,21,0.5384615384615384,1,0
169,Sliding - window - based matching on the original sentence will answer the question,Neural models,Dependency Sliding Window,question-answering,4,86,0.7747747747747747,168,0.5773195876288659,22,0.5641025641025641,1,0
170,Who called the police ? with Mrs. Mustard .,Neural models,Dependency Sliding Window,question-answering,4,87,0.7837837837837838,169,0.5807560137457045,23,0.5897435897435898,1,0
171,"The dependency reordering enables the window to determine the correct answer , Jenny .",Neural models,Dependency Sliding Window,question-answering,4,88,0.7927927927927928,170,0.584192439862543,24,0.6153846153846154,1,0
172,Combining Distributed,Neural models,,question-answering,4,89,0.8018018018018018,171,0.5876288659793815,25,0.6410256410256411,1,0
173,Evidence,Neural models,,question-answering,4,90,0.8108108108108109,172,0.5910652920962199,26,0.6666666666666666,1,0
174,It is important in comprehension to synthesize information found throughout a document .,Neural models,Evidence,question-answering,4,91,0.8198198198198198,173,0.5945017182130584,27,0.6923076923076923,1,0
175,"MCTest was explicitly designed to ensure that it could not be solved by lexical techniques alone , but would instead require some form of inference or limited reasoning .",Neural models,Evidence,question-answering,4,92,0.8288288288288288,174,0.5979381443298969,28,0.717948717948718,1,0
176,It therefore includes questions where the evidence for an answer spans several sentences .,Neural models,Evidence,question-answering,4,93,0.8378378378378378,175,0.6013745704467354,29,0.7435897435897436,1,0
177,"To perform synthesis , our model also takes in ngrams of sentences , i.e. , sentence pairs and triples strung together .",Neural models,Evidence,question-answering,4,94,0.8468468468468469,176,0.6048109965635738,30,0.7692307692307693,1,0
178,"The model treats these exactly as it does single sentences , applying all functions detailed above .",Neural models,Evidence,question-answering,4,95,0.8558558558558559,177,0.6082474226804123,31,0.7948717948717948,1,0
179,A later pooling operation combines scores across all n-grams ( including the singlesentence input ) .,Neural models,Evidence,question-answering,4,96,0.8648648648648649,178,0.6116838487972509,32,0.8205128205128205,1,0
180,This is described in the next subsection .,Neural models,Evidence,question-answering,4,97,0.8738738738738738,179,0.6151202749140894,33,0.8461538461538461,1,0
181,"With n-grams , the model can combine information distributed across contiguous sentences .",Neural models,Evidence,question-answering,4,98,0.8828828828828829,180,0.6185567010309279,34,0.8717948717948718,1,0
182,"In some cases , however , the required evidence is spread across distant sentences .",Neural models,Evidence,question-answering,4,99,0.8918918918918919,181,0.6219931271477663,35,0.8974358974358975,1,0
183,"To give our model some capacity to deal with this scenario , we take the top N sentences as scored by all the preceding functions , and then repeat the scoring computations viewing these top N as a single sentence .",Neural models,Evidence,question-answering,4,100,0.9009009009009008,182,0.6254295532646048,36,0.9230769230769232,1,0
184,The reasoning behind these approaches can be explained well in a probabilistic setting .,Neural models,Evidence,question-answering,4,101,0.90990990990991,183,0.6288659793814433,37,0.9487179487179488,1,0
185,"If we consider our similarity scores to model the likelihood of a text sentence given a hypothesis , p (t j |h i ) , then the n-gram and top N approaches model a joint probability p (t j 1 , t j 2 , . . . , t j k |h i ) .",Neural models,Evidence,question-answering,4,102,0.918918918918919,184,0.6323024054982818,38,0.9743589743589745,1,0
186,We can not model the joint probability as a product of individual terms ( score values ) because distributed pieces of evidence are likely not independent .,Neural models,Evidence,question-answering,4,103,0.927927927927928,185,0.6357388316151202,39,1.0,1,0
187,Combining Perspectives,Neural models,,question-answering,4,104,0.9369369369369368,186,0.6391752577319587,0,0.0,1,0
188,"We use a multilayer perceptron to combine M sem , M word , M swd , and M sws as a final matching score M i for each answer candidate .",Neural models,Combining Perspectives,question-answering,4,105,0.945945945945946,187,0.6426116838487973,1,0.1428571428571428,1,0
189,"This network also pools and combines the separate n-gram scores , and uses a linear activation function .",Neural models,Combining Perspectives,question-answering,4,106,0.954954954954955,188,0.6460481099656358,2,0.2857142857142857,1,0
190,Our overall training objective is to minimize the ranking loss,Neural models,Combining Perspectives,question-answering,4,107,0.963963963963964,189,0.6494845360824743,3,0.4285714285714285,1,0
191,"where is a constant margin , i * indexes the correct answer , and we take the maximum over i so that we are ranking the correct answer over the best - ranked incorrect answer ( of which there are three ) .",Neural models,Combining Perspectives,question-answering,4,108,0.972972972972973,190,0.6529209621993127,4,0.5714285714285714,1,0
192,This approach worked better than comparing the correct answer to the incorrect answers individually as in .,Neural models,Combining Perspectives,question-answering,4,109,0.981981981981982,191,0.6563573883161512,5,0.7142857142857143,1,0
193,"Our implementation of the Parallel - Hierarchical model , using the Keras framework , is available on Github .",Neural models,Combining Perspectives,question-answering,4,110,0.990990990990991,192,0.6597938144329897,6,0.8571428571428571,1,0
194,2,Neural models,Combining Perspectives,question-answering,4,111,1.0,193,0.6632302405498282,7,1.0,1,0
195,Training Wheels,,,question-answering,4,0,0.0,194,0.6666666666666666,0,0.0,1,0
196,"Before training , we initialized the neural - network components of our model to perform sensible heuristic functions .",Training Wheels,Training Wheels,question-answering,4,1,0.1,195,0.6701030927835051,1,0.1,1,0
197,Training did not converge on the small MCTest without this vital approach .,Training Wheels,Training Wheels,question-answering,4,2,0.2,196,0.6735395189003437,2,0.2,1,0
198,"Empirically , we found that we could achieve above 50 % accuracy on MCTest using a simple sum of word vectors followed by a dot product between the question sum and the hypothesis sum .",Training Wheels,Training Wheels,question-answering,4,3,0.3,197,0.6769759450171822,3,0.3,1,0
199,"Therefore , we initialized the network for the semantic perspective to perform this sum , by initializing A x as the identity matrix and bx A as the zero vector , x ? {t , h} .",Training Wheels,Training Wheels,question-answering,4,4,0.4,198,0.6804123711340206,4,0.4,1,0
200,Recall that the activation function is a ReLU so that positive outputs are unchanged .,Training Wheels,Training Wheels,question-answering,4,5,0.5,199,0.6838487972508591,5,0.5,1,0
201,"We also found basic word - matching scores to be helpful , so we initialized the word - by - word networks likewise .",Training Wheels,Training Wheels,question-answering,4,6,0.6,200,0.6872852233676976,6,0.6,1,0
202,"The network for perspectivecombination was initialized to perform a sum of individual scores , using a zero bias - vector and a weight matrix of ones , since we found that each perspective contributed positively to the overall result .",Training Wheels,Training Wheels,question-answering,4,7,0.7,201,0.6907216494845361,7,0.7,1,0
203,This training wheels approach is related to other techniques from the literature .,Training Wheels,Training Wheels,question-answering,4,8,0.8,202,0.6941580756013745,8,0.8,1,0
204,"For instance , proposed the identity - matrix initialization in the context of recurrent neural networks in order to preserve the error signal through backpropagation .",Training Wheels,Training Wheels,question-answering,4,9,0.9,203,0.697594501718213,9,0.9,1,0
205,"In residual networks , shortcut connections bypass certain layers in the network so that a simpler function can be trained in conjunction with the full model .",Training Wheels,Training Wheels,question-answering,4,10,1.0,204,0.7010309278350515,10,1.0,1,0
206,Experiments,,,question-answering,4,0,0.0,205,0.7044673539518901,0,0.0,1,0
207,The Dataset,Experiments,,question-answering,4,1,0.125,206,0.7079037800687286,0,0.0,1,0
208,"MCTest is a collection of 660 elementary - level children 's stories and associated questions , written by human subjects .",Experiments,The Dataset,question-answering,4,2,0.25,207,0.711340206185567,1,0.1428571428571428,1,0
209,"The stories are fictional , ensuring that the answer must be found in the text itself , and carefully limited to what a young child can understand .",Experiments,The Dataset,question-answering,4,3,0.375,208,0.7147766323024055,2,0.2857142857142857,1,0
210,The more challenging variant consists of 500 stories with four multiple - choice questions each .,Experiments,The Dataset,question-answering,4,4,0.5,209,0.718213058419244,3,0.4285714285714285,1,0
211,"Despite the elementary level , stories and questions are more natural and more complex than those found in synthetic MC datasets like bAb I and CNN .",Experiments,The Dataset,question-answering,4,5,0.625,210,0.7216494845360825,4,0.5714285714285714,1,0
212,MCTest is challenging because it is both complicated and small .,Experiments,The Dataset,question-answering,4,6,0.75,211,0.7250859106529209,5,0.7142857142857143,1,0
213,"As per , "" it is very difficult to train statistical models only on MCTest . """,Experiments,The Dataset,question-answering,4,7,0.875,212,0.7285223367697594,6,0.8571428571428571,1,0
214,"It s size limits the number of parameters that can be trained , and prevents learning any complex language modeling simultaneously with the capacity to answer questions .",Experiments,The Dataset,question-answering,4,8,1.0,213,0.7319587628865979,7,1.0,1,0
215,Training and Model Details,,,question-answering,4,0,0.0,214,0.7353951890034365,0,0.0,1,0
216,In this section we describe important details of the training procedure and model setup .,Training and Model Details,Training and Model Details,question-answering,4,1,0.04,215,0.738831615120275,1,0.04,1,0
217,"For a complete list of hyperparameter settings , our stopword list , and other minutiae , we refer interested readers to our Github repository .",Training and Model Details,Training and Model Details,question-answering,4,2,0.08,216,0.7422680412371134,2,0.08,1,0
218,"For word vectors we use Google 's publicly available embeddings , trained with word2vec on the 100 - billion - word News corpus .",Training and Model Details,Training and Model Details,question-answering,4,3,0.12,217,0.7457044673539519,3,0.12,1,1
219,"These vectors are kept fixed throughout training , since we found that training them was not helpful ( likely because of MCTest 's size ) .",Training and Model Details,Training and Model Details,question-answering,4,4,0.16,218,0.7491408934707904,4,0.16,1,0
220,The vectors are 300 - dimensional ( d = 300 ) .,Training and Model Details,Training and Model Details,question-answering,4,5,0.2,219,0.7525773195876289,5,0.2,1,0
221,"We do not use a stopword list for the text passage , instead relying on the trainable word weights to ascribe global importance ratings to words .",Training and Model Details,Training and Model Details,question-answering,4,6,0.24,220,0.7560137457044673,6,0.24,1,0
222,These weights are initialized with the inverse document frequency ( IDF ) statistic computed over the MCTest corpus .,Training and Model Details,Training and Model Details,question-answering,4,7,0.28,221,0.7594501718213058,7,0.28,1,0
223,3,Training and Model Details,Training and Model Details,question-answering,4,8,0.32,222,0.7628865979381443,8,0.32,1,0
224,"However , we douse a short stopword list for questions .",Training and Model Details,Training and Model Details,question-answering,4,9,0.36,223,0.7663230240549829,9,0.36,1,0
225,"This list nullifies query words such as { Who , what , when , where , how} , along with conjugations of the verbs to do and to be .",Training and Model Details,Training and Model Details,question-answering,4,10,0.4,224,0.7697594501718213,10,0.4,1,0
226,"Following earlier methods , we use a heuristic to improve performance on negation questions .",Training and Model Details,Training and Model Details,question-answering,4,11,0.44,225,0.7731958762886598,11,0.44,1,0
227,"When a question contains the words which and not , we negate the hypothesis ranking scores so that the minimum becomes the maximum .",Training and Model Details,Training and Model Details,question-answering,4,12,0.48,226,0.7766323024054983,12,0.48,1,0
228,The most important technique for training the model was the training wheels approach .,Training and Model Details,Training and Model Details,question-answering,4,13,0.52,227,0.7800687285223368,13,0.52,1,0
229,"Without this , training was not effective at all .",Training and Model Details,Training and Model Details,question-answering,4,14,0.56,228,0.7835051546391752,14,0.56,1,0
230,The identity initialization requires that the network weight matrices are square ( d = D ) .,Training and Model Details,Training and Model Details,question-answering,4,15,0.6,229,0.7869415807560137,15,0.6,1,0
231,"We found dropout to be particularly effective at improving generalization from the training to the test set , and used 0.5 as the dropout probability .",Training and Model Details,Training and Model Details,question-answering,4,16,0.64,230,0.7903780068728522,16,0.64,1,1
232,"Dropout occurs after all neural - network transformations , if those transformations are allowed to change with training .",Training and Model Details,Training and Model Details,question-answering,4,17,0.68,231,0.7938144329896907,17,0.68,1,0
233,Our best performing model held networks at the wordby - word level fixed .,Training and Model Details,Training and Model Details,question-answering,4,18,0.72,232,0.7972508591065293,18,0.72,1,0
234,"For combining distributed evidence , we used up to trigrams over sentences and our bestperforming model reiterated over the top two sentences ( N = 2 ) .",Training and Model Details,Training and Model Details,question-answering,4,19,0.76,233,0.8006872852233677,19,0.76,1,0
235,"We used the Adam optimizer with the standard settings ( Kingma and Ba , 2014 ) and a learning rate of 0.003 .",Training and Model Details,Training and Model Details,question-answering,4,20,0.8,234,0.8041237113402062,20,0.8,1,1
236,To determine the best hyperparameters we performed a grid search over 150 settings based on validation - set accuracy .,Training and Model Details,Training and Model Details,question-answering,4,21,0.84,235,0.8075601374570447,21,0.84,1,1
237,"MCTest 's original validation set is too small for reliable hyperparameter tuning , so , following , we merged the training and validation sets of MCTest - 160 and MCTest - 500 , then split them randomly into a 250 - story training set and a 200 - story validation set .",Training and Model Details,Training and Model Details,question-answering,4,22,0.88,236,0.8109965635738832,22,0.88,1,0
238,presents the performance of featureengineered and neural methods on the MCTest test set .,Training and Model Details,Training and Model Details,question-answering,4,23,0.92,237,0.8144329896907216,23,0.92,1,0
239,"Accuracy scores are divided among questions whose evidence lies in a single sentence ( single ) and across multiple sentences ( multi ) , and among the two variants .",Training and Model Details,Training and Model Details,question-answering,4,24,0.96,238,0.8178694158075601,24,0.96,1,0
240,"Clearly , MCTest - 160 is easier .",Training and Model Details,Training and Model Details,question-answering,4,25,1.0,239,0.8213058419243986,25,1.0,1,0
241,Results,,,question-answering,4,0,0.0,240,0.8247422680412371,0,0.0,1,0
242,The first three rows represent featureengineered methods .,Results,Results,question-answering,4,1,0.0238095238095238,241,0.8281786941580757,1,0.0714285714285714,1,0
243,+,Results,Results,question-answering,4,2,0.0476190476190476,242,0.8316151202749141,2,0.1428571428571428,1,0
244,RTE is the best - performing variant of the original baseline published along with MCTest .,Results,Results,question-answering,4,3,0.0714285714285714,243,0.8350515463917526,3,0.2142857142857142,1,0
245,"It uses a lexical sliding window and distance - based measure , augmented with rules for recognizing textual entailment .",Results,Results,question-answering,4,4,0.0952380952380952,244,0.8384879725085911,4,0.2857142857142857,1,0
246,We described the methods of and in Section 3 .,Results,Results,question-answering,4,5,0.119047619047619,245,0.8419243986254296,5,0.3571428571428571,1,0
247,"On MCTest - 500 , the Parallel Hierarchical model significantly outperforms these methods on single questions ( > 2 % ) and slightly outperforms the latter two on multi questions ( ? 0.3 % ) and overall ( ? 1 % ) .",Results,Results,question-answering,4,6,0.1428571428571428,246,0.845360824742268,6,0.4285714285714285,1,1
248,The method of achieves the best overall result on MCTest - 160 .,Results,Results,question-answering,4,7,0.1666666666666666,247,0.8487972508591065,7,0.5,1,0
249,We suspect this is because our neural method suffered from the relative lack of training data .,Results,Results,question-answering,4,8,0.1904761904761904,248,0.852233676975945,8,0.5714285714285714,1,0
250,The last four rows in are neural methods that we discussed in Section 3 .,Results,Results,question-answering,4,9,0.2142857142857142,249,0.8556701030927835,9,0.6428571428571429,1,0
251,Performance measures are taken from .,Results,Results,question-answering,4,10,0.238095238095238,250,0.8591065292096219,10,0.7142857142857143,1,0
252,Here we see our model outperforming the alternatives by a large margin across the board ( > 15 % ) .,Results,Results,question-answering,4,11,0.2619047619047619,251,0.8625429553264605,11,0.7857142857142857,1,0
253,"The Neural Reasoner and the Attentive Reader are large , deep models with hundreds of thousands of parameters , so it is unsurprising that they performed poorly on MCTest .",Results,Results,question-answering,4,12,0.2857142857142857,252,0.865979381443299,12,0.8571428571428571,1,0
254,"The specificallydesigned HABCNN fared better , its convolutional architecture cutting down on the parameter count .",Results,Results,question-answering,4,13,0.3095238095238095,253,0.8694158075601375,13,0.9285714285714286,1,0
255,"Because there are similarities between our model and the HABCNN , we hypothesize that much of the performance difference is attributable to our training wheels methodology .",Results,Results,question-answering,4,14,0.3333333333333333,254,0.872852233676976,14,1.0,1,0
256,Analysis and Discussion,Results,,question-answering,4,15,0.3571428571428571,255,0.8762886597938144,0,0.0,1,0
257,We measure the contribution of each component of the model by ablating it .,Results,Analysis and Discussion,question-answering,4,16,0.3809523809523809,256,0.8797250859106529,1,0.037037037037037,1,0
258,Results are given in .,Results,Analysis and Discussion,question-answering,4,17,0.4047619047619047,257,0.8831615120274914,2,0.074074074074074,1,0
259,"Not surprisingly , the n-gram functionality is important , contributing almost 5 % accuracy improvement .",Results,Analysis and Discussion,question-answering,4,18,0.4285714285714285,258,0.8865979381443299,3,0.1111111111111111,1,1
260,"Without this , the model has almost no Method MCTest - 160 accuracy ( % )",Results,Analysis and Discussion,question-answering,4,19,0.4523809523809524,259,0.8900343642611683,4,0.1481481481481481,1,0
261,MCTest - 500 accuracy ( % ) Single means for synthesizing distributed evidence .,Results,Analysis and Discussion,question-answering,4,20,0.4761904761904761,260,0.8934707903780069,5,0.1851851851851851,1,0
262,"The top N function contributes very little to the overall performance , suggesting that most multi questions have their evidence distributed across contiguous sentences .",Results,Analysis and Discussion,question-answering,4,21,0.5,261,0.8969072164948454,6,0.2222222222222222,1,1
263,"Ablating the sentential component made the most significant difference , reducing performance by more than 5 % .",Results,Analysis and Discussion,question-answering,4,22,0.5238095238095238,262,0.9003436426116839,7,0.2592592592592592,1,1
264,Simple word - by - word matching is obviously useful on MCTest .,Results,Analysis and Discussion,question-answering,4,23,0.5476190476190477,263,0.9037800687285223,8,0.2962962962962963,1,1
265,"The sequential sliding window makes a 3 % contribution , highlighting the importance of word - distance measures .",Results,Analysis and Discussion,question-answering,4,24,0.5714285714285714,264,0.9072164948453608,9,0.3333333333333333,1,1
266,"On the other hand , the dependency - based sliding window makes only a minor contribution .",Results,Analysis and Discussion,question-answering,4,25,0.5952380952380952,265,0.9106529209621992,10,0.3703703703703703,1,1
267,We found this surprising .,Results,Analysis and Discussion,question-answering,4,26,0.6190476190476191,266,0.9140893470790378,11,0.4074074074074074,1,0
268,It maybe that linearization of the dependency graph removes too much of its information .,Results,Analysis and Discussion,question-answering,4,27,0.6428571428571429,267,0.9175257731958762,12,0.4444444444444444,1,0
269,"Finally , the exogenous word weights make a significant contribution of almost 5 % .",Results,Analysis and Discussion,question-answering,4,28,0.6666666666666666,268,0.9209621993127148,13,0.4814814814814814,1,1
270,"Analysis reveals that most of our system 's test failures occur on questions about quantity ( e.g. , How many ...? ) and temporal order ( e.g. , Who was invited last ? ) .",Results,Analysis and Discussion,question-answering,4,29,0.6904761904761905,269,0.9243986254295532,14,0.5185185185185185,1,0
271,"Quantity questions makeup 9.5 % of our errors on the validation set , while order questions makeup 10.3 % .",Results,Analysis and Discussion,question-answering,4,30,0.7142857142857143,270,0.9278350515463918,15,0.5555555555555556,1,0
272,"This weakness is not unexpected , since our architecture lacks any capacity for counting or tracking temporal order .",Results,Analysis and Discussion,question-answering,4,31,0.7380952380952381,271,0.9312714776632304,16,0.5925925925925926,1,0
273,"Incorporating mechanisms for these forms of reasoning is a priority for future work ( in contrast , the Memory Network model is quite good at temporal reasoning ) .",Results,Analysis and Discussion,question-answering,4,32,0.7619047619047619,272,0.9347079037800688,17,0.6296296296296297,1,0
274,The Parallel - Hierarchical model is simple .,Results,Analysis and Discussion,question-answering,4,33,0.7857142857142857,273,0.9381443298969072,18,0.6666666666666666,1,0
275,It does no complex language or sequence modeling .,Results,Analysis and Discussion,question-answering,4,34,0.8095238095238095,274,0.9415807560137456,19,0.7037037037037037,1,0
276,It s simplicity is a response to the limited data of MCTest .,Results,Analysis and Discussion,question-answering,4,35,0.8333333333333334,275,0.9450171821305842,20,0.7407407407407407,1,0
277,"Nevertheless , the model achieves stateof - the - art results on the multi questions , which ( putatively ) require some limited reasoning .",Results,Analysis and Discussion,question-answering,4,36,0.8571428571428571,276,0.9484536082474226,21,0.7777777777777778,1,0
278,Our model is able to handle them reasonably well just by stringing important sentences together .,Results,Analysis and Discussion,question-answering,4,37,0.8809523809523809,277,0.9518900343642612,22,0.8148148148148148,1,0
279,"Thus , the model imitates reasoning with a heuristic .",Results,Analysis and Discussion,question-answering,4,38,0.9047619047619048,278,0.9553264604810996,23,0.8518518518518519,1,0
280,"This suggests that , to learn true reasoning abilities , MCTest is too simple a dataset - and it is almost certainly too small for this goal .",Results,Analysis and Discussion,question-answering,4,39,0.9285714285714286,279,0.9587628865979382,24,0.8888888888888888,1,0
281,"However , it maybe that human language processing can be factored into separate processes of comprehension and reasoning .",Results,Analysis and Discussion,question-answering,4,40,0.9523809523809524,280,0.9621993127147768,25,0.925925925925926,1,0
282,"If so , the Parallel - Hierarchical model is a good start on the former .",Results,Analysis and Discussion,question-answering,4,41,0.9761904761904762,281,0.9656357388316152,26,0.9629629629629628,1,0
283,"Indeed , if we train the method exclusively on single questions then its results become even more impressive : we can achieve a test accuracy of 79.1 % on MCTest - 500 .",Results,Analysis and Discussion,question-answering,4,42,1.0,282,0.9690721649484536,27,1.0,1,0
284,Conclusion,,,question-answering,4,0,0.0,283,0.972508591065292,0,0.0,1,0
285,"We have presented the novel Parallel - Hierarchical model for machine comprehension , and evaluated it on the small but complex MCTest .",Conclusion,Conclusion,question-answering,4,1,0.1428571428571428,284,0.9759450171821306,1,0.1428571428571428,0,0
286,"Our model achieves state - of - the - art results , outperforming several feature - engineered and neural approaches .",Conclusion,Conclusion,question-answering,4,2,0.2857142857142857,285,0.979381443298969,2,0.2857142857142857,0,0
287,"Working with our model has emphasized to us the following ( not necessarily novel ) concepts , which we record hereto promote further empirical validation .",Conclusion,Conclusion,question-answering,4,3,0.4285714285714285,286,0.9828178694158076,3,0.4285714285714285,0,0
288,Good comprehension of language is supported by hierarchical levels of understanding ( Cf. ) .,Conclusion,Conclusion,question-answering,4,4,0.5714285714285714,287,0.986254295532646,4,0.5714285714285714,0,0
289,Exogenous attention ( the trainable word weights ) maybe broadly helpful for NLP .,Conclusion,Conclusion,question-answering,4,5,0.7142857142857143,288,0.9896907216494846,5,0.7142857142857143,0,0
290,"The training wheels approach , that is , initializing neural networks to perform sensible heuristics , appears helpful for small datasets .",Conclusion,Conclusion,question-answering,4,6,0.8571428571428571,289,0.993127147766323,6,0.8571428571428571,0,0
291,"Reasoning over language is challenging , but easily simulated in some cases .",Conclusion,Conclusion,question-answering,4,7,1.0,290,0.9965635738831616,7,1.0,0,0
1,title,,,question-answering,5,0,0.0,0,0.0,0,0.0,1,0
2,Iterative Alternating Neural Attention for Machine Reading,title,,question-answering,5,1,0.0,1,0.004524886877828,1,0.0,1,1
3,abstract,,,question-answering,5,0,0.0,2,0.0090497737556561,0,0.0,1,0
4,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document .",abstract,abstract,question-answering,5,1,0.3333333333333333,3,0.0135746606334841,1,0.3333333333333333,1,1
5,"Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document .",abstract,abstract,question-answering,5,2,0.6666666666666666,4,0.0180995475113122,2,0.6666666666666666,1,0
6,Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .,abstract,abstract,question-answering,5,3,1.0,5,0.0226244343891402,3,1.0,1,0
7,Introduction,,,question-answering,5,0,0.0,6,0.0271493212669683,0,0.0,1,0
8,"Recently , the idea of training machine comprehension models that can read , understand , and answer questions about a text has come closer to reality principally through two factors .",Introduction,Introduction,question-answering,5,1,0.0109890109890109,7,0.0316742081447963,1,0.0454545454545454,1,0
9,"The first is the advent of deep learning techniques , which allow manipulation of natural language beyond its surface forms and generalize beyond relatively small amounts of labeled data .",Introduction,Introduction,question-answering,5,2,0.0219780219780219,8,0.0361990950226244,2,0.0909090909090909,1,0
10,"The second factor is the formulation of standard machine comprehension benchmarks based on Cloze - style queries , which permit fast integration loops between model conception and experimental evaluation .",Introduction,Introduction,question-answering,5,3,0.0329670329670329,9,0.0407239819004524,3,0.1363636363636363,1,0
11,Cloze - style queries are created by deleting a particular word in a natural - language statement .,Introduction,Introduction,question-answering,5,4,0.0439560439560439,10,0.0452488687782805,4,0.1818181818181818,1,0
12,The task is to guess which word was deleted .,Introduction,Introduction,question-answering,5,5,0.0549450549450549,11,0.0497737556561085,5,0.2272727272727272,1,0
13,"Ina pragmatic approach , recent work formed such questions by extracting a sentence from a larger document .",Introduction,Introduction,question-answering,5,6,0.0659340659340659,12,0.0542986425339366,6,0.2727272727272727,1,0
14,"In contrast to considering a stand - alone statement , the system is now required to handle a larger amount of information that may possibly influence the prediction of the missing word .",Introduction,Introduction,question-answering,5,7,0.0769230769230769,13,0.0588235294117647,7,0.3181818181818182,1,0
15,Such contextual dependencies may also be injected by removing a word from a short human - crafted summary of a larger body of text .,Introduction,Introduction,question-answering,5,8,0.0879120879120879,14,0.0633484162895927,8,0.3636363636363636,1,0
16,The abstractive nature of the summary is likely to demand a higher level of comprehension of the original text .,Introduction,Introduction,question-answering,5,9,0.0989010989010989,15,0.0678733031674208,9,0.4090909090909091,1,0
17,"In both cases , the machine comprehension system is presented with an ablated query and the document to which the original query refers .",Introduction,Introduction,question-answering,5,10,0.1098901098901098,16,0.0723981900452488,10,0.4545454545454545,1,0
18,The missing word is assumed to appear in the document .,Introduction,Introduction,question-answering,5,11,0.1208791208791208,17,0.0769230769230769,11,0.5,1,0
19,"Encouraged by the recent success of deep learning attention architectures , we propose a novel neural attention - based inference model designed to perform machine reading comprehension tasks .",Introduction,Introduction,question-answering,5,12,0.1318681318681318,18,0.0814479638009049,12,0.5454545454545454,1,1
20,The model first reads the document and the query using a recurrent neural network .,Introduction,Introduction,question-answering,5,13,0.1428571428571428,19,0.085972850678733,13,0.5909090909090909,1,1
21,"Then , it deploys an iterative inference process to uncover the inferential links that exist between the missing query word , the query , and the document .",Introduction,Introduction,question-answering,5,14,0.1538461538461538,20,0.090497737556561,14,0.6363636363636364,1,1
22,"This phase involves a novel alternating attention mechanism ; it first attends to some parts of the query , then finds their corresponding matches by attending to the document .",Introduction,Introduction,question-answering,5,15,0.1648351648351648,21,0.0950226244343891,15,0.6818181818181818,1,1
23,The result of this alternating search is fed back into the iterative inference process to seed the next search step .,Introduction,Introduction,question-answering,5,16,0.1758241758241758,22,0.0995475113122171,16,0.7272727272727273,1,1
24,"This permits our model to reason about different parts of the query in a sequential way , based on the information that has been gathered previously from the document .",Introduction,Introduction,question-answering,5,17,0.1868131868131868,23,0.1040723981900452,17,0.7727272727272727,1,0
25,"After a fixed number of iterations , the model uses a summary of its inference process to predict the answer .",Introduction,Introduction,question-answering,5,18,0.1978021978021978,24,0.1085972850678733,18,0.8181818181818182,1,1
26,This paper makes the following contributions .,Introduction,Introduction,question-answering,5,19,0.2087912087912088,25,0.1131221719457013,19,0.8636363636363636,1,0
27,"We present a novel iterative , alternating attention mechanism that , unlike existing models , does not compress the query to a single representation , but instead alternates its attention between the query and the document to obtain a fine - grained query representation within a fixed computation time .",Introduction,Introduction,question-answering,5,20,0.2197802197802197,26,0.1176470588235294,20,0.9090909090909092,1,0
28,Our architecture tightly integrates previous ideas related to bidirectional readers and iterative attention processes .,Introduction,Introduction,question-answering,5,21,0.2307692307692307,27,0.1221719457013574,21,0.9545454545454546,1,0
29,It obtains state - of - theart results on two machine comprehension datasets and shows promise for application to abroad range of natural language processing tasks .,Introduction,Introduction,question-answering,5,22,0.2417582417582417,28,0.1266968325791855,22,1.0,1,0
30,Task Description,Introduction,,question-answering,5,23,0.2527472527472527,29,0.1312217194570135,0,0.0,1,0
31,One of the advantages of using Cloze - style questions to evaluate machine comprehension systems is that a sufficient amount of training and test data can be obtained without human intervention .,Introduction,Task Description,question-answering,5,24,0.2637362637362637,30,0.1357466063348416,1,0.0769230769230769,1,0
32,The CBT and corpora are two such datasets .,Introduction,Task Description,question-answering,5,25,0.2747252747252747,31,0.1402714932126696,2,0.1538461538461538,1,0
33,The CBT 1 corpus was generated from well - known children 's books available through Project Gutenberg .,Introduction,Task Description,question-answering,5,26,0.2857142857142857,32,0.1447963800904977,3,0.2307692307692307,1,0
34,Documents consist of 20 - sentence excerpts from these books .,Introduction,Task Description,question-answering,5,27,0.2967032967032967,33,0.1493212669683258,4,0.3076923076923077,1,0
35,The related query is formed from an excerpt 's 21st sentence by replacing a single word with an anonymous placeholder token .,Introduction,Task Description,question-answering,5,28,0.3076923076923077,34,0.1538461538461538,5,0.3846153846153846,1,0
36,The dataset is divided into four subsets depending on the type of the word replaced .,Introduction,Task Description,question-answering,5,29,0.3186813186813186,35,0.1583710407239819,6,0.4615384615384615,1,0
37,"The subsets are named entity , common noun , verb , and preposition .",Introduction,Task Description,question-answering,5,30,0.3296703296703296,36,0.1628959276018099,7,0.5384615384615384,1,0
38,"We will focus our evaluation solely on the first two subsets , i.e. CBT - NE ( named entity ) and CBT - CN ( common nouns ) , since the latter two are relatively simple as demonstrated by .",Introduction,Task Description,question-answering,5,31,0.3406593406593406,37,0.167420814479638,8,0.6153846153846154,1,0
39,The CNN 2 corpus was generated from news articles available through the CNN website .,Introduction,Task Description,question-answering,5,32,0.3516483516483517,38,0.171945701357466,9,0.6923076923076923,1,0
40,"The documents are given by the full articles themselves , which are accompanied by short , bullet - point summary statements .",Introduction,Task Description,question-answering,5,33,0.3626373626373626,39,0.1764705882352941,10,0.7692307692307693,1,0
41,"Instead of extracting a query from the articles themselves , the authors replace a named entity within each article summary with an anonymous placeholder token .",Introduction,Task Description,question-answering,5,34,0.3736263736263736,40,0.1809954751131221,11,0.8461538461538461,1,0
42,"For both datasets , the training and evaluation data consist of tuples ( Q , D , A , a ) , where Q is the query ( represented as a sequence of words ) , Dis the document , A is the set of possible answers , and a ?",Introduction,Task Description,question-answering,5,35,0.3846153846153846,41,0.1855203619909502,12,0.9230769230769232,1,0
43,A is,Introduction,,question-answering,5,36,0.3956043956043956,42,0.1900452488687782,13,1.0,1,0
44,Alternating Iterative Attention,Introduction,,question-answering,5,37,0.4065934065934066,43,0.1945701357466063,0,0.0,1,0
45,Our model is represented in .,Introduction,Alternating Iterative Attention,question-answering,5,38,0.4175824175824176,44,0.1990950226244343,1,0.1428571428571428,1,0
46,It s workflow has three steps .,Introduction,Alternating Iterative Attention,question-answering,5,39,0.4285714285714285,45,0.2036199095022624,2,0.2857142857142857,1,0
47,"First is the encoding phase , in which we compute a set of vector representations , acting as a memory of the content of the input document and query .",Introduction,Alternating Iterative Attention,question-answering,5,40,0.4395604395604395,46,0.2081447963800905,3,0.4285714285714285,1,0
48,"Next , the inference phase aims to untangle the complex semantic relationships linking the document and the query in order to provide sufficiently strong evidence for the answer prediction to be successful .",Introduction,Alternating Iterative Attention,question-answering,5,41,0.4505494505494505,47,0.2126696832579185,4,0.5714285714285714,1,0
49,"To accomplish this , we use an iterative process that , at each iteration , alternates attentive memory accesses to the query and the document .",Introduction,Alternating Iterative Attention,question-answering,5,42,0.4615384615384615,48,0.2171945701357466,5,0.7142857142857143,1,0
50,"Finally , the prediction phase uses the information gathered from the repeated attentions through the query and the document to maximize the probability of the correct answer .",Introduction,Alternating Iterative Attention,question-answering,5,43,0.4725274725274725,49,0.2217194570135746,6,0.8571428571428571,1,0
51,We describe each of the phases in the following sections .,Introduction,Alternating Iterative Attention,question-answering,5,44,0.4835164835164835,50,0.2262443438914027,7,1.0,1,0
52,Bidirectional Encoding,Introduction,,question-answering,5,45,0.4945054945054945,51,0.2307692307692307,0,0.0,1,0
53,"The input to the encoding phase is a sequence of words X = ( x 1 , . . . , x | X | ) , such as a document or a query , drawn from a vocabulary V .",Introduction,Bidirectional Encoding,question-answering,5,46,0.5054945054945055,52,0.2352941176470588,1,0.0476190476190476,1,0
54,Each word is represented by a continuous word embedding x ?,Introduction,Bidirectional Encoding,question-answering,5,47,0.5164835164835165,53,0.2398190045248869,2,0.0952380952380952,1,0
55,Rd stored in a word embedding matrix X ? R | V |d .,Introduction,Bidirectional Encoding,question-answering,5,48,0.5274725274725275,54,0.2443438914027149,3,0.1428571428571428,1,0
56,The sequence X is processed using a recurrent neural network encoder with gated recurrent units ( GRU ) .,Introduction,Bidirectional Encoding,question-answering,5,49,0.5384615384615384,55,0.248868778280543,4,0.1904761904761904,1,0
57,"For each position i in the input sequence , the GRU takes as input the word embedding xi and updates a hidden Figure 1 : Our model first encodes the query and the document by means of bidirectional GRU networks .",Introduction,Bidirectional Encoding,question-answering,5,50,0.5494505494505495,56,0.253393665158371,5,0.238095238095238,1,0
58,"Then , it deploys an iterative inference mechanism that alternates between attending query encodings ( 1 ) and document encodings ( 2 ) given the query attended state .",Introduction,Bidirectional Encoding,question-answering,5,51,0.5604395604395604,57,0.2579185520361991,6,0.2857142857142857,1,0
59,The results of the alternating attention is gated and fed back into the inference GRU .,Introduction,Bidirectional Encoding,question-answering,5,52,0.5714285714285714,58,0.2624434389140271,7,0.3333333333333333,1,0
60,"Even if the encodings are computed only once , the query representation is dynamic and changes throughout the inference process .",Introduction,Bidirectional Encoding,question-answering,5,53,0.5824175824175825,59,0.2669683257918552,8,0.3809523809523809,1,0
61,"After a fixed number of steps T , the weights of the document attention are used to estimate the probability of the answer P ( a|Q , D ) .",Introduction,Bidirectional Encoding,question-answering,5,54,0.5934065934065934,60,0.2714932126696832,9,0.4285714285714285,1,0
62,by :,Introduction,Bidirectional Encoding,question-answering,5,55,0.6043956043956044,61,0.2760180995475113,10,0.4761904761904761,1,0
63,"where hi , r i and u i ?",Introduction,Bidirectional Encoding,question-answering,5,56,0.6153846153846154,62,0.2805429864253393,11,0.5238095238095238,1,0
64,"Rh are the recurrent state , the reset gate and update gate respectively , I {r , u , h} ?",Introduction,Bidirectional Encoding,question-answering,5,57,0.6263736263736264,63,0.2850678733031674,12,0.5714285714285714,1,0
65,"R hd , H {r , u , h} ?",Introduction,Bidirectional Encoding,question-answering,5,58,0.6373626373626373,64,0.2895927601809955,13,0.6190476190476191,1,0
66,"R hh are the parameters of the GRU , ?",Introduction,Bidirectional Encoding,question-answering,5,59,0.6483516483516484,65,0.2941176470588235,14,0.6666666666666666,1,0
67,is the sigmoid function and is the elementwise multiplication .,Introduction,Bidirectional Encoding,question-answering,5,60,0.6593406593406593,66,0.2986425339366516,15,0.7142857142857143,1,0
68,The hidden state hi acts as a representation of the word x i in the context of the preceding sequence inputs x < i .,Introduction,Bidirectional Encoding,question-answering,5,61,0.6703296703296703,67,0.3031674208144796,16,0.7619047619047619,1,0
69,"In order to incorporate information from the future tokens x > i , we choose to process the sequence in reverse with an additional GRU .",Introduction,Bidirectional Encoding,question-answering,5,62,0.6813186813186813,68,0.3076923076923077,17,0.8095238095238095,1,0
70,"Therefore , the encoding phase maps each token xi to a contextual representation given by the concatenation of the forward and backward GRU hidden",Introduction,Bidirectional Encoding,question-answering,5,63,0.6923076923076923,69,0.3122171945701357,18,0.8571428571428571,1,0
71,We denote byq i ?,Introduction,Bidirectional Encoding,question-answering,5,64,0.7032967032967034,70,0.3167420814479638,19,0.9047619047619048,1,0
72,R 2h andd i ?,Introduction,Bidirectional Encoding,question-answering,5,65,0.7142857142857143,71,0.3212669683257919,20,0.9523809523809524,1,0
73,R 2h the contextual encodings for word i in the query Q and the document D respectively .,Introduction,Bidirectional Encoding,question-answering,5,66,0.7252747252747253,72,0.3257918552036199,21,1.0,1,0
74,Iterative Alternating Attention,Introduction,,question-answering,5,67,0.7362637362637363,73,0.3303167420814479,0,0.0,1,0
75,This phase can be considered a means to uncover a possible inference chain that starts at the query and the document and leads to the answer .,Introduction,Iterative Alternating Attention,question-answering,5,68,0.7472527472527473,74,0.334841628959276,1,0.0625,1,0
76,The inference is modelled by an additional recurrent GRU network .,Introduction,Iterative Alternating Attention,question-answering,5,69,0.7582417582417582,75,0.3393665158371041,2,0.125,1,0
77,The recurrent network iteratively performs an alternating search step to gather information that maybe useful to predict the answer .,Introduction,Iterative Alternating Attention,question-answering,5,70,0.7692307692307693,76,0.3438914027149321,3,0.1875,1,0
78,"In particular , at each time step : ( 1 ) it performs an attentive read on the query encodings , resulting in a query glimpse , qt , and ( 2 ) given the current query glimpse , it extracts a conditional document glimpse , d t , representing the parts of the document that are relevant to the current query glimpse .",Introduction,Iterative Alternating Attention,question-answering,5,71,0.7802197802197802,77,0.3484162895927601,4,0.25,1,0
79,"In turn , both attentive reads are conditioned on the previous hidden state of the inference GRU s t ?1 , summarizing the information that has been gathered from the query and the document up to time t.",Introduction,Iterative Alternating Attention,question-answering,5,72,0.7912087912087912,78,0.3529411764705882,5,0.3125,1,0
80,The inference GRU uses both glimpses to update its recurrent state and thus decides which information needs to be gathered to complete the inference process .,Introduction,Iterative Alternating Attention,question-answering,5,73,0.8021978021978022,79,0.3574660633484163,6,0.375,1,0
81,Query Attentive Read,Introduction,,question-answering,5,74,0.8131868131868132,80,0.3619909502262443,7,0.4375,1,0
82,"Given the query encodings {q i } , we formulate a query glimpse qt at timestep t by :",Introduction,Query Attentive Read,question-answering,5,75,0.8241758241758241,81,0.3665158371040724,8,0.5,1,0
83,"where q i , tare the query attention weights and A q ?",Introduction,Query Attentive Read,question-answering,5,76,0.8351648351648352,82,0.3710407239819004,9,0.5625,1,0
84,"R 2hs , where sis the dimensionality of the inference GRU state , and a q ?",Introduction,Query Attentive Read,question-answering,5,77,0.8461538461538461,83,0.3755656108597285,10,0.625,1,0
85,R 2 h .,Introduction,Query Attentive Read,question-answering,5,78,0.8571428571428571,84,0.3800904977375565,11,0.6875,1,0
86,"The attention we use here is similar to the formulation used in , but with two differences .",Introduction,Query Attentive Read,question-answering,5,79,0.8681318681318682,85,0.3846153846153846,12,0.75,1,0
87,"First , we use a bilinear term instead of a simple dot product in order to compute the importance of each query term in the current time step .",Introduction,Query Attentive Read,question-answering,5,80,0.8791208791208791,86,0.3891402714932127,13,0.8125,1,0
88,This simple bilinear attention has been successfully used in .,Introduction,Query Attentive Read,question-answering,5,81,0.8901098901098901,87,0.3936651583710407,14,0.875,1,0
89,"Second , we add a term a q that allows to bias the attention mechanism towards words which tend to be important across the questions independently of the search key s t?1 .",Introduction,Query Attentive Read,question-answering,5,82,0.9010989010989012,88,0.3981900452488687,15,0.9375,1,0
90,This is similar to what is achieved by the original attention mechanism proposed in without the burden of the additional tanh layer .,Introduction,Query Attentive Read,question-answering,5,83,0.912087912087912,89,0.4027149321266968,16,1.0,1,0
91,Document Attentive Read,Introduction,,question-answering,5,84,0.9230769230769232,90,0.4072398190045249,0,0.0,1,0
92,The alternating attention continues by probing the document given the current query glimpse qt .,Introduction,Document Attentive Read,question-answering,5,85,0.934065934065934,91,0.4117647058823529,1,0.1428571428571428,1,0
93,"In particular , the document attention weights are computed based on both the previous search state and the currently selected query glimpse qt :",Introduction,Document Attentive Read,question-answering,5,86,0.945054945054945,92,0.416289592760181,2,0.2857142857142857,1,0
94,"where d i , tare the attention weights for each word in the document and A d ?",Introduction,Document Attentive Read,question-answering,5,87,0.956043956043956,93,0.420814479638009,3,0.4285714285714285,1,0
95,R 2 h ( s + 2h ) and ad ?,Introduction,Document Attentive Read,question-answering,5,88,0.967032967032967,94,0.4253393665158371,4,0.5714285714285714,1,0
96,R 2 h .,Introduction,Document Attentive Read,question-answering,5,89,0.978021978021978,95,0.4298642533936652,5,0.7142857142857143,1,0
97,Note that the document attention is also conditioned on s t?1 .,Introduction,Document Attentive Read,question-answering,5,90,0.989010989010989,96,0.4343891402714932,6,0.8571428571428571,1,0
98,"This allows the model to perform transitive reasoning on the document side , i.e. to use previously obtained document information to bias future attended locations , which is particularly important for natural language inference tasks .",Introduction,Document Attentive Read,question-answering,5,91,1.0,97,0.4389140271493212,7,1.0,1,0
99,Gating Search Results,,,question-answering,5,0,0.0,98,0.4434389140271493,0,0.0,1,0
100,"In order to update its recurrent state , the inference GRU may evolve on the basis of the information gathered from the current inference step , i.e.",Gating Search Results,Gating Search Results,question-answering,5,1,0.0588235294117647,99,0.4479638009049774,1,0.0833333333333333,1,0
101,"However , the current query glimpse maybe too general or the document may not contain the information specified in the query glimpse , i.e. the query or the document attention weights maybe nearly uniform .",Gating Search Results,Gating Search Results,question-answering,5,2,0.1176470588235294,100,0.4524886877828054,2,0.1666666666666666,1,0
102,We include a gating mechanism that is designed to reset the current query and document glimpses in the case that the current search is not fruitful .,Gating Search Results,Gating Search Results,question-answering,5,3,0.1764705882352941,101,0.4570135746606334,3,0.25,1,0
103,"Formally , we implement a gating mech -",Gating Search Results,Gating Search Results,question-answering,5,4,0.2352941176470588,102,0.4615384615384615,4,0.3333333333333333,1,0
104,", where is the element - wise multiplication and g :",Gating Search Results,Gating Search Results,question-answering,5,5,0.2941176470588235,103,0.4660633484162896,5,0.4166666666666667,1,0
105,R s+6h ?,Gating Search Results,Gating Search Results,question-answering,5,6,0.3529411764705882,104,0.4705882352941176,6,0.5,1,0
106,R 2 h .,Gating Search Results,Gating Search Results,question-answering,5,7,0.4117647058823529,105,0.4751131221719457,7,0.5833333333333334,1,0
107,The gate g takes the form of a 2 - layer feed - forward network with sigmoid output unit activation .,Gating Search Results,Gating Search Results,question-answering,5,8,0.4705882352941176,106,0.4796380090497738,8,0.6666666666666666,1,0
108,"The fourth argument of the gate takes into account multiplicative interactions between query and document glimpses , making it easier to determine the degree of matching between them .",Gating Search Results,Gating Search Results,question-answering,5,9,0.5294117647058824,107,0.4841628959276018,9,0.75,1,0
109,"Given a query gate g q , producing r q , and a document gate g d , producing rd , the inputs of the inference GRU are given by the reset version of the query and document glimpses , i.e. ,",Gating Search Results,Gating Search Results,question-answering,5,10,0.5882352941176471,108,0.4886877828054298,10,0.8333333333333334,1,0
110,.,Gating Search Results,Gating Search Results,question-answering,5,11,0.6470588235294118,109,0.4932126696832579,11,0.9166666666666666,1,0
111,"Intuitively , the model reviews the query glimpse with respect to the contents of the document glimpse and vice versa .",Gating Search Results,Gating Search Results,question-answering,5,12,0.7058823529411765,110,0.497737556561086,12,1.0,1,0
112,Answer Prediction,Gating Search Results,,question-answering,5,13,0.7647058823529411,111,0.502262443438914,0,0.0,1,0
113,"After a fixed number of time - steps T , the document attention weights obtained in the last search step d i , T are used to predict the probability of the answer given the document and the query P ( a|Q , D ) .",Gating Search Results,Answer Prediction,question-answering,5,14,0.8235294117647058,112,0.5067873303167421,1,0.25,1,0
114,"Formally , we follow and apply the "" pointer - sum "" loss :",Gating Search Results,Answer Prediction,question-answering,5,15,0.8823529411764706,113,0.5113122171945701,2,0.5,1,0
115,"where I ( a , D ) is a set of positions where a occurs in the document .",Gating Search Results,Answer Prediction,question-answering,5,16,0.9411764705882352,114,0.5158371040723982,3,0.75,1,0
116,"The model is trained to maximize log P ( a|Q , D ) over the training corpus .",Gating Search Results,Answer Prediction,question-answering,5,17,1.0,115,0.5203619909502263,4,1.0,1,0
117,Training Details,,,question-answering,5,0,0.0,116,0.5248868778280543,0,0.0,1,0
118,"To train our model , we used stochastic gradient descent with the ADAM optimizer ( Kingma and Ba , 2014 ) , with an initial learning rate of 0.001 .",Training Details,Training Details,question-answering,5,1,0.0833333333333333,117,0.5294117647058824,1,0.0833333333333333,1,1
119,"We set the batch size to 32 and we decay the learning rate by 0.8 if the accuracy on the validation set does not increase after a half - epoch , i.e. 2000 batches ( for CBT ) and 5000 batches for ( CNN ) .",Training Details,Training Details,question-answering,5,2,0.1666666666666666,118,0.5339366515837104,2,0.1666666666666666,1,1
120,"We initialize all weights of our model by sampling from the normal distribution N ( 0 , 0.05 ) .",Training Details,Training Details,question-answering,5,3,0.25,119,0.5384615384615384,3,0.25,1,1
121,"Following , the GRU recurrent weights are initialized to be orthogonal and biases are initialized to zero .",Training Details,Training Details,question-answering,5,4,0.3333333333333333,120,0.5429864253393665,4,0.3333333333333333,1,1
122,"In order to stabilize the learning , we clip the gradients if their norm is greater than 5 and those marked with 2 are from .",Training Details,Training Details,question-answering,5,5,0.4166666666666667,121,0.5475113122171946,5,0.4166666666666667,1,1
123,the inputs to both the query and the document attention mechanisms .,Training Details,Training Details,question-answering,5,6,0.5,122,0.5520361990950227,6,0.5,1,0
124,"We found that setting embedding regularization to 0.0001 , T = 8 , d = 384 , h = 128 , s = 512 worked robustly across the datasets .",Training Details,Training Details,question-answering,5,7,0.5833333333333334,123,0.5565610859728507,7,0.5833333333333334,1,0
125,"Our model is implemented in Theano , using the Keras library .",Training Details,Training Details,question-answering,5,8,0.6666666666666666,124,0.5610859728506787,8,0.6666666666666666,1,1
126,"Computational Complexity Similar to previous state - of - the - art models which use a bidirectional encoder , the major bottleneck of our method is computing the document and query encodings .",Training Details,Training Details,question-answering,5,9,0.75,125,0.5656108597285068,9,0.75,1,0
127,"The alternating attention mechanism runs only fora fixed number of steps ( T = 8 in our tests ) , which is orders of magnitude smaller than atypical document or query in our datasets ( see ) .",Training Details,Training Details,question-answering,5,10,0.8333333333333334,126,0.5701357466063348,10,0.8333333333333334,1,0
128,The repeated attentions each require a softmax over ? 1000 locations which is typically fast on recent GPU architectures .,Training Details,Training Details,question-answering,5,11,0.9166666666666666,127,0.5746606334841629,11,0.9166666666666666,1,0
129,"Thus , our computation cost is comparable to , but we outperform the latter models on the datasets tested .",Training Details,Training Details,question-answering,5,12,1.0,128,0.579185520361991,12,1.0,1,0
130,Results,,,question-answering,5,0,0.0,129,0.583710407239819,0,0.0,1,0
131,"We report the results of our model on the CBT - CN , CBT - NE and CNN datasets , previously described in Section 2 . reports our results on the CBT - CN and CBT - NE dataset .",Results,Results,question-answering,5,1,0.3333333333333333,130,0.5882352941176471,1,0.5,1,0
132,"The Humans , LSTMs and Memory Networks ( Mem NNs ) results are taken from and the Attention - Sum Reader ( AS Reader ) is a state - of - the - art result recently obtained by .",Results,Results,question-answering,5,2,0.6666666666666666,131,0.5927601809954751,2,1.0,1,0
133,CBT,Results,,question-answering,5,3,1.0,132,0.5972850678733032,0,0.0,1,0
134,Main result,,,question-answering,5,0,0.0,133,0.6018099547511312,0,0.0,1,0
135,Our model ( line 7 ) sets anew stateof - the - art on the common noun category by gaining 3.6 and 5.6 points in validation and test over the best baseline AS Reader ( line 5 ) .,Main result,Main result,question-answering,5,1,0.0666666666666666,134,0.6063348416289592,1,0.0714285714285714,1,1
136,This performance gap is only partially reflected on the CBT - NE dataset .,Main result,Main result,question-answering,5,2,0.1333333333333333,135,0.6108597285067874,2,0.1428571428571428,1,0
137,"We observe that the 1.4 accuracy points on the validation set do not reflect better performance on the test set , which sits on par with the best baseline .",Main result,Main result,question-answering,5,3,0.2,136,0.6153846153846154,3,0.2142857142857142,1,0
138,"In CBT - NE , the missing word is a named entity appearing in the story which is likely to be less frequent than a common noun .",Main result,Main result,question-answering,5,4,0.2666666666666666,137,0.6199095022624435,4,0.2857142857142857,1,0
139,We found that approximatively 27.5 % of validation examples and 29.6 % of test examples contain an answer that has never been predicted in the training set .,Main result,Main result,question-answering,5,5,0.3333333333333333,138,0.6244343891402715,5,0.3571428571428571,1,0
140,"These numbers are considerably lower for the CBT - CN , for which only 2.5 % and 4.6 % of validation and test examples respectively contain an answer that has not been previously seen .",Main result,Main result,question-answering,5,6,0.4,139,0.6289592760180995,6,0.4285714285714285,1,0
141,Ensembles Fusing multiple models generally achieves better generalization .,Main result,Main result,question-answering,5,7,0.4666666666666667,140,0.6334841628959276,7,0.5,1,0
142,"In order to investigate whether this could help achieving better held - out performance on CBT - NE , we adopt a simple strategy and average the predictions of 5 models trained with different random seeds ( line 9 , 3 from and 4 from .",Main result,Main result,question-answering,5,8,0.5333333333333333,141,0.6380090497737556,8,0.5714285714285714,1,0
143,improvements over the single model and sits at 74.1 on validation and 71.0 on test .,Main result,Main result,question-answering,5,9,0.6,142,0.6425339366515838,9,0.6428571428571429,1,0
144,Fixed query attention,Main result,Main result,question-answering,5,10,0.6666666666666666,143,0.6470588235294118,10,0.7142857142857143,1,0
145,"In order to measure the impact of the query attention step in our model , we constrain the query attention weights q i , t to be uniform , i.e. q i , t = 1 / | Q | , for all t = 1 , . . . , T ( line 6 ) .",Main result,Main result,question-answering,5,11,0.7333333333333333,144,0.6515837104072398,11,0.7857142857142857,1,0
146,This corresponds to fixing the query representation to the average pooling over the bidirectional query encodings and is similar in spirit to previous work .,Main result,Main result,question-answering,5,12,0.8,145,0.6561085972850679,12,0.8571428571428571,1,0
147,"By comparing line 6 and line 7 , we see that the query attention mechanism allows improvements up to 2.3 points in validation and 4.9 points in test with respect to fixing the query representation throughout the search process .",Main result,Main result,question-answering,5,13,0.8666666666666667,146,0.6606334841628959,13,0.9285714285714286,1,0
148,A similar scenario was observed on the CNN dataset ..,Main result,Main result,question-answering,5,14,0.9333333333333332,147,0.665158371040724,14,1.0,1,0
149,CNN,Main result,,question-answering,5,15,1.0,148,0.669683257918552,0,0.0,1,0
150,Main result,,,question-answering,5,0,0.0,149,0.6742081447963801,0,0.0,1,0
151,The results show that our model ( line 8 ) improves state - of - the - art accuracy by 4 percent absolute on validation and 3.4 on test with respect to the most recent published result ( AS Reader ) ( line 7 ) .,Main result,Main result,question-answering,5,1,0.0232558139534883,150,0.6787330316742082,1,0.05,1,1
152,We also report the very recent results of the Stanford AR system that came to our attention during the writeup of this article ) ( line 9 ) .,Main result,Main result,question-answering,5,2,0.0465116279069767,151,0.6832579185520362,2,0.1,1,0
153,Our model slightly improves over this strong baseline by 0.2 percent on validation and 0.9 percent on test .,Main result,Main result,question-answering,5,3,0.0697674418604651,152,0.6877828054298643,3,0.15,1,0
154,We note that the latter comparison maybe influenced by different training and initialization strategies .,Main result,Main result,question-answering,5,4,0.0930232558139534,153,0.6923076923076923,4,0.2,1,0
155,"First , Stanford AS uses Glo Ve embeddings , pre-trained from a large external corpus .",Main result,Main result,question-answering,5,5,0.1162790697674418,154,0.6968325791855203,5,0.25,1,0
156,"Second , the system normalizes the output probabilities only over the candidate answers in the document .",Main result,Main result,question-answering,5,6,0.1395348837209302,155,0.7013574660633484,6,0.3,1,0
157,Ensembles,Main result,,question-answering,5,7,0.1627906976744186,156,0.7058823529411765,7,0.35,1,0
158,We also report the results using ensembled models .,Main result,Ensembles,question-answering,5,8,0.1860465116279069,157,0.7104072398190046,8,0.4,1,0
159,"Similarly to the single model case , our ensembles achieve state - of - the - art test performance of 75.2 and 76.1 on validation and test respectively , outperforming previously published results .",Main result,Ensembles,question-answering,5,9,0.2093023255813953,158,0.7149321266968326,9,0.45,1,1
160,Category analysis classified a sample of 100 CNN stories based on the type of inference required to guess the answer .,Main result,Ensembles,question-answering,5,10,0.2325581395348837,159,0.7194570135746606,10,0.5,1,0
161,"Categories that only require local context matching around the placeholder and the answer in the text are Exact Match , Paraphrasing , and Partial Clue , while those which require higher reasoning skills are Multiple Sentences and Ambiguous .",Main result,Ensembles,question-answering,5,11,0.2558139534883721,160,0.7239819004524887,11,0.55,1,0
162,"For example , in Exact Match examples , the question placeholder and the answer in the document share several neighboring exact words .",Main result,Ensembles,question-answering,5,12,0.2790697674418604,161,0.7285067873303167,12,0.6,1,0
163,Category - specific results are reported in : Per-category performance of the Stanford AR and our system .,Main result,Ensembles,question-answering,5,13,0.3023255813953488,162,0.7330316742081447,13,0.65,1,0
164,"The first three categories require local context matching , the next two global context matching and coreference errors are unanswerable questions .",Main result,Ensembles,question-answering,5,14,0.3255813953488372,163,0.7375565610859729,14,0.7,1,0
165,"tackled by the neural models , which perform similarly .",Main result,Ensembles,question-answering,5,15,0.3488372093023256,164,0.7420814479638009,15,0.75,1,0
166,It seems that the iterative alternating attention inference is better able to solve more difficult examples such as Ambiguous / Hard .,Main result,Ensembles,question-answering,5,16,0.3720930232558139,165,0.746606334841629,16,0.8,1,0
167,"One hypothesis is that , in contrast to Stanford AR , which uses only one fixedquery attention step , our iterative attention may better explore the documents and queries .",Main result,Ensembles,question-answering,5,17,0.3953488372093023,166,0.751131221719457,17,0.85,1,0
168,"Finally , Coreference Errors ( ? 25 % of the corpus ) includes examples with critical coreference resolution errors which may make the questions "" unanswerable "" .",Main result,Ensembles,question-answering,5,18,0.4186046511627907,167,0.755656108597285,18,0.9,1,0
169,This is a barrier to achieving accuracies considerably above 75 % .,Main result,Ensembles,question-answering,5,19,0.4418604651162791,168,0.7601809954751131,19,0.95,1,0
170,"If this estimate is accurate , our ensemble model ( 76.1 % ) maybe approaching near-optimal performance on this dataset .",Main result,Ensembles,question-answering,5,20,0.4651162790697674,169,0.7647058823529411,20,1.0,1,0
171,Discussion,Main result,,question-answering,5,21,0.4883720930232558,170,0.7692307692307693,0,0.0,1,0
172,We inspect the query and document attention weights for an example article from the CNN dataset .,Main result,Discussion,question-answering,5,22,0.5116279069767442,171,0.7737556561085973,1,0.0454545454545454,1,0
173,"The title of the article is "" Dante turns in his grave as Italian language declines "" , and it discusses the decline of Italian language in schools .",Main result,Discussion,question-answering,5,23,0.5348837209302325,172,0.7782805429864253,2,0.0909090909090909,1,0
174,"The plot is shown in . 2 , where locations attended to in the query and document are in the left and right column respectively .",Main result,Discussion,question-answering,5,24,0.5581395348837209,173,0.7828054298642534,3,0.1363636363636363,1,0
175,Each row corresponds to an inference timestep 1 ? t ?,Main result,Discussion,question-answering,5,25,0.5813953488372093,174,0.7873303167420814,4,0.1818181818181818,1,0
176,"8 . At the first step , the query attention focuses on the placeholder token , as its local context is generally important to discriminate the answer .",Main result,Discussion,question-answering,5,26,0.6046511627906976,175,0.7918552036199095,5,0.2272727272727272,1,0
177,"The model first focuses on @entity148 , which corresponds to "" Greek "" in this The approach to teaching @entity6 in @placeholder schools needs a makeover , she says : Visualization of the alternated attention mechanism for an article in CNN , treating about the decline of the Italian language in schools .",Main result,Discussion,question-answering,5,27,0.627906976744186,176,0.7963800904977375,6,0.2727272727272727,1,0
178,The title of the plot is the query .,Main result,Discussion,question-answering,5,28,0.6511627906976745,177,0.8009049773755657,7,0.3181818181818182,1,0
179,Each row correspond to a timestep .,Main result,Discussion,question-answering,5,29,0.6744186046511628,178,0.8054298642533937,8,0.3636363636363636,1,0
180,"The target is @entity3 which corresponds to the word "" Italian "" .",Main result,Discussion,question-answering,5,30,0.6976744186046512,179,0.8099547511312217,9,0.4090909090909091,1,0
181,across document locations ) .,Main result,Discussion,question-answering,5,31,0.7209302325581395,180,0.8144796380090498,10,0.4545454545454545,1,0
182,"At t = 2 , the query attention moves towards "" schools "" and the model hesitates between "" Italian "" and "" European Union "" ( @entity28 , see step 3 ) , both of which may satisfy the query .",Main result,Discussion,question-answering,5,32,0.7441860465116279,181,0.8190045248868778,11,0.5,1,0
183,"At step 3 , the most likely candidates are "" European Union "" and "" Rome "" ( @entity159 ) .",Main result,Discussion,question-answering,5,33,0.7674418604651163,182,0.8235294117647058,12,0.5454545454545454,1,0
184,"As the timesteps unfold , the model learns that "" needs "" maybe important to infer the correct entity , i.e. "" Italian "" .",Main result,Discussion,question-answering,5,34,0.7906976744186046,183,0.8280542986425339,13,0.5909090909090909,1,0
185,"The query sits on the same attended location , while the document attention evolves to become more confident about the answer .",Main result,Discussion,question-answering,5,35,0.813953488372093,184,0.832579185520362,14,0.6363636363636364,1,0
186,"We find that , across CBT and CNN examples , the query attention wanders near or focuses on the placeholder location , attempting to discriminate its identity using only local context .",Main result,Discussion,question-answering,5,36,0.8372093023255814,185,0.8371040723981901,15,0.6818181818181818,1,0
187,"For these particular datasets , the majority of questions can be answered after attending only to the words directly neighbouring the placeholder .",Main result,Discussion,question-answering,5,37,0.8604651162790697,186,0.8416289592760181,16,0.7272727272727273,1,0
188,"This aligns with the findings of concerning CNN , which state that the required reasoning and inference levels for this dataset are quite simple .",Main result,Discussion,question-answering,5,38,0.8837209302325582,187,0.8461538461538461,17,0.7727272727272727,1,0
189,"It would be worthwhile to formulate a dataset in which the placeholder is harder to infer using only local neighboring words , and thereby necessitates deeper query exploration .",Main result,Discussion,question-answering,5,39,0.9069767441860463,188,0.8506787330316742,18,0.8181818181818182,1,0
190,"Finally , across this work we fixed the number of inference steps T .",Main result,Discussion,question-answering,5,40,0.9302325581395348,189,0.8552036199095022,19,0.8636363636363636,1,0
191,We found that using 8 timesteps works well consistently across the tested datasets .,Main result,Discussion,question-answering,5,41,0.9534883720930232,190,0.8597285067873304,20,0.9090909090909092,1,0
192,"However , we hypothesize that more ( fewer ) timesteps would benefit harder ( easier ) examples .",Main result,Discussion,question-answering,5,42,0.9767441860465116,191,0.8642533936651584,21,0.9545454545454546,1,0
193,A straight - forward extension of the model would be to dynamically select the number of inference steps conditioned on each example .,Main result,Discussion,question-answering,5,43,1.0,192,0.8687782805429864,22,1.0,1,0
194,Related Works,,,question-answering,5,0,0.0,193,0.8733031674208145,0,0.0,1,0
195,Neural attention models have been applied recently to a smrgsbord of machine learning and natural language processing problems .,Related Works,Related Works,question-answering,5,1,0.0526315789473684,194,0.8778280542986425,1,0.0526315789473684,0,0
196,"These include , but are not limited to , handwriting recognition , digit classification , machine translation , question answering and caption generation .",Related Works,Related Works,question-answering,5,2,0.1052631578947368,195,0.8823529411764706,2,0.1052631578947368,0,0
197,"In general , attention models keep a memory of states that can be accessed at will by learned attention policies .",Related Works,Related Works,question-answering,5,3,0.1578947368421052,196,0.8868778280542986,3,0.1578947368421052,0,0
198,"In our case , the memory is represented by the set of document and query contextual encodings .",Related Works,Related Works,question-answering,5,4,0.2105263157894736,197,0.8914027149321267,4,0.2105263157894736,0,0
199,"Our model is closely related to , which were also applied to question answering .",Related Works,Related Works,question-answering,5,5,0.2631578947368421,198,0.8959276018099548,5,0.2631578947368421,0,0
200,"The pointer - style attention mechanism that we use to perform the final answer prediction has been proposed by , which in turn was based on the earlier Pointer Networks of .",Related Works,Related Works,question-answering,5,6,0.3157894736842105,199,0.9004524886877828,6,0.3157894736842105,0,0
201,"However , differently from our work , perform only one attention step and embed the query into a single vector representation , corresponding to the concatenation of the last state of the forward and backward GRU networks .",Related Works,Related Works,question-answering,5,7,0.3684210526315789,200,0.9049773755656108,7,0.3684210526315789,0,0
202,"To our knowledge , embedding the query into a single vector representation is a choice that is shared by most machine reading comprehension models .",Related Works,Related Works,question-answering,5,8,0.4210526315789473,201,0.9095022624434388,8,0.4210526315789473,0,0
203,"In our model , the repeated , tight integration between query attention and document attention allows the model to explore dynamically which parts of the query are most important to predict the answer , and then to focus on the parts of the document that are most salient to the currently - attended query components .",Related Works,Related Works,question-answering,5,9,0.4736842105263157,202,0.9140271493212668,9,0.4736842105263157,0,0
204,A similar attempt in attending different components of the query maybe found in .,Related Works,Related Works,question-answering,5,10,0.5263157894736842,203,0.918552036199095,10,0.5263157894736842,0,0
205,"In that model , the document is processed once for each query word .",Related Works,Related Works,question-answering,5,11,0.5789473684210527,204,0.9230769230769232,11,0.5789473684210527,0,0
206,"This can be computationally intractable for large documents , since it involves unrolling a bidirectional recurrent neural network over the entire document multiple times .",Related Works,Related Works,question-answering,5,12,0.631578947368421,205,0.9276018099547512,12,0.631578947368421,0,0
207,"In contrast , our model only estimates query and document encodings once and can learn how to attend different parts of those encodings in a fixed number of steps .",Related Works,Related Works,question-answering,5,13,0.6842105263157895,206,0.9321266968325792,13,0.6842105263157895,0,0
208,The inference network is responsible for making sense of the current attention step with respect to what has been gathered before .,Related Works,Related Works,question-answering,5,14,0.7368421052631579,207,0.9366515837104072,14,0.7368421052631579,0,0
209,"In addition to achieving state - of the - art performance , this technique may also prove to be more scalable than alternative query attention models .",Related Works,Related Works,question-answering,5,15,0.7894736842105263,208,0.9411764705882352,15,0.7894736842105263,0,0
210,"Finally , our iterative inference process shares similarities to the iterative hops in Memory Networks .",Related Works,Related Works,question-answering,5,16,0.8421052631578947,209,0.9457013574660632,16,0.8421052631578947,0,0
211,"In that model , the query representation is updated iteratively from hop to hop , although its different components are not attended to separately .",Related Works,Related Works,question-answering,5,17,0.8947368421052632,210,0.9502262443438914,17,0.8947368421052632,0,0
212,"Moreover , we substitute the simple linear update with a GRU network .",Related Works,Related Works,question-answering,5,18,0.9473684210526316,211,0.9547511312217196,18,0.9473684210526316,0,0
213,The gating mechanism of the GRU network made it possible to use multiple steps of attention and to propagate the learning signal effectively back through to the first timestep .,Related Works,Related Works,question-answering,5,19,1.0,212,0.9592760180995475,19,1.0,0,0
214,Conclusion,,,question-answering,5,0,0.0,213,0.9638009049773756,0,0.0,1,0
215,We presented an iterative neural attention model and applied it to machine comprehension tasks .,Conclusion,Conclusion,question-answering,5,1,0.1428571428571428,214,0.9683257918552036,1,0.1428571428571428,0,0
216,"Our architecture deploys a novel alternating attention mechanism , and tightly integrates successful ideas from past works in machine reading comprehension to obtain state - of - the - art results on three datasets .",Conclusion,Conclusion,question-answering,5,2,0.2857142857142857,215,0.9728506787330315,2,0.2857142857142857,0,0
217,The iterative alternating attention mechanism continually refines its view of the query and document while aggregating the information required to answer a query .,Conclusion,Conclusion,question-answering,5,3,0.4285714285714285,216,0.9773755656108596,3,0.4285714285714285,0,0
218,Multiple future research directions maybe envisioned .,Conclusion,Conclusion,question-answering,5,4,0.5714285714285714,217,0.9819004524886876,4,0.5714285714285714,0,0
219,We plan to dynamically select the optimal number of inference steps required for each example .,Conclusion,Conclusion,question-answering,5,5,0.7142857142857143,218,0.986425339366516,5,0.7142857142857143,0,0
220,"Moreover , we suspect that shifting towards stochastic attention should permit us to learn more interesting search policies .",Conclusion,Conclusion,question-answering,5,6,0.8571428571428571,219,0.990950226244344,6,0.8571428571428571,0,0
221,"Finally , we believe that our model is fully general and maybe applied in a straightforward way to other tasks such as information retrieval .",Conclusion,Conclusion,question-answering,5,7,1.0,220,0.995475113122172,7,1.0,0,0
1,title,,,question-answering,6,0,0.0,0,0.0,0,0.0,1,0
2,Published as a conference paper at ICLR 2017 QUERY - REDUCTION NETWORKS FOR QUESTION ANSWERING,title,title,question-answering,6,1,0.0,1,0.003030303030303,1,0.0,1,1
3,abstract,,,question-answering,6,0,0.0,2,0.006060606060606,0,0.0,1,0
4,"In this paper , we study the problem of question answering when reasoning over multiple facts is required .",abstract,abstract,question-answering,6,1,0.2,3,0.009090909090909,1,0.2,1,1
5,"We propose Query - Reduction Network ( QRN ) , a variant of Recurrent Neural Network ( RNN ) that effectively handles both short - term ( local ) and long - term ( global ) sequential dependencies to reason over multiple facts .",abstract,abstract,question-answering,6,2,0.4,4,0.0121212121212121,2,0.4,1,0
6,"QRN considers the context sentences as a sequence of state - changing triggers , and reduces the original query to a more informed query as it observes each trigger ( context sentence ) through time .",abstract,abstract,question-answering,6,3,0.6,5,0.0151515151515151,3,0.6,1,0
7,"Our experiments show that QRN produces the state - of - the - art results in bAbI QA and dialog tasks , and in areal goal - oriented dialog dataset .",abstract,abstract,question-answering,6,4,0.8,6,0.0181818181818181,4,0.8,1,0
8,"In addition , QRN formulation allows parallelization on RNN 's time axis , saving an order of magnitude in time complexity for training and inference .",abstract,abstract,question-answering,6,5,1.0,7,0.0212121212121212,5,1.0,1,0
9,INTRODUCTION,,,question-answering,6,0,0.0,8,0.0242424242424242,0,0.0,1,0
10,"In this paper , we address the problem of question answering ( QA ) when reasoning over multiple facts is required .",INTRODUCTION,INTRODUCTION,question-answering,6,1,0.032258064516129,9,0.0272727272727272,1,0.032258064516129,1,0
11,"For example , consider we know that Frogs eat insects and Flies are insects .",INTRODUCTION,INTRODUCTION,question-answering,6,2,0.064516129032258,10,0.0303030303030303,2,0.064516129032258,1,0
12,Then answering Do frogs eat flies ?,INTRODUCTION,INTRODUCTION,question-answering,6,3,0.0967741935483871,11,0.0333333333333333,3,0.0967741935483871,1,0
13,requires reasoning over both of the above facts .,INTRODUCTION,INTRODUCTION,question-answering,6,4,0.1290322580645161,12,0.0363636363636363,4,0.1290322580645161,1,0
14,"Question answering , more specifically context - based QA , has been extensively studied in machine comprehension tasks .",INTRODUCTION,INTRODUCTION,question-answering,6,5,0.1612903225806451,13,0.0393939393939393,5,0.1612903225806451,1,0
15,"However , most of the datasets are primarily focused on lexical and syntactic understanding , and hardly concentrate on inference over multiple facts .",INTRODUCTION,INTRODUCTION,question-answering,6,6,0.1935483870967742,14,0.0424242424242424,6,0.1935483870967742,1,0
16,"Recently , several datasets aimed for testing multi-hop reasoning have emerged ; among them are story - based QA and the dialog task .",INTRODUCTION,INTRODUCTION,question-answering,6,7,0.2258064516129032,15,0.0454545454545454,7,0.2258064516129032,1,0
17,"Recurrent Neural Network ( RNN ) and its variants , such as Long Short - Term Memory ( LSTM ) and Gated Recurrent Unit ( GRU ) , are popular choices for modeling natural language .",INTRODUCTION,INTRODUCTION,question-answering,6,8,0.2580645161290322,16,0.0484848484848484,8,0.2580645161290322,1,0
18,"However , when used for multi-hop reasoning in question answering , purely RNN - based models have shown to perform poorly .",INTRODUCTION,INTRODUCTION,question-answering,6,9,0.2903225806451613,17,0.0515151515151515,9,0.2903225806451613,1,0
19,This is largely due to the fact that RNN 's internal memory is inherently unstable over along term .,INTRODUCTION,INTRODUCTION,question-answering,6,10,0.3225806451612903,18,0.0545454545454545,10,0.3225806451612903,1,0
20,"For this reason , most recent approaches in the literature have mainly relied on global attention mechanism and shared external memory .",INTRODUCTION,INTRODUCTION,question-answering,6,11,0.3548387096774194,19,0.0575757575757575,11,0.3548387096774194,1,0
21,The attention mechanism allows these models to focus on a single sentence in each layer .,INTRODUCTION,INTRODUCTION,question-answering,6,12,0.3870967741935484,20,0.0606060606060606,12,0.3870967741935484,1,0
22,They can sequentially read multiple relevant sentences from the memory with multiple layers to perform multi-hop reasoning .,INTRODUCTION,INTRODUCTION,question-answering,6,13,0.4193548387096774,21,0.0636363636363636,13,0.4193548387096774,1,0
23,"However , one major drawback of these standard attention mechanisms is that they are insensitive to the time step ( memory address ) of the sentences when accessing them .",INTRODUCTION,INTRODUCTION,question-answering,6,14,0.4516129032258064,22,0.0666666666666666,14,0.4516129032258064,1,0
24,"Our proposed model , Query - Reduction Network 1 ( QRN ) , is a single recurrent unit that addresses the long - term dependency problem of most RNN - based models by simplifying the recurrent update , while taking the advantage of RNN 's capability to model sequential data ) .",INTRODUCTION,INTRODUCTION,question-answering,6,15,0.4838709677419355,23,0.0696969696969697,15,0.4838709677419355,1,1
25,"QRN considers the context sentences as a sequence of state - changing triggers , and transforms ( reduces ) the original query to a more informed query as it observes each trigger through time .",INTRODUCTION,INTRODUCTION,question-answering,6,16,0.5161290322580645,24,0.0727272727272727,16,0.5161290322580645,1,1
26,"For instance in , the original question , Where is the apple ? , can not be directly answered by any single sentence from the story .",INTRODUCTION,INTRODUCTION,question-answering,6,17,0.5483870967741935,25,0.0757575757575757,17,0.5483870967741935,1,0
27,"After observing the first sentence , Sandra got the apple there , QRN transforms the original question to a reduced query",INTRODUCTION,INTRODUCTION,question-answering,6,18,0.5806451612903226,26,0.0787878787878787,18,0.5806451612903226,1,0
28,"Where is Sandra ? , which is presumably . ? and ?",INTRODUCTION,INTRODUCTION,question-answering,6,19,0.6129032258064516,27,0.0818181818181818,19,0.6129032258064516,1,0
29,"are update gate and reduce functions , respectively .?",INTRODUCTION,INTRODUCTION,question-answering,6,20,0.6451612903225806,28,0.0848484848484848,20,0.6451612903225806,1,0
30,"is assigned to be h 2 5 , the local query at the last time step in the last layer .",INTRODUCTION,INTRODUCTION,question-answering,6,21,0.6774193548387096,29,0.0878787878787878,21,0.6774193548387096,1,0
31,"Also , red-colored text is the inferred meanings of the vectors ( see. easier to answer than the original question given the context provided by the first sentence .",INTRODUCTION,INTRODUCTION,question-answering,6,22,0.7096774193548387,30,0.0909090909090909,22,0.7096774193548387,1,0
32,"2 Unlike RNN - based models , QRN 's candidate state ( h t in ) does not depend on the previous hidden state ( h t?1 ) .",INTRODUCTION,INTRODUCTION,question-answering,6,23,0.7419354838709677,31,0.0939393939393939,23,0.7419354838709677,1,0
33,"Compared to memory - based approaches , QRN can better encodes locality information because it does not use a global memory access controller ( circle nodes in ) , and the query updates are performed locally .",INTRODUCTION,INTRODUCTION,question-answering,6,24,0.7741935483870968,32,0.0969696969696969,24,0.7741935483870968,1,0
34,"In short , the main contribution of QRN is threefold .",INTRODUCTION,INTRODUCTION,question-answering,6,25,0.8064516129032258,33,0.1,25,0.8064516129032258,1,0
35,"First , QRN is a simple variant of RNN that reduces the query given the context sentences in a differentiable manner .",INTRODUCTION,INTRODUCTION,question-answering,6,26,0.8387096774193549,34,0.103030303030303,26,0.8387096774193549,1,0
36,"Second , QRN is situated between the attention mechanism and RNN , effectively handling time dependency and long - term dependency problems of each technique , respectively .",INTRODUCTION,INTRODUCTION,question-answering,6,27,0.8709677419354839,35,0.106060606060606,27,0.8709677419354839,1,0
37,"Hence it is well - suited for sequential data with both local and global interactions ( note that QRN is not the replacement of RNN , which is arguably better for modeling complex local interactions ) .",INTRODUCTION,INTRODUCTION,question-answering,6,28,0.9032258064516128,36,0.109090909090909,28,0.9032258064516128,1,0
38,"Third , unlike most RNN - based models , QRN can be parallelized overtime by computing candidate reduced queries ( h t ) directly from local input queries ( q t ) and context sentence vectors ( x t ) .",INTRODUCTION,INTRODUCTION,question-answering,6,29,0.935483870967742,37,0.1121212121212121,29,0.935483870967742,1,0
39,"In fact , the parallelizability of QRN implies that QRN does not suffer from the vanishing gradient problem of RNN , hence effectively addressing the long - term dependency .",INTRODUCTION,INTRODUCTION,question-answering,6,30,0.967741935483871,38,0.1151515151515151,30,0.967741935483871,1,0
40,We experimentally demonstrate these contributions by achieving the state - of - the - art results on story - based QA and interactive dialog datasets .,INTRODUCTION,INTRODUCTION,question-answering,6,31,1.0,39,0.1181818181818181,31,1.0,1,0
41,MODEL,,,question-answering,6,0,0.0,40,0.1212121212121212,0,0.0,1,0
42,"In story - based QA ( or dialog dataset ) , the input is the context as a sequence of sentences ( story or past conversations ) and a question in natural language ( equivalent to the user 's last utterance in the dialog ) .",MODEL,MODEL,question-answering,6,1,0.0083333333333333,41,0.1242424242424242,1,0.04,1,0
43,The output is the predicted answer to the question in natural language ( the system 's next utterance in the dialog ) .,MODEL,MODEL,question-answering,6,2,0.0166666666666666,42,0.1272727272727272,2,0.08,1,0
44,The only supervision provided during training is the answer to the question .,MODEL,MODEL,question-answering,6,3,0.025,43,0.1303030303030303,3,0.12,1,0
45,"In this paper we particularly focus on end - to - end solutions , i.e. , the only supervision comes from questions and answers , and we restrain from using manually defined rules or external language resources , such as lexicon or dependency parser .",MODEL,MODEL,question-answering,6,4,0.0333333333333333,44,0.1333333333333333,4,0.16,1,0
46,"Let x 1 , . . . , x",MODEL,,question-answering,6,5,0.0416666666666666,45,0.1363636363636363,5,0.2,1,0
47,"T denote the sequence of sentences , where T is the number of sentences in the story , and let q denote the question .",MODEL,"Let x 1 , . . . , x",question-answering,6,6,0.05,46,0.1393939393939394,6,0.24,1,0
48,Let ?,MODEL,"Let x 1 , . . . , x",question-answering,6,7,0.0583333333333333,47,0.1424242424242424,7,0.28,1,0
49,"denote the predicted answer , and y denote the true answer .",MODEL,"Let x 1 , . . . , x",question-answering,6,8,0.0666666666666666,48,0.1454545454545454,8,0.32,1,0
50,"Our proposed system for end - to - end QA task is divided into three modules ( ) : input module , QRN layers , and output module .",MODEL,"Let x 1 , . . . , x",question-answering,6,9,0.075,49,0.1484848484848485,9,0.36,1,0
51,Input module .,MODEL,,question-answering,6,10,0.0833333333333333,50,0.1515151515151515,10,0.4,1,0
52,"Input module maps each sentence x t and the question q to d-dimensional vector space , x t ?",MODEL,Input module .,question-answering,6,11,0.0916666666666666,51,0.1545454545454545,11,0.44,1,0
53,Rd and qt ?,MODEL,Input module .,question-answering,6,12,0.1,52,0.1575757575757575,12,0.48,1,0
54,Rd .,MODEL,,question-answering,6,13,0.1083333333333333,53,0.1606060606060606,13,0.52,1,0
55,We adopt a previous solution for the input module ( details in Section 5 ) .,MODEL,Rd .,question-answering,6,14,0.1166666666666666,54,0.1636363636363636,14,0.56,1,0
56,QRN layers .,MODEL,,question-answering,6,15,0.125,55,0.1666666666666666,15,0.6,1,0
57,"QRN layers use the sentence vectors and the question vector from the input module to obtain the predicted answer in vector space , ? ?",MODEL,QRN layers .,question-answering,6,16,0.1333333333333333,56,0.1696969696969697,16,0.64,1,0
58,"Rd . A QRN layer refers to the recurrent application of a QRN unit , which can be considered as a variant of RNN with two inputs , two outputs , and a hidden state ( reduced query ) , all of which operate in vector space .",MODEL,QRN layers .,question-answering,6,17,0.1416666666666666,57,0.1727272727272727,17,0.68,1,0
59,"The details of the QRN module is explained throughout this section ( 2.1 , 2.2 ) .",MODEL,QRN layers .,question-answering,6,18,0.15,58,0.1757575757575757,18,0.72,1,0
60,Output module .,MODEL,,question-answering,6,19,0.1583333333333333,59,0.1787878787878788,19,0.76,1,0
61,Output module maps ?,MODEL,Output module .,question-answering,6,20,0.1666666666666666,60,0.1818181818181818,20,0.8,1,0
62,obtained from QRN to a natural language answer ?.,MODEL,Output module .,question-answering,6,21,0.175,61,0.1848484848484848,21,0.84,1,0
63,"Similar to the input module , we adopt a standard solution for the output module ( details in Section 5 ) .",MODEL,Output module .,question-answering,6,22,0.1833333333333333,62,0.1878787878787878,22,0.88,1,0
64,"We first formally define the base model of a QRN unit , and then we explain how we connect the input and output modules to it ( Section 2.1 ) .",MODEL,Output module .,question-answering,6,23,0.1916666666666666,63,0.1909090909090909,23,0.92,1,0
65,We also present a few extensions to the network that can improve QRN 's performance ( Section 2.2 ) .,MODEL,Output module .,question-answering,6,24,0.2,64,0.1939393939393939,24,0.96,1,0
66,"Finally , we show that QRN can be parallelized overtime , giving computational advantage over most RNN - based models by one order of magnitude ( Section 3 ) .",MODEL,Output module .,question-answering,6,25,0.2083333333333333,65,0.1969696969696969,25,1.0,1,0
67,QRN UNIT,MODEL,Output module .,question-answering,6,26,0.2166666666666666,66,0.2,0,0.0,1,0
68,"As an RNN - based model , QRN is a single recurrent unit that updates its hidden state ( reduced query ) through time and layers .",MODEL,Output module .,question-answering,6,27,0.225,67,0.203030303030303,1,0.0555555555555555,1,0
69,"depicts the schematic structure of a QRN unit , and demonstrates how layers are stacked .",MODEL,Output module .,question-answering,6,28,0.2333333333333333,68,0.206060606060606,2,0.1111111111111111,1,0
70,A QRN unit accepts two inputs ( local query vector qt ?,MODEL,Output module .,question-answering,6,29,0.2416666666666666,69,0.209090909090909,3,0.1666666666666666,1,0
71,"Rd and sentence vector x t ? Rd ) , and two outputs ( reduced query vector ht ?",MODEL,Output module .,question-answering,6,30,0.25,70,0.2121212121212121,4,0.2222222222222222,1,0
72,"Rd , which is similar to the hidden state in RNN , and the sentence vector x t from the input without modification ) .",MODEL,Output module .,question-answering,6,31,0.2583333333333333,71,0.2151515151515151,5,0.2777777777777778,1,0
73,The local query vector is not necessarily identical to the original query ( question ) vector q .,MODEL,Output module .,question-answering,6,32,0.2666666666666666,72,0.2181818181818181,6,0.3333333333333333,1,0
74,"In order to compute the outputs , we use update gate function ? :",MODEL,Output module .,question-answering,6,33,0.275,73,0.2212121212121212,7,0.3888888888888889,1,0
75,"Intuitively , the update gate function measures the relevance between the sentence and the local query and is used to update the hidden state .",MODEL,Output module .,question-answering,6,34,0.2833333333333333,74,0.2242424242424242,8,0.4444444444444444,1,0
76,The reduce function transforms the local query input to a candidate state which is anew reduced ( easier ) query given the sentence .,MODEL,Output module .,question-answering,6,35,0.2916666666666667,75,0.2272727272727272,9,0.5,1,0
77,The outputs are calculated with the following equations :,MODEL,Output module .,question-answering,6,36,0.3,76,0.2303030303030303,10,0.5555555555555556,1,0
78,"where z t is the scalar update gate , h t is the candidate reduced query , and ht is the final reduced query at time step t , ? ( ) is sigmoid activation , tanh ( ) is hyperboolic tangent activation ( applied element - wise ) ,",MODEL,Output module .,question-answering,6,37,0.3083333333333333,77,0.2333333333333333,11,0.6111111111111112,1,0
79,"is element - wise vector multiplication , and [ ; ] is vector concatenation along the row .",MODEL,Output module .,question-answering,6,38,0.3166666666666666,78,0.2363636363636363,12,0.6666666666666666,1,0
80,"As abase case , h 0 = 0 .",MODEL,,question-answering,6,39,0.325,79,0.2393939393939393,13,0.7222222222222222,1,0
81,Here we have explicitly defined ?,MODEL,"As abase case , h 0 = 0 .",question-answering,6,40,0.3333333333333333,80,0.2424242424242424,14,0.7777777777777778,1,0
82,"and ? , but they can be any reasonable differentiable functions .",MODEL,"As abase case , h 0 = 0 .",question-answering,6,41,0.3416666666666667,81,0.2454545454545454,15,0.8333333333333334,1,0
83,The update gate is similar to the global attention mechanism in that it measures the similarity between the sentence ( a memory slot ) and the query .,MODEL,"As abase case , h 0 = 0 .",question-answering,6,42,0.35,82,0.2484848484848484,16,0.8888888888888888,1,0
84,"However , a significant difference is that the update gate is computed using sigmoid ( ? ) function on the current memory slot only ( hence internally embedded within the unit ) , whereas the global attention is computed using softmax function over the entire memory ( hence globally defined ) .",MODEL,"As abase case , h 0 = 0 .",question-answering,6,43,0.3583333333333333,83,0.2515151515151515,17,0.9444444444444444,1,0
85,The update gate can be rather considered as local sigmoid attention .,MODEL,"As abase case , h 0 = 0 .",question-answering,6,44,0.3666666666666666,84,0.2545454545454545,18,1.0,1,0
86,Stacking layers,MODEL,,question-answering,6,45,0.375,85,0.2575757575757575,0,0.0,1,0
87,"We just showed the single - layer case of QRN , but QRN with multiple layers is able to perform reasoning over multiple facts more effectively , as shown in the example of .",MODEL,Stacking layers,question-answering,6,46,0.3833333333333333,86,0.2606060606060606,1,0.0476190476190476,1,0
88,"In order to stack several layers of QRN , the outputs of the current layer are used as the inputs to the next layer .",MODEL,Stacking layers,question-answering,6,47,0.3916666666666666,87,0.2636363636363636,2,0.0952380952380952,1,0
89,"That is , using superscript k to denote the current layer 's index ( assuming 1 - based indexing ) , we let q k + 1 t = h kt .",MODEL,Stacking layers,question-answering,6,48,0.4,88,0.2666666666666666,3,0.1428571428571428,1,0
90,"Note that x t is passed to the next layer without any modification , so we do not put a layer index on it .",MODEL,Stacking layers,question-answering,6,49,0.4083333333333333,89,0.2696969696969697,4,0.1904761904761904,1,0
91,Bi-direction .,MODEL,,question-answering,6,50,0.4166666666666667,90,0.2727272727272727,5,0.238095238095238,1,0
92,"So far we have assumed that QRN only needs to look at past sentences , whereas oftentimes , query answers can depend on future sentences .",MODEL,Bi-direction .,question-answering,6,51,0.425,91,0.2757575757575757,6,0.2857142857142857,1,0
93,"For instance , consider a sentence "" John dropped the football . "" at time t.",MODEL,Bi-direction .,question-answering,6,52,0.4333333333333333,92,0.2787878787878788,7,0.3333333333333333,1,0
94,"Then , even if there is no mention about the "" football "" in the past ( at time i < t ) , it can be implied that "" John "" has the "" football "" at the current time t.",MODEL,Bi-direction .,question-answering,6,53,0.4416666666666666,93,0.2818181818181818,8,0.3809523809523809,1,0
95,"In order to incorporate the future dependency , we obtain ? ?",MODEL,Bi-direction .,question-answering,6,54,0.45,94,0.2848484848484848,9,0.4285714285714285,1,0
96,ht and ? ?,MODEL,Bi-direction .,question-answering,6,55,0.4583333333333333,95,0.2878787878787879,10,0.4761904761904761,1,0
97,"ht in both forward and backward directions , respectively , using Equation",MODEL,Bi-direction .,question-answering,6,56,0.4666666666666667,96,0.2909090909090909,11,0.5238095238095238,1,0
98,3 .,MODEL,Bi-direction .,question-answering,6,57,0.475,97,0.2939393939393939,12,0.5714285714285714,1,0
99,We then add them together to get qt for the next layer .,MODEL,Bi-direction .,question-answering,6,58,0.4833333333333333,98,0.296969696969697,13,0.6190476190476191,1,0
100,"That is , are shared between the two directions .",MODEL,Bi-direction .,question-answering,6,59,0.4916666666666666,99,0.3,14,0.6666666666666666,1,0
101,Connecting input and output modules .,MODEL,,question-answering,6,60,0.5,100,0.303030303030303,15,0.7142857142857143,1,0
102,depicts how QRN is connected with the input and output modules .,MODEL,Connecting input and output modules .,question-answering,6,61,0.5083333333333333,101,0.306060606060606,16,0.7619047619047619,1,0
103,"In the first layer of QRN , q 1 t = q for all t , where q is obtained from the input module by processing the natural language question input q. x t is also obtained from x t by the same input module .",MODEL,Connecting input and output modules .,question-answering,6,62,0.5166666666666667,102,0.3090909090909091,17,0.8095238095238095,1,0
104,The output at the last time step in the last layer is passed to the output module .,MODEL,Connecting input and output modules .,question-answering,6,63,0.525,103,0.3121212121212121,18,0.8571428571428571,1,0
105,"That is , y = h K t where K represent the number of layers in the network .",MODEL,Connecting input and output modules .,question-answering,6,64,0.5333333333333333,104,0.3151515151515151,19,0.9047619047619048,1,0
106,Then the output module gives the predicted answer ?,MODEL,Connecting input and output modules .,question-answering,6,65,0.5416666666666666,105,0.3181818181818182,20,0.9523809523809524,1,0
107,in natural language .,MODEL,Connecting input and output modules .,question-answering,6,66,0.55,106,0.3212121212121212,21,1.0,1,0
108,EXTENSIONS,MODEL,,question-answering,6,67,0.5583333333333333,107,0.3242424242424242,0,0.0,1,0
109,"Here we introduce a few extensions of QRN , and later in our experiments , we test QRN 's performance with and without each of these extensions .",MODEL,EXTENSIONS,question-answering,6,68,0.5666666666666667,108,0.3272727272727272,1,0.0666666666666666,1,0
110,Reset gate .,MODEL,,question-answering,6,69,0.575,109,0.3303030303030303,2,0.1333333333333333,1,0
111,"Inspired by GRU , we found that it is useful to allow the QRN unit to reset ( nullify ) the candidate reduced query ( i.e. , h t ) when necessary .",MODEL,Reset gate .,question-answering,6,70,0.5833333333333334,110,0.3333333333333333,3,0.2,1,0
112,For this we use a reset gate function ? :,MODEL,Reset gate .,question-answering,6,71,0.5916666666666667,111,0.3363636363636363,4,0.2666666666666666,1,0
113,", which can be defined similarly to the update gate function :",MODEL,Reset gate .,question-answering,6,72,0.6,112,0.3393939393939394,5,0.3333333333333333,1,0
114,where W ( r ) ?,MODEL,Reset gate .,question-answering,6,73,0.6083333333333333,113,0.3424242424242424,6,0.4,1,0
115,"R 1d is a weight matrix , and b ( r ) ?",MODEL,Reset gate .,question-answering,6,74,0.6166666666666667,114,0.3454545454545454,7,0.4666666666666667,1,0
116,R is a bias term .,MODEL,,question-answering,6,75,0.625,115,0.3484848484848485,8,0.5333333333333333,1,0
117,Equation 3 is rewritten as,MODEL,R is a bias term .,question-answering,6,76,0.6333333333333333,116,0.3515151515151515,9,0.6,1,0
118,Note that we do not use the reset gate in the last layer .,MODEL,R is a bias term .,question-answering,6,77,0.6416666666666667,117,0.3545454545454545,10,0.6666666666666666,1,0
119,Vector gates .,MODEL,,question-answering,6,78,0.65,118,0.3575757575757576,11,0.7333333333333333,1,0
120,"As in LSTM and GRU , update and reset gates can be vectors instead of scalar values for fine - controlled gating .",MODEL,Vector gates .,question-answering,6,79,0.6583333333333333,119,0.3606060606060606,12,0.8,1,0
121,"For vector gates , we modify the row dimension of weights and biases in Equation 1 and 5 from 1 to d .",MODEL,Vector gates .,question-answering,6,80,0.6666666666666666,120,0.3636363636363636,13,0.8666666666666667,1,0
122,"Then we obtain z t , rt ?",MODEL,Vector gates .,question-answering,6,81,0.675,121,0.3666666666666666,14,0.9333333333333332,1,0
123,"Rd ( instead of z t , rt ? R ) , and these can be element - wise multiplied ( ) instead of being broadcasted in Equation 3 and 6 .",MODEL,Vector gates .,question-answering,6,82,0.6833333333333333,122,0.3696969696969697,15,1.0,1,0
124,PARALLELIZATION,MODEL,,question-answering,6,83,0.6916666666666667,123,0.3727272727272727,0,0.0,1,0
125,An important advantage of QRN is that the recurrent updates in Equation 3 and 5 can be computed in parallel across time .,MODEL,PARALLELIZATION,question-answering,6,84,0.7,124,0.3757575757575757,1,0.0833333333333333,1,0
126,"This is in contrast with most RNN - based models that can not be parallelized , where computing the candidate hidden state at time t explicitly requires the previous hidden state .",MODEL,PARALLELIZATION,question-answering,6,85,0.7083333333333334,125,0.3787878787878788,2,0.1666666666666666,1,0
127,"In QRN , the final reduced queries ( h t ) can be decomposed into computing over candidate reduced queries ( h t ) , without looking at the previous reduced query .",MODEL,PARALLELIZATION,question-answering,6,86,0.7166666666666667,126,0.3818181818181818,3,0.25,1,0
128,Here we primarily show that the query update in Equation 3 can be parallelized by rewriting the equation with matrix operations .,MODEL,PARALLELIZATION,question-answering,6,87,0.725,127,0.3848484848484848,4,0.3333333333333333,1,0
129,The extension to Equation 5 is straightforward .,MODEL,,question-answering,6,88,0.7333333333333333,128,0.3878787878787879,5,0.4166666666666667,1,0
130,The proof for QRN with vector gates is shown in Appendix B .,MODEL,The extension to Equation 5 is straightforward .,question-answering,6,89,0.7416666666666667,129,0.3909090909090909,6,0.5,1,0
131,The recursive definition of Equation 3 can be explicitly written as,MODEL,The extension to Equation 5 is straightforward .,question-answering,6,90,0.75,130,0.3939393939393939,7,0.5833333333333334,1,0
132,Let bi = log ( 1 ? z i ) for brevity .,MODEL,The extension to Equation 5 is straightforward .,question-answering,6,91,0.7583333333333333,131,0.396969696969697,8,0.6666666666666666,1,0
133,Then we can rewrite Equation 7 as the following equation :,MODEL,The extension to Equation 5 is straightforward .,question-answering,6,92,0.7666666666666667,132,0.4,9,0.75,1,0
134,Figure,MODEL,,question-answering,6,93,0.775,133,0.403030303030303,10,0.8333333333333334,1,0
135,"2 : The schematics of QRN and the two state - of - the - art models , End - to - End Memory Networks and Improved Dynamic Memory Networks ( DMN + ) , simplified to emphasize the differences among the models .",MODEL,Figure,question-answering,6,94,0.7833333333333333,134,0.406060606060606,11,0.9166666666666666,1,0
136,"AGRU is a variant of GRU where the update gate is replaced with soft attention , proposed by .",MODEL,Figure,question-answering,6,95,0.7916666666666666,135,0.4090909090909091,12,1.0,1,0
137,RELATED WORK,MODEL,Figure,question-answering,6,96,0.8,136,0.4121212121212121,0,0.0,1,0
138,"QRN is inspired by RNN - based models with gating mechanism , such as LSTM and GRU .",MODEL,Figure,question-answering,6,97,0.8083333333333333,137,0.4151515151515151,1,0.0416666666666666,1,0
139,"While GRU and LSTM use the previous hidden state and the current input to obtain the candidate hidden state , QRN only uses the current two inputs to obtain the candidate reduced query ( equivalent to candidate hidden state ) .",MODEL,Figure,question-answering,6,98,0.8166666666666667,138,0.4181818181818181,2,0.0833333333333333,1,0
140,"We conjecture that this not only gives computational advantage via parallelization , but also makes training easier , i.e. , avoiding vanishing gradient ( which is critical for long - term dependency ) , overfitting ( by simplifying the model ) , and converging to local minima .",MODEL,Figure,question-answering,6,99,0.825,139,0.4212121212121212,3,0.125,1,0
141,"The idea of structurally simplifying ( constraining ) RNNs for learning longer - term patterns has been explored in recent previous work , such as Structurally Constrained Recurrent Network and Strongly - Typed Recurrent Neural Network ( STRNN ) .",MODEL,Figure,question-answering,6,100,0.8333333333333334,140,0.4242424242424242,4,0.1666666666666666,1,0
142,"QRN is similar to STRNN in that both architectures use gating mechanism , and the gates and the candidate hidden states do not depend on the previous hidden states , which simplifies the recurrent relation .",MODEL,Figure,question-answering,6,101,0.8416666666666667,141,0.4272727272727272,5,0.2083333333333333,1,0
143,"However , QRN can be distinguished from STRNN in three ways .",MODEL,Figure,question-answering,6,102,0.85,142,0.4303030303030303,6,0.25,1,0
144,"First , QRN 's update gate simulates attention mechanism , measuring the relevance between the input sentence and query .",MODEL,Figure,question-answering,6,103,0.8583333333333333,143,0.4333333333333333,7,0.2916666666666667,1,0
145,"On the other hand , the gates in STRNN can be considered as the simplification of LSTM / GRU by removing their dependency on previous hidden state .",MODEL,Figure,question-answering,6,104,0.8666666666666667,144,0.4363636363636363,8,0.3333333333333333,1,0
146,"Second , QRN is an RNN that is natively compatible with context - based QA tasks , where the QRN unit accepts two inputs , i.e. each context sentence and query .",MODEL,Figure,question-answering,6,105,0.875,145,0.4393939393939394,9,0.375,1,0
147,This is distinct from STRNN which has only one input .,MODEL,Figure,question-answering,6,106,0.8833333333333333,146,0.4424242424242424,10,0.4166666666666667,1,0
148,"Third , we show that QRN is timewise - parallelizable on GPUs .",MODEL,Figure,question-answering,6,107,0.8916666666666667,147,0.4454545454545454,11,0.4583333333333333,1,0
149,Our parallelization algorithm is also applicable to STRNN .,MODEL,,question-answering,6,108,0.9,148,0.4484848484848485,12,0.5,1,0
150,End - to - end Memory Network ( N2N ) uses external memory with multi - layer attention mechanism to focus on sentences that are relevant to the question .,MODEL,Our parallelization algorithm is also applicable to STRNN .,question-answering,6,109,0.9083333333333332,149,0.4515151515151515,13,0.5416666666666666,1,0
151,There are two key differences between N2N and our QRN .,MODEL,Our parallelization algorithm is also applicable to STRNN .,question-answering,6,110,0.9166666666666666,150,0.4545454545454545,14,0.5833333333333334,1,0
152,"First , N2N summarizes the entire memory in each layer to control the attention in the next layer ( circle nodes in ) .",MODEL,Our parallelization algorithm is also applicable to STRNN .,question-answering,6,111,0.925,151,0.4575757575757576,15,0.625,1,0
153,"Instead , QRN does not have any controller node ) and is able to focus on relevant sentences through the update gate that is internally embodied within its unit .",MODEL,Our parallelization algorithm is also applicable to STRNN .,question-answering,6,112,0.9333333333333332,152,0.4606060606060606,16,0.6666666666666666,1,0
154,"Second , N2N adds time - dependent trainable weights to the sentence representations to model the time dependency of the sentences ( as discussed in Section 1 ) .",MODEL,Our parallelization algorithm is also applicable to STRNN .,question-answering,6,113,0.9416666666666668,153,0.4636363636363636,17,0.7083333333333334,1,0
155,QRN does not need such additional weights as its inherent RNN architecture allows QRN to effectively model the time dependency .,MODEL,Our parallelization algorithm is also applicable to STRNN .,question-answering,6,114,0.95,154,0.4666666666666667,18,0.75,1,0
156,Neural Reasoner and Gated End - toend Memory Network ) are variants of MemN2N that share its fundamental characteristics .,MODEL,Our parallelization algorithm is also applicable to STRNN .,question-answering,6,115,0.9583333333333334,155,0.4696969696969697,19,0.7916666666666666,1,0
157,Improved Dynamic Memory Network ( DMN + ) uses the hybrid of the attention mechanism and the RNN architecture to model the sequence of sentences .,MODEL,Our parallelization algorithm is also applicable to STRNN .,question-answering,6,116,0.9666666666666668,156,0.4727272727272727,20,0.8333333333333334,1,0
158,"It consists of two distinct GRUs , one for the time axis ( rectangle nodes in ) and one for the layer axis ( circle nodes in ) .",MODEL,Our parallelization algorithm is also applicable to STRNN .,question-answering,6,117,0.975,157,0.4757575757575757,21,0.875,1,0
159,Note that the update gate of the GRU for the time axis is replaced with external softmax attention weights .,MODEL,Our parallelization algorithm is also applicable to STRNN .,question-answering,6,118,0.9833333333333332,158,0.4787878787878787,22,0.9166666666666666,1,0
160,"DMN + uses the time - axis GRU to summarizes the entire memory in each layer , and then the layer - axis GRU controls the attention weights in each layer .",MODEL,Our parallelization algorithm is also applicable to STRNN .,question-answering,6,119,0.9916666666666668,159,0.4818181818181818,23,0.9583333333333334,1,0
161,"In contrast , QRN is simply a single recurrent unit without any controller node .",MODEL,Our parallelization algorithm is also applicable to STRNN .,question-answering,6,120,1.0,160,0.4848484848484848,24,1.0,1,0
162,EXPERIMENTS,,,question-answering,6,0,0.0,161,0.4878787878787878,0,0.0,1,0
163,5.1 DATA bAb,EXPERIMENTS,EXPERIMENTS,question-answering,6,1,0.025,162,0.4909090909090909,1,0.0555555555555555,1,0
164,"I story - based QA dataset bAb I story - based QA dataset is composed of 20 different tasks ( Appendix A ) , each of which has 1,000 ( 1 k ) synthetically - generated story - question pair .",EXPERIMENTS,EXPERIMENTS,question-answering,6,2,0.05,163,0.4939393939393939,2,0.1111111111111111,1,0
165,A story can be as short as two sentences and as long as 200 + sentences .,EXPERIMENTS,EXPERIMENTS,question-answering,6,3,0.075,164,0.4969696969696969,3,0.1666666666666666,1,0
166,A system is evaluated on the accuracy of getting the correct answers to the questions .,EXPERIMENTS,EXPERIMENTS,question-answering,6,4,0.1,165,0.5,4,0.2222222222222222,1,0
167,"The answers are single words or lists ( e.g. "" football , apple "" ) .",EXPERIMENTS,EXPERIMENTS,question-answering,6,5,0.125,166,0.503030303030303,5,0.2777777777777778,1,0
168,Answering questions in each task requires selecting a set of relevant sentences and applying different kinds of logical reasoning over them .,EXPERIMENTS,EXPERIMENTS,question-answering,6,6,0.15,167,0.5060606060606061,6,0.3333333333333333,1,0
169,"The dataset also includes 10 k training data ( for each task ) , which allows training more complex models .",EXPERIMENTS,EXPERIMENTS,question-answering,6,7,0.175,168,0.509090909090909,7,0.3888888888888889,1,0
170,Note that DMN + only reports on the 10k dataset .,EXPERIMENTS,EXPERIMENTS,question-answering,6,8,0.2,169,0.5121212121212121,8,0.4444444444444444,1,0
171,bAb,EXPERIMENTS,EXPERIMENTS,question-answering,6,9,0.225,170,0.5151515151515151,9,0.5,1,0
172,"I dialog dataset bAb I dialog dataset consists of 5 different tasks , each of which has 1 k synthetically - generated goal - oriented dialogs between a user and the system in the domain of restaurant reservation .",EXPERIMENTS,EXPERIMENTS,question-answering,6,10,0.25,171,0.5181818181818182,10,0.5555555555555556,1,0
173,Each dialog is as long as 96 utterances and comes with external knowledge base ( KB ) providing information of each restaurant .,EXPERIMENTS,EXPERIMENTS,question-answering,6,11,0.275,172,0.5212121212121212,11,0.6111111111111112,1,0
174,"The authors also provide Out - Of - Vocabulary ( OOV ) version of the dataset , where many of the words and KB keywords in test data are not seen during training .",EXPERIMENTS,EXPERIMENTS,question-answering,6,12,0.3,173,0.5242424242424243,12,0.6666666666666666,1,0
175,"A system is evaluated on the accuracy of its response to each utterance of the user , choosing from up to 2500 possible candidate responses .",EXPERIMENTS,EXPERIMENTS,question-answering,6,13,0.325,174,0.5272727272727272,13,0.7222222222222222,1,0
176,A system is required not only to understand the user 's request but also refer to previous conversations in order to obtain the context information of the current conversation .,EXPERIMENTS,EXPERIMENTS,question-answering,6,14,0.35,175,0.5303030303030303,14,0.7777777777777778,1,0
177,"DSTC2 ( Task 6 ) dialog dataset transformed the Second Dialog State Tracking Challenge ( DSTC2 ) dataset into the same format as the bAbI dialog dataset , for the measurement of performance on areal dataset .",EXPERIMENTS,EXPERIMENTS,question-answering,6,15,0.375,176,0.5333333333333333,15,0.8333333333333334,1,0
178,"Each dialog can be as long as 800 + utterances , and a system needs to choose from 2407 possible candidate responses for each utterance of the user .",EXPERIMENTS,EXPERIMENTS,question-answering,6,16,0.4,177,0.5363636363636364,16,0.8888888888888888,1,0
179,"Note that the evaluation metric of the original DSTC2 is different from that of the transformed DSTC2 , so previous work on the original DSTC2 should not be directly compared to our work .",EXPERIMENTS,EXPERIMENTS,question-answering,6,17,0.425,178,0.5393939393939394,17,0.9444444444444444,1,0
180,"We will refer to this transformed DSTC2 dataset by "" Task 6 "" of dialog dataset .",EXPERIMENTS,EXPERIMENTS,question-answering,6,18,0.45,179,0.5424242424242425,18,1.0,1,0
181,MODEL DETAILS,EXPERIMENTS,EXPERIMENTS,question-answering,6,19,0.475,180,0.5454545454545454,0,0.0,1,0
182,Input Module .,EXPERIMENTS,,question-answering,6,20,0.5,181,0.5484848484848485,1,0.0128205128205128,1,0
183,"In the input module , we are given sentences ( previous conversations in dialog ) x t and a question ( most recent user utterance ) q , and we want to obtain their vector representations , x t , q ?",EXPERIMENTS,Input Module .,question-answering,6,21,0.525,182,0.5515151515151515,2,0.0256410256410256,1,0
184,Rd .,EXPERIMENTS,,question-answering,6,22,0.55,183,0.5545454545454546,3,0.0384615384615384,1,0
185,We use a trainable embedding matrix A ?,EXPERIMENTS,Rd .,question-answering,6,23,0.575,184,0.5575757575757576,4,0.0512820512820512,1,0
186,R d V to encode the one - hot vector of each word x tj in each sentence x t into a d-dimensional vector x tj ?,EXPERIMENTS,Rd .,question-answering,6,24,0.6,185,0.5606060606060606,5,0.0641025641025641,1,0
187,Rd .,EXPERIMENTS,,question-answering,6,25,0.625,186,0.5636363636363636,6,0.0769230769230769,1,0
188,Then the sentence representation x t is obtained by Position Encoder .,EXPERIMENTS,Rd .,question-answering,6,26,0.65,187,0.5666666666666667,7,0.0897435897435897,1,0
189,The same encoder with the same embedding matrix is also used to obtain the question vector q from q.,EXPERIMENTS,Rd .,question-answering,6,27,0.675,188,0.5696969696969697,8,0.1025641025641025,1,0
190,Output Module for story - based QA .,EXPERIMENTS,,question-answering,6,28,0.7,189,0.5727272727272728,9,0.1153846153846153,1,0
191,"In the output module , we are given the vector representation of the predicted answer ?",EXPERIMENTS,Output Module for story - based QA .,question-answering,6,29,0.725,190,0.5757575757575758,10,0.1282051282051282,1,0
192,"and we want to obtain the natural language form of the answer , ?.",EXPERIMENTS,Output Module for story - based QA .,question-answering,6,30,0.75,191,0.5787878787878787,11,0.141025641025641,1,0
193,We use a V - way single - layer softmax classifier to map ?,EXPERIMENTS,Output Module for story - based QA .,question-answering,6,31,0.775,192,0.5818181818181818,12,0.1538461538461538,1,0
194,"to a V - dimensional sparse vector , v = softmax W ( y ) ? ? RV , where W ( y ) ?",EXPERIMENTS,Output Module for story - based QA .,question-answering,6,32,0.8,193,0.5848484848484848,13,0.1666666666666666,1,0
195,RV d is a weight matrix .,EXPERIMENTS,,question-answering,6,33,0.825,194,0.5878787878787879,14,0.1794871794871795,1,0
196,Then the final answer ?,EXPERIMENTS,RV d is a weight matrix .,question-answering,6,34,0.85,195,0.5909090909090909,15,0.1923076923076923,1,0
197,is simply the argmax word inv .,EXPERIMENTS,RV d is a weight matrix .,question-answering,6,35,0.875,196,0.593939393939394,16,0.2051282051282051,1,0
198,"To handle questions with multiple - word answers , we consider each of them as a single word that contains punctuations such as space and comma , and put it in the vocabulary .",EXPERIMENTS,RV d is a weight matrix .,question-answering,6,36,0.9,197,0.5969696969696969,17,0.2179487179487179,1,0
199,Output Module for dialog .,EXPERIMENTS,,question-answering,6,37,0.925,198,0.6,18,0.2307692307692307,1,0
200,"We use a fixed number single - layer softmax classifiers , each of which is similar to that of the sotry - based QA model , to sequentially output each word of the system 's response .",EXPERIMENTS,Output Module for dialog .,question-answering,6,38,0.95,199,0.603030303030303,19,0.2435897435897435,1,0
201,"While it is similar in spirit to the RNN decoder , our output module does not have a recurrent hidden state or gating mechanism .",EXPERIMENTS,Output Module for dialog .,question-answering,6,39,0.975,200,0.6060606060606061,20,0.2564102564102564,1,0
202,"Instead , it solely uses the final ouptut of the QRN , ? , and the current word output to influence the prediction of the next word among possible candidates .",EXPERIMENTS,Output Module for dialog .,question-answering,6,40,1.0,201,0.6090909090909091,21,0.2692307692307692,1,0
203,Training .,,,question-answering,6,0,0.0,202,0.6121212121212121,22,0.282051282051282,1,0
204,We withhold 10 % of the training for development .,Training .,Training .,question-answering,6,1,0.0909090909090909,203,0.6151515151515151,23,0.2948717948717949,1,1
205,We use the hidden state size of 50 by deafult .,Training .,Training .,question-answering,6,2,0.1818181818181818,204,0.6181818181818182,24,0.3076923076923077,1,1
206,"Batch sizes of 32 for bAbI story - based QA 1k , bAb I dialog and DSTC2 dialog , and 128 for bAbI QA 10 k are used .",Training .,Training .,question-answering,6,3,0.2727272727272727,205,0.6212121212121212,25,0.3205128205128205,1,1
207,The weights in the input and output modules are initialized with zero mean and the standard deviation of 1 / ? d.,Training .,Training .,question-answering,6,4,0.3636363636363636,206,0.6242424242424243,26,0.3333333333333333,1,0
208,"Weights in the QRN unit are initialized using techniques by , and are tied across the layers .",Training .,Training .,question-answering,6,5,0.4545454545454545,207,0.6272727272727273,27,0.3461538461538461,1,0
209,Forget bias of 2.5 is used for update gates ( no bias for reset gates ) .,Training .,Training .,question-answering,6,6,0.5454545454545454,208,0.6303030303030303,28,0.358974358974359,1,0
210,L2 weight decay of 0.001 ( 0.0005 for QA 10 k ) is used for all weights .,Training .,Training .,question-answering,6,7,0.6363636363636364,209,0.6333333333333333,29,0.3717948717948718,1,1
211,The loss function is the cross entropy betweenv and the one - hot vector of the true answer .,Training .,Training .,question-answering,6,8,0.7272727272727273,210,0.6363636363636364,30,0.3846153846153846,1,1
212,"The loss is minimized by stochastic gradient descent for maximally 500 epochs , but training is early stopped if the loss on the development data does not decrease for 50 epochs .",Training .,Training .,question-answering,6,9,0.8181818181818182,211,0.6393939393939394,31,0.3974358974358974,1,1
213,The learning rate is controlled by AdaGrad with the initial learning rate of 0.5 ( 0.1 for QA 10 k ) .,Training .,Training .,question-answering,6,10,0.9090909090909092,212,0.6424242424242425,32,0.4102564102564102,1,1
214,"Since the model is sensitive to the weight initialization , we repeat each training procedure 10 times ( 50 times for 10 k ) with the new random initialization of the weights and report the result on the test data with the lowest loss on the development data .",Training .,Training .,question-answering,6,11,1.0,213,0.6454545454545455,33,0.4230769230769231,1,0
215,RESULTS .,,,question-answering,6,0,0.0,214,0.6484848484848484,34,0.4358974358974359,1,0
216,We compare our model with baselines and previous state - of - the - art models on story - based and dialog tasks .,RESULTS .,RESULTS .,question-answering,6,1,0.1,215,0.6515151515151515,35,0.4487179487179487,1,0
217,"These include LSTM , End - to - end Memory Networks ( N2N ) , Dynamic Memory Networks ( DMN + ) , Gated End - to - end Memory Networks ( GMe m N2N ) , and Differentiable Neural Computer ( DNC ) .",RESULTS .,RESULTS .,question-answering,6,2,0.2,216,0.6545454545454545,36,0.4615384615384615,1,0
218,Story - based QA .,RESULTS .,,question-answering,6,3,0.3,217,0.6575757575757576,37,0.4743589743589743,1,0
219,Table 1 ( top ) reports the summary of results of our model ( QRN ) and previous work on b AbI QA ( task - wise results are shown in in Appendix ) .,RESULTS .,Story - based QA .,question-answering,6,4,0.4,218,0.6606060606060606,38,0.4871794871794871,1,0
220,"In 1 k data , QRN 's ' 2 r' ( 2 layers + reset gate + d = 50 ) outperforms all other models by a large margin ( 2.8 + % ) .",RESULTS .,Story - based QA .,question-answering,6,5,0.5,219,0.6636363636363637,39,0.5,1,1
221,"In 10 k dataset , the average accuracy of QRN 's ' 6r200 ' ( 6 layers + reset gate + d = 200 ) model outperforms all previous models by a large margin ( 2.5 + % ) , achieving a nearly perfect score of 99.7 % .",RESULTS .,Story - based QA .,question-answering,6,6,0.6,220,0.6666666666666666,40,0.5128205128205128,1,1
222,Dialog . Table 1 ( bottom ) reports the summary of the results of our model ( QRN ) and previous work on bAbI dialog and Task 6 dialog ( task - wise results are shown in in Appendix ) .,RESULTS .,Story - based QA .,question-answering,6,7,0.7,221,0.6696969696969697,41,0.5256410256410257,1,0
223,"As done in previous work , we also report results when we use ' Match ' for dialogs .",RESULTS .,Story - based QA .,question-answering,6,8,0.8,222,0.6727272727272727,42,0.5384615384615384,1,0
224,' Match ' is the extension to the model which additionally takes as input whether each answer candidate matches with context ( more details on Appendix ) .,RESULTS .,Story - based QA .,question-answering,6,9,0.9,223,0.6757575757575758,43,0.5512820512820513,1,0
225,QRN outperforms previous work by a large margin ( 2.0 + % ) in every comparison .,RESULTS .,Story - based QA .,question-answering,6,10,1.0,224,0.6787878787878788,44,0.5641025641025641,1,1
226,Ablations .,,,question-answering,6,0,0.0,225,0.6818181818181818,45,0.5769230769230769,1,0
227,"We test four types of ablations ( also discussed in Section 2.2 ) : number of layers ( 1 , 2 , 3 , or 6 ) , reset gate ( r ) , and gate vectorization ( v ) and the dimension of the hidden vector ( 50 , 100 ) .",Ablations .,Ablations .,question-answering,6,1,0.0303030303030303,226,0.6848484848484848,46,0.5897435897435898,1,0
228,We show a subset of combinations of the ablations for bAbI QA in ; other combinations performed poorly and / or did not give interesting observations .,Ablations .,Ablations .,question-answering,6,2,0.0606060606060606,227,0.6878787878787879,47,0.6025641025641025,1,0
229,"According to the ablation results , we infer that : ( a ) When the number of layers is only one , the model lacks reasoning capability .",Ablations .,Ablations .,question-answering,6,3,0.0909090909090909,228,0.6909090909090909,48,0.6153846153846154,1,1
230,"In the case of 1 k dataset , when there are too many layers ( 6 ) , it seems correctly training the model becomes increasingly difficult .",Ablations .,Ablations .,question-answering,6,4,0.1212121212121212,229,0.693939393939394,49,0.6282051282051282,1,0
231,"In the case of 10 k dataset , many layers ( 6 ) and hidden dimensions ( 200 ) helps reasoning , most notably in difficult task such as task 16 .",Ablations .,Ablations .,question-answering,6,5,0.1515151515151515,230,0.696969696969697,50,0.6410256410256411,1,0
232,( b ) Adding the reset gate helps .,Ablations .,Ablations .,question-answering,6,6,0.1818181818181818,231,0.7,51,0.6538461538461539,1,1
233,"( c ) Including vector gates hurts in 1 k datasets , as the model either overfits to the training data or converges to local minima .",Ablations .,Ablations .,question-answering,6,7,0.2121212121212121,232,0.703030303030303,52,0.6666666666666666,1,1
234,"On the other hand , vector gates in bAbI story - based QA 10 k dataset sometimes help .",Ablations .,Ablations .,question-answering,6,8,0.2424242424242424,233,0.706060606060606,53,0.6794871794871795,1,1
235,"( d ) Increasing the dimension of the hidden state to 100 in the dialog 's Task 6 ( DSTC2 ) helps , while there is not much improvement in the dialog 's Task 1 - 5 .",Ablations .,Ablations .,question-answering,6,9,0.2727272727272727,234,0.7090909090909091,54,0.6923076923076923,1,0
236,It can be hypothesized that a larger hidden state is required for real data . Parallelization .,Ablations .,Ablations .,question-answering,6,10,0.303030303030303,235,0.7121212121212122,55,0.7051282051282052,1,0
237,We implement QRN with and without parallelization in TensorFlow ) on a single Titan X GPU to qunaitify the computational gain of the parallelization .,Ablations .,Ablations .,question-answering,6,11,0.3333333333333333,236,0.7151515151515152,56,0.717948717948718,1,0
238,"For QRN without parallelization , we use the RNN library provided by TensorFlow .",Ablations .,Ablations .,question-answering,6,12,0.3636363636363636,237,0.7181818181818181,57,0.7307692307692307,1,0
239,QRN with parallelization gives 6.2 times faster training and inference than QRN without parallelization on average .,Ablations .,Ablations .,question-answering,6,13,0.3939393939393939,238,0.7212121212121212,58,0.7435897435897436,1,0
240,We expect that the speedup can be even higher for datasets with larger context .,Ablations .,Ablations .,question-answering,6,14,0.4242424242424242,239,0.7242424242424242,59,0.7564102564102564,1,0
241,Interpretations .,Ablations .,,question-answering,6,15,0.4545454545454545,240,0.7272727272727273,60,0.7692307692307693,1,0
242,An advantage of QRN is that the intermediate query updates are interpretable .,Ablations .,Interpretations .,question-answering,6,16,0.4848484848484848,241,0.7303030303030303,61,0.782051282051282,1,0
243,"shows intermediate local queries ( q kt ) interpreted in natural language , such as "" Where is Sandra ? "" .",Ablations .,Interpretations .,question-answering,6,17,0.5151515151515151,242,0.7333333333333333,62,0.7948717948717948,1,0
244,"In order to obtain these , we place a decoder on the input question embedding q and add it s loss for recovering the question to the classification loss ( similarly to ) .",Ablations .,Interpretations .,question-answering,6,18,0.5454545454545454,243,0.7363636363636363,63,0.8076923076923077,1,0
245,We then use the same decoder to decode the intermediate queries .,Ablations .,Interpretations .,question-answering,6,19,0.5757575757575758,244,0.7393939393939394,64,0.8205128205128205,1,0
246,This helps us understand the flow of information in the networks .,Ablations .,Interpretations .,question-answering,6,20,0.6060606060606061,245,0.7424242424242424,65,0.8333333333333334,1,0
247,"In , the question Where is apple ?",Ablations .,Interpretations .,question-answering,6,21,0.6363636363636364,246,0.7454545454545455,66,0.8461538461538461,1,0
248,is transformed into,Ablations .,Interpretations .,question-answering,6,22,0.6666666666666666,247,0.7484848484848485,67,0.8589743589743589,1,0
249,"Where is Sandra ? at t = 1 . At t = 2 , as Sandra dropped the apple , the apple is no more relevant to Sandra .",Ablations .,Interpretations .,question-answering,6,23,0.696969696969697,248,0.7515151515151515,68,0.8717948717948718,1,0
250,We obtain Where is Daniel ?,Ablations .,Interpretations .,question-answering,6,24,0.7272727272727273,249,0.7545454545454545,69,0.8846153846153846,1,0
251,"at time t = 3 , and it is propagated until t = 5 , where we observe a sentence ( fact ) that can be used to answer the query .",Ablations .,Interpretations .,question-answering,6,25,0.7575757575757576,250,0.7575757575757576,70,0.8974358974358975,1,0
252,Visualization .,Ablations .,,question-answering,6,26,0.7878787878787878,251,0.7606060606060606,71,0.9102564102564102,1,0
253,shows vizualization of the ( scalar ) magnitudes of update and reset gates on story sentences and dialog utterances .,Ablations .,Visualization .,question-answering,6,27,0.8181818181818182,252,0.7636363636363637,72,0.9230769230769232,1,0
254,More visualizations are shown in Appendices : and .,Ablations .,,question-answering,6,28,0.8484848484848485,253,0.7666666666666667,73,0.935897435897436,1,0
255,"In , we observe high values on facts that provide information to answer question ( the system 's next utterance for dialog ) .",Ablations .,More visualizations are shown in Appendices : and .,question-answering,6,29,0.8787878787878788,254,0.7696969696969697,74,0.9487179487179488,1,0
256,"In QA Task 2 example ( top left ) , we observe high update gate values in the first layer on facts that state who has the apple , and in the second layer , the high update gate values are on those that inform where that person went to .",Ablations .,More visualizations are shown in Appendices : and .,question-answering,6,30,0.9090909090909092,255,0.7727272727272727,75,0.9615384615384616,1,0
257,"We also observe that the forward reset gate at t = 2 in the first layer ( ? ? r 1 2 ) is low , which is signifying that apple no more belongs to Sandra .",Ablations .,More visualizations are shown in Appendices : and .,question-answering,6,31,0.9393939393939394,256,0.7757575757575758,76,0.9743589743589745,1,0
258,"In dialog Task 3 ( bottom left ) , the model is able to infer that three restaurants are already recommended so that it can recommend another one .",Ablations .,More visualizations are shown in Appendices : and .,question-answering,6,32,0.9696969696969696,257,0.7787878787878788,77,0.9871794871794872,1,0
259,"In dialog Task 6 ( bottom ) , the model focuses on the sentences containing Spanish , and does not concentrate much on other facts such as I do n't care .",Ablations .,More visualizations are shown in Appendices : and .,question-answering,6,33,1.0,258,0.7818181818181819,78,1.0,1,0
260,CONCLUSION,,,question-answering,6,0,0.0,259,0.7848484848484848,0,0.0,1,0
261,"In this paper , we introduce Query - Reduction Network ( QRN ) to answer context - based questions and carryout conversations with users that require multi-hop reasoning .",CONCLUSION,CONCLUSION,question-answering,6,1,0.0142857142857142,260,0.7878787878787878,1,0.0666666666666666,0,0
262,We show the state - of - theart results in the three datasets of story - based QA and dialog .,CONCLUSION,CONCLUSION,question-answering,6,2,0.0285714285714285,261,0.7909090909090909,2,0.1333333333333333,0,0
263,We model a story or a dialog as a sequence of state - changing triggers and compute the final answer to the question or the system 's next utterance by recurrently updating ( or reducing ) the query .,CONCLUSION,CONCLUSION,question-answering,6,3,0.0428571428571428,262,0.793939393939394,3,0.2,0,0
264,"QRN is situated between the attention mechanism and RNN , effectively handling time dependency and long - term dependency problems of each technique , respectively .",CONCLUSION,CONCLUSION,question-answering,6,4,0.0571428571428571,263,0.796969696969697,4,0.2666666666666666,0,0
265,"It addresses the long - term dependency problem of most RNNs by simplifying the recurrent update , in which the candidate hidden state ( reduced query ) does not depend on the previous state .",CONCLUSION,CONCLUSION,question-answering,6,5,0.0714285714285714,264,0.8,5,0.3333333333333333,0,0
266,"Moreover , QRN can be parallelized and can address the well - known problem of RNN 's vanishing gradients .",CONCLUSION,CONCLUSION,question-answering,6,6,0.0857142857142857,265,0.803030303030303,6,0.4,0,0
267,A TASK - WISE RESULTS,CONCLUSION,CONCLUSION,question-answering,6,7,0.1,266,0.806060606060606,7,0.4666666666666667,0,0
268,"Here we provide detailed per- task breakdown of our results in QA ) and dialog datasets . error rates ( % ) of QRN and previous work : LSTM , End - to - end Memory Networks ( N2N ) , Dynamic Memory Networks ( DMN + ) , Gated End - to - end Memory Networks ( GMemN2N ) .",CONCLUSION,CONCLUSION,question-answering,6,8,0.1142857142857142,267,0.8090909090909091,8,0.5333333333333333,0,0
269,Results within each task of Differentiable Neural Computer ( DNC ) were not provided in its paper ) .,CONCLUSION,CONCLUSION,question-answering,6,9,0.1285714285714285,268,0.8121212121212121,9,0.6,0,0
270,"For QRN , a number in the front ( 1 , 2 , 3 , 6 ) indicates the number of layers .",CONCLUSION,CONCLUSION,question-answering,6,10,0.1428571428571428,269,0.8151515151515152,10,0.6666666666666666,0,0
271,"A number in the back indicates the dimension of hidden vector , while the default value is 50 .",CONCLUSION,CONCLUSION,question-answering,6,11,0.1571428571428571,270,0.8181818181818182,11,0.7333333333333333,0,0
272,"' r ' indicates that the reset gate is used , and ' v ' indicates that the gates were vectorized .",CONCLUSION,CONCLUSION,question-answering,6,12,0.1714285714285714,271,0.8212121212121212,12,0.8,0,0
273,'*' indicates joint training . ) and Gated End - to - end Memory Networks ( GMem N2N ) .,CONCLUSION,CONCLUSION,question-answering,6,13,0.1857142857142857,272,0.8242424242424242,13,0.8666666666666667,0,0
274,"For QRN , a number in the front ( 1 , 2 , 3 , 6 ) indicates the number of layers and a number in the back ( 100 ) indicates the dimension of hidden vector , while the default value is 50 .",CONCLUSION,CONCLUSION,question-answering,6,14,0.2,273,0.8272727272727273,14,0.9333333333333332,0,0
275,"' r ' indicates that the reset gate is used , ' v ' indicates that the gates were vectorized , and '+ ' indicates that ' match ' was used .",CONCLUSION,CONCLUSION,question-answering,6,15,0.2142857142857142,274,0.8303030303030303,15,1.0,0,0
276,B VECTOR GATE PARALLELIZATION,CONCLUSION,CONCLUSION,question-answering,6,16,0.2285714285714285,275,0.8333333333333334,0,0.0,0,0
277,"For vector gates , we have z t ?",CONCLUSION,CONCLUSION,question-answering,6,17,0.2428571428571428,276,0.8363636363636363,1,0.0909090909090909,0,0
278,Rd instead of z t ?,CONCLUSION,CONCLUSION,question-answering,6,18,0.2571428571428571,277,0.8393939393939394,2,0.1818181818181818,0,0
279,R .,CONCLUSION,,question-answering,6,19,0.2714285714285714,278,0.8424242424242424,3,0.2727272727272727,0,0
280,Therefore the following equation replaces Equation :,CONCLUSION,R .,question-answering,6,20,0.2857142857142857,279,0.8454545454545455,4,0.3636363636363636,0,0
281,where z j k is the k - th column vector of z j .,CONCLUSION,R .,question-answering,6,21,0.3,280,0.8484848484848485,5,0.4545454545454545,0,0
282,Let b ij = log ( 1 ?,CONCLUSION,R .,question-answering,6,22,0.3142857142857143,281,0.8515151515151516,6,0.5454545454545454,0,0
283,z i j ) for brevity .,CONCLUSION,R .,question-answering,6,23,0.3285714285714285,282,0.8545454545454545,7,0.6363636363636364,0,0
284,"Then , we can rewrite Equation 8 as following :",CONCLUSION,R .,question-answering,6,24,0.3428571428571428,283,0.8575757575757575,8,0.7272727272727273,0,0
285,"where L , L ? R T T are lower and strictly lower triangular matrices of 1's are tiled across the column .",CONCLUSION,R .,question-answering,6,25,0.3571428571428571,284,0.8606060606060606,9,0.8181818181818182,0,0
286,"Z = [ z 1 , . . . , z d ] ?",CONCLUSION,R .,question-answering,6,26,0.3714285714285714,285,0.8636363636363636,10,0.9090909090909092,0,0
287,R T d .,CONCLUSION,,question-answering,6,27,0.3857142857142857,286,0.8666666666666667,11,1.0,0,0
288,C MODEL DETAILS,CONCLUSION,R T d .,question-answering,6,28,0.4,287,0.8696969696969697,0,0.0,0,0
289,Match .,CONCLUSION,,question-answering,6,29,0.4142857142857143,288,0.8727272727272727,1,0.1666666666666666,0,0
290,"While similar in spirit , our ' Match ' model is slightly different from previous work ( Bordes and .",CONCLUSION,Match .,question-answering,6,30,0.4285714285714285,289,0.8757575757575757,2,0.3333333333333333,0,0
291,"We use answer candidate embedding matrix , and add 2 dimension of 0 - 1 matrix which expresses whether the answer candidate matches with any word in the paragraph and the question .",CONCLUSION,Match .,question-answering,6,31,0.4428571428571428,290,0.8787878787878788,3,0.5,0,0
292,"In other words , the softmax is computed b? v = softmax W [ W ( y ) ; M ( y ) ]? ? RV , where W ?",CONCLUSION,Match .,question-answering,6,32,0.4571428571428571,291,0.8818181818181818,4,0.6666666666666666,0,0
293,"R dd and W ( y ) ? RV ( d?2 ) are trainable weight matrices , and M ( y ) ?",CONCLUSION,Match .,question-answering,6,33,0.4714285714285714,292,0.8848484848484849,5,0.8333333333333334,0,0
294,RV 2 is the 0 - 1 match matrix .,CONCLUSION,Match .,question-answering,6,34,0.4857142857142857,293,0.8878787878787879,6,1.0,0,0
295,D VISUALIZATIONS,CONCLUSION,,question-answering,6,35,0.5,294,0.8909090909090909,0,0.0,0,0
296,Visualization of Story - based QA . shows visualization of models for story - based QA tasks .,CONCLUSION,D VISUALIZATIONS,question-answering,6,36,0.5142857142857142,295,0.8939393939393939,1,0.0285714285714285,0,0
297,"In the task 3 ( left ) , the model focuses on the facts that contain ' football ' in the first layer , and found out where Mary journeyed to before the bathroom in the second layer .",CONCLUSION,D VISUALIZATIONS,question-answering,6,37,0.5285714285714286,296,0.896969696969697,2,0.0571428571428571,0,0
298,"In task 7 ( right ) , the model focuses on the facts that provide information about the location of Sandra . 0.00 0.87 1.00 1.00 I 'm on it .",CONCLUSION,D VISUALIZATIONS,question-answering,6,38,0.5428571428571428,297,0.9,3,0.0857142857142857,0,0
299,0.73 0.97 0.38 0.00,CONCLUSION,D VISUALIZATIONS,question-answering,6,39,0.5571428571428572,298,0.9030303030303032,4,0.1142857142857142,0,0
300,How many people would you in your party .,CONCLUSION,,question-answering,6,40,0.5714285714285714,299,0.906060606060606,5,0.1428571428571428,0,0
301,1.00 1.00 0.00 0.41,CONCLUSION,How many people would you in your party .,question-answering,6,41,0.5857142857142857,300,0.9090909090909092,6,0.1714285714285714,0,0
302,For four people please .,CONCLUSION,,question-answering,6,42,0.6,301,0.912121212121212,7,0.2,0,0
303,Which price range are you looking for .,CONCLUSION,,question-answering,6,43,0.6142857142857143,302,0.9151515151515152,8,0.2285714285714285,0,0
304,Layer 1 Layer 2 Task 1 Issuing API calls z 1 ? ? r 1 ? ? r 1 z,CONCLUSION,Which price range are you looking for .,question-answering,6,44,0.6285714285714286,303,0.9181818181818182,9,0.2571428571428571,0,0
305,2 Can you make a restaurant reservation for eight in a cheap price range in madrid 0.00 1.00 0.93 1.00 I 'm on it .,CONCLUSION,Which price range are you looking for .,question-answering,6,45,0.6428571428571429,304,0.9212121212121211,10,0.2857142857142857,0,0
306,0.00 1.00 0.74 0.00,CONCLUSION,Which price range are you looking for .,question-answering,6,46,0.6571428571428571,305,0.9242424242424242,11,0.3142857142857143,0,0
307,Any preference on a type of cuisine .,CONCLUSION,,question-answering,6,47,0.6714285714285714,306,0.9272727272727272,12,0.3428571428571428,0,0
308,0.00 0.11 1.00 0.01,CONCLUSION,Any preference on a type of cuisine .,question-answering,6,48,0.6857142857142857,307,0.9303030303030304,13,0.3714285714285714,0,0
309,I love british food .,CONCLUSION,,question-answering,6,49,0.7,308,0.9333333333333332,14,0.4,0,0
310,0.00 0.99 0.99 0.57,CONCLUSION,I love british food .,question-answering,6,50,0.7142857142857143,309,0.9363636363636364,15,0.4285714285714285,0,0
311,Okay let me look into some options for you .,CONCLUSION,I love british food .,question-answering,6,51,0.7285714285714285,310,0.9393939393939394,16,0.4571428571428571,0,0
312,1.00 0.00 0.00 0.02 < SILENCE > API CALL british madrid eight cheap Layer 1 Layer 2 Task 4 Providing extra-information z 1 ? ? r 1 ? ? r 1 z 2 resto-paris-expen-spanish-8stars,CONCLUSION,I love british food .,question-answering,6,52,0.7428571428571429,311,0.9424242424242424,17,0.4857142857142857,0,0
313,R-phone resto-paris -expen-spanish-8stars-phone 0.71 0.84 0.99 0.36 resto-paris-expen-spanish-8stars,CONCLUSION,,question-answering,6,53,0.7571428571428571,312,0.9454545454545454,18,0.5142857142857142,0,0
314,R-address resto-paris - expen-spanish-8stars-address 1.00 0.99 1.00 1.00 resto-paris-expen-spanish-8stars R-location paris 0.05 0.01 1.00 0.00 resto-paris-expen-spanish-8stars R-number four 0.02 0.95 0.97 0.00 resto-paris-expen-spanish-8stars,CONCLUSION,R-phone resto-paris -expen-spanish-8stars-phone 0.71 0.84 0.99 0.36 resto-paris-expen-spanish-8stars,question-answering,6,54,0.7714285714285715,313,0.9484848484848484,19,0.5428571428571428,0,0
315,R-price expensive 0.00 0.05 0.92 0.00 resto-paris-expen-spanish-8stars,CONCLUSION,,question-answering,6,55,0.7857142857142857,314,0.9515151515151516,20,0.5714285714285714,0,0
316,R-rating 8 0.38 0.91 1.00 0.10,CONCLUSION,,question-answering,6,56,0.8,315,0.9545454545454546,21,0.6,0,0
317,What do you think of this option : resto-paris-expen-spanish-8stars 0.90 0.93 0.99 1.00,CONCLUSION,R-rating 8 0.38 0.91 1.00 0.10,question-answering,6,57,0.8142857142857143,316,0.9575757575757576,22,0.6285714285714286,0,0
318,Let 's do it .,CONCLUSION,,question-answering,6,58,0.8285714285714286,317,0.9606060606060606,23,0.6571428571428571,0,0
319,0.00 0.00 1.00 0.00,CONCLUSION,Let 's do it .,question-answering,6,59,0.8428571428571429,318,0.9636363636363636,24,0.6857142857142857,0,0
320,Great let me do the reservation .,CONCLUSION,,question-answering,6,60,0.8571428571428571,319,0.9666666666666668,25,0.7142857142857143,0,0
321,0.98 0.99 0.97 0.00,CONCLUSION,Great let me do the reservation .,question-answering,6,61,0.8714285714285714,320,0.9696969696969696,26,0.7428571428571429,0,0
322,Do you have its address .,CONCLUSION,,question-answering,6,62,0.8857142857142857,321,0.9727272727272728,27,0.7714285714285715,0,0
323,Here it is : resto - paris - expen-spanish -8stars - address : Visualization of update and reset gates in QRN ' 2 r ' model for on several tasks of bAbI dialog and DSTC2 dialog .,CONCLUSION,Do you have its address .,question-answering,6,63,0.9,322,0.9757575757575756,28,0.8,0,0
324,We do not put reset gate in the last layer .,CONCLUSION,Do you have its address .,question-answering,6,64,0.9142857142857144,323,0.9787878787878788,29,0.8285714285714286,0,0
325,"Note that we only show some of recent sentences here , even the dialog has more sentences .",CONCLUSION,Do you have its address .,question-answering,6,65,0.9285714285714286,324,0.9818181818181818,30,0.8571428571428571,0,0
326,Visualization of Dialog .,CONCLUSION,,question-answering,6,66,0.9428571428571428,325,0.9848484848484848,31,0.8857142857142857,0,0
327,shows visualization of models for dialog tasks .,CONCLUSION,Visualization of Dialog .,question-answering,6,67,0.9571428571428572,326,0.987878787878788,32,0.9142857142857144,0,0
328,"In the first dialog of task 1 , the model focuses on the user utterance that mentions the user 's desired cuisine and location , and the current query ( user 's last utterance ) informs the system of the number of people , so the system is able to learn that it now needs to ask the user about the desired price range .",CONCLUSION,Visualization of Dialog .,question-answering,6,68,0.9714285714285714,327,0.990909090909091,33,0.9428571428571428,0,0
329,"In the second dialog of task 1 , the model focuses on the facts that provide information about the requests of the user .",CONCLUSION,Visualization of Dialog .,question-answering,6,69,0.9857142857142858,328,0.993939393939394,34,0.9714285714285714,0,0
330,"In task 4 ( third ) , the model focuses on what restaurant a user is talking about and the information about the restaurant .",CONCLUSION,Visualization of Dialog .,question-answering,6,70,1.0,329,0.996969696969697,35,1.0,0,0
1,title,,,question-answering,7,0,0.0,0,0.0,0,0.0,1,0
2,Neural Semantic Encoders,title,title,question-answering,7,1,0.0,1,0.0036363636363636,1,0.0,1,0
3,abstract,,,question-answering,7,0,0.0,2,0.0072727272727272,0,0.0,1,0
4,We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders .,abstract,abstract,question-answering,7,1,0.0357142857142857,3,0.0109090909090909,1,0.0144927536231884,1,1
5,"NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations .",abstract,abstract,question-answering,7,2,0.0714285714285714,4,0.0145454545454545,2,0.0289855072463768,1,0
6,NSE can also access 1 multiple and shared memories .,abstract,abstract,question-answering,7,3,0.1071428571428571,5,0.0181818181818181,3,0.0434782608695652,1,0
7,"In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks .",abstract,abstract,question-answering,7,4,0.1428571428571428,6,0.0218181818181818,4,0.0579710144927536,1,0
8,"For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",abstract,abstract,question-answering,7,5,0.1785714285714285,7,0.0254545454545454,5,0.072463768115942,1,0
9,Recurrent neural networks ( RNNs ) have been successful for modeling sequences,abstract,abstract,question-answering,7,6,0.2142857142857142,8,0.029090909090909,6,0.0869565217391304,1,0
10,[ 1 ] .,abstract,abstract,question-answering,7,7,0.25,9,0.0327272727272727,7,0.1014492753623188,1,0
11,"Particularly , RNNs equipped with internal short memories , such as long short - term memories ( LSTM )",abstract,abstract,question-answering,7,8,0.2857142857142857,10,0.0363636363636363,8,0.1159420289855072,1,0
12,"[ 2 ] have achieved a notable success in sequential tasks [ 3 , 4 ] .",abstract,abstract,question-answering,7,9,0.3214285714285714,11,0.04,9,0.1304347826086956,1,0
13,LSTM is powerful because it learns to control it s short term memories .,abstract,abstract,question-answering,7,10,0.3571428571428571,12,0.0436363636363636,10,0.144927536231884,1,0
14,"However , the short term memories in LSTM area part of the training parameters .",abstract,abstract,question-answering,7,11,0.3928571428571428,13,0.0472727272727272,11,0.1594202898550724,1,0
15,This imposes some practical difficulties in training and modeling long sequences with LSTM .,abstract,abstract,question-answering,7,12,0.4285714285714285,14,0.0509090909090909,12,0.1739130434782608,1,0
16,Recently several studies have explored ways of extending the neural networks with an external memory [ 5 ] [ 6 ] [ 7 ] .,abstract,abstract,question-answering,7,13,0.4642857142857143,15,0.0545454545454545,13,0.1884057971014492,1,0
17,"Unlike LSTM , the short term memories and the training parameters of such a neural network are no longer coupled and can be adapted .",abstract,abstract,question-answering,7,14,0.5,16,0.0581818181818181,14,0.2028985507246377,1,0
18,In this paper we propose a novel class of memory augmented neural networks called Neural Semantic Encoders ( NSE ) for natural language understanding .,abstract,abstract,question-answering,7,15,0.5357142857142857,17,0.0618181818181818,15,0.217391304347826,1,1
19,NSE offers several desirable properties .,abstract,,question-answering,7,16,0.5714285714285714,18,0.0654545454545454,16,0.2318840579710145,1,0
20,NSE has a variable sized encoding memory which allows the model to access entire input sequence during the reading process ; therefore efficiently delivering long - term dependencies overtime .,abstract,NSE offers several desirable properties .,question-answering,7,17,0.6071428571428571,19,0.069090909090909,17,0.2463768115942029,1,1
21,"The encoding memory evolves overtime and maintains the memory of the input sequence through read , compose and write operations .",abstract,NSE offers several desirable properties .,question-answering,7,18,0.6428571428571429,20,0.0727272727272727,18,0.2608695652173913,1,1
22,NSE sequentially processes the input and supports word compositionality inheriting both temporal and hierarchical nature of human language .,abstract,NSE offers several desirable properties .,question-answering,7,19,0.6785714285714286,21,0.0763636363636363,19,0.2753623188405797,1,1
23,NSE can read from and write to a set of relevant encoding memories simultaneously or multiple NSEs can access a shared encoding memory effectively supporting knowledge and representation sharing .,abstract,NSE offers several desirable properties .,question-answering,7,20,0.7142857142857143,22,0.08,20,0.2898550724637681,1,1
24,"NSE is flexible , robust and suitable for practical NLU tasks and can be trained easily by any gradient descent optimizer .",abstract,NSE offers several desirable properties .,question-answering,7,21,0.75,23,0.0836363636363636,21,0.3043478260869565,1,1
25,We evaluate NSE on five different real tasks .,abstract,,question-answering,7,22,0.7857142857142857,24,0.0872727272727272,22,0.3188405797101449,1,0
26,"For four of them , our models set new state - of - theart results .",abstract,We evaluate NSE on five different real tasks .,question-answering,7,23,0.8214285714285714,25,0.0909090909090909,23,0.3333333333333333,1,0
27,Our results suggest that a NN model with the shared memory between encoder and decoder is a promising approach for sequence transduction problems such as machine translation and abstractive summarization .,abstract,We evaluate NSE on five different real tasks .,question-answering,7,24,0.8571428571428571,26,0.0945454545454545,24,0.3478260869565217,1,0
28,"In particular , we observe that the attention - based neural machine translation can be further improved by shared - memory models .",abstract,We evaluate NSE on five different real tasks .,question-answering,7,25,0.8928571428571429,27,0.0981818181818181,25,0.3623188405797101,1,0
29,We also analyze memory access pattern and compositionality in NSE and show that our model captures semantic and syntactic structures of input sentence .,abstract,We evaluate NSE on five different real tasks .,question-answering,7,26,0.9285714285714286,28,0.1018181818181818,26,0.3768115942028985,1,0
30,1,abstract,We evaluate NSE on five different real tasks .,question-answering,7,27,0.9642857142857144,29,0.1054545454545454,27,0.391304347826087,1,0
31,"By access we mean changing the memory states by the read , compose and write operations .",abstract,We evaluate NSE on five different real tasks .,question-answering,7,28,1.0,30,0.109090909090909,28,0.4057971014492754,1,0
32,Related Work,,,question-answering,7,0,0.0,31,0.1127272727272727,29,0.4202898550724637,1,0
33,One of the pioneering work that attempts to extend deep neural networks with an external memory is Neural Turing Machines ( NTM ) .,Related Work,Related Work,question-answering,7,1,0.025,32,0.1163636363636363,30,0.4347826086956521,0,0
34,NTM implements a centralized controller and a fixed - sized random access memory .,Related Work,Related Work,question-answering,7,2,0.05,33,0.12,31,0.4492753623188406,0,0
35,The NTM memory is addressable by both content ( i.e. soft attention ) and location based access mechanisms .,Related Work,Related Work,question-answering,7,3,0.075,34,0.1236363636363636,32,0.463768115942029,0,0
36,The authors evaluated NTM on algorithmic tasks such as copying and sorting sequences .,Related Work,Related Work,question-answering,7,4,0.1,35,0.1272727272727272,33,0.4782608695652174,0,0
37,Comparison with Neural Turing Machines : NSE addresses certain drawbacks of NTM .,Related Work,Related Work,question-answering,7,5,0.125,36,0.1309090909090909,34,0.4927536231884058,0,0
38,"NTM has a single centralized controller , which is usually an MLP or RNN while NSE takes a modular approach .",Related Work,Related Work,question-answering,7,6,0.15,37,0.1345454545454545,35,0.5072463768115942,0,0
39,"The main controller in NSE is decomposed into three separate modules , each of which performs for read , compose or write operation .",Related Work,Related Work,question-answering,7,7,0.175,38,0.1381818181818181,36,0.5217391304347826,0,0
40,"In NSE , the compose module is introduced in addition to the standard memory update operations ( i.e. read - write ) in order to process the memory entries and input information .",Related Work,Related Work,question-answering,7,8,0.2,39,0.1418181818181818,37,0.5362318840579711,0,0
41,The main advantage of NSE over NTM is in its memory update .,Related Work,Related Work,question-answering,7,9,0.225,40,0.1454545454545454,38,0.5507246376811594,0,0
42,"Despite its sophisticated addressing mechanism , the NTM controller does not have mechanism to avoid information collision in the memory .",Related Work,Related Work,question-answering,7,10,0.25,41,0.149090909090909,39,0.5652173913043478,0,0
43,Particularly the NTM controller emits two separate set of access weights ( i.e. read weight and erase and write weights ) that do not explicitly encode the knowledge about where information is read from and written to .,Related Work,Related Work,question-answering,7,11,0.275,42,0.1527272727272727,40,0.5797101449275363,0,0
44,Moreover the fixed - size memory in NTM has no memory allocation or de-allocation protocol .,Related Work,Related Work,question-answering,7,12,0.3,43,0.1563636363636363,41,0.5942028985507246,0,0
45,"Therefore unless the controller is intelligent enough to track the previous read / write information , which is hard for an RNN when processing long sequences , the memory content is overlapped and information is overwritten throughout different time scales .",Related Work,Related Work,question-answering,7,13,0.325,44,0.16,42,0.6086956521739131,0,0
46,We think that this is a potential reason that makes NTM hard to train and makes the training not stable .,Related Work,Related Work,question-answering,7,14,0.35,45,0.1636363636363636,43,0.6231884057971014,0,0
47,We also note that the effectiveness of the location based addressing introduced in NTM is unclear .,Related Work,Related Work,question-answering,7,15,0.375,46,0.1672727272727272,44,0.6376811594202898,0,0
48,"In NSE , we introduce a novel and systematic memory update approach based on the soft attention mechanism .",Related Work,Related Work,question-answering,7,16,0.4,47,0.1709090909090909,45,0.6521739130434783,0,0
49,NSE writes new information to the most recently read memory locations .,Related Work,Related Work,question-answering,7,17,0.425,48,0.1745454545454545,46,0.6666666666666666,0,0
50,This is accomplished by sharing the same memory key vector between the read and write modules .,Related Work,Related Work,question-answering,7,18,0.45,49,0.1781818181818182,47,0.6811594202898551,0,0
51,The NSE memory update is scalable and potentially more robust to train .,Related Work,Related Work,question-answering,7,19,0.475,50,0.1818181818181818,48,0.6956521739130435,0,0
52,"NSE is provided with a variable sized memory and thus unlike NTM , the size of the NSE memory is more relaxed .",Related Work,Related Work,question-answering,7,20,0.5,51,0.1854545454545454,49,0.7101449275362319,0,0
53,The novel memory update mechanism and the variable sized memory together prevent NSE from the information collision issue and avoid the need of the memory allocation or de-allocation protocols .,Related Work,Related Work,question-answering,7,21,0.525,52,0.1890909090909091,50,0.7246376811594203,0,0
54,Each memory location of the NSE memory stores a token representation in input sequence during encoding .,Related Work,Related Work,question-answering,7,22,0.55,53,0.1927272727272727,51,0.7391304347826086,0,0
55,"This provides NSE with an anytime - access to the entire input sequence including the tokens from the future time scales , which is not permitted in NTM , RNN and attention - based encoders .",Related Work,Related Work,question-answering,7,23,0.575,54,0.1963636363636363,52,0.7536231884057971,0,0
56,"Lastly , NTM addresses small algorithmic problems while NSE focuses on a set of large - scale language understanding tasks .",Related Work,Related Work,question-answering,7,24,0.6,55,0.2,53,0.7681159420289855,0,0
57,The RNNSearch model proposed in can be seen as a variation of memory augmented networks due to its ability to read the historic output states of RNNs with soft attention .,Related Work,Related Work,question-answering,7,25,0.625,56,0.2036363636363636,54,0.782608695652174,0,0
58,The work of combines the soft attention with Memory Networks ( Mem NNs ) .,Related Work,Related Work,question-answering,7,26,0.65,57,0.2072727272727272,55,0.7971014492753623,0,0
59,"Similar to RNNSearch , MemNNs are designed with non-writable memories .",Related Work,Related Work,question-answering,7,27,0.675,58,0.2109090909090909,56,0.8115942028985508,0,0
60,It constructs layered memory representations and showed promising results on both artificial and real question answering tasks .,Related Work,Related Work,question-answering,7,28,0.7,59,0.2145454545454545,57,0.8260869565217391,0,0
61,We note that RNNSearch and MemNNs avoid the memory update and management overhead by simply using a non-writable memory storage .,Related Work,Related Work,question-answering,7,29,0.725,60,0.2181818181818181,58,0.8405797101449275,0,0
62,Another variation of MemNNs is Dynamic Memory Network that is equipped with an episodic memory and seems to be flexible in different settings .,Related Work,Related Work,question-answering,7,30,0.75,61,0.2218181818181818,59,0.855072463768116,0,0
63,"Although NSE differs from other memory - augumented NN models in many aspects , they all use soft attention mechanism with a type of similarity measures to retrieve relevant information from the external memory .",Related Work,Related Work,question-answering,7,31,0.775,62,0.2254545454545454,60,0.8695652173913043,0,0
64,"For example , NTM implements cosine similarity and MemNNs use vector dot product .",Related Work,Related Work,question-answering,7,32,0.8,63,0.2290909090909091,61,0.8840579710144928,0,0
65,NSE uses the vector dot product for the similarity measure in NSE because it is faster to compute .,Related Work,Related Work,question-answering,7,33,0.825,64,0.2327272727272727,62,0.8985507246376812,0,0
66,"Other related work includes Neural Program - Interpreters , which learns to run sub-programs and to compose them for high - level programs .",Related Work,Related Work,question-answering,7,34,0.85,65,0.2363636363636363,63,0.9130434782608696,0,0
67,It uses execution traces to provide the full supervision .,Related Work,Related Work,question-answering,7,35,0.875,66,0.24,64,0.927536231884058,0,0
68,Researchers have also explored ways to add unbounded memory to LSTM using a particular data structure .,Related Work,Related Work,question-answering,7,36,0.9,67,0.2436363636363636,65,0.9420289855072465,0,0
69,"Although this type of architecture provides a flexible capacity to store information , the memory access is constrained by the data structure used for the memory bank , such as stack and queue .",Related Work,Related Work,question-answering,7,37,0.925,68,0.2472727272727272,66,0.9565217391304348,0,0
70,Overall it is expensive to train and to scale the previously proposed memory - based models .,Related Work,Related Work,question-answering,7,38,0.95,69,0.2509090909090909,67,0.9710144927536232,0,0
71,Most models required a set of clever engineering tricks to work successfully .,Related Work,Related Work,question-answering,7,39,0.975,70,0.2545454545454545,68,0.9855072463768116,0,0
72,Most of the aforementioned memory augmented neural networks have been tested on synthetic tasks whereas in this paper we evaluated NSE on a wide range of real and large - scale natural language applications .,Related Work,Related Work,question-answering,7,40,1.0,71,0.2581818181818182,69,1.0,0,0
73,Proposed Approach,,,question-answering,7,0,0.0,72,0.2618181818181818,0,0.0,1,0
74,Our training set consists,,,question-answering,7,0,0.0,73,0.2654545454545454,1,0.0277777777777777,1,0
75,". . , w i Ti of tokens while the output Y i can be either a single target or a sequence .",Our training set consists,Our training set consists,question-answering,7,1,0.0208333333333333,74,0.2690909090909091,2,0.0555555555555555,1,0
76,We transform each input token wt to its word embedding x t .,Our training set consists,Our training set consists,question-answering,7,2,0.0416666666666666,75,0.2727272727272727,3,0.0833333333333333,1,0
77,"Our Neural Semantic Encoders ( NSE ) model has four main components : read , compose and write modules and an encoding memory M ?",Our training set consists,Our training set consists,question-answering,7,3,0.0625,76,0.2763636363636363,4,0.1111111111111111,1,0
78,"R kl with a variable number of slots , where k is the embedding dimension and l is the length of the input sequence .",Our training set consists,Our training set consists,question-answering,7,4,0.0833333333333333,77,0.28,5,0.1388888888888889,1,0
79,Each memory slot vector mt ?,Our training set consists,Our training set consists,question-answering,7,5,0.1041666666666666,78,0.2836363636363636,6,0.1666666666666666,1,0
80,R k corresponds to the vector representation of information about word wt in memory .,Our training set consists,Our training set consists,question-answering,7,6,0.125,79,0.2872727272727273,7,0.1944444444444444,1,0
81,"In particular , the memory is initialized by the embedding vectors {x t } l t=1 and is evolved overtime , through read , compose and write operations .",Our training set consists,Our training set consists,question-answering,7,7,0.1458333333333333,80,0.2909090909090909,8,0.2222222222222222,1,0
82,"Read , Compose and Write",Our training set consists,,question-answering,7,8,0.1666666666666666,81,0.2945454545454545,9,0.25,1,0
83,NSE performs three main operations in every time step .,Our training set consists,"Read , Compose and Write",question-answering,7,9,0.1875,82,0.2981818181818181,10,0.2777777777777778,1,0
84,"After initializing the memory slots with the corresponding input representations , NSE processes an embedding vector x t and retrieves a memory slot m r,t that is expected to be associatively coherent ( i.e. semantically associated ) with the current input word wt .",Our training set consists,"Read , Compose and Write",question-answering,7,10,0.2083333333333333,83,0.3018181818181818,11,0.3055555555555556,1,0
85,The slot location r ( ranging from 1 to l ) is defined by a key vector z t which the read module emits by attending over the memory slots .,Our training set consists,"Read , Compose and Write",question-answering,7,11,0.2291666666666666,84,0.3054545454545455,12,0.3333333333333333,1,0
86,The compose module implements a composition operation that combines the memory slot with the current input .,Our training set consists,"Read , Compose and Write",question-answering,7,12,0.25,85,0.3090909090909091,13,0.3611111111111111,1,0
87,The write module then transforms the composition output to the encoding memory space and writes the resulting new representation into the slot location of the memory .,Our training set consists,"Read , Compose and Write",question-answering,7,13,0.2708333333333333,86,0.3127272727272727,14,0.3888888888888889,1,0
88,"Instead of composing the raw embedding vector x t , we use the hidden state o t produced by the read module at time t",Our training set consists,"Read , Compose and Write",question-answering,7,14,0.2916666666666667,87,0.3163636363636363,15,0.4166666666666667,1,0
89,"where 1 is a matrix of ones , ?",Our training set consists,"Read , Compose and Write",question-answering,7,15,0.3125,88,0.32,16,0.4444444444444444,1,0
90,denotes the outer product which duplicates it s left vector l or k times to form a matrix .,Our training set consists,"Read , Compose and Write",question-answering,7,16,0.3333333333333333,89,0.3236363636363636,17,0.4722222222222222,1,0
91,The read function f LST,Our training set consists,,question-answering,7,17,0.3541666666666667,90,0.3272727272727272,18,0.5,1,0
92,Mr sequentially maps the word embeddings to the internal space of the memory M t?1 .,Our training set consists,The read function f LST,question-answering,7,18,0.375,91,0.3309090909090909,19,0.5277777777777778,1,0
93,Then Equation 2 looks for the slots related to the input by computing association degree between each memory slot and the hidden state o t .,Our training set consists,The read function f LST,question-answering,7,19,0.3958333333333333,92,0.3345454545454545,20,0.5555555555555556,1,0
94,We calculate the association degree by the dot product and transform this scores to the fuzzy key vector z t by normalizing with sof tmax function .,Our training set consists,The read function f LST,question-answering,7,20,0.4166666666666667,93,0.3381818181818182,21,0.5833333333333334,1,0
95,"Since our key vector is fuzzy , the slot to be composed is retrieved by taking weighted sum of the all slots as in Equation .",Our training set consists,The read function f LST,question-answering,7,21,0.4375,94,0.3418181818181818,22,0.6111111111111112,1,0
96,This process can also be seen as the soft attention mechanism .,Our training set consists,The read function f LST,question-answering,7,22,0.4583333333333333,95,0.3454545454545454,23,0.6388888888888888,1,0
97,"In Equation 4 and 5 , we compose and process the retrieved slot with the current hidden state and map the resulting vector to the encoder output space .",Our training set consists,The read function f LST,question-answering,7,23,0.4791666666666667,96,0.3490909090909091,24,0.6666666666666666,1,0
98,"Finally , we write the new representation to the memory location pointed by the key vector in where the key vector z t emitted by the read module is reused to inform the write module of the most recently read slots .",Our training set consists,The read function f LST,question-answering,7,24,0.5,97,0.3527272727272727,25,0.6944444444444444,1,0
99,First the slot information that was retrieved is erased and then the new representation is located .,Our training set consists,The read function f LST,question-answering,7,25,0.5208333333333334,98,0.3563636363636364,26,0.7222222222222222,1,0
100,NSE performs this iterative process until all words in the input sequence are read .,Our training set consists,The read function f LST,question-answering,7,26,0.5416666666666666,99,0.36,27,0.75,1,0
101,The encoding memories { M } T t=1 and output states {h} T t=1 are further used for the tasks .,Our training set consists,The read function f LST,question-answering,7,27,0.5625,100,0.3636363636363636,28,0.7777777777777778,1,0
102,"Although NSE reads a single word at a time , it has an anytime - access to the entire sequence stored in the encoding memory .",Our training set consists,The read function f LST,question-answering,7,28,0.5833333333333334,101,0.3672727272727272,29,0.8055555555555556,1,0
103,"With the encoding memory , NSE maintains a mental image of the input sequence .",Our training set consists,The read function f LST,question-answering,7,29,0.6041666666666666,102,0.3709090909090909,30,0.8333333333333334,1,0
104,The memory is initialized with the raw embedding vector at time t = 0 .,Our training set consists,The read function f LST,question-answering,7,30,0.625,103,0.3745454545454545,31,0.8611111111111112,1,0
105,We term such a freshly initialized memory a baby memory .,Our training set consists,The read function f LST,question-answering,7,31,0.6458333333333334,104,0.3781818181818182,32,0.8888888888888888,1,0
106,"As NSE reads more input content in time , the baby memory evolves and refines the encoded mental image .",Our training set consists,The read function f LST,question-answering,7,32,0.6666666666666666,105,0.3818181818181818,33,0.9166666666666666,1,0
107,functions are neural networks and are the training parameters in our NSE .,Our training set consists,The read function f LST,question-answering,7,33,0.6875,106,0.3854545454545454,34,0.9444444444444444,1,0
108,"As the name suggests , we use LSTM and multi -layer perceptron ( MLP ) in this paper .",Our training set consists,The read function f LST,question-answering,7,34,0.7083333333333334,107,0.3890909090909091,35,0.9722222222222222,1,0
109,"Since NSE is fully differentiable , it can be trained with any gradient descent optimizer .",Our training set consists,The read function f LST,question-answering,7,35,0.7291666666666666,108,0.3927272727272727,36,1.0,1,0
110,Shared and Multiple Memory Accesses,Our training set consists,The read function f LST,question-answering,7,36,0.75,109,0.3963636363636363,0,0.0,1,0
111,"For sequence to sequence transduction tasks like question answering , natural language inference and machine translation , it is beneficial to access other relevant memories in addition to its own one .",Our training set consists,The read function f LST,question-answering,7,37,0.7708333333333334,110,0.4,1,0.0833333333333333,1,0
112,The shared or the multiple memory access allows a set of NSEs to exchange knowledge representations and to communicate with each other to accomplish a particular task throughout the encoding memory .,Our training set consists,The read function f LST,question-answering,7,38,0.7916666666666666,111,0.4036363636363636,2,0.1666666666666666,1,0
113,"NSE can be extended easily , so that it is able to read from and write to multiple memories simultaneously or multiple NSEs are able to access a shared memory .",Our training set consists,The read function f LST,question-answering,7,39,0.8125,112,0.4072727272727273,3,0.25,1,0
114,( b ) depicts a high - level architectural diagram of a multiple memory access - NSE ( MMA - NSE ) .,Our training set consists,The read function f LST,question-answering,7,40,0.8333333333333334,113,0.4109090909090909,4,0.3333333333333333,1,0
115,The first memory ( in green ) is the shared memory accessed by more than one NSEs .,Our training set consists,The read function f LST,question-answering,7,41,0.8541666666666666,114,0.4145454545454545,5,0.4166666666666667,1,0
116,Given a shared memory Mn ?,Our training set consists,The read function f LST,question-answering,7,42,0.875,115,0.4181818181818181,6,0.5,1,0
117,"R kn that has been encoded by processing a relevant sequence with length n , MMA - NSE with the access to one relevant memory is defined as",Our training set consists,The read function f LST,question-answering,7,43,0.8958333333333334,116,0.4218181818181818,7,0.5833333333333334,1,0
118,and this is almost the same as standard NSE .,Our training set consists,The read function f LST,question-answering,7,44,0.9166666666666666,117,0.4254545454545455,8,0.6666666666666666,1,0
119,The read module now emits the additional key vector z n t for the shared memory and the composition function f M LP c combines more than one slots .,Our training set consists,The read function f LST,question-answering,7,45,0.9375,118,0.4290909090909091,9,0.75,1,0
120,"In MMA - NSE , the different memory slots are retrieved from the shared memories depending on their encoded semantic representations .",Our training set consists,The read function f LST,question-answering,7,46,0.9583333333333334,119,0.4327272727272727,10,0.8333333333333334,1,0
121,They are then composed together with the current input and written back to their corresponding slots .,Our training set consists,The read function f LST,question-answering,7,47,0.9791666666666666,120,0.4363636363636363,11,0.9166666666666666,1,0
122,Note that MMA - NSE is capable of accessing a variable number of relevant shared memories once a composition function that takes in dynamic inputs is chosen .,Our training set consists,The read function f LST,question-answering,7,48,1.0,121,0.44,12,1.0,1,0
123,Experiments,,,question-answering,7,0,0.0,122,0.4436363636363636,0,0.0,1,0
124,"We describe in this section experiments on five different tasks , in order to show that NSE can be effective and flexible in different settings .",Experiments,Experiments,question-answering,7,1,0.0071428571428571,123,0.4472727272727272,1,0.0909090909090909,1,0
125,"We report results on natural language inference , question answering ( QA ) , sentence classification , document sentiment analysis and machine translation .",Experiments,Experiments,question-answering,7,2,0.0142857142857142,124,0.4509090909090909,2,0.1818181818181818,1,0
126,All five tasks challenge a model in terms of language understanding and semantic reasoning .,Experiments,Experiments,question-answering,7,3,0.0214285714285714,125,0.4545454545454545,3,0.2727272727272727,1,0
127,The models are trained using Adam with hyperparameters selected on development set .,Experiments,Experiments,question-answering,7,4,0.0285714285714285,126,0.4581818181818182,4,0.3636363636363636,1,1
128,We chose two one - layer LSTM for read / write modules on the tasks other than QA on which we used two - layer LSTM .,Experiments,Experiments,question-answering,7,5,0.0357142857142857,127,0.4618181818181818,5,0.4545454545454545,1,1
129,The pre-trained 300 - D Glove 840B vectors and 100 - D Glove 6B vectors were obtained for the word embeddings .,Experiments,Experiments,question-answering,7,6,0.0428571428571428,128,0.4654545454545454,6,0.5454545454545454,1,1
130,The word embeddings are fixed during training .,Experiments,,question-answering,7,7,0.05,129,0.4690909090909091,7,0.6363636363636364,1,0
131,The embeddings for out - of - vocabulary words were set to zero vector .,Experiments,The word embeddings are fixed during training .,question-answering,7,8,0.0571428571428571,130,0.4727272727272727,8,0.7272727272727273,1,0
132,We crop or pad the input sequence to a fixed length .,Experiments,The word embeddings are fixed during training .,question-answering,7,9,0.0642857142857142,131,0.4763636363636364,9,0.8181818181818182,1,1
133,A padding vector was inserted when padding .,Experiments,,question-answering,7,10,0.0714285714285714,132,0.48,10,0.9090909090909092,1,0
134,The models were regularized by using dropouts and an l 2 weight decay .,Experiments,A padding vector was inserted when padding .,question-answering,7,11,0.0785714285714285,133,0.4836363636363636,11,1.0,1,1
135,Natural Language Inference,Experiments,A padding vector was inserted when padding .,question-answering,7,12,0.0857142857142857,134,0.4872727272727272,0,0.0,1,0
136,The natural language inference is one of the main tasks in language understanding .,Experiments,A padding vector was inserted when padding .,question-answering,7,13,0.0928571428571428,135,0.4909090909090909,1,0.0384615384615384,1,0
137,This task tests the ability of a model to reason about the semantic relationship between two sentences .,Experiments,A padding vector was inserted when padding .,question-answering,7,14,0.1,136,0.4945454545454545,2,0.0769230769230769,1,0
138,"In order to perform well on the task , NSE should be able to capture sentence semantics and be able to reason the relation between a sentence pair , i.e. , whether a premise - hypothesis pair is entailing , contradictory or neutral .",Experiments,A padding vector was inserted when padding .,question-answering,7,15,0.1071428571428571,137,0.4981818181818181,3,0.1153846153846153,1,0
139,"We conducted experiments on the Stanford Natural Language Inference ( SNLI ) dataset , which consists of 549,367/9,842/9,824 premise-hypothesis pairs for train / dev / test sets and target label indicating their relation .",Experiments,A padding vector was inserted when padding .,question-answering,7,16,0.1142857142857142,138,0.5018181818181818,4,0.1538461538461538,1,0
140,"Following the setting in the NSE output for each sentence was the input to a MLP , where the input layer computes the concatenation [ h pl ; h h l ] , absolute difference hp l ?",Experiments,A padding vector was inserted when padding .,question-answering,7,17,0.1214285714285714,139,0.5054545454545455,5,0.1923076923076923,1,0
141,h h land elementwise product hp l h h l of the two sentence representations .,Experiments,A padding vector was inserted when padding .,question-answering,7,18,0.1285714285714285,140,0.509090909090909,6,0.2307692307692307,1,0
142,"In addition , the MLP has a hidden layer with 1024 units with ReLU activation and a sof tmax layer .",Experiments,A padding vector was inserted when padding .,question-answering,7,19,0.1357142857142857,141,0.5127272727272727,7,0.2692307692307692,1,1
143,"We set the batch size to 128 , the initial learning rate to 3e - 4 and l 2 regularizer strength to 3 e - 5 , and train each model for 40 epochs .",Experiments,A padding vector was inserted when padding .,question-answering,7,20,0.1428571428571428,142,0.5163636363636364,8,0.3076923076923077,1,1
144,The write / read neural nets and the last linear layer were regularized by using 30 % dropouts .,Experiments,A padding vector was inserted when padding .,question-answering,7,21,0.15,143,0.52,9,0.3461538461538461,1,0
145,We evaluated three different variations of NSE show in .,Experiments,A padding vector was inserted when padding .,question-answering,7,22,0.1571428571428571,144,0.5236363636363637,10,0.3846153846153846,1,0
146,The NSE model encodes each sentence simultaneously by using a separate memory for each sentence .,Experiments,A padding vector was inserted when padding .,question-answering,7,23,0.1642857142857142,145,0.5272727272727272,11,0.4230769230769231,1,0
147,The second model - MMA - NSE first encodes the premise and then the hypothesis sentence by sharing the premise encoded memory in addition to the hypothesis memory .,Experiments,A padding vector was inserted when padding .,question-answering,7,24,0.1714285714285714,146,0.5309090909090909,12,0.4615384615384615,1,0
148,"For the third model , we use inter-sentence attention which selectively reconstructs the premise representation .",Experiments,A padding vector was inserted when padding .,question-answering,7,25,0.1785714285714285,147,0.5345454545454545,13,0.5,1,0
149,shows the results of our models along with the results of published methods for the task .,Experiments,A padding vector was inserted when padding .,question-answering,7,26,0.1857142857142857,148,0.5381818181818182,14,0.5384615384615384,1,0
150,The classifier with handcrafted features extracts a set of lexical features .,Experiments,A padding vector was inserted when padding .,question-answering,7,27,0.1928571428571428,149,0.5418181818181819,15,0.5769230769230769,1,0
151,The next group of models are based on sentence encoding .,Experiments,A padding vector was inserted when padding .,question-answering,7,28,0.2,150,0.5454545454545454,16,0.6153846153846154,1,0
152,"While most of the sentence encoder models rely solely on word embeddings , the dependency tree CNN and the SPINN - PI models make use of sentence parser output .",Experiments,A padding vector was inserted when padding .,question-answering,7,29,0.2071428571428571,151,0.5490909090909091,17,0.6538461538461539,1,0
153,The SPINN - PI model is similar to NSE in spirit that it also explicitly computes word composition .,Experiments,A padding vector was inserted when padding .,question-answering,7,30,0.2142857142857142,152,0.5527272727272727,18,0.6923076923076923,1,0
154,"However , the composition in the SPINN - PI is guided by supervisions from a dependency parser .",Experiments,A padding vector was inserted when padding .,question-answering,7,31,0.2214285714285714,153,0.5563636363636364,19,0.7307692307692307,1,0
155,NSE outperformed the previous sentence encoders on this task .,Experiments,A padding vector was inserted when padding .,question-answering,7,32,0.2285714285714285,154,0.56,20,0.7692307692307693,1,0
156,"The MMA - SNE further slightly improved the result , indicating that reading the premise memory is helpful while encoding the hypothesis .",Experiments,A padding vector was inserted when padding .,question-answering,7,33,0.2357142857142857,155,0.5636363636363636,21,0.8076923076923077,1,0
157,The last set of methods designs inter-sentence relation with parameterized soft attention .,Experiments,A padding vector was inserted when padding .,question-answering,7,34,0.2428571428571428,156,0.5672727272727273,22,0.8461538461538461,1,0
158,Our MMA - NSE attention model is similar to the LSTM attention model .,Experiments,A padding vector was inserted when padding .,question-answering,7,35,0.25,157,0.5709090909090909,23,0.8846153846153846,1,1
159,"Particularly , it attends over the premise encoder outputs {h p } T t= 1 in respect to the final hypothesis representation h h land constructs an attentively blended vector of the premise .",Experiments,A padding vector was inserted when padding .,question-answering,7,36,0.2571428571428571,158,0.5745454545454546,24,0.9230769230769232,1,0
160,This model obtained 85.4 % accuracy score .,Experiments,,question-answering,7,37,0.2642857142857143,159,0.5781818181818181,25,0.9615384615384616,1,1
161,The best performing model for this task performs tree matching with attention mechanism and LSTM .,Experiments,This model obtained 85.4 % accuracy score .,question-answering,7,38,0.2714285714285714,160,0.5818181818181818,26,1.0,1,0
162,Answer Sentence Selection,Experiments,This model obtained 85.4 % accuracy score .,question-answering,7,39,0.2785714285714286,161,0.5854545454545454,0,0.0,1,0
163,Answer sentence selection is an integral part of the open - domain question answering .,Experiments,This model obtained 85.4 % accuracy score .,question-answering,7,40,0.2857142857142857,162,0.5890909090909091,1,0.0434782608695652,1,0
164,"For this task , a model is trained to identify the correct sentences that answer a factual question , from a set of candidate sentences .",Experiments,This model obtained 85.4 % accuracy score .,question-answering,7,41,0.2928571428571428,163,0.5927272727272728,2,0.0869565217391304,1,0
165,We experiment on WikiQA dataset constructed from Wikipedia .,Experiments,,question-answering,7,42,0.3,164,0.5963636363636363,3,0.1304347826086956,1,0
166,"The dataset contains 20,360/2,733/6,165 QA pairs for train / dev / test sets .",Experiments,We experiment on WikiQA dataset constructed from Wikipedia .,question-answering,7,43,0.3071428571428571,165,0.6,4,0.1739130434782608,1,0
167,"The MLP setup used in the language inference task is kept same , except that we now replace the sof tmax layer with a sigmoid layer and model the following conditional probability distribution .",Experiments,We experiment on WikiQA dataset constructed from Wikipedia .,question-answering,7,44,0.3142857142857143,166,0.6036363636363636,5,0.217391304347826,1,0
168,where h q land ha l are the question and the answer encoded vectors and o QA denotes the output of the hidden layer of the MLP .,Experiments,We experiment on WikiQA dataset constructed from Wikipedia .,question-answering,7,45,0.3214285714285714,167,0.6072727272727273,6,0.2608695652173913,1,0
169,We trained the MMA - NSE attention model to minimize the sigmoid cross entropy loss .,Experiments,We experiment on WikiQA dataset constructed from Wikipedia .,question-answering,7,46,0.3285714285714285,168,0.610909090909091,7,0.3043478260869565,1,0
170,MMA - NSE first encodes the answers and then the questions by accessing its own and the answer encoding memories .,Experiments,We experiment on WikiQA dataset constructed from Wikipedia .,question-answering,7,47,0.3357142857142857,169,0.6145454545454545,8,0.3478260869565217,1,0
171,"In our preliminary experiment , we found that the multiple memory access and the attention over answer encoder outputs {h a } T t= 1 are crucial to this problem .",Experiments,We experiment on WikiQA dataset constructed from Wikipedia .,question-answering,7,48,0.3428571428571428,170,0.6181818181818182,9,0.391304347826087,1,0
172,"Following previous work , we adopt MAP and MRR as the evaluation metrics for this task .",Experiments,We experiment on WikiQA dataset constructed from Wikipedia .,question-answering,7,49,0.35,171,0.6218181818181818,10,0.4347826086956521,1,0
173,"We set the batch size to 4 and the initial learning rate to 1 e - 5 , and train the model for 10 epochs .",Experiments,We experiment on WikiQA dataset constructed from Wikipedia .,question-answering,7,50,0.3571428571428571,172,0.6254545454545455,11,0.4782608695652174,1,1
174,We used 40 % dropouts afterword embeddings and no l 2 weight decay .,Experiments,We experiment on WikiQA dataset constructed from Wikipedia .,question-answering,7,51,0.3642857142857142,173,0.6290909090909091,12,0.5217391304347826,1,1
175,The word embeddings are pre-trained 300 - D Glove 840B vectors .,Experiments,We experiment on WikiQA dataset constructed from Wikipedia .,question-answering,7,52,0.3714285714285714,174,0.6327272727272727,13,0.5652173913043478,1,1
176,"For this task , a linear mapping layer transforms the 300 - D word embeddings to the 512- D LSTM inputs .",Experiments,We experiment on WikiQA dataset constructed from Wikipedia .,question-answering,7,53,0.3785714285714285,175,0.6363636363636364,14,0.6086956521739131,1,1
177,presents the results of our model and the previous models for the task .,Experiments,We experiment on WikiQA dataset constructed from Wikipedia .,question-answering,7,54,0.3857142857142857,176,0.64,15,0.6521739130434783,1,0
178,The classifier with handcrafted features is a SVM model trained with a set of features .,Experiments,We experiment on WikiQA dataset constructed from Wikipedia .,question-answering,7,55,0.3928571428571428,177,0.6436363636363637,16,0.6956521739130435,1,0
179,The Bigram - CNN model is a simple convolutional neural net .,Experiments,We experiment on WikiQA dataset constructed from Wikipedia .,question-answering,7,56,0.4,178,0.6472727272727272,17,0.7391304347826086,1,0
180,"While the LSTM and LSTM attention models outperform the previous best result by nearly 5 - 6 % by implementing deep LSTM with three hidden layers , NASM improves it further and sets a strong baseline by combining variational auto - encoder with the soft attention .",Experiments,We experiment on WikiQA dataset constructed from Wikipedia .,question-answering,7,57,0.4071428571428571,179,0.6509090909090909,18,0.782608695652174,1,0
181,Our MMA - NSE attention model exceeds the NASM by approximately 1 % on MAP and 0.8 % on MRR for this task .,Experiments,We experiment on WikiQA dataset constructed from Wikipedia .,question-answering,7,58,0.4142857142857143,180,0.6545454545454545,19,0.8260869565217391,1,1
182,We used trec_eval script to calculate the evaluation metrics 7 Inclusion of simple word count feature improves the performance by around 0.15 - 0.3 across the board Model MAP MRR Classifier with features 0.5993 0.6068 Paragraph Vector 0.5110 0.5160,Experiments,We experiment on WikiQA dataset constructed from Wikipedia .,question-answering,7,59,0.4214285714285714,181,0.6581818181818182,20,0.8695652173913043,1,0
183,Bigram- CNN 0.6190 0.6281 3 - layer LSTM 0.6552 0.6747 3 - layer LSTM attention 0.6639 0.6828 NASM 0.6705 0.6914 MMA - NSE attention 0.6811 0.6993 88.1 47.4 DRNN 86.6 49.8 2 - layer LSTM 86.3 46.0 Bi-LSTM 87.5 49.1 CT- LSTM 88.0 51.0 DMN 88.6 52.1 NSE 89.7 52.8 : Test accuracy for sentence classification .,Experiments,We experiment on WikiQA dataset constructed from Wikipedia .,question-answering,7,60,0.4285714285714285,182,0.6618181818181819,21,0.9130434782608696,1,0
184,Bin :,Experiments,We experiment on WikiQA dataset constructed from Wikipedia .,question-answering,7,61,0.4357142857142857,183,0.6654545454545454,22,0.9565217391304348,1,0
185,"Binary , FG : fine - grained 5 classes .",Experiments,We experiment on WikiQA dataset constructed from Wikipedia .,question-answering,7,62,0.4428571428571428,184,0.6690909090909091,23,1.0,1,0
186,Sentence Classification,Experiments,,question-answering,7,63,0.45,185,0.6727272727272727,0,0.0,1,0
187,We evaluated NSE on the Stanford Sentiment Treebank ( SST ) .,Experiments,Sentence Classification,question-answering,7,64,0.4571428571428571,186,0.6763636363636364,1,0.0769230769230769,1,0
188,This dataset comes with standard train / dev / test sets and two subtasks : binary sentence classification or fine - grained classification of five classes .,Experiments,Sentence Classification,question-answering,7,65,0.4642857142857143,187,0.68,2,0.1538461538461538,1,0
189,We trained our model on the text spans corresponding to labeled phrases in the training set and evaluated the model on the full sentences .,Experiments,Sentence Classification,question-answering,7,66,0.4714285714285714,188,0.6836363636363636,3,0.2307692307692307,1,0
190,The sentence representations were passed to a two - layer MLP for classification .,Experiments,Sentence Classification,question-answering,7,67,0.4785714285714286,189,0.6872727272727273,4,0.3076923076923077,1,0
191,The first layer of the MLP has ReLU activation and 1024 or 300 units for binary or fine - grained setting .,Experiments,Sentence Classification,question-answering,7,68,0.4857142857142857,190,0.6909090909090909,5,0.3846153846153846,1,1
192,The second layer is a sof tmax layer .,Experiments,,question-answering,7,69,0.4928571428571429,191,0.6945454545454546,6,0.4615384615384615,1,1
193,The read / write modules are two one - layer LSTM with 300 hidden units and the word embeddings are the pre-trained 300 - D Glove 840B vectors .,Experiments,The second layer is a sof tmax layer .,question-answering,7,70,0.5,192,0.6981818181818182,7,0.5384615384615384,1,1
194,"We set the batch size to 64 , the initial learning rate to 3e - 4 and l 2 regularizer strength to 3 e - 5 , and train each model for 25 epochs .",Experiments,The second layer is a sof tmax layer .,question-answering,7,71,0.5071428571428571,193,0.7018181818181818,8,0.6153846153846154,1,1
195,The write / read neural nets and the last linear layer were regularized by 50 % dropouts .,Experiments,The second layer is a sof tmax layer .,question-answering,7,72,0.5142857142857142,194,0.7054545454545454,9,0.6923076923076923,1,0
196,compares the result of our model with the state - of - the - art methods on the two subtasks .,Experiments,The second layer is a sof tmax layer .,question-answering,7,73,0.5214285714285715,195,0.7090909090909091,10,0.7692307692307693,1,0
197,Most best performing methods exploited the parse tree provided in the treebank on this task with the exception of the DMN .,Experiments,The second layer is a sof tmax layer .,question-answering,7,74,0.5285714285714286,196,0.7127272727272728,11,0.8461538461538461,1,0
198,The Dynamic Memory Network ( DMN ) model is a memory - augmented network .,Experiments,The second layer is a sof tmax layer .,question-answering,7,75,0.5357142857142857,197,0.7163636363636363,12,0.9230769230769232,1,0
199,Our model outperformed the DMN and set the state - of - the - art results on both subtasks .,Experiments,The second layer is a sof tmax layer .,question-answering,7,76,0.5428571428571428,198,0.72,13,1.0,1,1
200,Document Sentiment Analysis,Experiments,The second layer is a sof tmax layer .,question-answering,7,77,0.55,199,0.7236363636363636,0,0.0,1,0
201,"We evaluated our models for document - level sentiment analysis on two publically available largescale datasets : the IMDB consisting of 335,018 movie reviews and 10 different classes and Yelp 13 consisting of 348,415 restaurant reviews and 5 different classes .",Experiments,The second layer is a sof tmax layer .,question-answering,7,78,0.5571428571428572,200,0.7272727272727273,1,0.05,1,0
202,Each document in the datasets is associated with human ratings and we used these ratings as gold labels for sentiment classification .,Experiments,The second layer is a sof tmax layer .,question-answering,7,79,0.5642857142857143,201,0.730909090909091,2,0.1,1,0
203,"Particularly , we used the pre-split datasets of .",Experiments,,question-answering,7,80,0.5714285714285714,202,0.7345454545454545,3,0.15,1,0
204,We stack a NSE or LSTM on the top of another NSE for document modeling .,Experiments,"Particularly , we used the pre-split datasets of .",question-answering,7,81,0.5785714285714286,203,0.7381818181818182,4,0.2,1,1
205,The first NSE encodes the sentences and the second NSE or LSTM takes sentence encoded outputs and constructs document representations .,Experiments,"Particularly , we used the pre-split datasets of .",question-answering,7,82,0.5857142857142857,204,0.7418181818181818,5,0.25,1,0
206,The document representation is given to a output sof tmax layer .,Experiments,"Particularly , we used the pre-split datasets of .",question-answering,7,83,0.5928571428571429,205,0.7454545454545455,6,0.3,1,0
207,The whole network is trained jointly by backpropagating the cross entropy loss .,Experiments,"Particularly , we used the pre-split datasets of .",question-answering,7,84,0.6,206,0.7490909090909091,7,0.35,1,1
208,We used one - layer LSTM with 100 hidden units for the read / write modules and the pre-trained 100 - D Glove 6B vectors for this task .,Experiments,"Particularly , we used the pre-split datasets of .",question-answering,7,85,0.6071428571428571,207,0.7527272727272727,8,0.4,1,1
209,"We set the batch size to 32 , the initial learning rate to 3e - 4 and l 2 regularizer strength to 1 e - 5 , and trained each model for 50 epochs .",Experiments,"Particularly , we used the pre-split datasets of .",question-answering,7,86,0.6142857142857143,208,0.7563636363636363,9,0.45,1,1
210,The write / read neural nets and the document - level NSE / LSTM were regularized by 15 % dropouts and the softmax layer by 20 % dropouts .,Experiments,"Particularly , we used the pre-split datasets of .",question-answering,7,87,0.6214285714285714,209,0.76,10,0.5,1,0
211,"In order to speedup the training , we created document buckets by considering the number of sentences per document , i.e. , documents with the same number of sentences were put together in the same bucket .",Experiments,"Particularly , we used the pre-split datasets of .",question-answering,7,88,0.6285714285714286,210,0.7636363636363637,11,0.55,1,0
212,The buckets were shuffled and updated per epoch .,Experiments,,question-answering,7,89,0.6357142857142857,211,0.7672727272727272,12,0.6,1,0
213,"We did not use curriculum scheduling , although it is observed to help sequence training .",Experiments,The buckets were shuffled and updated per epoch .,question-answering,7,90,0.6428571428571429,212,0.7709090909090909,13,0.65,1,0
214,shows our results .,Experiments,The buckets were shuffled and updated per epoch .,question-answering,7,91,0.65,213,0.7745454545454545,14,0.7,1,0
215,We report two performance metrics : accuracy and MSE .,Experiments,The buckets were shuffled and updated per epoch .,question-answering,7,92,0.6571428571428571,214,0.7781818181818182,15,0.75,1,0
216,"The best results on the task were previously obtained by Conv - GRNN and LSTM - GRNN , which are also stacked models .",Experiments,The buckets were shuffled and updated per epoch .,question-answering,7,93,0.6642857142857143,215,0.7818181818181819,16,0.8,1,0
217,These models first learn the sentence representations with a CNN or LSTM and then combine them for document representation using a gated recurrent neural network ( GRNN : BLEU scores for English - German translation task .,Experiments,The buckets were shuffled and updated per epoch .,question-answering,7,94,0.6714285714285714,216,0.7854545454545454,17,0.85,1,0
218,Yelp 13 dataset has five classes to distinguish .,Experiments,,question-answering,7,95,0.6785714285714286,217,0.7890909090909091,18,0.9,1,0
219,The stacked NSEs ( NSE - NSE ) performed slightly better than the NSE - LSTM on the IMDB dataset .,Experiments,Yelp 13 dataset has five classes to distinguish .,question-answering,7,96,0.6857142857142857,218,0.7927272727272727,19,0.95,1,0
220,This is possibly due to the encoding memory of the document level NSE that preserves the long dependency in documents with a large number of sentences .,Experiments,Yelp 13 dataset has five classes to distinguish .,question-answering,7,97,0.6928571428571428,219,0.7963636363636364,20,1.0,1,0
221,Machine Translation,Experiments,,question-answering,7,98,0.7,220,0.8,0,0.0,1,0
222,"Lastly , we conducted an experiment on neural machine translation ( NMT ) .",Experiments,Machine Translation,question-answering,7,99,0.7071428571428572,221,0.8036363636363636,1,0.0434782608695652,1,0
223,The NMT problem is mostly defined within the encoder - decoder framework .,Experiments,Machine Translation,question-answering,7,100,0.7142857142857143,222,0.8072727272727273,2,0.0869565217391304,1,0
224,The encoder provides the semantic and syntactic information about the source sentences to the decoder and the decoder generates the target sentences by conditioning on this information and its partially produced translation .,Experiments,Machine Translation,question-answering,7,101,0.7214285714285714,223,0.8109090909090909,3,0.1304347826086956,1,0
225,"For an efficient encoding , the attention - based NTM was introduced .",Experiments,Machine Translation,question-answering,7,102,0.7285714285714285,224,0.8145454545454546,4,0.1739130434782608,1,0
226,"For NTM , we implemented three different models .",Experiments,,question-answering,7,103,0.7357142857142858,225,0.8181818181818182,5,0.217391304347826,1,0
227,The first model is a baseline model and is similar to the one proposed in ( RNNSearch ) .,Experiments,"For NTM , we implemented three different models .",question-answering,7,104,0.7428571428571429,226,0.8218181818181818,6,0.2608695652173913,1,0
228,"This model ( LSTM - LSTM ) has two LSTM for the encoder / decoder and has the soft attention neural net , which attends over the source sentence and constructs a focused encoding vector for each target word .",Experiments,"For NTM , we implemented three different models .",question-answering,7,105,0.75,227,0.8254545454545454,7,0.3043478260869565,1,0
229,The second model is an NSE - LSTM encoder - decoder which encodes the source sentence with NSE and generates the targets with the LSTM network by using the NSE output states and the attention network .,Experiments,"For NTM , we implemented three different models .",question-answering,7,106,0.7571428571428571,228,0.8290909090909091,8,0.3478260869565217,1,0
230,"The last model is an NSE - NSE setup , where the encoding part is the same as the NSE - LSTM while the decoder NSE now uses the output state and has an access to the encoder memory , i.e. , the encoder and the decoder NSEs access a shared memory .",Experiments,"For NTM , we implemented three different models .",question-answering,7,107,0.7642857142857142,229,0.8327272727272728,9,0.391304347826087,1,0
231,The memory is encoded by the first NSEs and then read / written by the decoder NSEs .,Experiments,"For NTM , we implemented three different models .",question-answering,7,108,0.7714285714285715,230,0.8363636363636363,10,0.4347826086956521,1,0
232,We used the English - German translation corpus from the IWSLT 2014 evaluation campaign .,Experiments,"For NTM , we implemented three different models .",question-answering,7,109,0.7785714285714286,231,0.84,11,0.4782608695652174,1,0
233,The corpus consists of sentence - aligned translation of TED talks .,Experiments,"For NTM , we implemented three different models .",question-answering,7,110,0.7857142857142857,232,0.8436363636363636,12,0.5217391304347826,1,0
234,The data was pre-processed and lowercased with the Moses toolkit .,Experiments,"For NTM , we implemented three different models .",question-answering,7,111,0.7928571428571428,233,0.8472727272727273,13,0.5652173913043478,1,0
235,"We merged the dev2010 and dev2012 sets for development and the tst2010 , tst2011 and tst 2012 sets for test data :",Experiments,"For NTM , we implemented three different models .",question-answering,7,112,0.8,234,0.850909090909091,14,0.6086956521739131,1,0
236,Word association or composition graphs produced by NSE memory access .,Experiments,"For NTM , we implemented three different models .",question-answering,7,113,0.8071428571428572,235,0.8545454545454545,15,0.6521739130434783,1,0
237,The directed arcs connect the words that are composed via compose module .,Experiments,"For NTM , we implemented three different models .",question-answering,7,114,0.8142857142857143,236,0.8581818181818182,16,0.6956521739130435,1,0
238,The source nodes are input words and the destination nodes ( pointed by the arrows ) correspond to the accessed memory slots .,Experiments,"For NTM , we implemented three different models .",question-answering,7,115,0.8214285714285714,237,0.8618181818181818,17,0.7391304347826086,1,0
239,< S > denotes the beginning of sequence .,Experiments,"For NTM , we implemented three different models .",question-answering,7,116,0.8285714285714286,238,0.8654545454545455,18,0.782608695652174,1,0
240,the number of parameters of the models is roughly the equal .,Experiments,"For NTM , we implemented three different models .",question-answering,7,117,0.8357142857142857,239,0.8690909090909091,19,0.8260869565217391,1,0
241,The models were trained to minimize word - level cross entropy loss and were regularized by 20 % input dropouts and the 30 % output dropouts .,Experiments,"For NTM , we implemented three different models .",question-answering,7,118,0.8428571428571429,240,0.8727272727272727,20,0.8695652173913043,1,1
242,"We set the batch size to 128 , the initial learning rate to 1e - 3 for LSTM - LSTM and 3e - 4 for the other models and l 2 regularizer strength to 3 e - 5 , and train each model for 40 epochs .",Experiments,"For NTM , we implemented three different models .",question-answering,7,119,0.85,241,0.8763636363636363,21,0.9130434782608696,1,1
243,We report BLEU score for each models .,Experiments,,question-answering,7,120,0.8571428571428571,242,0.88,22,0.9565217391304348,1,0
244,11 5 Qualitative Analysis,Experiments,We report BLEU score for each models .,question-answering,7,121,0.8642857142857143,243,0.8836363636363637,23,1.0,1,0
245,Memory Access and Compositionality,Experiments,,question-answering,7,122,0.8714285714285714,244,0.8872727272727273,0,0.0,1,0
246,NSE is capabable of performing multiscale composition by retrieving associative slots fora particular input at a time step .,Experiments,Memory Access and Compositionality,question-answering,7,123,0.8785714285714286,245,0.8909090909090909,1,0.0555555555555555,1,0
247,We analyzed the memory access order and the compositionality of memory slot and the input word in the NSE model trained on the SNLI data .,Experiments,Memory Access and Compositionality,question-answering,7,124,0.8857142857142857,246,0.8945454545454545,2,0.1111111111111111,1,0
248,shows the word association graphs for the two sentence picked from SNLI test set .,Experiments,Memory Access and Compositionality,question-answering,7,125,0.8928571428571429,247,0.8981818181818182,3,0.1666666666666666,1,0
249,The association graph was constructed by inspecting the key vector z .,Experiments,Memory Access and Compositionality,question-answering,7,126,0.9,248,0.901818181818182,4,0.2222222222222222,1,0
250,"For an input word , we connect it to the most active slot pointed by z 12 .",Experiments,Memory Access and Compositionality,question-answering,7,127,0.9071428571428573,249,0.9054545454545454,5,0.2777777777777778,1,0
251,"Note the graph components clustered around the semantically rich words : "" sits "" , "" wall "" and "" autumn "" ( a ) and "" Three "" , "" puppies "" , "" tub "" and "" vet "" ( b ) .",Experiments,Memory Access and Compositionality,question-answering,7,128,0.9142857142857144,250,0.9090909090909092,6,0.3333333333333333,1,0
252,The memory slots corresponding to words that are semantically rich in the current context are the most frequently accessed .,Experiments,Memory Access and Compositionality,question-answering,7,129,0.9214285714285714,251,0.9127272727272728,7,0.3888888888888889,1,0
253,"The graph is able to capture certain syntactic structures including phrases ( e.g. , "" hand built rock wall "" ) and modifier relations ( between "" sits "" and "" quietly "" and between "" tub "" and "" sprayed with water "" ) .",Experiments,Memory Access and Compositionality,question-answering,7,130,0.9285714285714286,252,0.9163636363636364,8,0.4444444444444444,1,0
254,Another interesting property is that the model tends to perform sensible compositions while processing the input sentence .,Experiments,Memory Access and Compositionality,question-answering,7,131,0.9357142857142856,253,0.92,9,0.5,1,0
255,"For example , NSE retrieved the memory slot corresponding to "" wall "" or "" Three "" when reading the input "" rock "" or "" are "" .",Experiments,Memory Access and Compositionality,question-answering,7,132,0.9428571428571428,254,0.9236363636363636,10,0.5555555555555556,1,0
256,In Appendix,Experiments,,question-answering,7,133,0.95,255,0.9272727272727272,11,0.6111111111111112,1,0
257,"A , we show a step - by - step visualization of NSE memory states for the first sentence .",Experiments,In Appendix,question-answering,7,134,0.9571428571428572,256,0.9309090909090908,12,0.6666666666666666,1,0
258,Note how the encoding memory is evolved overtime .,Experiments,,question-answering,7,135,0.9642857142857144,257,0.9345454545454546,13,0.7222222222222222,1,0
259,"In time step four ( t = 4 ) , the memory slot for "" quietly "" encodes information about "" quiet ( ly ) little child "" .",Experiments,Note how the encoding memory is evolved overtime .,question-answering,7,136,0.9714285714285714,258,0.9381818181818182,14,0.7777777777777778,1,0
260,"When t = 6 , the model forms another composition involving "" quietly "" , "" quietly sits "" .",Experiments,Note how the encoding memory is evolved overtime .,question-answering,7,137,0.9785714285714284,259,0.9418181818181818,15,0.8333333333333334,1,0
261,"In the last time step , we are able to find the most or the least frequently accessed slots in the memory .",Experiments,Note how the encoding memory is evolved overtime .,question-answering,7,138,0.9857142857142858,260,0.9454545454545454,16,0.8888888888888888,1,0
262,The least accessed slots correspond to function words while the frequently accessed slots are content words and tend to carryout rich semantics and intrinsic compositions found in the input sentence .,Experiments,Note how the encoding memory is evolved overtime .,question-answering,7,139,0.9928571428571428,261,0.9490909090909092,17,0.9444444444444444,1,0
263,Overall the model is less constrained and is able to compose multiword expressions .,Experiments,Note how the encoding memory is evolved overtime .,question-answering,7,140,1.0,262,0.9527272727272728,18,1.0,1,0
264,Conclusion,,,question-answering,7,0,0.0,263,0.9563636363636364,0,0.0,1,0
265,Our proposed memory augmented neural networks have achieved the state - of - the - art results when evaluated on five representative NLP tasks .,Conclusion,Conclusion,question-answering,7,1,0.0909090909090909,264,0.96,1,0.0909090909090909,0,0
266,"NSE is capable of building an efficient architecture of the single , shared and multiple memory accesses fora specific NLP task .",Conclusion,Conclusion,question-answering,7,2,0.1818181818181818,265,0.9636363636363636,2,0.1818181818181818,0,0
267,"For example , for the NLI task NSE accesses premise encoded memory when processing hypothesis .",Conclusion,Conclusion,question-answering,7,3,0.2727272727272727,266,0.9672727272727272,3,0.2727272727272727,0,0
268,"For the QA task , NSE accesses answer encoded memory when reading question for QA .",Conclusion,Conclusion,question-answering,7,4,0.3636363636363636,267,0.9709090909090908,4,0.3636363636363636,0,0
269,"In machine translation , NSE shares a single encoded memory between encoder and decoder .",Conclusion,Conclusion,question-answering,7,5,0.4545454545454545,268,0.9745454545454544,5,0.4545454545454545,0,0
270,Such flexibility in the architectural choice of the NSE memory access allows for the robust models fora better performance .,Conclusion,Conclusion,question-answering,7,6,0.5454545454545454,269,0.9781818181818182,6,0.5454545454545454,0,0
271,The initial state of the NSE memory stores information about each word in the input sequence .,Conclusion,Conclusion,question-answering,7,7,0.6363636363636364,270,0.9818181818181818,7,0.6363636363636364,0,0
272,We in this paper used word embeddings to represent the words in the memory .,Conclusion,Conclusion,question-answering,7,8,0.7272727272727273,271,0.9854545454545456,8,0.7272727272727273,0,0
273,Different variations of word representations such as character - based models are left to be evaluated for memory initialization in the future .,Conclusion,Conclusion,question-answering,7,9,0.8181818181818182,272,0.9890909090909092,9,0.8181818181818182,0,0
274,We plan to extend NSE so that it learns to select and access a relevant subset from a memory set .,Conclusion,Conclusion,question-answering,7,10,0.9090909090909092,273,0.9927272727272728,10,0.9090909090909092,0,0
275,"One could also explore unsupervised variations of NSE , for example , to train them to produce encoding memory and representation vector of entire sentences or documents using either new or existing models such as the skip - gram model .",Conclusion,Conclusion,question-answering,7,11,1.0,274,0.9963636363636365,11,1.0,0,0
1,title,,,question-answering,8,1,0.1,0,0.0,0,0.0,1,0
2,MACHINE COMPREHENSION USING MATCH - LSTM AND ANSWER POINTER,,,question-answering,8,2,0.2,1,0.0040160642570281,1,0.0,1,1
3,abstract,,MACHINE COMPREHENSION USING MATCH - LSTM AND ANSWER POINTER,question-answering,8,3,0.3,2,0.0080321285140562,0,0.0,1,0
4,Machine comprehension of text is an important problem in natural language processing .,,MACHINE COMPREHENSION USING MATCH - LSTM AND ANSWER POINTER,question-answering,8,4,0.4,3,0.0120481927710843,1,0.1428571428571428,1,1
5,"A recently released dataset , the Stanford Question Answering Dataset ( SQuAD ) , offers a large number of real questions and their answers created by humans through crowdsourcing .",,MACHINE COMPREHENSION USING MATCH - LSTM AND ANSWER POINTER,question-answering,8,5,0.5,4,0.0160642570281124,2,0.2857142857142857,1,0
6,"SQuAD provides a challenging testbed for evaluating machine comprehension algorithms , partly because compared with previous datasets , in SQuAD the answers do not come from a small set of candidate answers and they have variable lengths .",,MACHINE COMPREHENSION USING MATCH - LSTM AND ANSWER POINTER,question-answering,8,6,0.6,5,0.0200803212851405,3,0.4285714285714285,1,0
7,We propose an end - to - end neural architecture for the task .,,MACHINE COMPREHENSION USING MATCH - LSTM AND ANSWER POINTER,question-answering,8,7,0.7,6,0.0240963855421686,4,0.5714285714285714,1,0
8,"The architecture is based on match - LSTM , a model we proposed previously for textual entailment , and Pointer Net , a sequence - to - sequence model proposed by Vinyals et al. ( 2015 ) to constrain the output tokens to be from the input sequences .",,MACHINE COMPREHENSION USING MATCH - LSTM AND ANSWER POINTER,question-answering,8,8,0.8,7,0.0281124497991967,5,0.7142857142857143,1,0
9,We propose two ways of using Pointer Net for our task .,,MACHINE COMPREHENSION USING MATCH - LSTM AND ANSWER POINTER,question-answering,8,9,0.9,8,0.0321285140562249,6,0.8571428571428571,1,0
10,Our experiments show that both of our two models substantially outperform the best results obtained by Rajpurkar et al. ( 2016 ) using logistic regression and manually crafted features .,,MACHINE COMPREHENSION USING MATCH - LSTM AND ANSWER POINTER,question-answering,8,10,1.0,9,0.036144578313253,7,1.0,1,0
11,INTRODUCTION,,,question-answering,8,0,0.0,10,0.0401606425702811,0,0.0,1,0
12,Machine comprehension of text is one of the ultimate goals of natural language processing .,INTRODUCTION,INTRODUCTION,question-answering,8,1,0.0256410256410256,11,0.0441767068273092,1,0.0256410256410256,1,0
13,"While the ability of a machine to understand text can be assessed in many different ways , in recent years , several benchmark datasets have been created to focus on answering questions as away to evaluate machine comprehension .",INTRODUCTION,INTRODUCTION,question-answering,8,2,0.0512820512820512,12,0.0481927710843373,2,0.0512820512820512,1,0
14,"In this setup , typically the machine is first presented with apiece of text such as a news article or a story .",INTRODUCTION,INTRODUCTION,question-answering,8,3,0.0769230769230769,13,0.0522088353413654,3,0.0769230769230769,1,0
15,The machine is then expected to answer one or multiple questions related to the text .,INTRODUCTION,INTRODUCTION,question-answering,8,4,0.1025641025641025,14,0.0562248995983935,4,0.1025641025641025,1,0
16,"In most of the benchmark datasets , a question can be treated as a multiple choice question , whose correct answer is to be chosen from a set of provided candidate answers .",INTRODUCTION,INTRODUCTION,question-answering,8,5,0.1282051282051282,15,0.0602409638554216,5,0.1282051282051282,1,0
17,"Presumably , questions with more given candidate answers are more challenging .",INTRODUCTION,INTRODUCTION,question-answering,8,6,0.1538461538461538,16,0.0642570281124498,6,0.1538461538461538,1,0
18,The Stanford Question Answering Dataset ( SQuAD ) introduced recently by contains such more challenging questions whose correct answers can be any sequence of tokens from the given text .,INTRODUCTION,INTRODUCTION,question-answering,8,7,0.1794871794871795,17,0.0682730923694779,7,0.1794871794871795,1,0
19,"Moreover , unlike some other datasets whose questions and answers were created automatically in Cloze style , the questions and answers in SQu AD were created by humans through crowdsourcing , which makes the dataset more realistic .",INTRODUCTION,INTRODUCTION,question-answering,8,8,0.2051282051282051,18,0.072289156626506,8,0.2051282051282051,1,0
20,"Given these advantages of the SQuAD dataset , in this paper , we focus on this new dataset to study machine comprehension of text .",INTRODUCTION,INTRODUCTION,question-answering,8,9,0.2307692307692307,19,0.0763052208835341,9,0.2307692307692307,1,0
21,A sample piece of text and three of its associated questions are shown in .,INTRODUCTION,INTRODUCTION,question-answering,8,10,0.2564102564102564,20,0.0803212851405622,10,0.2564102564102564,1,0
22,"Traditional solutions to this kind of question answering tasks rely on NLP pipelines that involve multiple steps of linguistic analyses and feature engineering , including syntactic parsing , named entity recognition , question classification , semantic parsing , etc .",INTRODUCTION,INTRODUCTION,question-answering,8,11,0.282051282051282,21,0.0843373493975903,11,0.282051282051282,1,0
23,"Recently , with the advances of applying neural network models in NLP , there has been much interest in building end - to - end neural architectures for various NLP tasks , including several pieces of work on machine comprehension .",INTRODUCTION,INTRODUCTION,question-answering,8,12,0.3076923076923077,22,0.0883534136546184,12,0.3076923076923077,1,0
24,"However , given the properties of previous machine comprehension datasets , existing end - to - end neural architectures for the task either rely on the candidate answers or assume that the In 1870 , Tesla moved to Karlovac , to attend school at the Higher Real Gymnasium , where he was profoundly influenced by a math teacher Martin Sekuli ?.",INTRODUCTION,INTRODUCTION,question-answering,8,13,0.3333333333333333,23,0.0923694779116465,13,0.3333333333333333,1,0
25,"The classes were held in German , as it was a school within the Austro-Hungarian Military Frontier .",INTRODUCTION,INTRODUCTION,question-answering,8,14,0.358974358974359,24,0.0963855421686747,14,0.358974358974359,1,0
26,"Tesla was able to perform integral calculus in his head , which prompted his teachers to believe that he was cheating .",INTRODUCTION,INTRODUCTION,question-answering,8,15,0.3846153846153846,25,0.1004016064257028,15,0.3846153846153846,1,0
27,"He finished a four - year term in three years , graduating in 1873 .",INTRODUCTION,INTRODUCTION,question-answering,8,16,0.4102564102564102,26,0.1044176706827309,16,0.4102564102564102,1,0
28,1 .,INTRODUCTION,INTRODUCTION,question-answering,8,17,0.4358974358974359,27,0.108433734939759,17,0.4358974358974359,1,0
29,In what language were the classes given ?,INTRODUCTION,INTRODUCTION,question-answering,8,18,0.4615384615384615,28,0.1124497991967871,18,0.4615384615384615,1,0
30,German,INTRODUCTION,INTRODUCTION,question-answering,8,19,0.4871794871794871,29,0.1164658634538152,19,0.4871794871794871,1,0
31,2 . Who was Tesla 's main influence in Karlovac ?,INTRODUCTION,INTRODUCTION,question-answering,8,20,0.5128205128205128,30,0.1204819277108433,20,0.5128205128205128,1,0
32,Martin Sekuli ?,INTRODUCTION,INTRODUCTION,question-answering,8,21,0.5384615384615384,31,0.1244979919678714,21,0.5384615384615384,1,0
33,3 . Why did Tesla go to Karlovac ?,INTRODUCTION,INTRODUCTION,question-answering,8,22,0.5641025641025641,32,0.1285140562248996,22,0.5641025641025641,1,0
34,attend school at the Higher Real Gymnasium :,INTRODUCTION,INTRODUCTION,question-answering,8,23,0.5897435897435898,33,0.1325301204819277,23,0.5897435897435898,1,0
35,"A paragraph from Wikipedia and three associated questions together with their answers , taken from the SQuAD dataset .",INTRODUCTION,INTRODUCTION,question-answering,8,24,0.6153846153846154,34,0.1365461847389558,24,0.6153846153846154,1,0
36,The tokens in bold in the paragraph are our predicted answers while the texts next to the questions are the ground truth answers .,INTRODUCTION,INTRODUCTION,question-answering,8,25,0.6410256410256411,35,0.1405622489959839,25,0.6410256410256411,1,0
37,"answer is a single token , which make these methods unsuitable for the SQuAD dataset .",INTRODUCTION,INTRODUCTION,question-answering,8,26,0.6666666666666666,36,0.144578313253012,26,0.6666666666666666,1,0
38,"In this paper , we propose anew end - to - end neural architecture to address the machine comprehension problem as defined in the SQuAD dataset .",INTRODUCTION,INTRODUCTION,question-answering,8,27,0.6923076923076923,37,0.1485943775100401,27,0.6923076923076923,1,1
39,"Specifically , observing that in the SQuAD dataset many questions are paraphrases of sentences from the original text , we adopt a match - LSTM model that we developed earlier for textual entailment .",INTRODUCTION,INTRODUCTION,question-answering,8,28,0.717948717948718,38,0.1526104417670682,28,0.717948717948718,1,1
40,"We further adopt the Pointer Net ( Ptr - Net ) model developed by , which enables the predictions of tokens from the input sequence only rather than from a larger fixed vocabulary and thus allows us to generate answers that consist of multiple tokens from the original text .",INTRODUCTION,INTRODUCTION,question-answering,8,29,0.7435897435897436,39,0.1566265060240964,29,0.7435897435897436,1,1
41,We propose two ways to apply the Ptr - Net model for our task : a sequence model and a boundary model .,INTRODUCTION,INTRODUCTION,question-answering,8,30,0.7692307692307693,40,0.1606425702811245,30,0.7692307692307693,1,1
42,We also further extend the boundary model with a search mechanism .,INTRODUCTION,INTRODUCTION,question-answering,8,31,0.7948717948717948,41,0.1646586345381526,31,0.7948717948717948,1,1
43,Experiments on the SQuAD dataset show that our two models both outperform the best performance reported by .,INTRODUCTION,INTRODUCTION,question-answering,8,32,0.8205128205128205,42,0.1686746987951807,32,0.8205128205128205,1,0
44,"Moreover , using an ensemble of several of our models , we can achieve very competitive performance on SQuAD .",INTRODUCTION,INTRODUCTION,question-answering,8,33,0.8461538461538461,43,0.1726907630522088,33,0.8461538461538461,1,0
45,Our contributions can be summarized as follows :,INTRODUCTION,INTRODUCTION,question-answering,8,34,0.8717948717948718,44,0.1767068273092369,34,0.8717948717948718,1,0
46,"( 1 ) We propose two new end - to - end neural network models for machine comprehension , which combine match - LSTM and Ptr- Net to handle the special properties of the SQuAD dataset .",INTRODUCTION,INTRODUCTION,question-answering,8,35,0.8974358974358975,45,0.180722891566265,35,0.8974358974358975,1,0
47,"( 2 ) We have achieved the performance of an exact match score of 67.9 % and an F1 score of 77.0 % on the unseen test dataset , which is much better than the featureengineered solution .",INTRODUCTION,INTRODUCTION,question-answering,8,36,0.9230769230769232,46,0.1847389558232931,36,0.9230769230769232,1,0
48,"Our performance is also close to the state of the art on SQuAD , which is 71.6 % in terms of exact match and 80.4 % in terms of F1 from Salesforce Research .",INTRODUCTION,INTRODUCTION,question-answering,8,37,0.9487179487179488,47,0.1887550200803212,37,0.9487179487179488,1,0
49,( 3 ) Our further analyses of the models reveal some useful insights for further improving the method .,INTRODUCTION,INTRODUCTION,question-answering,8,38,0.9743589743589745,48,0.1927710843373494,38,0.9743589743589745,1,0
50,"Beisdes , we also made our code available online 1 .",INTRODUCTION,INTRODUCTION,question-answering,8,39,1.0,49,0.1967871485943775,39,1.0,1,0
51,METHOD,,,question-answering,8,0,0.0,50,0.2008032128514056,0,0.0,1,0
52,"In this section , we first briefly review match - LSTM and Pointer Net .",METHOD,METHOD,question-answering,8,1,0.05,51,0.2048192771084337,1,0.0526315789473684,1,0
53,These two pieces of existing work lay the foundation of our method .,METHOD,METHOD,question-answering,8,2,0.1,52,0.2088353413654618,2,0.1052631578947368,1,0
54,We then present our end - to - end neural architecture for machine comprehension .,METHOD,METHOD,question-answering,8,3,0.15,53,0.2128514056224899,3,0.1578947368421052,1,0
55,MATCH - LSTM,METHOD,,question-answering,8,4,0.2,54,0.216867469879518,4,0.2105263157894736,1,0
56,"I na recent work on learning natural language inference , we proposed a match - LSTM model for predicting textual entailment .",METHOD,MATCH - LSTM,question-answering,8,5,0.25,55,0.2208835341365461,5,0.2631578947368421,1,0
57,"In textual entailment , two sentences are given where one is a premise and the other is a hypothesis .",METHOD,MATCH - LSTM,question-answering,8,6,0.3,56,0.2248995983935743,6,0.3157894736842105,1,0
58,"To predict whether the premise entails the hypothesis , the match - LSTM model goes through the tokens of the hypothesis sequentially .",METHOD,MATCH - LSTM,question-answering,8,7,0.35,57,0.2289156626506024,7,0.3684210526315789,1,0
59,"At each position of the hypothesis , attention mechanism is used to obtain a weighted vector representation of the premise .",METHOD,MATCH - LSTM,question-answering,8,8,0.4,58,0.2329317269076305,8,0.4210526315789473,1,0
60,"This weighted premise is then to be combined with a vector representation of the current token of the hypothesis and fed into an LSTM , which we call the match - LSTM .",METHOD,MATCH - LSTM,question-answering,8,9,0.45,59,0.2369477911646586,9,0.4736842105263157,1,0
61,The match - LSTM essentially sequentially aggregates the matching of the attention - weighted premise to each token of the hypothesis and uses the aggregated matching result to make a final prediction . :,METHOD,MATCH - LSTM,question-answering,8,10,0.5,60,0.2409638554216867,10,0.5263157894736842,1,0
62,An overview of our two models .,METHOD,MATCH - LSTM,question-answering,8,11,0.55,61,0.2449799196787148,11,0.5789473684210527,1,0
63,"Both models consist of an LSTM preprocessing layer , a match - LSTM layer and an Answer Pointer layer .",METHOD,MATCH - LSTM,question-answering,8,12,0.6,62,0.2489959839357429,12,0.631578947368421,1,0
64,"For each match - LSTM in a particular direction , h q i , which is defined as H q ?",METHOD,MATCH - LSTM,question-answering,8,13,0.65,63,0.253012048192771,13,0.6842105263157895,1,0
65,"i , is computed using the ?",METHOD,MATCH - LSTM,question-answering,8,14,0.7,64,0.2570281124497992,14,0.7368421052631579,1,0
66,"in the corresponding direction , as described in either Eqn. ( 2 ) or Eqn. ( 5 ) .",METHOD,MATCH - LSTM,question-answering,8,15,0.75,65,0.2610441767068273,15,0.7894736842105263,1,0
67,proposed a Pointer Network ( Ptr - Net ) model to solve a special kind of problems where we want to generate an output sequence whose tokens must come from the input sequence .,METHOD,MATCH - LSTM,question-answering,8,16,0.8,66,0.2650602409638554,16,0.8421052631578947,1,0
68,"Instead of picking an output token from a fixed vocabulary , Ptr - Net uses attention mechanism as a pointer to select a position from the input sequence as an output symbol .",METHOD,MATCH - LSTM,question-answering,8,17,0.85,67,0.2690763052208835,17,0.8947368421052632,1,0
69,The pointer mechanism has inspired some recent work on language processing .,METHOD,MATCH - LSTM,question-answering,8,18,0.9,68,0.2730923694779116,18,0.9473684210526316,1,0
70,Here we adopt Ptr- Net in order to construct answers using tokens from the input text .,METHOD,MATCH - LSTM,question-answering,8,19,0.95,69,0.2771084337349397,19,1.0,1,0
71,POINTER NET,METHOD,,question-answering,8,20,1.0,70,0.2811244979919678,0,0.0,1,0
72,OUR METHOD,,,question-answering,8,0,0.0,71,0.285140562248996,0,0.0,1,0
73,"Formally , the problem we are trying to solve can be formulated as follows .",OUR METHOD,OUR METHOD,question-answering,8,1,0.0108695652173913,72,0.2891566265060241,1,0.0163934426229508,1,0
74,"We are given apiece of text , which we refer to as a passage , and a question related to the passage .",OUR METHOD,OUR METHOD,question-answering,8,2,0.0217391304347826,73,0.2931726907630522,2,0.0327868852459016,1,0
75,The passage is represented by matrix P ?,OUR METHOD,OUR METHOD,question-answering,8,3,0.0326086956521739,74,0.2971887550200803,3,0.0491803278688524,1,0
76,"R dP , where P is the length ( number of tokens ) of the passage and dis the dimensionality of word embeddings .",OUR METHOD,OUR METHOD,question-answering,8,4,0.0434782608695652,75,0.3012048192771084,4,0.0655737704918032,1,0
77,"Similarly , the question is represented by matrix Q ?",OUR METHOD,OUR METHOD,question-answering,8,5,0.0543478260869565,76,0.3052208835341365,5,0.081967213114754,1,0
78,R d Q where Q is the length of the question .,OUR METHOD,OUR METHOD,question-answering,8,6,0.0652173913043478,77,0.3092369477911647,6,0.0983606557377049,1,0
79,Our goal is to identify a subsequence from the passage as the answer to the question .,OUR METHOD,OUR METHOD,question-answering,8,7,0.0760869565217391,78,0.3132530120481928,7,0.1147540983606557,1,0
80,"As pointed out earlier , since the output tokens are from the input , we would like to adopt the Pointer Net for this problem .",OUR METHOD,OUR METHOD,question-answering,8,8,0.0869565217391304,79,0.3172690763052209,8,0.1311475409836065,1,0
81,"A straightforward way of applying Ptr - Net here is to treat an answer as a sequence of tokens from the input passage but ignore the fact that these tokens are consecutive in the original passage , because Ptr - Net does not make the consecutivity assumption .",OUR METHOD,OUR METHOD,question-answering,8,9,0.0978260869565217,80,0.321285140562249,9,0.1475409836065573,1,0
82,"Specifically , we represent the answer as a sequence of integers a = ( a 1 , a 2 , . . . ) , where each a i is an integer between 1 and P , indicating a certain position in the passage .",OUR METHOD,OUR METHOD,question-answering,8,10,0.108695652173913,81,0.3253012048192771,10,0.1639344262295081,1,0
83,"Alternatively , if we want to ensure consecutivity , that is , if we want to ensure that we indeed select a subsequence from the passage as an answer , we can use the Ptr-Net to predict only the start and the end of an answer .",OUR METHOD,OUR METHOD,question-answering,8,11,0.1195652173913043,82,0.3293172690763052,11,0.180327868852459,1,0
84,"In this case , the Ptr - Net only needs to select two tokens from the input passage , and all the tokens between these two tokens in the passage are treated as the answer .",OUR METHOD,OUR METHOD,question-answering,8,12,0.1304347826086956,83,0.3333333333333333,12,0.1967213114754098,1,0
85,"Specifically , we can represent the answer to be predicted as two integers a = ( a s , a e ) , where a s an a e are integers between 1 and P .",OUR METHOD,OUR METHOD,question-answering,8,13,0.1413043478260869,84,0.3373493975903614,13,0.2131147540983606,1,0
86,We refer to the first setting above as a sequence model and the second setting above as a boundary model .,OUR METHOD,OUR METHOD,question-answering,8,14,0.1521739130434782,85,0.3413654618473896,14,0.2295081967213114,1,0
87,"For either model , we assume that a set of training examples in the form of triplets {( P n , Q n , an ) } N n=1 are given .",OUR METHOD,OUR METHOD,question-answering,8,15,0.1630434782608695,86,0.3453815261044177,15,0.2459016393442623,1,0
88,An overview of the two neural network models are shown in .,OUR METHOD,OUR METHOD,question-answering,8,16,0.1739130434782608,87,0.3493975903614458,16,0.2622950819672131,1,0
89,Both models consist of three layers :,OUR METHOD,OUR METHOD,question-answering,8,17,0.1847826086956521,88,0.3534136546184739,17,0.2786885245901639,1,0
90,( 1 ) An LSTM preprocessing layer that preprocesses the passage and the question using LSTMs .,OUR METHOD,OUR METHOD,question-answering,8,18,0.1956521739130435,89,0.357429718875502,18,0.2950819672131147,1,0
91,( 2 ) A match - LSTM layer that tries to match the passage against the question .,OUR METHOD,OUR METHOD,question-answering,8,19,0.2065217391304347,90,0.3614457831325301,19,0.3114754098360656,1,0
92,( 3 ) An Answer Pointer ( Ans - Ptr ) layer that uses Ptr-Net to select a set of tokens from the passage as the answer .,OUR METHOD,OUR METHOD,question-answering,8,20,0.217391304347826,91,0.3654618473895582,20,0.3278688524590163,1,0
93,The difference between the two models only lies in the third layer .,OUR METHOD,OUR METHOD,question-answering,8,21,0.2282608695652173,92,0.3694779116465863,21,0.3442622950819672,1,0
94,LSTM,OUR METHOD,,question-answering,8,22,0.2391304347826087,93,0.3734939759036144,22,0.360655737704918,1,0
95,Preprocessing Layer,OUR METHOD,LSTM,question-answering,8,23,0.25,94,0.3775100401606425,23,0.3770491803278688,1,0
96,The purpose for the LSTM preprocessing layer is to incorporate contextual information into the representation of each token in the passage and the question .,OUR METHOD,LSTM,question-answering,8,24,0.2608695652173913,95,0.3815261044176707,24,0.3934426229508196,1,0
97,"We use a standard one - directional LSTM 2 to process the passage and the question separately , as shown below :",OUR METHOD,LSTM,question-answering,8,25,0.2717391304347826,96,0.3855421686746988,25,0.4098360655737705,1,0
98,The resulting matrices H p ?,OUR METHOD,LSTM,question-answering,8,26,0.2826086956521739,97,0.3895582329317269,26,0.4262295081967213,1,0
99,R lP and H q ?,OUR METHOD,LSTM,question-answering,8,27,0.2934782608695652,98,0.393574297188755,27,0.4426229508196721,1,0
100,"R lQ are hidden representations of the passage and the question , where l is the dimensionality of the hidden vectors .",OUR METHOD,LSTM,question-answering,8,28,0.3043478260869565,99,0.3975903614457831,28,0.4590163934426229,1,0
101,"In other words , the i th column vector hp i ( or h q i ) in H p ( or H q ) represents the i th token in the passage ( or the question ) together with some contextual information from the left .",OUR METHOD,LSTM,question-answering,8,29,0.3152173913043478,100,0.4016064257028112,29,0.4754098360655737,1,0
102,Match- LSTM,OUR METHOD,LSTM,question-answering,8,30,0.3260869565217391,101,0.4056224899598393,30,0.4918032786885246,1,0
103,Layer,OUR METHOD,LSTM,question-answering,8,31,0.3369565217391304,102,0.4096385542168674,31,0.5081967213114754,1,0
104,We apply the match - LSTM model proposed for textual entailment to our machine comprehension problem by treating the question as a premise and the passage as a hypothesis .,OUR METHOD,LSTM,question-answering,8,32,0.3478260869565217,103,0.4136546184738955,32,0.5245901639344263,1,0
105,The match - LSTM sequentially goes through the passage .,OUR METHOD,LSTM,question-answering,8,33,0.358695652173913,104,0.4176706827309236,33,0.5409836065573771,1,0
106,"At position i of the passage , it first uses the standard word - by - word attention mechanism to obtain attention weight vector ? ? ?",OUR METHOD,LSTM,question-answering,8,34,0.3695652173913043,105,0.4216867469879518,34,0.5573770491803278,1,0
107,i ?,OUR METHOD,LSTM,question-answering,8,35,0.3804347826086957,106,0.4257028112449799,35,0.5737704918032787,1,0
108,R Q as follows :,OUR METHOD,LSTM,question-answering,8,36,0.391304347826087,107,0.429718875502008,36,0.5901639344262295,1,0
109,"where W q , W p , W r ?",OUR METHOD,LSTM,question-answering,8,37,0.4021739130434782,108,0.4337349397590361,37,0.6065573770491803,1,0
110,"R ll , b p , w ?",OUR METHOD,LSTM,question-answering,8,38,0.4130434782608695,109,0.4377510040160642,38,0.6229508196721312,1,0
111,R land b ?,OUR METHOD,LSTM,question-answering,8,39,0.4239130434782608,110,0.4417670682730923,39,0.639344262295082,1,0
112,"R are parameters to be learned , ? ? hr i?1 ?",OUR METHOD,LSTM,question-answering,8,40,0.4347826086956521,111,0.4457831325301205,40,0.6557377049180327,1,0
113,"R l is the hidden vector of the one -directional match - LSTM ( to be explained below ) at position i ? 1 , and the outer product ( ? e Q ) produces a matrix or row vector by repeating the vector or scalar on the left for Q times .",OUR METHOD,LSTM,question-answering,8,41,0.4456521739130434,112,0.4497991967871486,41,0.6721311475409836,1,0
114,"Essentially , the resulting attention weight ? ? ?",OUR METHOD,LSTM,question-answering,8,42,0.4565217391304347,113,0.4538152610441767,42,0.6885245901639344,1,0
115,"i , j above indicates the degree of matching between the i th token in the passage with the j th token in the question .",OUR METHOD,LSTM,question-answering,8,43,0.4673913043478261,114,0.4578313253012048,43,0.7049180327868853,1,0
116,"Next , we use the attention weight vector ? ? ?",OUR METHOD,LSTM,question-answering,8,44,0.4782608695652174,115,0.4618473895582329,44,0.7213114754098361,1,0
117,i to obtain a weighted version of the question and combine it with the current token of the passage to form a vector ? ?,OUR METHOD,LSTM,question-answering,8,45,0.4891304347826087,116,0.465863453815261,45,0.7377049180327869,1,0
118,z i :,OUR METHOD,LSTM,question-answering,8,46,0.5,117,0.4698795180722891,46,0.7540983606557377,1,0
119,This vector ? ?,OUR METHOD,LSTM,question-answering,8,47,0.5108695652173914,118,0.4738955823293173,47,0.7704918032786885,1,0
120,z i is fed into a standard one - directional LSTM to form our so - called match - LSTM :,OUR METHOD,LSTM,question-answering,8,48,0.5217391304347826,119,0.4779116465863454,48,0.7868852459016393,1,0
121,where ? ?,OUR METHOD,LSTM,question-answering,8,49,0.532608695652174,120,0.4819277108433735,49,0.8032786885245902,1,0
122,hr i ?,OUR METHOD,LSTM,question-answering,8,50,0.5434782608695652,121,0.4859437751004016,50,0.819672131147541,1,0
123,R l .,OUR METHOD,LSTM,question-answering,8,51,0.5543478260869565,122,0.4899598393574297,51,0.8360655737704918,1,0
124,We further build a similar match - LSTM in the reverse direction .,OUR METHOD,LSTM,question-answering,8,52,0.5652173913043478,123,0.4939759036144578,52,0.8524590163934426,1,0
125,The purpose is to obtain a representation that encodes the contexts from both directions for each token in the passage .,OUR METHOD,LSTM,question-answering,8,53,0.5760869565217391,124,0.4979919678714859,53,0.8688524590163934,1,0
126,"To build this reverse match - LSTM , we first define",OUR METHOD,LSTM,question-answering,8,54,0.5869565217391305,125,0.5020080321285141,54,0.8852459016393442,1,0
127,"Note that the parameters here ( W q , W p , W r , b p , wand b ) are the same as used in Eqn .",OUR METHOD,LSTM,question-answering,8,55,0.5978260869565217,126,0.5060240963855421,55,0.9016393442622952,1,0
128,( 2 ) .,OUR METHOD,LSTM,question-answering,8,56,0.6086956521739131,127,0.5100401606425703,56,0.918032786885246,1,0
129,We then define ? ?,OUR METHOD,LSTM,question-answering,8,57,0.6195652173913043,128,0.5140562248995983,57,0.9344262295081968,1,0
130,z i in a similar way and finally define ? ?,OUR METHOD,LSTM,question-answering,8,58,0.6304347826086957,129,0.5180722891566265,58,0.9508196721311476,1,0
131,hr i to be the hidden representation at position i produced by the match - LSTM in the reverse direction .,OUR METHOD,LSTM,question-answering,8,59,0.6413043478260869,130,0.5220883534136547,59,0.9672131147540984,1,0
132,We define H r ?,OUR METHOD,LSTM,question-answering,8,60,0.6521739130434783,131,0.5261044176706827,60,0.9836065573770492,1,0
133,R 2 lP as the concatenation of the two :,OUR METHOD,LSTM,question-answering,8,61,0.6630434782608695,132,0.5301204819277109,61,1.0,1,0
134,Answer Pointer Layer,OUR METHOD,LSTM,question-answering,8,62,0.6739130434782609,133,0.5341365461847389,0,0.0,1,0
135,"The top layer , the Answer Pointer ( Ans - Ptr ) layer , is motivated by the Pointer Net introduced by .",OUR METHOD,LSTM,question-answering,8,63,0.6847826086956522,134,0.5381526104417671,1,0.0333333333333333,1,0
136,This layer uses the sequence H r as input .,OUR METHOD,LSTM,question-answering,8,64,0.6956521739130435,135,0.5421686746987951,2,0.0666666666666666,1,0
137,Recall that we have two different models :,OUR METHOD,LSTM,question-answering,8,65,0.7065217391304348,136,0.5461847389558233,3,0.1,1,0
138,The sequence model produces a sequence of answer tokens but these tokens may not be consecutive in the original passage .,OUR METHOD,LSTM,question-answering,8,66,0.717391304347826,137,0.5502008032128514,4,0.1333333333333333,1,0
139,"The boundary model produces only the start token and the end token of the answer , and then all the tokens between these two in the original passage are considered to be the answer .",OUR METHOD,LSTM,question-answering,8,67,0.7282608695652174,138,0.5542168674698795,5,0.1666666666666666,1,0
140,We now explain the two models separately .,OUR METHOD,LSTM,question-answering,8,68,0.7391304347826086,139,0.5582329317269076,6,0.2,1,0
141,The Sequence Model :,OUR METHOD,LSTM,question-answering,8,69,0.75,140,0.5622489959839357,7,0.2333333333333333,1,0
142,"Recall that in the sequence model , the answer is represented by a sequence of integers a = ( a 1 , a 2 , . . . ) indicating the positions of the selected tokens in the original passage .",OUR METHOD,LSTM,question-answering,8,70,0.7608695652173914,141,0.5662650602409639,8,0.2666666666666666,1,0
143,The Ans - Ptr layer models the generation of these integers in a sequential manner .,OUR METHOD,LSTM,question-answering,8,71,0.7717391304347826,142,0.570281124497992,9,0.3,1,0
144,"Because the length of an answer is not fixed , in order to stop generating answer tokens at certain point , we allow each a k to take up an integer value between 1 and P + 1 , where P + 1 is a special value indicating the end of the answer .",OUR METHOD,LSTM,question-answering,8,72,0.782608695652174,143,0.5742971887550201,10,0.3333333333333333,1,0
145,"Once a k is set to be P + 1 , the generation of the answer stops .",OUR METHOD,LSTM,question-answering,8,73,0.7934782608695652,144,0.5783132530120482,11,0.3666666666666666,1,0
146,"In order to generate the k th answer token indicated by a k , first , the attention mechanism is used again to obtain an attention weight vector ? k ? R ( P + 1 ) , where ? k , j ( 1 ? j ? P + 1 ) is the probability of selecting the j th token from the passage as the k th token in the answer , and ? k , ( P + 1 ) is the probability of stopping the answer generation at position k. ?",OUR METHOD,LSTM,question-answering,8,74,0.8043478260869565,145,0.5823293172690763,12,0.4,1,0
147,k is modeled as follows :,OUR METHOD,LSTM,question-answering,8,75,0.8152173913043478,146,0.5863453815261044,13,0.4333333333333333,1,0
148,where H r ?,OUR METHOD,LSTM,question-answering,8,76,0.8260869565217391,147,0.5903614457831325,14,0.4666666666666667,1,0
149,"R 2 l ( P + 1 ) is the concatenation of H r with a zero vector , defined as H r = [ H r ; 0 ] , V ? R l2 l , W a ?",OUR METHOD,LSTM,question-answering,8,77,0.8369565217391305,148,0.5943775100401606,15,0.5,1,0
150,"R ll , b a , v ?",OUR METHOD,LSTM,question-answering,8,78,0.8478260869565217,149,0.5983935742971888,16,0.5333333333333333,1,0
151,R land c ?,OUR METHOD,LSTM,question-answering,8,79,0.8586956521739131,150,0.6024096385542169,17,0.5666666666666667,1,0
152,"R are parameters to be learned , ( ? e ( P + 1 ) ) follows the same definition as before , and ha k?1 ?",OUR METHOD,LSTM,question-answering,8,80,0.8695652173913043,151,0.606425702811245,18,0.6,1,0
153,R l is the hidden vector at position k ?,OUR METHOD,LSTM,question-answering,8,81,0.8804347826086957,152,0.6104417670682731,19,0.6333333333333333,1,0
154,1 of an answer LSTM as defined below :,OUR METHOD,LSTM,question-answering,8,82,0.8913043478260869,153,0.6144578313253012,20,0.6666666666666666,1,0
155,"We can then model the probability of generating the answer sequence as p ( a| H r ) = k p ( a k | a 1 , a 2 , . . . , a k?1 , H r ) ,",OUR METHOD,LSTM,question-answering,8,83,0.9021739130434784,154,0.6184738955823293,21,0.7,1,0
156,"and p ( a k = j|a 1 , a 2 , . . . , a k?1 , H r ) = ? k , j .",OUR METHOD,LSTM,question-answering,8,84,0.9130434782608696,155,0.6224899598393574,22,0.7333333333333333,1,0
157,"To train the model , we minimize the following loss function based on the training examples :",OUR METHOD,LSTM,question-answering,8,85,0.9239130434782608,156,0.6265060240963856,23,0.7666666666666667,1,0
158,"log p ( a n | P n , Q n ) .",OUR METHOD,LSTM,question-answering,8,86,0.9347826086956522,157,0.6305220883534136,24,0.8,1,0
159,The Boundary Model :,OUR METHOD,LSTM,question-answering,8,87,0.9456521739130436,158,0.6345381526104418,25,0.8333333333333334,1,0
160,"The boundary model works in away very similar to the sequence model above , except that instead of predicting a sequence of indices a 1 , a 2 , . . . , we only need to predict two indices a sand a e .",OUR METHOD,LSTM,question-answering,8,88,0.9565217391304348,159,0.6385542168674698,26,0.8666666666666667,1,0
161,"So the main difference from the sequence model above is that in the boundary model we do not need to add the zero padding to H r , and the probability of generating an answer is simply modeled as p ( a| H r ) = p ( a s | H r ) p ( a e | a s , H r ) . Here "" Search "" refers to globally searching the spans with no more than 15 tokens , "" b "" refers to using bi-directional pre-processing LSTM , and "" en "" refers to ensemble method .",OUR METHOD,LSTM,question-answering,8,89,0.967391304347826,160,0.642570281124498,27,0.9,1,0
162,We further extend the boundary model by incorporating a search mechanism .,OUR METHOD,LSTM,question-answering,8,90,0.9782608695652174,161,0.6465863453815262,28,0.9333333333333332,1,0
163,"Specifically , during prediction , we try to limit the length of the span and globally search the span with the highest probability computed by p ( a s ) p ( a e ) .",OUR METHOD,LSTM,question-answering,8,91,0.9891304347826086,162,0.6506024096385542,29,0.9666666666666668,1,0
164,"Besides , as the boundary has a sequence of fixed number of values , bi-directional Ans - Ptr can be simply combined to fine - tune the correct span .",OUR METHOD,LSTM,question-answering,8,92,1.0,163,0.6546184738955824,30,1.0,1,0
165,EXPERIMENTS,,,question-answering,8,0,0.0,164,0.6586345381526104,0,0.0,1,0
166,"In this section , we present our experiment results and perform some analyses to better understand how our models works .",EXPERIMENTS,EXPERIMENTS,question-answering,8,1,0.1428571428571428,165,0.6626506024096386,1,0.0,1,0
167,DATA,EXPERIMENTS,,question-answering,8,2,0.2857142857142857,166,0.6666666666666666,0,0.0,1,0
168,We use the Stanford Question Answering Dataset ( SQuAD ) v 1.1 to conduct our experiments .,EXPERIMENTS,DATA,question-answering,8,3,0.4285714285714285,167,0.6706827309236948,1,0.2,1,0
169,Passages in SQuAD come from 536 articles from Wikipedia covering a wide range of topics .,EXPERIMENTS,DATA,question-answering,8,4,0.5714285714285714,168,0.6746987951807228,2,0.4,1,0
170,"Each passage is a single paragraph from a Wikipedia article , and each passage has around 5 questions associated with it .",EXPERIMENTS,DATA,question-answering,8,5,0.7142857142857143,169,0.678714859437751,3,0.6,1,0
171,"In total , there are 23,215 passages and 107,785 questions .",EXPERIMENTS,DATA,question-answering,8,6,0.8571428571428571,170,0.6827309236947792,4,0.8,1,0
172,"The data has been split into a training set ( with 87,599 question - answer pairs ) , a development set ( with 10,570 questionanswer pairs ) and a hidden test set .",EXPERIMENTS,DATA,question-answering,8,7,1.0,171,0.6867469879518072,5,1.0,1,0
173,EXPERIMENT SETTINGS,,,question-answering,8,0,0.0,172,0.6907630522088354,0,0.0,1,0
174,"We first tokenize all the passages , questions and answers .",EXPERIMENT SETTINGS,EXPERIMENT SETTINGS,question-answering,8,1,0.0833333333333333,173,0.6947791164658634,1,0.0833333333333333,1,0
175,The resulting vocabulary contains 117K unique words .,EXPERIMENT SETTINGS,EXPERIMENT SETTINGS,question-answering,8,2,0.1666666666666666,174,0.6987951807228916,2,0.1666666666666666,1,0
176,We use word embeddings from GloVe to initialize the model .,EXPERIMENT SETTINGS,EXPERIMENT SETTINGS,question-answering,8,3,0.25,175,0.7028112449799196,3,0.25,1,1
177,Words not found in Glo Ve are initialized as zero vectors .,EXPERIMENT SETTINGS,EXPERIMENT SETTINGS,question-answering,8,4,0.3333333333333333,176,0.7068273092369478,4,0.3333333333333333,1,0
178,The word embeddings are not updated during the training of the model .,EXPERIMENT SETTINGS,EXPERIMENT SETTINGS,question-answering,8,5,0.4166666666666667,177,0.7108433734939759,5,0.4166666666666667,1,0
179,The dimensionality l of the hidden layers is set to be 150 or 300 .,EXPERIMENT SETTINGS,EXPERIMENT SETTINGS,question-answering,8,6,0.5,178,0.714859437751004,6,0.5,1,1
180,We use ADAMAX with the coefficients ? 1 = 0.9 and ? 2 = 0.999 to optimize the model .,EXPERIMENT SETTINGS,EXPERIMENT SETTINGS,question-answering,8,7,0.5833333333333334,179,0.7188755020080321,7,0.5833333333333334,1,1
181,Each update is computed through a minibatch of 30 instances .,EXPERIMENT SETTINGS,EXPERIMENT SETTINGS,question-answering,8,8,0.6666666666666666,180,0.7228915662650602,8,0.6666666666666666,1,1
182,We do not use L2-regularization .,EXPERIMENT SETTINGS,EXPERIMENT SETTINGS,question-answering,8,9,0.75,181,0.7269076305220884,9,0.75,1,0
183,"The performance is measured by two metrics : percentage of exact match with the ground truth answers , and word - level F 1 score when comparing the tokens in the predicted answers with the tokens in the ground truth answers .",EXPERIMENT SETTINGS,EXPERIMENT SETTINGS,question-answering,8,10,0.8333333333333334,182,0.7309236947791165,10,0.8333333333333334,1,0
184,Note that in the development set and the test set each question has around three ground truth answers .,EXPERIMENT SETTINGS,EXPERIMENT SETTINGS,question-answering,8,11,0.9166666666666666,183,0.7349397590361446,11,0.9166666666666666,1,0
185,F1 scores with the best matching answers are used to compute the average F1 score .,EXPERIMENT SETTINGS,EXPERIMENT SETTINGS,question-answering,8,12,1.0,184,0.7389558232931727,12,1.0,1,0
186,RESULTS,,,question-answering,8,0,0.0,185,0.7429718875502008,0,0.0,1,0
187,The results of our models as well as the results of the baselines given by and are shown in .,RESULTS,RESULTS,question-answering,8,1,0.0256410256410256,186,0.7469879518072289,1,0.0625,1,0
188,We can see that both of our two models have clearly outper - :,RESULTS,RESULTS,question-answering,8,2,0.0512820512820512,187,0.751004016064257,2,0.125,1,0
189,Visualization of the attention weights ?,RESULTS,RESULTS,question-answering,8,3,0.0769230769230769,188,0.7550200803212851,3,0.1875,1,0
190,for three questions associated with the same passage .,RESULTS,RESULTS,question-answering,8,4,0.1025641025641025,189,0.7590361445783133,4,0.25,1,0
191,"outperformed the logistic regression model by , which relies on carefully designed features .",RESULTS,RESULTS,question-answering,8,5,0.1282051282051282,190,0.7630522088353414,5,0.3125,1,1
192,"Furthermore , our boundary model has outperformed the sequence model , achieving an exact match score of 61.1 % and an F1 score of 71.2 % .",RESULTS,RESULTS,question-answering,8,6,0.1538461538461538,191,0.7670682730923695,6,0.375,1,1
193,"In particular , in terms of the exact match score , the boundary model has a clear advantage over the sequence model .",RESULTS,RESULTS,question-answering,8,7,0.1794871794871795,192,0.7710843373493976,7,0.4375,1,0
194,The improvement of our models over the logistic regression model shows that our end - to - end neural network models without much feature engineering are very effective on this task and this dataset .,RESULTS,RESULTS,question-answering,8,8,0.2051282051282051,193,0.7751004016064257,8,0.5,1,0
195,"Considering the effectiveness of boundary model , we further explore this model .",RESULTS,RESULTS,question-answering,8,9,0.2307692307692307,194,0.7791164658634538,9,0.5625,1,0
196,"Observing that most of the answers are the spans with relatively small sizes , we simply limit the largest predicted span to have no more than 15 tokens and conducted experiment with span searching",RESULTS,RESULTS,question-answering,8,10,0.2564102564102564,195,0.7831325301204819,10,0.625,1,0
197,"This resulted in 1.5 % improvement in F1 on the development data and that outperformed the DCR model , which also introduced some language features such as POS and NE into their model .",RESULTS,RESULTS,question-answering,8,11,0.282051282051282,196,0.7871485943775101,11,0.6875,1,0
198,"Besides , we tried to increase the memory dimension l in the model or add bi-directional pre-processing LSTM or add bi-directional Ans - Ptr .",RESULTS,RESULTS,question-answering,8,12,0.3076923076923077,197,0.7911646586345381,12,0.75,1,0
199,The improvement on the development data using the first two methods is quite small .,RESULTS,RESULTS,question-answering,8,13,0.3333333333333333,198,0.7951807228915663,13,0.8125,1,0
200,"While by adding Bi - Ans - Ptr with bi-directional pre-processing LSTM , we can get 1.2 % improvement in F1 .",RESULTS,RESULTS,question-answering,8,14,0.358974358974359,199,0.7991967871485943,14,0.875,1,1
201,"Finally , we explore the ensemble method by simply computing the product of the boundary probabilities collected from 5 boundary models and then searching the most likely span with no more than 15 tokens .",RESULTS,RESULTS,question-answering,8,15,0.3846153846153846,200,0.8032128514056225,15,0.9375,1,0
202,This ensemble method achieved the best performance as shown in the table .,RESULTS,RESULTS,question-answering,8,16,0.4102564102564102,201,0.8072289156626506,16,1.0,1,0
203,FURTHER ANALYSES,RESULTS,,question-answering,8,17,0.4358974358974359,202,0.8112449799196787,0,0.0,1,0
204,"To better understand the strengths and weaknesses of our models , we perform some further analyses of the results below .",RESULTS,FURTHER ANALYSES,question-answering,8,18,0.4615384615384615,203,0.8152610441767069,1,0.0454545454545454,1,0
205,"First , we suspect that longer answers are harder to predict .",RESULTS,FURTHER ANALYSES,question-answering,8,19,0.4871794871794871,204,0.8192771084337349,2,0.0909090909090909,1,0
206,"To verify this hypothesis , we analysed the performance in terms of both exact match and F 1 score with respect to the answer length on the development set .",RESULTS,FURTHER ANALYSES,question-answering,8,20,0.5128205128205128,205,0.8232931726907631,3,0.1363636363636363,1,0
207,"For example , for questions whose answers contain more than 9 tokens , the F 1 score of the boundary model drops to around 55 % and the exact match score drops to only around 30 % , compared to the F 1 score and exact match score of close to 72 % and 67 % , respectively , for questions with single - token answers .",RESULTS,FURTHER ANALYSES,question-answering,8,21,0.5384615384615384,206,0.8273092369477911,4,0.1818181818181818,1,0
208,And that supports our hypothesis .,RESULTS,FURTHER ANALYSES,question-answering,8,22,0.5641025641025641,207,0.8313253012048193,5,0.2272727272727272,1,0
209,"Next , we analyze the performance of our models on different groups of questions .",RESULTS,FURTHER ANALYSES,question-answering,8,23,0.5897435897435898,208,0.8353413654618473,6,0.2727272727272727,1,0
210,"We use a crude way to split the questions into different groups based on a set of question words we have defined , including "" what , "" "" how , "" "" who , "" "" when , "" "" which , "" "" where , "" and "" why . """,RESULTS,FURTHER ANALYSES,question-answering,8,24,0.6153846153846154,209,0.8393574297188755,7,0.3181818181818182,1,0
211,These different question words roughly refer to questions with different types of answers .,RESULTS,FURTHER ANALYSES,question-answering,8,25,0.6410256410256411,210,0.8433734939759037,8,0.3636363636363636,1,0
212,"For example , "" when "" questions look for temporal expressions as answers , whereas "" where "" questions look for locations as answers .",RESULTS,FURTHER ANALYSES,question-answering,8,26,0.6666666666666666,211,0.8473895582329317,9,0.4090909090909091,1,0
213,"According to the performance on the development data set , our models work the best for "" when "" questions .",RESULTS,FURTHER ANALYSES,question-answering,8,27,0.6923076923076923,212,0.8514056224899599,10,0.4545454545454545,1,0
214,This maybe because in this dataset temporal expressions are relatively easier to recognize .,RESULTS,FURTHER ANALYSES,question-answering,8,28,0.717948717948718,213,0.8554216867469879,11,0.5,1,0
215,"Other groups of questions whose answers are noun phrases , such as "" what "" questions , "" which "" questions and "" where "" questions , also get relatively better results .",RESULTS,FURTHER ANALYSES,question-answering,8,29,0.7435897435897436,214,0.8594377510040161,12,0.5454545454545454,1,0
216,"On the other hand , "" why "" questions are the hardest to answer .",RESULTS,FURTHER ANALYSES,question-answering,8,30,0.7692307692307693,215,0.8634538152610441,13,0.5909090909090909,1,0
217,"This is not surprising because the answers to "" why "" questions can be very diverse , and they are not restricted to any certain type of phrases .",RESULTS,FURTHER ANALYSES,question-answering,8,31,0.7948717948717948,216,0.8674698795180723,14,0.6363636363636364,1,0
218,"Finally , we would like to check whether the attention mechanism used in the match - LSTM layer is effective in helping the model locate the answer .",RESULTS,FURTHER ANALYSES,question-answering,8,32,0.8205128205128205,217,0.8714859437751004,15,0.6818181818181818,1,0
219,We show the attention weights ?,RESULTS,FURTHER ANALYSES,question-answering,8,33,0.8461538461538461,218,0.8755020080321285,16,0.7272727272727273,1,0
220,in .,RESULTS,FURTHER ANALYSES,question-answering,8,34,0.8717948717948718,219,0.8795180722891566,17,0.7727272727272727,1,0
221,In the figure the darker the color is the higher the weight is .,RESULTS,FURTHER ANALYSES,question-answering,8,35,0.8974358974358975,220,0.8835341365461847,18,0.8181818181818182,1,0
222,We can see that some words have been well aligned based on the attention weights .,RESULTS,FURTHER ANALYSES,question-answering,8,36,0.9230769230769232,221,0.8875502008032129,19,0.8636363636363636,1,0
223,"For example , the word "" German "" in the passage is aligned well to the word "" language "" in the first question , and the model successfully predicts "" German "" as the answer to the question .",RESULTS,FURTHER ANALYSES,question-answering,8,37,0.9487179487179488,222,0.891566265060241,20,0.9090909090909092,1,0
224,"For the question word "" who "" in the second question , the word "" teacher "" actually receives relatively higher attention weight , and the model has predicted the phrase "" Martin Sekulic "" after that as the answer , which is correct .",RESULTS,FURTHER ANALYSES,question-answering,8,38,0.9743589743589745,223,0.8955823293172691,21,0.9545454545454546,1,0
225,"For the last question that starts with "" why "" , the attention weights are more evenly distributed and it is not clear which words have been aligned to "" why "" .",RESULTS,FURTHER ANALYSES,question-answering,8,39,1.0,224,0.8995983935742972,22,1.0,1,0
226,RELATED WORK,,,question-answering,8,0,0.0,225,0.9036144578313252,0,0.0,1,0
227,"Machine comprehension of text has gained much attention in recent years , and increasingly researchers are building data - drive , end - to - end neural network models for the task .",RELATED WORK,RELATED WORK,question-answering,8,1,0.0769230769230769,226,0.9076305220883534,1,0.5,0,0
228,We will first review the recently released datasets and then some end - to - end models on this task .,RELATED WORK,RELATED WORK,question-answering,8,2,0.1538461538461538,227,0.9116465863453816,2,1.0,0,0
229,DATASETS,RELATED WORK,,question-answering,8,3,0.2307692307692307,228,0.9156626506024096,0,0.0,0,0
230,"A number of datasets for studying machine comprehension were created in Cloze style by removing a single token from a sentence in the original corpus , and the task is to predict the missing word .",RELATED WORK,DATASETS,question-answering,8,4,0.3076923076923077,229,0.9196787148594378,1,0.1,0,0
231,was also created by human annotators .,RELATED WORK,DATASETS,question-answering,8,5,0.3846153846153846,230,0.923694779116466,2,0.2,0,0
232,"Different from the previous two , however , the SQuAD dataset does not provide candidate answers , and thus all possible subsequences from the given passage have to be considered as candidate answers .",RELATED WORK,DATASETS,question-answering,8,6,0.4615384615384615,231,0.927710843373494,3,0.3,0,0
233,"Besides the datasets above , there are also a few other datasets created for machine comprehension , such as WikiReading dataset and bAbI dataset , but they are quite different from the datasets above in nature .",RELATED WORK,DATASETS,question-answering,8,7,0.5384615384615384,232,0.931726907630522,4,0.4,0,0
234,END- TO- END NEURAL NETWORK MODELS FOR MACHINE COMPREHENSION,RELATED WORK,,question-answering,8,8,0.6153846153846154,233,0.9357429718875502,5,0.5,0,0
235,There have been a number of studies proposing end - to - end neural network models for machine comprehension .,RELATED WORK,END- TO- END NEURAL NETWORK MODELS FOR MACHINE COMPREHENSION,question-answering,8,9,0.6923076923076923,234,0.9397590361445785,6,0.6,0,0
236,A common approach is to use recurrent neural networks ( RNNs ) to process the given text and the question in order to predictor generate the answers .,RELATED WORK,END- TO- END NEURAL NETWORK MODELS FOR MACHINE COMPREHENSION,question-answering,8,10,0.7692307692307693,235,0.9437751004016064,7,0.7,0,0
237,Attention mechanism is also widely used on top of RNNs in order to match the question with the given passage .,RELATED WORK,END- TO- END NEURAL NETWORK MODELS FOR MACHINE COMPREHENSION,question-answering,8,11,0.8461538461538461,236,0.9477911646586346,8,0.8,0,0
238,"Given that answers often come from the given passage , Pointer Network has been adopted in a few studies in order to copy tokens from the given passage as answers .",RELATED WORK,END- TO- END NEURAL NETWORK MODELS FOR MACHINE COMPREHENSION,question-answering,8,12,0.9230769230769232,237,0.9518072289156626,9,0.9,0,0
239,"Compared with existing work , we use match - LSTM to match a question and a given passage , and we use Pointer Network in a different way such that we can generate answers that contain multiple tokens from the given passage .",RELATED WORK,END- TO- END NEURAL NETWORK MODELS FOR MACHINE COMPREHENSION,question-answering,8,13,1.0,238,0.9558232931726908,10,1.0,0,0
240,CONCLUSIONS,,,question-answering,8,0,0.0,239,0.9598393574297188,0,0.0,1,0
241,"In this paper , We developed two models for the machine comprehension problem defined in the Stanford Question Answering ( SQuAD ) dataset , both making use of match - LSTM and Pointer Network .",CONCLUSIONS,CONCLUSIONS,question-answering,8,1,0.1111111111111111,240,0.963855421686747,1,0.1428571428571428,0,0
242,"Experiments on the SQuAD dataset showed that our second model , the boundary model , could achieve an exact match score of 67.6 % and an F 1 score of 77 % on the test dataset , which is better than our sequence model and 's feature - engineered model .",CONCLUSIONS,CONCLUSIONS,question-answering,8,2,0.2222222222222222,241,0.9678714859437751,2,0.2857142857142857,0,0
243,"In the future , we plan to look further into the different types of questions and focus on those questions which currently have low performance , such as the "" why ' questions .",CONCLUSIONS,CONCLUSIONS,question-answering,8,3,0.3333333333333333,242,0.9718875502008032,3,0.4285714285714285,0,0
244,We also plan to test how our models could be applied to other machine comprehension datasets .,CONCLUSIONS,CONCLUSIONS,question-answering,8,4,0.4444444444444444,243,0.9759036144578314,4,0.5714285714285714,0,0
245,shows the numbers of answers with different lengths .,CONCLUSIONS,CONCLUSIONS,question-answering,8,5,0.5555555555555556,244,0.9799196787148594,5,0.7142857142857143,0,0
246,Bottom : Plot ( 3 ) shows the performance our the two models on different types of questions .,CONCLUSIONS,CONCLUSIONS,question-answering,8,6,0.6666666666666666,245,0.9839357429718876,6,0.8571428571428571,0,0
247,Plot ( 4 ) shows the numbers of different types of questions .,CONCLUSIONS,CONCLUSIONS,question-answering,8,7,0.7777777777777778,246,0.9879518072289156,7,1.0,0,0
248,A APPENDIX,CONCLUSIONS,,question-answering,8,8,0.8888888888888888,247,0.9919678714859438,0,0.0,0,0
249,"We show the performance breakdown by answer lengths and question types for our sequence model , boundary model and the ensemble model in .",CONCLUSIONS,A APPENDIX,question-answering,8,9,1.0,248,0.9959839357429718,1,0.0,0,0
1,title,,,question-answering,9,1,0.0666666666666666,0,0.0,0,0.0,1,0
2,LEARNING RECURRENT SPAN REPRESENTATIONS FOR EXTRACTIVE QUESTION ANSWERING,,,question-answering,9,2,0.1333333333333333,1,0.0056179775280898,1,0.0,1,1
3,abstract,,LEARNING RECURRENT SPAN REPRESENTATIONS FOR EXTRACTIVE QUESTION ANSWERING,question-answering,9,3,0.2,2,0.0112359550561797,0,0.0,1,0
4,"The reading comprehension task , that asks questions about a given evidence document , is a central problem in natural language understanding .",,LEARNING RECURRENT SPAN REPRESENTATIONS FOR EXTRACTIVE QUESTION ANSWERING,question-answering,9,4,0.2666666666666666,3,0.0168539325842696,1,0.0833333333333333,1,1
5,Recent formulations of this task have typically focused on answer selection from a set of candidates pre-defined manually or through the use of an external NLP pipeline .,,LEARNING RECURRENT SPAN REPRESENTATIONS FOR EXTRACTIVE QUESTION ANSWERING,question-answering,9,5,0.3333333333333333,4,0.0224719101123595,2,0.1666666666666666,1,0
6,"However , Rajpurkar et al . ( 2016 ) recently released the SQUAD dataset in which the answers can be arbitrary strings from the supplied text .",,LEARNING RECURRENT SPAN REPRESENTATIONS FOR EXTRACTIVE QUESTION ANSWERING,question-answering,9,6,0.4,5,0.0280898876404494,3,0.25,1,0
7,"In this paper , we focus on this answer extraction task , presenting a novel model architecture that efficiently builds fixed length representations of all spans in the evidence document with a recurrent network .",,LEARNING RECURRENT SPAN REPRESENTATIONS FOR EXTRACTIVE QUESTION ANSWERING,question-answering,9,7,0.4666666666666667,6,0.0337078651685393,4,0.3333333333333333,1,0
8,We show that scoring explicit span representations significantly improves performance over other approaches that factor the prediction into separate predictions about words or start and end markers .,,LEARNING RECURRENT SPAN REPRESENTATIONS FOR EXTRACTIVE QUESTION ANSWERING,question-answering,9,8,0.5333333333333333,7,0.0393258426966292,5,0.4166666666666667,1,0
9,Our approach improves upon the best published results of Wang & Jiang ( 2016 ) by 5 % and decreases the error of Rajpurkar et al. 's baseline by > 50 %.,,LEARNING RECURRENT SPAN REPRESENTATIONS FOR EXTRACTIVE QUESTION ANSWERING,question-answering,9,9,0.6,8,0.0449438202247191,6,0.5,1,0
10,"Recently , Rajpurkar et al. ( 2016 ) released the less restricted SQUAD dataset 1 that does not place any constraints on the set of allowed answers , other than that they should be drawn from the evidence document .",,LEARNING RECURRENT SPAN REPRESENTATIONS FOR EXTRACTIVE QUESTION ANSWERING,question-answering,9,10,0.6666666666666666,9,0.0505617977528089,7,0.5833333333333334,1,0
11,Rajpurkar et al. proposed a baseline system that chooses answers from the constituents identified by an existing syntactic parser .,,LEARNING RECURRENT SPAN REPRESENTATIONS FOR EXTRACTIVE QUESTION ANSWERING,question-answering,9,11,0.7333333333333333,10,0.0561797752808988,8,0.6666666666666666,1,0
12,"This allows them to prune the O ( N 2 ) answer candidates in each document of length N , but it also effectively renders 20.7 % of all questions unanswerable .",,LEARNING RECURRENT SPAN REPRESENTATIONS FOR EXTRACTIVE QUESTION ANSWERING,question-answering,9,12,0.8,11,0.0617977528089887,9,0.75,1,0
13,"Subsequent work by Wang & Jiang ( 2016 ) significantly improve upon this baseline by using an endto - end neural network architecture to identify answer spans by labeling either individual words , or the start and end of the answer span .",,LEARNING RECURRENT SPAN REPRESENTATIONS FOR EXTRACTIVE QUESTION ANSWERING,question-answering,9,13,0.8666666666666667,12,0.0674157303370786,10,0.8333333333333334,1,0
14,"Both of these methods do not make independence assumptions about substructures , but they are susceptible to search errors due to greedy training and decoding .",,LEARNING RECURRENT SPAN REPRESENTATIONS FOR EXTRACTIVE QUESTION ANSWERING,question-answering,9,14,0.9333333333333332,13,0.0730337078651685,11,0.9166666666666666,1,0
15,1,,LEARNING RECURRENT SPAN REPRESENTATIONS FOR EXTRACTIVE QUESTION ANSWERING,question-answering,9,15,1.0,14,0.0786516853932584,12,1.0,1,0
16,INTRODUCTION,,,question-answering,9,0,0.0,15,0.0842696629213483,0,0.0,1,0
17,A primary goal of natural language processing is to develop systems that can answer questions about the contents of documents .,INTRODUCTION,INTRODUCTION,question-answering,9,1,0.0625,16,0.0898876404494382,1,0.0909090909090909,1,1
18,"The reading comprehension task is of practical interest - we want computers to be able to read the world 's text and then answer our questions - and , since we believe it requires deep language understanding , it has also become a flagship task in NLP research .",INTRODUCTION,INTRODUCTION,question-answering,9,2,0.125,17,0.095505617977528,2,0.1818181818181818,1,0
19,A number of reading comprehension datasets have been developed that focus on answer selection from a small set of alternatives defined by annotators or existing NLP pipelines that can not be trained end - to - end .,INTRODUCTION,INTRODUCTION,question-answering,9,3,0.1875,18,0.1011235955056179,3,0.2727272727272727,1,0
20,"Subsequently , the models proposed for this task have tended to make use of the limited set of candidates , basing their predictions on mention - level attention weights , or centering classifiers , or network memories on candidate locations .",INTRODUCTION,INTRODUCTION,question-answering,9,4,0.25,19,0.1067415730337078,4,0.3636363636363636,1,0
21,"In contrast , here we argue that it is beneficial to simplify the decoding procedure by enumerating all possible answer spans .",INTRODUCTION,INTRODUCTION,question-answering,9,5,0.3125,20,0.1123595505617977,5,0.4545454545454545,1,0
22,"By explicitly representing each answer span , our model can be globally normalized during training and decoded exactly during evaluation .",INTRODUCTION,INTRODUCTION,question-answering,9,6,0.375,21,0.1179775280898876,6,0.5454545454545454,1,0
23,"A naive approach to building the O ( N 2 ) spans of up to length N would require a network that is cubic in size with respect to the passage length , and such a network would be untrainable .",INTRODUCTION,INTRODUCTION,question-answering,9,7,0.4375,22,0.1235955056179775,7,0.6363636363636364,1,0
24,"To overcome this , we present a novel neural architecture called RASOR that builds fixed - length span representations , reusing recurrent computations for shared substructures .",INTRODUCTION,INTRODUCTION,question-answering,9,8,0.5,23,0.1292134831460674,8,0.7272727272727273,1,1
25,"We demonstrate that directly classifying each of the competing spans , and training with global normalization overall possible spans , leads to a significant increase in performance .",INTRODUCTION,INTRODUCTION,question-answering,9,9,0.5625,24,0.1348314606741573,9,0.8181818181818182,1,1
26,"In our experiments , we show an increase in performance over of 5 % in terms of exact match to a reference answer , and 3.6 % in terms of predicted answer F1 with respect to the reference .",INTRODUCTION,INTRODUCTION,question-answering,9,10,0.625,25,0.1404494382022472,10,0.9090909090909092,1,0
27,"On both of these metrics , we close the gap between Rajpurkar et al. 's baseline and the human - performance upper-bound by > 50 %.",INTRODUCTION,INTRODUCTION,question-answering,9,11,0.6875,26,0.146067415730337,11,1.0,1,0
28,EXTRACTIVE QUESTION ANSWERING,INTRODUCTION,,question-answering,9,12,0.75,27,0.1516853932584269,0,0.0,1,0
29,TASK DEFINITION,INTRODUCTION,,question-answering,9,13,0.8125,28,0.1573033707865168,0,0.0,1,0
30,"Extractive question answering systems take as input a question q = {q 0 , . . . , q n } and a passage of text p = {p 0 , . . . , pm } from which they predict a single answer span a = a start , a end , represented as a pair of indices into p.",INTRODUCTION,TASK DEFINITION,question-answering,9,14,0.875,29,0.1629213483146067,1,0.3333333333333333,1,0
31,"Machine learned extractive question answering systems , such as the one presented here , learn a predictor function f ( q , p ) ?",INTRODUCTION,TASK DEFINITION,question-answering,9,15,0.9375,30,0.1685393258426966,2,0.6666666666666666,1,0
32,"a from a training dataset of q , p , a triples .",INTRODUCTION,TASK DEFINITION,question-answering,9,16,1.0,31,0.1741573033707865,3,1.0,1,0
33,RELATED WORK,,,question-answering,9,0,0.0,32,0.1797752808988764,0,0.0,1,0
34,"For the SQUAD dataset , the original paper from implemented a linear model with sparse features based on n-grams and part - of - speech tags present in the question and the candidate answer .",RELATED WORK,RELATED WORK,question-answering,9,1,0.0555555555555555,33,0.1853932584269663,1,0.0555555555555555,0,0
35,"Other than lexical features , they also used syntactic information in the form of dependency paths to extract more general features .",RELATED WORK,RELATED WORK,question-answering,9,2,0.1111111111111111,34,0.1910112359550561,2,0.1111111111111111,0,0
36,"They set a strong baseline for following work and also presented an in depth analysis , showing that lexical and syntactic features contribute most strongly to their model 's performance .",RELATED WORK,RELATED WORK,question-answering,9,3,0.1666666666666666,35,0.196629213483146,3,0.1666666666666666,0,0
37,"Subsequent work by use an end - to - end neural network method that uses a Match - LSTM to model the question and the passage , and uses pointer networks to extract the answer span from the passage .",RELATED WORK,RELATED WORK,question-answering,9,4,0.2222222222222222,36,0.2022471910112359,4,0.2222222222222222,0,0
38,This model resorts to greedy decoding and falls short in terms of performance compared to our model ( see Section 5 for more detail ) .,RELATED WORK,RELATED WORK,question-answering,9,5,0.2777777777777778,37,0.2078651685393258,5,0.2777777777777778,0,0
39,"While we only compare to published baselines , there are other unpublished competitive systems on the SQUAD leaderboard , as listed in footnote",RELATED WORK,RELATED WORK,question-answering,9,6,0.3333333333333333,38,0.2134831460674157,6,0.3333333333333333,0,0
40,4 .,RELATED WORK,RELATED WORK,question-answering,9,7,0.3888888888888889,39,0.2191011235955056,7,0.3888888888888889,0,0
41,"A task that is closely related to extractive question answering is the Cloze task , in which the goal is to predict a concealed span from a declarative sentence given a passage of supporting text .",RELATED WORK,RELATED WORK,question-answering,9,8,0.4444444444444444,40,0.2247191011235955,8,0.4444444444444444,0,0
42,presented a Cloze dataset in which the task is to predict the correct entity in an incomplete sentence given an abstractive summary of a news article .,RELATED WORK,RELATED WORK,question-answering,9,9,0.5,41,0.2303370786516854,9,0.5,0,0
43,Hermann et al .,RELATED WORK,RELATED WORK,question-answering,9,10,0.5555555555555556,42,0.2359550561797752,10,0.5555555555555556,0,0
44,also present various neural architectures to solve the problem .,RELATED WORK,RELATED WORK,question-answering,9,11,0.6111111111111112,43,0.2415730337078651,11,0.6111111111111112,0,0
45,"Although this dataset is large and varied in domain , recent analysis by shows that simple models can achieve close to the human upper bound .",RELATED WORK,RELATED WORK,question-answering,9,12,0.6666666666666666,44,0.247191011235955,12,0.6666666666666666,0,0
46,"As noted by the authors of the SQUAD paper , the annotated answers in the SQUAD dataset are often spans that include non-entities and can be longer phrases , unlike the Cloze datasets , thus making the task more challenging .",RELATED WORK,RELATED WORK,question-answering,9,13,0.7222222222222222,45,0.2528089887640449,13,0.7222222222222222,0,0
47,"Another , more traditional line of work has focused on extractive question answering on sentences , where the task is to extract a sentence from a document , given a question .",RELATED WORK,RELATED WORK,question-answering,9,14,0.7777777777777778,46,0.2584269662921348,14,0.7777777777777778,0,0
48,"Relevant datasets include datasets from the annual TREC evaluations and WikiQA , where the latter dataset specifically focused on Wikipedia passages .",RELATED WORK,RELATED WORK,question-answering,9,15,0.8333333333333334,47,0.2640449438202247,15,0.8333333333333334,0,0
49,"There has been a line of interesting recent publications using neural architectures , focused on this variety of extractive question answering .",RELATED WORK,RELATED WORK,question-answering,9,16,0.8888888888888888,48,0.2696629213483146,16,0.8888888888888888,0,0
50,"These methods model the question and a candidate answer sentence , but do not focus on possible candidate answer spans that may contain the answer to the given question .",RELATED WORK,RELATED WORK,question-answering,9,17,0.9444444444444444,49,0.2752808988764045,17,0.9444444444444444,0,0
51,"In this work , we focus on the more challenging problem of extracting the precise answer span .",RELATED WORK,RELATED WORK,question-answering,9,18,1.0,50,0.2808988764044944,18,1.0,0,0
52,MODEL,,,question-answering,9,0,0.0,51,0.2865168539325842,0,0.0,1,0
53,"We propose a model architecture called RASOR 2 illustrated in , that explicitly computes embedding representations for candidate answer spans .",MODEL,MODEL,question-answering,9,1,0.0192307692307692,52,0.2921348314606741,1,0.0909090909090909,1,0
54,"In most structured prediction problems ( e.g. sequence labeling or parsing ) , the number of possible output structures is exponential in the input length , and computing representations for every candidate is prohibitively expensive .",MODEL,MODEL,question-answering,9,2,0.0384615384615384,53,0.297752808988764,2,0.1818181818181818,1,0
55,"However , we exploit the simplicity of our task , where we can trivially and tractably enumerate all candidates .",MODEL,MODEL,question-answering,9,3,0.0576923076923076,54,0.3033707865168539,3,0.2727272727272727,1,0
56,"This facilitates an expressive model that computes joint representations of every answer span , that can be globally normalized during learning .",MODEL,MODEL,question-answering,9,4,0.0769230769230769,55,0.3089887640449438,4,0.3636363636363636,1,0
57,"In order to compute these span representations , we must aggregate information from the passage and the question for every answer candidate .",MODEL,MODEL,question-answering,9,5,0.0961538461538461,56,0.3146067415730337,5,0.4545454545454545,1,0
58,"For the example in , RASOR computes an embedding for the candidate answer spans : fixed to , fixed to the , to the , etc .",MODEL,MODEL,question-answering,9,6,0.1153846153846153,57,0.3202247191011236,6,0.5454545454545454,1,0
59,A naive approach for these aggregations would require a network that is cubic in size with respect to the passage length .,MODEL,MODEL,question-answering,9,7,0.1346153846153846,58,0.3258426966292135,7,0.6363636363636364,1,0
60,"Instead , our model reduces this to a quadratic size by reusing recurrent computations for shared substructures ( i.e. common passage words ) from different spans .",MODEL,MODEL,question-answering,9,8,0.1538461538461538,59,0.3314606741573033,8,0.7272727272727273,1,0
61,"Since the choice of answer span depends on the original question , we must incorporate this information into the computation of the span representation .",MODEL,MODEL,question-answering,9,9,0.173076923076923,60,0.3370786516853932,9,0.8181818181818182,1,0
62,We model this by augmenting the passage word embeddings with additional embedding representations of the question .,MODEL,MODEL,question-answering,9,10,0.1923076923076923,61,0.3426966292134831,10,0.9090909090909092,1,0
63,"In this section , we motivate and describe the architecture for RASOR in a top - down manner .",MODEL,MODEL,question-answering,9,11,0.2115384615384615,62,0.348314606741573,11,1.0,1,0
64,SCORING ANSWER SPANS,MODEL,,question-answering,9,12,0.2307692307692307,63,0.3539325842696629,0,0.0,1,0
65,"The goal of our extractive question answering system is to predict the single best answer span among all candidates from the passage p , denoted as A ( p ) .",MODEL,SCORING ANSWER SPANS,question-answering,9,13,0.25,64,0.3595505617977528,1,0.027027027027027,1,0
66,"Therefore , we define a probability distribution overall possible answer spans given the question q and passage p , and the predictor function finds the answer span with the maximum likelihood :",MODEL,SCORING ANSWER SPANS,question-answering,9,14,0.2692307692307692,65,0.3651685393258427,2,0.054054054054054,1,0
67,One might be tempted to introduce independence assumptions that would enable cheaper decoding .,MODEL,SCORING ANSWER SPANS,question-answering,9,15,0.2884615384615384,66,0.3707865168539326,3,0.081081081081081,1,0
68,"For example , this distribution can be modeled as ( 1 ) a product of conditionally independent distributions ( binary ) for every word or ( 2 ) a product of conditionally independent distributions ( over words ) for the start and end indices of the answer span .",MODEL,SCORING ANSWER SPANS,question-answering,9,16,0.3076923076923077,67,0.3764044943820224,4,0.1081081081081081,1,0
69,"However , we show in Section 5.2 that such independence assumptions hurt the accuracy of the model , and instead we only assume a fixed - length representation ha of each candidate span that is scored and normalized with a softmax layer ( Span score and Softmax in ) :",MODEL,SCORING ANSWER SPANS,question-answering,9,17,0.3269230769230769,68,0.3820224719101123,5,0.1351351351351351,1,0
70,where FFNN ( ) denotes a fully connected feed - forward neural network that provides a non-linear mapping of its input embedding .,MODEL,SCORING ANSWER SPANS,question-answering,9,18,0.3461538461538461,69,0.3876404494382022,6,0.1621621621621621,1,0
71,RASOR : RECURRENT SPAN REPRESENTATION,MODEL,,question-answering,9,19,0.3653846153846153,70,0.3932584269662921,7,0.1891891891891892,1,0
72,"The previously defined probability distribution depends on the answer span representations , ha .",MODEL,RASOR : RECURRENT SPAN REPRESENTATION,question-answering,9,20,0.3846153846153846,71,0.398876404494382,8,0.2162162162162162,1,0
73,"When computing ha , we assume access to representations of individual passage words that have been augmented with a representation of the question .",MODEL,RASOR : RECURRENT SPAN REPRESENTATION,question-answering,9,21,0.4038461538461538,72,0.4044943820224719,9,0.2432432432432432,1,0
74,"We denote these question - focused passage word embeddings as {p * 1 , . . . , p * m } and describe their creation in Section 3.3 .",MODEL,RASOR : RECURRENT SPAN REPRESENTATION,question-answering,9,22,0.4230769230769231,73,0.4101123595505618,10,0.2702702702702703,1,0
75,"In order to reuse computation for shared substructures , we use a bidirectional LSTM to encode the left and right context of every p * i ( Passage - level BiLSTM in ) .",MODEL,RASOR : RECURRENT SPAN REPRESENTATION,question-answering,9,23,0.4423076923076923,74,0.4157303370786517,11,0.2972972972972973,1,0
76,This allows us to simply concatenate the bidirectional LSTM ( BiLSTM ) outputs at the endpoints of a span to jointly encode its inside and outside information ( Span embedding in :,MODEL,RASOR : RECURRENT SPAN REPRESENTATION,question-answering,9,24,0.4615384615384615,75,0.4213483146067415,12,0.3243243243243243,1,0
77,where BILSTM ( ) denotes a BiLSTM over it s input embedding sequence and p * i is the concatenation of forward and backward outputs at time - step i .,MODEL,RASOR : RECURRENT SPAN REPRESENTATION,question-answering,9,25,0.4807692307692308,76,0.4269662921348314,13,0.3513513513513513,1,0
78,"While the visualization in shows a single layer BiLSTM for simplicity , we use a multi - layer BiLSTM in our experiments .",MODEL,RASOR : RECURRENT SPAN REPRESENTATION,question-answering,9,26,0.5,77,0.4325842696629213,14,0.3783783783783784,1,0
79,"The concatenated output of each layer is used as input for the subsequent layer , allowing the upper layers to depend on the entire passage .",MODEL,RASOR : RECURRENT SPAN REPRESENTATION,question-answering,9,27,0.5192307692307693,78,0.4382022471910112,15,0.4054054054054054,1,0
80,QUESTION - FOCUSED PASSAGE WORD EMBEDDING,MODEL,,question-answering,9,28,0.5384615384615384,79,0.4438202247191011,16,0.4324324324324324,1,0
81,"Computing the question - focused passage word embeddings {p * 1 , . . . , p * m } requires integrating question information into the passage .",MODEL,QUESTION - FOCUSED PASSAGE WORD EMBEDDING,question-answering,9,29,0.5576923076923077,80,0.449438202247191,17,0.4594594594594595,1,0
82,The architecture for this integration is flexible and likely depends on the nature of the dataset .,MODEL,QUESTION - FOCUSED PASSAGE WORD EMBEDDING,question-answering,9,30,0.5769230769230769,81,0.4550561797752809,18,0.4864864864864865,1,0
83,"For the SQUAD dataset , we find that both passage - aligned and passageindependent question representations are effective at incorporating this contextual information , and experiments will show that their benefits are complementary .",MODEL,QUESTION - FOCUSED PASSAGE WORD EMBEDDING,question-answering,9,31,0.5961538461538461,82,0.4606741573033708,19,0.5135135135135135,1,0
84,"To incorporate these question representations , we simply concatenate them with the passage word embeddings ( Question - focused passage word embedding in ) .",MODEL,QUESTION - FOCUSED PASSAGE WORD EMBEDDING,question-answering,9,32,0.6153846153846154,83,0.4662921348314606,20,0.5405405405405406,1,0
85,We use fixed pretrained embeddings to represent question and passage words .,MODEL,QUESTION - FOCUSED PASSAGE WORD EMBEDDING,question-answering,9,33,0.6346153846153846,84,0.4719101123595505,21,0.5675675675675675,1,0
86,"Therefore , in the following discussion , notation for the words are interchangeable with their embedding representations .",MODEL,QUESTION - FOCUSED PASSAGE WORD EMBEDDING,question-answering,9,34,0.6538461538461539,85,0.4775280898876404,22,0.5945945945945946,1,0
87,Question - independent passage word embedding,MODEL,QUESTION - FOCUSED PASSAGE WORD EMBEDDING,question-answering,9,35,0.6730769230769231,86,0.4831460674157303,23,0.6216216216216216,1,0
88,"The first component simply looks up the pretrained word embedding for the passage word , pi .",MODEL,QUESTION - FOCUSED PASSAGE WORD EMBEDDING,question-answering,9,36,0.6923076923076923,87,0.4887640449438202,24,0.6486486486486487,1,0
89,Passage - aligned question representation,MODEL,QUESTION - FOCUSED PASSAGE WORD EMBEDDING,question-answering,9,37,0.7115384615384616,88,0.4943820224719101,25,0.6756756756756757,1,0
90,"In this dataset , the question - passage pairs often contain large lexical overlap or similarity near the correct answer span .",MODEL,QUESTION - FOCUSED PASSAGE WORD EMBEDDING,question-answering,9,38,0.7307692307692307,89,0.5,26,0.7027027027027027,1,0
91,"To encourage the model to exploit these similarities , we include a fixed - length representation of the question based on soft - alignments with the passage word .",MODEL,QUESTION - FOCUSED PASSAGE WORD EMBEDDING,question-answering,9,39,0.75,90,0.5056179775280899,27,0.7297297297297297,1,0
92,"The alignments are computed via neural attention , and we use the variant proposed by , where attention scores are dot products between non-linear mappings of word embeddings .",MODEL,QUESTION - FOCUSED PASSAGE WORD EMBEDDING,question-answering,9,40,0.7692307692307693,91,0.5112359550561798,28,0.7567567567567568,1,0
93,q align i = n j=1 a ij q j,MODEL,QUESTION - FOCUSED PASSAGE WORD EMBEDDING,question-answering,9,41,0.7884615384615384,92,0.5168539325842697,29,0.7837837837837838,1,0
94,Passage - independent question representation,MODEL,QUESTION - FOCUSED PASSAGE WORD EMBEDDING,question-answering,9,42,0.8076923076923077,93,0.5224719101123596,30,0.8108108108108109,1,0
95,We also include a representation of the question that does not depend on the passage and is shared for all passage words .,MODEL,QUESTION - FOCUSED PASSAGE WORD EMBEDDING,question-answering,9,43,0.8269230769230769,94,0.5280898876404494,31,0.8378378378378378,1,0
96,"Similar to the previous question representation , an attention score is computed via a dot -product , except the question word is compared to a universal learned embedding rather any particular passage word .",MODEL,QUESTION - FOCUSED PASSAGE WORD EMBEDDING,question-answering,9,44,0.8461538461538461,95,0.5337078651685393,32,0.8648648648648649,1,0
97,"Additionally , we incorporate contextual information with a BiLSTM before aggregating the outputs using this attention mechanism .",MODEL,QUESTION - FOCUSED PASSAGE WORD EMBEDDING,question-answering,9,45,0.8653846153846154,96,0.5393258426966292,33,0.8918918918918919,1,0
98,The goal is to generate a coarse - grained summary of the question that depends on word order .,MODEL,QUESTION - FOCUSED PASSAGE WORD EMBEDDING,question-answering,9,46,0.8846153846153846,97,0.5449438202247191,34,0.918918918918919,1,0
99,"Formally , the passage - independent question representation q indep is computed as follows :",MODEL,QUESTION - FOCUSED PASSAGE WORD EMBEDDING,question-answering,9,47,0.903846153846154,98,0.550561797752809,35,0.945945945945946,1,0
100,This representation is a bidirectional generalization of the question representation recently proposed by fora different question - answering task .,MODEL,QUESTION - FOCUSED PASSAGE WORD EMBEDDING,question-answering,9,48,0.9230769230769232,99,0.5561797752808989,36,0.972972972972973,1,0
101,"Given the above three components , the complete question - focused passage word embedding for pi is their concatenation :",MODEL,QUESTION - FOCUSED PASSAGE WORD EMBEDDING,question-answering,9,49,0.9423076923076924,100,0.5617977528089888,37,1.0,1,0
102,LEARNING,MODEL,,question-answering,9,50,0.9615384615384616,101,0.5674157303370787,0,0.0,1,0
103,"Given the above model specification , learning is straightforward .",MODEL,LEARNING,question-answering,9,51,0.9807692307692308,102,0.5730337078651685,1,0.5,1,0
104,We simply maximize the loglikelihood of the correct answer candidates and backpropagate the errors end - to - end .,MODEL,LEARNING,question-answering,9,52,1.0,103,0.5786516853932584,2,1.0,1,0
105,EXPERIMENTAL SETUP,,,question-answering,9,0,0.0,104,0.5842696629213483,0,0.0,1,0
106,We represent each of the words in the question and document using 300 dimensional GloVe embeddings trained on a corpus of 840 bn words .,EXPERIMENTAL SETUP,EXPERIMENTAL SETUP,question-answering,9,1,0.125,105,0.5898876404494382,1,0.125,1,1
107,These embeddings cover 200 k words and all out of vocabulary ( OOV ) words are projected onto one of 1 m randomly initialized 300d embeddings .,EXPERIMENTAL SETUP,EXPERIMENTAL SETUP,question-answering,9,2,0.25,106,0.5955056179775281,2,0.25,1,0
108,"We couple the input and forget gates in our LSTMs , as described in , and we use a single dropout mask to apply dropout across all LSTM time - steps as proposed by .",EXPERIMENTAL SETUP,EXPERIMENTAL SETUP,question-answering,9,3,0.375,107,0.601123595505618,3,0.375,1,1
109,Hidden layers in the feed forward neural networks use rectified linear units .,EXPERIMENTAL SETUP,EXPERIMENTAL SETUP,question-answering,9,4,0.5,108,0.6067415730337079,4,0.5,1,1
110,Answer candidates are limited to spans with at most 30 words .,EXPERIMENTAL SETUP,EXPERIMENTAL SETUP,question-answering,9,5,0.625,109,0.6123595505617978,5,0.625,1,0
111,"To choose the final model configuration , we ran grid searches over : the dimensionality of the LSTM hidden states ; the width and depth of the feed forward neural networks ; dropout for the LSTMs ; the number of stacked LSTM layers ; and the decay multiplier [ 0.9 , 0.95 , 1.0 ] with which we multiply the learning rate every 10 k steps .",EXPERIMENTAL SETUP,EXPERIMENTAL SETUP,question-answering,9,6,0.75,110,0.6179775280898876,6,0.75,1,1
112,The best model uses 50d LSTM states ; two - layer BiLSTMs for the span encoder and the passage - independent question representation ; dropout of 0.1 throughout ; and a learning rate decay of 5 % every 10 k steps .,EXPERIMENTAL SETUP,EXPERIMENTAL SETUP,question-answering,9,7,0.875,111,0.6235955056179775,7,0.875,1,1
113,All models are implemented using TensorFlow 3 and trained on the SQUAD training set using the ADAM optimizer with a mini-batch size of 4 and trained using 10 asynchronous training threads on a single machine .,EXPERIMENTAL SETUP,EXPERIMENTAL SETUP,question-answering,9,8,1.0,112,0.6292134831460674,8,1.0,1,1
114,RESULTS,,,question-answering,9,0,0.0,113,0.6348314606741573,0,0.0,1,0
115,"We train on the 80 k ( question , passage , answer span ) triples in the SQUAD training set and report results on the 10k examples in the SQUAD development and test sets .",RESULTS,RESULTS,question-answering,9,1,0.0909090909090909,114,0.6404494382022472,1,0.5,1,0
116,"All results are calculated using the official SQUAD evaluation script , which reports exact answer match and F1 overlap of the unigrams between the predicted answer and the closest labeled answer from the 3 reference answers given in the SQUAD development set .",RESULTS,RESULTS,question-answering,9,2,0.1818181818181818,115,0.6460674157303371,2,1.0,1,0
117,COMPARISONS TO OTHER WORK,RESULTS,,question-answering,9,3,0.2727272727272727,116,0.651685393258427,0,0.0,1,0
118,Our model with recurrent span representations ( RASOR ) is compared to all previously published systems,RESULTS,COMPARISONS TO OTHER WORK,question-answering,9,4,0.3636363636363636,117,0.6573033707865169,1,0.125,1,0
119,4 . published a logistic regression baseline as well as human performance on the SQUAD task .,RESULTS,COMPARISONS TO OTHER WORK,question-answering,9,5,0.4545454545454545,118,0.6629213483146067,2,0.25,1,0
120,"The logistic regression baseline uses the output of an existing syntactic parser both as a constraint on the set of allowed answer spans , and as a method of creating sparse features for an answer -centric scoring model .",RESULTS,COMPARISONS TO OTHER WORK,question-answering,9,6,0.5454545454545454,119,0.6685393258426966,3,0.375,1,0
121,"Despite not having access to any external representation of linguistic structure , RASOR achieves an error reduction of more than 50 % over this baseline , both in terms of exact match and F1 , relative to the human performance upper bound .",RESULTS,COMPARISONS TO OTHER WORK,question-answering,9,7,0.6363636363636364,120,0.6741573033707865,4,0.5,1,1
122,More closely related to RASOR is the boundary model with Match - LSTMs and Pointer Networks by .,RESULTS,COMPARISONS TO OTHER WORK,question-answering,9,8,0.7272727272727273,121,0.6797752808988764,5,0.625,1,0
123,"Their model similarly uses recurrent networks to learn embeddings of each passage word in the context of the question , and it can also capture interactions between endpoints , since the end index probability distribution is conditioned on the start index .",RESULTS,COMPARISONS TO OTHER WORK,question-answering,9,9,0.8181818181818182,122,0.6853932584269663,6,0.75,1,0
124,"However , both training and evaluation are greedy , making their system susceptible to search errors when decoding .",RESULTS,COMPARISONS TO OTHER WORK,question-answering,9,10,0.9090909090909092,123,0.6910112359550562,7,0.875,1,0
125,"In contrast , RASOR can efficiently and explicitly model the quadratic number of possible answers , which leads to a 14 % error reduction over the best performing Match - LSTM model .",RESULTS,COMPARISONS TO OTHER WORK,question-answering,9,11,1.0,124,0.6966292134831461,8,1.0,1,1
126,MODEL VARIATIONS,,,question-answering,9,0,0.0,125,0.702247191011236,0,0.0,1,0
127,We investigate two main questions in the following ablations and comparisons .,MODEL VARIATIONS,MODEL VARIATIONS,question-answering,9,1,0.0217391304347826,126,0.7078651685393258,1,0.125,1,0
128,( 1 ) How important are the two methods of representing the question described in Section 3.3 ?,MODEL VARIATIONS,MODEL VARIATIONS,question-answering,9,2,0.0434782608695652,127,0.7134831460674157,2,0.25,1,0
129,( 2 ) What is the impact of learning a loss function that accurately reflects the span prediction task ?,MODEL VARIATIONS,MODEL VARIATIONS,question-answering,9,3,0.0652173913043478,128,0.7191011235955056,3,0.375,1,0
130,Question representations shows the performance of RASOR when either of the two question representations described in Section 3.3 is removed .,MODEL VARIATIONS,MODEL VARIATIONS,question-answering,9,4,0.0869565217391304,129,0.7247191011235955,4,0.5,1,0
131,"The passage - aligned question representation is crucial , since lexically similar regions of the passage provide strong signal for relevant answer spans .",MODEL VARIATIONS,MODEL VARIATIONS,question-answering,9,5,0.108695652173913,130,0.7303370786516854,5,0.625,1,1
132,"If the question is only integrated through the inclusion of a passage - independent representation , performance drops drastically .",MODEL VARIATIONS,MODEL VARIATIONS,question-answering,9,6,0.1304347826086956,131,0.7359550561797753,6,0.75,1,0
133,"The passage - independent question representation over the BiLSTM is less important , but it still accounts for over 3 % exact match and F 1 .",MODEL VARIATIONS,MODEL VARIATIONS,question-answering,9,7,0.1521739130434782,132,0.7415730337078652,7,0.875,1,0
134,The input of both of these components is analyzed qualitatively in Section 6 .,MODEL VARIATIONS,MODEL VARIATIONS,question-answering,9,8,0.1739130434782608,133,0.7471910112359551,8,1.0,1,0
135,Question representation EM F1,MODEL VARIATIONS,MODEL VARIATIONS,question-answering,9,9,0.1956521739130435,134,0.7528089887640449,0,0.0,1,0
136,Only passage - independent 48.7 56.6 Only passage - aligned 63.1 71.3 RASOR 66.4 74.9,MODEL VARIATIONS,MODEL VARIATIONS,question-answering,9,10,0.217391304347826,135,0.7584269662921348,1,0.5,1,0
137,( a ) Ablation of question representations .,MODEL VARIATIONS,MODEL VARIATIONS,question-answering,9,11,0.2391304347826087,136,0.7640449438202247,2,1.0,1,0
138,Learning objective EM F1,MODEL VARIATIONS,MODEL VARIATIONS,question-answering,9,12,0.2608695652173913,137,0.7696629213483146,0,0.0,1,0
139,Membership prediction 57.9 69.7 BIO sequence prediction 63.9 73.0 Endpoints prediction 65.3 75.1 Span prediction w/ log loss 65.2 73.6 ( b ) Comparisons for different learning objectives given the same passage - level BiLSTM .,MODEL VARIATIONS,MODEL VARIATIONS,question-answering,9,13,0.2826086956521739,138,0.7752808988764045,1,0.0303030303030303,1,0
140,Learning objectives,MODEL VARIATIONS,MODEL VARIATIONS,question-answering,9,14,0.3043478260869565,139,0.7808988764044944,2,0.0606060606060606,1,0
141,"Given a fixed architecture that is capable of encoding the input questionpassage pairs , there are many ways of setting up a learning objective to encourage the model to predict the correct span .",MODEL VARIATIONS,MODEL VARIATIONS,question-answering,9,15,0.3260869565217391,140,0.7865168539325843,3,0.0909090909090909,1,0
142,"In , we provide comparisons of some alternatives ( learned end - toend ) given only the passage - level BiLSTM from RASOR .",MODEL VARIATIONS,MODEL VARIATIONS,question-answering,9,16,0.3478260869565217,141,0.7921348314606742,4,0.1212121212121212,1,0
143,"In order to provide clean comparisons , we restrict the alternatives to objectives that are trained and evaluated with exact decoding .",MODEL VARIATIONS,MODEL VARIATIONS,question-answering,9,17,0.3695652173913043,142,0.797752808988764,5,0.1515151515151515,1,0
144,The simplest alternative is to consider this task as binary classification for every word ( Membership prediction in ) .,MODEL VARIATIONS,MODEL VARIATIONS,question-answering,9,18,0.391304347826087,143,0.8033707865168539,6,0.1818181818181818,1,0
145,"In this baseline , we optimize the logistic loss for binary labels indicating whether passage words belong to the correct answer span .",MODEL VARIATIONS,MODEL VARIATIONS,question-answering,9,19,0.4130434782608695,144,0.8089887640449438,7,0.2121212121212121,1,0
146,"At prediction time , a valid span can be recovered in linear time by finding the maximum contiguous sum of scores .",MODEL VARIATIONS,MODEL VARIATIONS,question-answering,9,20,0.4347826086956521,145,0.8146067415730337,8,0.2424242424242424,1,0
147,proposed a sequence - labeling scheme that is similar to the above baseline ( BIO sequence prediction in ) .,MODEL VARIATIONS,MODEL VARIATIONS,question-answering,9,21,0.4565217391304347,146,0.8202247191011236,9,0.2727272727272727,1,0
148,We follow their proposed model and learn a conditional random field ( CRF ) layer after the passage - level BiLSTM to model transitions between the different labels .,MODEL VARIATIONS,MODEL VARIATIONS,question-answering,9,22,0.4782608695652174,147,0.8258426966292135,10,0.303030303030303,1,0
149,"At prediction time , a valid span can be recovered in linear time using Viterbi decoding , with hard transition constraints to enforce a single contiguous output .",MODEL VARIATIONS,MODEL VARIATIONS,question-answering,9,23,0.5,148,0.8314606741573034,11,0.3333333333333333,1,0
150,We also consider a model that independently predicts the two endpoints of the answer span ( Endpoints prediction in ) .,MODEL VARIATIONS,MODEL VARIATIONS,question-answering,9,24,0.5217391304347826,149,0.8370786516853933,12,0.3636363636363636,1,0
151,This model uses the softmax loss over passage words during learning .,MODEL VARIATIONS,MODEL VARIATIONS,question-answering,9,25,0.5434782608695652,150,0.8426966292134831,13,0.3939393939393939,1,0
152,"When decoding , we only need to enforce the constraint that the start index is no greater than the end index .",MODEL VARIATIONS,MODEL VARIATIONS,question-answering,9,26,0.5652173913043478,151,0.848314606741573,14,0.4242424242424242,1,0
153,"Without the interactions between the endpoints , this can be computed in linear time .",MODEL VARIATIONS,MODEL VARIATIONS,question-answering,9,27,0.5869565217391305,152,0.8539325842696629,15,0.4545454545454545,1,0
154,Note that this model has the same expressivity as RASOR if the span - level FFNN were removed .,MODEL VARIATIONS,MODEL VARIATIONS,question-answering,9,28,0.6086956521739131,153,0.8595505617977528,16,0.4848484848484848,1,0
155,"Lastly , we compare with a model using the same architecture as RASOR but is trained with a binary logistic loss rather than a softmax loss over spans ( Span prediction w/ logistic loss in ) .",MODEL VARIATIONS,MODEL VARIATIONS,question-answering,9,29,0.6304347826086957,154,0.8651685393258427,17,0.5151515151515151,1,0
156,The trend in shows that the model is better at leveraging the supervision as the learning objective more accurately reflects the fundamental task at hand : determining the best answer span .,MODEL VARIATIONS,MODEL VARIATIONS,question-answering,9,30,0.6521739130434783,155,0.8707865168539326,18,0.5454545454545454,1,0
157,"First , we observe general improvements when using labels that closely align with the task .",MODEL VARIATIONS,MODEL VARIATIONS,question-answering,9,31,0.6739130434782609,156,0.8764044943820225,19,0.5757575757575758,1,1
158,"For example , the labels for membership prediction simply happens to provide single contiguous spans in the supervision .",MODEL VARIATIONS,MODEL VARIATIONS,question-answering,9,32,0.6956521739130435,157,0.8820224719101124,20,0.6060606060606061,1,0
159,The model must consider far more possible answers than it needs to ( the power set of all words ) .,MODEL VARIATIONS,MODEL VARIATIONS,question-answering,9,33,0.717391304347826,158,0.8876404494382022,21,0.6363636363636364,1,0
160,The same problem holds for BIO sequence predictionthe model must do additional work to learn the semantics of the BIO tags .,MODEL VARIATIONS,MODEL VARIATIONS,question-answering,9,34,0.7391304347826086,159,0.8932584269662921,22,0.6666666666666666,1,0
161,"On the other hand , in RASOR , the semantics of an answer span is naturally encoded by the set of labels .",MODEL VARIATIONS,MODEL VARIATIONS,question-answering,9,35,0.7608695652173914,160,0.898876404494382,23,0.696969696969697,1,0
162,"Second , we observe the importance of allowing interactions between the endpoints using the spanlevel FFNN .",MODEL VARIATIONS,MODEL VARIATIONS,question-answering,9,36,0.782608695652174,161,0.904494382022472,24,0.7272727272727273,1,1
163,"RASOR outperforms the endpoint prediction model by 1.1 inexact match , The interaction between endpoints enables RASOR to enforce consistency across its two substructures .",MODEL VARIATIONS,MODEL VARIATIONS,question-answering,9,37,0.8043478260869565,162,0.9101123595505618,25,0.7575757575757576,1,1
164,"While this does not provide improvements for predicting the correct region of the answer ( captured by the F1 metric , which drops by 0.2 ) , it is more likely to predict a clean answer span that matches human judgment exactly ( captured by the exact - match metric ) .",MODEL VARIATIONS,MODEL VARIATIONS,question-answering,9,38,0.8260869565217391,163,0.9157303370786516,26,0.7878787878787878,1,0
165,shows how the performances of RASOR and the endpoint predictor introduced in Section 5.2 degrade as the lengths of their predictions increase .,MODEL VARIATIONS,MODEL VARIATIONS,question-answering,9,39,0.8478260869565217,164,0.9213483146067416,27,0.8181818181818182,1,0
166,It is clear that explicitly modeling interactions between end markers is increasingly important as the span grows in length .,MODEL VARIATIONS,MODEL VARIATIONS,question-answering,9,40,0.8695652173913043,165,0.9269662921348316,28,0.8484848484848485,1,0
167,"The passageindependent question representation pays most attention to the words that could attach to the answer in the passage ( "" brought "" , "" against "" ) or describe the answer category ( "" people "" ) .",MODEL VARIATIONS,MODEL VARIATIONS,question-answering,9,41,0.8913043478260869,166,0.9325842696629212,29,0.8787878787878788,1,0
168,"Meanwhile , the passage - aligned question representation pays attention to similar words .",MODEL VARIATIONS,MODEL VARIATIONS,question-answering,9,42,0.9130434782608696,167,0.9382022471910112,30,0.9090909090909092,1,0
169,"The top predictions for both examples are all valid syntactic constituents , and they all have the correct semantic category .",MODEL VARIATIONS,MODEL VARIATIONS,question-answering,9,43,0.9347826086956522,168,0.9438202247191012,31,0.9393939393939394,1,0
170,"However , RASOR assigns almost as much probability mass to it 's incorrect third prediction "" British "" as it does to the top scoring correct prediction "" Egyptian "" .",MODEL VARIATIONS,MODEL VARIATIONS,question-answering,9,44,0.9565217391304348,169,0.949438202247191,32,0.9696969696969696,1,0
171,"This showcases a common failure case for RASOR , where it can find an answer of the correct type close to a phrase that overlaps with the question - but it can not accurately represent the semantic dependency on that phrase .",MODEL VARIATIONS,MODEL VARIATIONS,question-answering,9,45,0.9782608695652174,170,0.9550561797752808,33,1.0,1,0
172,ANALYSIS,MODEL VARIATIONS,,question-answering,9,46,1.0,171,0.9606741573033708,0,0.0,1,0
173,CONCLUSION,,,question-answering,9,0,0.0,172,0.9662921348314608,0,0.0,1,0
174,We have shown a novel approach for perform extractive question answering on the SQUAD dataset by explicitly representing and scoring answer span candidates .,CONCLUSION,CONCLUSION,question-answering,9,1,0.2,173,0.9719101123595506,1,0.2,0,0
175,The core of our model relies on a recurrent network that enables shared computation for the shared substructure across span candidates .,CONCLUSION,CONCLUSION,question-answering,9,2,0.4,174,0.9775280898876404,2,0.4,0,0
176,"We explore different methods of encoding the passage and question , showing the benefits of including both passage - independent and passage - aligned question representations .",CONCLUSION,CONCLUSION,question-answering,9,3,0.6,175,0.9831460674157304,3,0.6,0,0
177,"While we show that this encoding method is beneficial for the task , this is orthogonal to the core contribution of efficiently computing span representation .",CONCLUSION,CONCLUSION,question-answering,9,4,0.8,176,0.9887640449438202,4,0.8,0,0
178,"In future work , we plan to explore alternate architectures that provide input to the recurrent span representations .",CONCLUSION,CONCLUSION,question-answering,9,5,1.0,177,0.99438202247191,5,1.0,0,0
1,title,,,relation-classification,0,0,0.0,0,0.0,0,0.0,1,0
2,End - to - End Relation Extraction using LSTMs on Sequences and Tree Structures,title,title,relation-classification,0,1,0.0,1,0.0044247787610619,1,0.0,1,1
3,abstract,,,relation-classification,0,0,0.0,2,0.0088495575221238,0,0.0,1,0
4,We present a novel end - to - end neural model to extract entities and relations between them .,abstract,abstract,relation-classification,0,1,0.1428571428571428,3,0.0132743362831858,1,0.1428571428571428,1,1
5,Our recurrent neural network based model captures both word sequence and dependency tree substructure information by stacking bidirectional treestructured LSTM - RNNs on bidirectional sequential LSTM - RNNs .,abstract,abstract,relation-classification,0,2,0.2857142857142857,4,0.0176991150442477,2,0.2857142857142857,1,0
6,This allows our model to jointly represent both entities and relations with shared parameters in a single model .,abstract,abstract,relation-classification,0,3,0.4285714285714285,5,0.0221238938053097,3,0.4285714285714285,1,1
7,We further encourage detection of entities during training and use of entity information in relation extraction via entity pretraining and scheduled sampling .,abstract,abstract,relation-classification,0,4,0.5714285714285714,6,0.0265486725663716,4,0.5714285714285714,1,0
8,"Our model improves over the stateof - the - art feature - based model on end -toend relation extraction , achieving 12.1 % and 5.7 % relative error reductions in F1score on ACE2005 and ACE2004 , respectively .",abstract,abstract,relation-classification,0,5,0.7142857142857143,7,0.0309734513274336,5,0.7142857142857143,1,0
9,We also show that our LSTM - RNN based model compares favorably to the state - of - the - art CNN based model ( in F1-score ) on nominal relation classification ( Sem Eval - 2010 Task 8 ) .,abstract,abstract,relation-classification,0,6,0.8571428571428571,8,0.0353982300884955,6,0.8571428571428571,1,0
10,"Finally , we present an extensive ablation analysis of several model components .",abstract,abstract,relation-classification,0,7,1.0,9,0.0398230088495575,7,1.0,1,0
11,Introduction,,,relation-classification,0,0,0.0,10,0.0442477876106194,0,0.0,1,0
12,Extracting semantic relations between entities in text is an important and well - studied task in information extraction and natural language processing ( NLP ) .,Introduction,Introduction,relation-classification,0,1,0.0434782608695652,11,0.0486725663716814,1,0.0434782608695652,1,1
13,"Traditional systems treat this task as a pipeline of two separated tasks , i.e. , named entity recognition ( NER ) ) and relation extraction , but recent studies show that end - to - end ( joint ) modeling of entity and relation is important for high performance since relations interact closely with entity information .",Introduction,Introduction,relation-classification,0,2,0.0869565217391304,12,0.0530973451327433,2,0.0869565217391304,1,1
14,"For instance , to learn that Toefting and Bolton have an Organization - Affiliation ( ORG - AFF ) relation in the sentence Toefting transferred to Bolton , the entity information that Toefting and Bolton are Person and Organization entities is important .",Introduction,Introduction,relation-classification,0,3,0.1304347826086956,13,0.0575221238938053,3,0.1304347826086956,1,0
15,"Extraction of these entities is in turn encouraged by the presence of the context words transferred to , which indicate an employment relation .",Introduction,Introduction,relation-classification,0,4,0.1739130434782608,14,0.0619469026548672,4,0.1739130434782608,1,0
16,Previous joint models have employed feature - based structured learning .,Introduction,Introduction,relation-classification,0,5,0.217391304347826,15,0.0663716814159292,5,0.217391304347826,1,0
17,An alternative approach to this end - to - end relation extraction task is to employ automatic feature learning via neural network ( NN ) based models .,Introduction,Introduction,relation-classification,0,6,0.2608695652173913,16,0.0707964601769911,6,0.2608695652173913,1,0
18,There are two ways to represent relations between entities using neural networks : recurrent / recursive neural networks ( RNNs ) and convolutional neural networks ( CNNs ) .,Introduction,Introduction,relation-classification,0,7,0.3043478260869565,17,0.0752212389380531,7,0.3043478260869565,1,0
19,"Among these , RNNs can directly represent essential linguistic structures , i.e. , word sequences and constituent / dependency trees .",Introduction,Introduction,relation-classification,0,8,0.3478260869565217,18,0.079646017699115,8,0.3478260869565217,1,0
20,"Despite this representation ability , for relation classification tasks , the previously reported performance using long short - term memory ( LSTM ) based RNNs is worse than one using CNNs .",Introduction,Introduction,relation-classification,0,9,0.391304347826087,19,0.084070796460177,9,0.391304347826087,1,0
21,"These previous LSTM - based systems mostly include limited linguistic structures and neural architectures , and do not model entities and relations jointly .",Introduction,Introduction,relation-classification,0,10,0.4347826086956521,20,0.0884955752212389,10,0.4347826086956521,1,0
22,We are able to achieve improvements over state - of - the - art models via endto - end modeling of entities and relations based on richer LSTM - RNN architectures that incorporate complementary linguistic structures .,Introduction,Introduction,relation-classification,0,11,0.4782608695652174,21,0.0929203539823008,11,0.4782608695652174,1,0
23,Word sequence and tree structure are known to be complementary information for extracting relations .,Introduction,Introduction,relation-classification,0,12,0.5217391304347826,22,0.0973451327433628,12,0.5217391304347826,1,0
24,"For instance , dependencies between words are not enough to predict that source and U.S. have an ORG - AFF relation in the sentence "" This is ... "" , one U.S. source said , and the context word said is required for this prediction .",Introduction,Introduction,relation-classification,0,13,0.5652173913043478,23,0.1017699115044247,13,0.5652173913043478,1,0
25,"Many traditional , feature - based relation classification models extract features from both sequences and parse trees .",Introduction,Introduction,relation-classification,0,14,0.6086956521739131,24,0.1061946902654867,14,0.6086956521739131,1,0
26,"However , previous RNNbased models focus on only one of these linguistic structures .",Introduction,Introduction,relation-classification,0,15,0.6521739130434783,25,0.1106194690265486,15,0.6521739130434783,1,0
27,We present a novel end - to - end model to extract relations between entities on both word sequence and dependency tree structures .,Introduction,Introduction,relation-classification,0,16,0.6956521739130435,26,0.1150442477876106,16,0.6956521739130435,1,1
28,Our model allows joint modeling of entities and relations in a single model by using both bidirectional sequential ( left - to - right and right - to - left ) and bidirectional tree - structured ( bottom - up and top - down ) LSTM - RNNs .,Introduction,Introduction,relation-classification,0,17,0.7391304347826086,27,0.1194690265486725,17,0.7391304347826086,1,1
29,"Our model first detects entities and then extracts relations between the detected entities using a single incrementally - decoded NN structure , and the NN parameters are jointly updated using both entity and relation labels .",Introduction,Introduction,relation-classification,0,18,0.782608695652174,28,0.1238938053097345,18,0.782608695652174,1,1
30,"Unlike traditional incremental end - to - end relation extraction models , our model further incorporates two enhancements into training : entity pretraining , which pretrains the entity model , and scheduled sampling , which replaces ( unreliable ) predicted labels with gold labels in a certain probability .",Introduction,Introduction,relation-classification,0,19,0.8260869565217391,29,0.1283185840707964,19,0.8260869565217391,1,1
31,"These enhancements alleviate the problem of low - performance entity detection in early stages of training , as well as allow entity information to further help downstream relation classification .",Introduction,Introduction,relation-classification,0,20,0.8695652173913043,30,0.1327433628318584,20,0.8695652173913043,1,1
32,"On end - to - end relation extraction , we improve over the state - of - the - art feature - based model , with 12.1 % ( ACE2005 ) and 5.7 % ( ACE2004 ) relative error reductions in F1-score .",Introduction,Introduction,relation-classification,0,21,0.9130434782608696,31,0.1371681415929203,21,0.9130434782608696,1,0
33,"On nominal relation classification ( Sem Eval - 2010 Task 8 ) , our model compares favorably to the state - of - the - art CNNbased model in F1-score .",Introduction,Introduction,relation-classification,0,22,0.9565217391304348,32,0.1415929203539823,22,0.9565217391304348,1,0
34,"Finally , we also ablate and compare our various model components , which leads to some key findings ( both positive and negative ) about the contribution and effectiveness of different RNN structures , input dependency relation structures , different parsing models , external resources , and joint learning settings .",Introduction,Introduction,relation-classification,0,23,1.0,33,0.1460176991150442,23,1.0,1,0
35,Related Work,,,relation-classification,0,0,0.0,34,0.1504424778761062,0,0.0,1,0
36,"LSTM - RNNs have been widely used for sequential labeling , such as clause identification , phonetic labeling , and NER .",Related Work,Related Work,relation-classification,0,1,0.0769230769230769,35,0.1548672566371681,1,0.0769230769230769,0,0
37,"showed that building a conditional random field ( CRF ) layer on top of bidirectional LSTM - RNNs performs comparably to the state - of - the - art methods in the partof - speech ( POS ) tagging , chunking , and NER .",Related Work,Related Work,relation-classification,0,2,0.1538461538461538,36,0.1592920353982301,2,0.1538461538461538,0,0
38,"For relation classification , in addition to traditional feature / kernel - based approaches , several neural models have been proposed in the , including embedding - based models , , and RNN - based models .",Related Work,Related Work,relation-classification,0,3,0.2307692307692307,37,0.163716814159292,3,0.2307692307692307,0,0
39,"Recently , and showed that the shortest dependency paths between relation arguments , which were used in feature / kernel - based systems , are also useful in NN - based models .",Related Work,Related Work,relation-classification,0,4,0.3076923076923077,38,0.168141592920354,4,0.3076923076923077,0,0
40,"also showed that LSTM - RNNs are useful for relation classification , but the performance was worse than CNN - based models .",Related Work,Related Work,relation-classification,0,5,0.3846153846153846,39,0.1725663716814159,5,0.3846153846153846,0,0
41,"compared separate sequence - based and tree - structured LSTM - RNNs on relation classification , using basic RNN model structures .",Related Work,Related Work,relation-classification,0,6,0.4615384615384615,40,0.1769911504424778,6,0.4615384615384615,0,0
42,"Research on tree - structured LSTM - RNNs fixes the direction of information propagation from bottom to top , and also can not handle an arbitrary number of typed children as in a typed dependency tree .",Related Work,Related Work,relation-classification,0,7,0.5384615384615384,41,0.1814159292035398,7,0.5384615384615384,0,0
43,"Furthermore , no RNNbased relation classification model simultaneously uses word sequence and dependency tree information .",Related Work,Related Work,relation-classification,0,8,0.6153846153846154,42,0.1858407079646017,8,0.6153846153846154,0,0
44,"We propose several such novel model structures and training settings , investigating the simultaneous use of bidirectional sequential and bidirectional tree - structured LSTM - RNNs to jointly capture linear and dependency context for end - toend extraction of relations between entities .",Related Work,Related Work,relation-classification,0,9,0.6923076923076923,43,0.1902654867256637,9,0.6923076923076923,0,0
45,"As for end - to - end ( joint ) extraction of relations between entities , all existing models are featurebased systems ( and no NN - based model has been proposed ) .",Related Work,Related Work,relation-classification,0,10,0.7692307692307693,44,0.1946902654867256,10,0.7692307692307693,0,0
46,"Such models include structured prediction , integer linear programming , card - pyramid parsing ( Kate and Mooney , 2010 ) , and global probabilistic graphical models .",Related Work,Related Work,relation-classification,0,11,0.8461538461538461,45,0.1991150442477876,11,0.8461538461538461,0,0
47,"Among these , structured prediction methods are state - of - the - art on several corpora .",Related Work,Related Work,relation-classification,0,12,0.9230769230769232,46,0.2035398230088495,12,0.9230769230769232,0,0
48,"We present an improved , NN - based alternative for the end - to - end relation extraction .",Related Work,Related Work,relation-classification,0,13,1.0,47,0.2079646017699115,13,1.0,0,0
49,Model,,,relation-classification,0,0,0.0,48,0.2123893805309734,0,0.0,1,0
50,"We design our model with LSTM - RNNs that represent both word sequences and dependency tree structures , and perform end - to - end extraction of relations between entities on top of these RNNs.",Model,Model,relation-classification,0,1,0.0131578947368421,49,0.2168141592920354,1,0.1666666666666666,1,0
51,illustrates the overview of the model .,Model,Model,relation-classification,0,2,0.0263157894736842,50,0.2212389380530973,2,0.3333333333333333,1,0
52,"The model mainly consists of three representation layers : a word embeddings layer ( embedding layer ) , a word sequence based LSTM - RNN layer ( sequence layer ) , and finally a dependency subtree based LSTM - RNN layer ( dependency layer ) .",Model,Model,relation-classification,0,3,0.0394736842105263,51,0.2256637168141592,3,0.5,1,0
53,"During decoding , we build greedy , left - to - right entity detection on the sequence layer and realize relation classification on the dependency layers , where each subtree based LSTM - RNN corresponds to a relation candidate between two detected entities .",Model,Model,relation-classification,0,4,0.0526315789473684,52,0.2300884955752212,4,0.6666666666666666,1,0
54,"After decoding the entire model structure , we update the parameters simultaneously via backpropagation through time ( BPTT ) .",Model,Model,relation-classification,0,5,0.0657894736842105,53,0.2345132743362832,5,0.8333333333333334,1,0
55,"The dependency layers are stacked on the sequence layer , so the embedding and sequence layers are shared by both entity detection and relation classification , and the shared parameters are affected by both entity and relation labels .",Model,Model,relation-classification,0,6,0.0789473684210526,54,0.2389380530973451,6,1.0,1,0
56,Embedding Layer,Model,,relation-classification,0,7,0.0921052631578947,55,0.243362831858407,0,0.0,1,0
57,The embedding layer handles embedding representations .,Model,Embedding Layer,relation-classification,0,8,0.1052631578947368,56,0.247787610619469,1,0.5,1,0
58,"n w , n p , n d and n e - dimensional vectors v , v ( p ) , v and v ( e ) are embedded to words , part - of - speech ( POS ) tags , dependency types , and entity labels , respectively .",Model,Embedding Layer,relation-classification,0,9,0.1184210526315789,57,0.252212389380531,2,1.0,1,0
59,Sequence Layer,Model,,relation-classification,0,10,0.131578947368421,58,0.2566371681415929,0,0.0,1,0
60,The sequence layer represents words in a linear sequence using the representations from the embedding layer .,Model,Sequence Layer,relation-classification,0,11,0.1447368421052631,59,0.2610619469026549,1,0.0769230769230769,1,0
61,"This layer represents sentential context information and maintains entities , as shown in bottom - left part of .",Model,Sequence Layer,relation-classification,0,12,0.1578947368421052,60,0.2654867256637168,2,0.1538461538461538,1,0
62,We represent the word sequence in a sentence with bidirectional LSTM - RNNs .,Model,Sequence Layer,relation-classification,0,13,0.1710526315789473,61,0.2699115044247787,3,0.2307692307692307,1,0
63,"The LSTM unit at t-th word consists of a collection of n ls - dimensional vectors : an input gate it , a forget gate ft , an output gate o t , a memory cell ct , and a hidden state ht .",Model,Sequence Layer,relation-classification,0,14,0.1842105263157894,62,0.2743362831858407,4,0.3076923076923077,1,0
64,"The unit receives an n-dimensional input vector x t , the previous hidden state h t?1 , and the memory cell c t?1 , and calculates the new vectors using the following equations :",Model,Sequence Layer,relation-classification,0,15,0.1973684210526315,63,0.2787610619469026,5,0.3846153846153846,1,0
65,( 1 ),Model,Sequence Layer,relation-classification,0,16,0.2105263157894736,64,0.2831858407079646,6,0.4615384615384615,1,0
66,where ?,Model,Sequence Layer,relation-classification,0,17,0.2236842105263158,65,0.2876106194690265,7,0.5384615384615384,1,0
67,"denotes the logistic function , denotes element - wise multiplication , W and U are weight matrices , and bare bias vectors .",Model,Sequence Layer,relation-classification,0,18,0.2368421052631578,66,0.2920353982300885,8,0.6153846153846154,1,0
68,The LSTM unit at t-th word receives the concatenation of word and POS embeddings as its input vector :,Model,Sequence Layer,relation-classification,0,19,0.25,67,0.2964601769911504,9,0.6923076923076923,1,0
69,.,Model,Sequence Layer,relation-classification,0,20,0.2631578947368421,68,0.3008849557522124,10,0.7692307692307693,1,0
70,We also concatenate the hidden state vectors of the two directions ' LSTM units corresponding to each word ( denoted as ? ?,Model,Sequence Layer,relation-classification,0,21,0.2763157894736842,69,0.3053097345132743,11,0.8461538461538461,1,0
71,"ht and ? ? ht ) as its output vector , st = ? ? ht ; ? ?",Model,Sequence Layer,relation-classification,0,22,0.2894736842105263,70,0.3097345132743362,12,0.9230769230769232,1,0
72,"ht , and pass it to the subsequent layers .",Model,Sequence Layer,relation-classification,0,23,0.3026315789473684,71,0.3141592920353982,13,1.0,1,0
73,Entity Detection,Model,,relation-classification,0,24,0.3157894736842105,72,0.3185840707964602,0,0.0,1,0
74,We treat entity detection as a sequence labeling task .,Model,Entity Detection,relation-classification,0,25,0.3289473684210526,73,0.3230088495575221,1,0.1111111111111111,1,0
75,"We assign an entity tag to each word using a commonly used encoding scheme BILOU ( Begin , Inside , Last , Outside , Unit ) ( Ratinov and , where each entity tag represents the entity type and the position of a word in the entity .",Model,Entity Detection,relation-classification,0,26,0.3421052631578947,74,0.3274336283185841,2,0.2222222222222222,1,0
76,"For example , in , we assign B - PER and L - PER ( which denote the beginning and last words of a person entity type , respectively ) to each word in Sidney Yates to represent this phrase as a PER ( person ) entity type .",Model,Entity Detection,relation-classification,0,27,0.3552631578947368,75,0.331858407079646,3,0.3333333333333333,1,0
77,We perform entity detection on top of the sequence layer .,Model,Entity Detection,relation-classification,0,28,0.3684210526315789,76,0.336283185840708,4,0.4444444444444444,1,0
78,We employ a two - layered NN with an n he - dimensional hidden layer h ( e ) and a softmax output layer for entity detection .,Model,Entity Detection,relation-classification,0,29,0.3815789473684211,77,0.3407079646017699,5,0.5555555555555556,1,0
79,"Here , Ware weight matrices and bare bias vectors .",Model,Entity Detection,relation-classification,0,30,0.3947368421052631,78,0.3451327433628318,6,0.6666666666666666,1,0
80,"We assign entity labels to words in a greedy , left - to - right manner .",Model,Entity Detection,relation-classification,0,31,0.4078947368421052,79,0.3495575221238938,7,0.7777777777777778,1,0
81,"1 During this decoding , we use the predicted label of a word to predict the label of the next word so as to take label dependencies into account .",Model,Entity Detection,relation-classification,0,32,0.4210526315789473,80,0.3539823008849557,8,0.8888888888888888,1,0
82,The NN above receives the concatenation of its corresponding outputs in the sequence layer and the label embedding for its previous word ) .,Model,Entity Detection,relation-classification,0,33,0.4342105263157895,81,0.3584070796460177,9,1.0,1,0
83,Dependency Layer,Model,,relation-classification,0,34,0.4473684210526316,82,0.3628318584070796,0,0.0,1,0
84,"The dependency layer represents a relation between a pair of two target words ( corresponding to a relation candidate in relation classification ) in the dependency tree , and is in charge of relationspecific representations , as is shown in top - right part of .",Model,Dependency Layer,relation-classification,0,35,0.4605263157894737,83,0.3672566371681416,1,0.0526315789473684,1,0
85,"This layer mainly focuses on the shortest path between a pair of target words in the dependency tree ( i.e. , the path between the least common node and the two target words ) since these paths are shown to be effective in relation classification .",Model,Dependency Layer,relation-classification,0,36,0.4736842105263157,84,0.3716814159292035,2,0.1052631578947368,1,0
86,"For example , we show the shortest path between Yates and Chicago in the bottom of , and this path well captures the key phrase of their relation , i.e. , born in .",Model,Dependency Layer,relation-classification,0,37,0.4868421052631579,85,0.3761061946902654,3,0.1578947368421052,1,0
87,"We employ bidirectional tree - structured LSTM - RNNs ( i.e. , bottom - up and top - down ) to represent a relation candidate by capturing the dependency structure around the target word pair .",Model,Dependency Layer,relation-classification,0,38,0.5,86,0.3805309734513274,4,0.2105263157894736,1,0
88,This bidirectional structure propagates to each node not only the information from the leaves but also information from the root .,Model,Dependency Layer,relation-classification,0,39,0.5131578947368421,87,0.3849557522123893,5,0.2631578947368421,1,0
89,"This is especially important for relation classification , which makes use of argument nodes near the bottom of the tree , and our top - down LSTM - RNN sends information from the top of the tree to such near - leaf nodes ( unlike in standard bottom - up LSTM - RNNs ) .",Model,Dependency Layer,relation-classification,0,40,0.5263157894736842,88,0.3893805309734513,6,0.3157894736842105,1,0
90,2 Note that the two variants of tree - structured LSTM - RNNs by are notable to represent our target structures which have a variable number of typed children : the Child - Sum Tree - LSTM does not deal with types and the N - ary Tree assumes a fixed number of children .,Model,Dependency Layer,relation-classification,0,41,0.5394736842105263,89,0.3938053097345133,7,0.3684210526315789,1,0
91,We thus propose anew variant of tree - structured LSTM - RNN that shares weight matrices U s for same - type children and also allows variable number of children .,Model,Dependency Layer,relation-classification,0,42,0.5526315789473685,90,0.3982300884955752,8,0.4210526315789473,1,0
92,"For this variant , we calculate n lt - dimensional vectors in the LSTM unit at t-th node with C ( t ) children using following equations :",Model,Dependency Layer,relation-classification,0,43,0.5657894736842105,91,0.4026548672566372,9,0.4736842105263157,1,0
93,where m ( ) is a type mapping function .,Model,Dependency Layer,relation-classification,0,44,0.5789473684210527,92,0.4070796460176991,10,0.5263157894736842,1,0
94,"To investigate appropriate structures to represent relations between two target word pairs , we experiment with three structure options .",Model,Dependency Layer,relation-classification,0,45,0.5921052631578947,93,0.411504424778761,11,0.5789473684210527,1,0
95,"We primarily employ the shortest path structure ( SP - Tree ) , which captures the core dependency path between a target word pair and is widely used in relation classification models , e.g. , .",Model,Dependency Layer,relation-classification,0,46,0.6052631578947368,94,0.415929203539823,12,0.631578947368421,1,0
96,We also try two other dependency structures : SubTree and Full - Tree .,Model,Dependency Layer,relation-classification,0,47,0.618421052631579,95,0.4203539823008849,13,0.6842105263157895,1,0
97,SubTree is the subtree under the lowest common ancestor of the target word pair .,Model,Dependency Layer,relation-classification,0,48,0.631578947368421,96,0.4247787610619469,14,0.7368421052631579,1,0
98,This provides additional modifier information to the path and the word pair in SPTree .,Model,Dependency Layer,relation-classification,0,49,0.6447368421052632,97,0.4292035398230088,15,0.7894736842105263,1,0
99,FullTree is the full dependency tree .,Model,Dependency Layer,relation-classification,0,50,0.6578947368421053,98,0.4336283185840708,16,0.8421052631578947,1,0
100,This captures context from the entire sentence .,Model,Dependency Layer,relation-classification,0,51,0.6710526315789473,99,0.4380530973451327,17,0.8947368421052632,1,0
101,"While we use one node type for SPTree , we define two node types for SubTree and FullTree , i.e. , one for nodes on shortest paths and one for all other nodes .",Model,Dependency Layer,relation-classification,0,52,0.6842105263157895,100,0.4424778761061947,18,0.9473684210526316,1,0
102,We use the type mapping function m ( ) to distinguish these two nodes types .,Model,Dependency Layer,relation-classification,0,53,0.6973684210526315,101,0.4469026548672566,19,1.0,1,0
103,Stacking Sequence and Dependency Layers,Model,,relation-classification,0,54,0.7105263157894737,102,0.4513274336283185,0,0.0,1,0
104,We stack the dependency layers ( corresponding to relation candidates ) on top of the sequence layer to incorporate both word sequence and dependency tree structure information into the output .,Model,Stacking Sequence and Dependency Layers,relation-classification,0,55,0.7236842105263158,103,0.4557522123893805,1,0.3333333333333333,1,0
105,The dependency - layer LSTM unit at the t - th word receives as input,Model,Stacking Sequence and Dependency Layers,relation-classification,0,56,0.7368421052631579,104,0.4601769911504424,2,0.6666666666666666,1,0
106,"i.e. , the concatenation of its corresponding hidden state vectors st in the sequence layer , dependency type embedding v",Model,Stacking Sequence and Dependency Layers,relation-classification,0,57,0.75,105,0.4646017699115044,3,1.0,1,0
107,Relation Classification,Model,,relation-classification,0,58,0.7631578947368421,106,0.4690265486725664,0,0.0,1,0
108,"We incrementally build relation candidates using all possible combinations of the last words of detected entities , i.e. , words with L or U labels in the BILOU scheme , during decoding .",Model,Relation Classification,relation-classification,0,59,0.7763157894736842,107,0.4734513274336283,1,0.0555555555555555,1,0
109,"For instance , in , we build a relation candidate using Yates with an L - PER label and Chicago with an U - LOC label .",Model,Relation Classification,relation-classification,0,60,0.7894736842105263,108,0.4778761061946903,2,0.1111111111111111,1,0
110,"For each relation candidate , we realize the dependency layer d p ( described above ) corresponding to the path between the word pair pin the relation candidate , and the NN receives a relation candidate vector constructed from the output of the dependency tree layer , and predicts its relation label .",Model,Relation Classification,relation-classification,0,61,0.8026315789473685,109,0.4823008849557522,3,0.1666666666666666,1,0
111,We treat a pair as a negative relation when the detected entities are wrong or when the pair has no relation .,Model,Relation Classification,relation-classification,0,62,0.8157894736842105,110,0.4867256637168141,4,0.2222222222222222,1,0
112,"We represent relation labels by type and direction , except for negative relations that have no direction .",Model,Relation Classification,relation-classification,0,63,0.8289473684210527,111,0.4911504424778761,5,0.2777777777777778,1,0
113,"The relation candidate vector is constructed as the concatenation d p = [?h p A ; ?h p 1 ; ?h p 2 ] , where ?h p",Model,Relation Classification,relation-classification,0,64,0.8421052631578947,112,0.495575221238938,6,0.3333333333333333,1,0
114,A is the hidden state vector of the top LSTM,Model,Relation Classification,relation-classification,0,65,0.8552631578947368,113,0.5,7,0.3888888888888889,1,0
115,We use the dependency to the parent since the number of children varies .,Model,Relation Classification,relation-classification,0,66,0.868421052631579,114,0.504424778761062,8,0.4444444444444444,1,0
116,"Dependency types can also be incorporated into m ( ) , but this did not help in initial experiments .",Model,Relation Classification,relation-classification,0,67,0.881578947368421,115,0.5088495575221239,9,0.5,1,0
117,"unit in the bottom - up LSTM - RNN ( representing the lowest common ancestor of the target word pair p ) , and ?h p 1 , ?h p 2 are the hidden state vectors of the two LSTM units representing the first and second target words in the top - down LSTM - RNN .",Model,Relation Classification,relation-classification,0,68,0.8947368421052632,116,0.5132743362831859,10,0.5555555555555556,1,0
118,All the corresponding arrows are shown in .,Model,Relation Classification,relation-classification,0,69,0.9078947368421052,117,0.5176991150442478,11,0.6111111111111112,1,0
119,"Similarly to the entity detection , we employ a two - layered NN with an n hr -dimensional hidden layer h ( r ) and a softmax output layer ( with weight matrices W , bias vectors b ) .",Model,Relation Classification,relation-classification,0,70,0.9210526315789472,118,0.5221238938053098,12,0.6666666666666666,1,0
120,"We construct the input d p for relation classification from tree - structured LSTM - RNNs stacked on sequential LSTM - RNNs , so the contribution of sequence layer to the input is indirect .",Model,Relation Classification,relation-classification,0,71,0.9342105263157896,119,0.5265486725663717,13,0.7222222222222222,1,0
121,"Furthermore , our model uses words for representing entities , so it can not fully use the entity information .",Model,Relation Classification,relation-classification,0,72,0.9473684210526316,120,0.5309734513274337,14,0.7777777777777778,1,0
122,"To alleviate these problems , we directly concatenate the average of hidden state vectors for each entity from the sequence layer to the input d p to relation classification , i.e. , d p =",Model,Relation Classification,relation-classification,0,73,0.9605263157894736,121,0.5353982300884956,15,0.8333333333333334,1,0
123,", where I p 1 and I p 2 represent sets of word indices in the first and second entities .",Model,Relation Classification,relation-classification,0,74,0.9736842105263158,122,0.5398230088495575,16,0.8888888888888888,1,0
124,"Also , we assign two labels to each word pair in prediction since we consider both left - to - right and right - to - left directions .",Model,Relation Classification,relation-classification,0,75,0.986842105263158,123,0.5442477876106194,17,0.9444444444444444,1,0
125,"When the predicted labels are inconsistent , we select the positive and more confident label , similar to .",Model,Relation Classification,relation-classification,0,76,1.0,124,0.5486725663716814,18,1.0,1,0
126,Training,,,relation-classification,0,0,0.0,125,0.5530973451327433,0,0.0,1,0
127,"We update the model parameters including weights , biases , and embeddings by BPTT and Adam ( Kingma and Ba , 2015 ) with gradient clipping , parameter averaging , and L2-regularization ( we regularize weights W and U , not the bias terms b ) .",Training,Training,relation-classification,0,1,0.1428571428571428,126,0.5575221238938053,1,0.1428571428571428,1,0
128,We also apply dropout to the embedding layer and to the final hidden layers for entity detection and relation classification .,Training,Training,relation-classification,0,2,0.2857142857142857,127,0.5619469026548672,2,0.2857142857142857,1,0
129,"We employ two enhancements , scheduled sampling and entity pretraining , to alleviate the problem of unreliable prediction of entities in the early stage of training , and to encourage building positive relation instances from the detected entities .",Training,Training,relation-classification,0,3,0.4285714285714285,128,0.5663716814159292,3,0.4285714285714285,1,0
130,"In scheduled sampling , we use gold labels as prediction in the probability of i that depends on the number of epochs i during training if the gold labels are legal .",Training,Training,relation-classification,0,4,0.5714285714285714,129,0.5707964601769911,4,0.5714285714285714,1,0
131,"As for i , we choose the inverse sigmoid decay i = k / ( k + exp ( i / k ) ) , where k( ?",Training,Training,relation-classification,0,5,0.7142857142857143,130,0.5752212389380531,5,0.7142857142857143,1,0
132,1 ) is a hyper - parameter that adjusts how often we use the gold labels as prediction .,Training,Training,relation-classification,0,6,0.8571428571428571,131,0.5796460176991151,6,0.8571428571428571,1,0
133,"Entity pretraining is inspired by , and we pretrain the entity detection model using the training data before training the entire model parameters .",Training,Training,relation-classification,0,7,1.0,132,0.584070796460177,7,1.0,1,0
134,Results and Discussion,,,relation-classification,0,0,0.0,133,0.588495575221239,0,0.0,1,0
135,Data and Task Settings,Results and Discussion,,relation-classification,0,1,0.0555555555555555,134,0.5929203539823009,0,0.0,1,0
136,"We evaluate on three datasets : ACE05 and ACE04 for end - to - end relation extraction , and SemEval - 2010 Task 8 for relation classification .",Results and Discussion,Data and Task Settings,relation-classification,0,2,0.1111111111111111,135,0.5973451327433629,1,0.0588235294117647,1,0
137,"We use the first two datasets as our primary target , and use the last one to thoroughly analyze and ablate the relation classification part of our model .",Results and Discussion,Data and Task Settings,relation-classification,0,3,0.1666666666666666,136,0.6017699115044248,2,0.1176470588235294,1,0
138,ACE05 defines 7 coarse - grained entity types and 6 coarse - grained relation types between entities .,Results and Discussion,Data and Task Settings,relation-classification,0,4,0.2222222222222222,137,0.6061946902654868,3,0.1764705882352941,1,0
139,"We use the same data splits , preprocessing , and task settings as .",Results and Discussion,Data and Task Settings,relation-classification,0,5,0.2777777777777778,138,0.6106194690265486,4,0.2352941176470588,1,0
140,We report the primary micro F1 -scores as well as micro precision and recall on both entity and relation extraction to better explain model performance .,Results and Discussion,Data and Task Settings,relation-classification,0,6,0.3333333333333333,139,0.6150442477876106,5,0.2941176470588235,1,0
141,We treat an entity as correct when it s type and the region of its head are correct .,Results and Discussion,Data and Task Settings,relation-classification,0,7,0.3888888888888889,140,0.6194690265486725,6,0.3529411764705882,1,0
142,We treat a relation as correct when it s type and argument entities are correct ; we thus treat all non-negative relations on wrong entities as false positives .,Results and Discussion,Data and Task Settings,relation-classification,0,8,0.4444444444444444,141,0.6238938053097345,7,0.4117647058823529,1,0
143,"ACE04 defines the same 7 coarse - grained entity types as ACE05 , but defines 7 coarse - grained relation types .",Results and Discussion,Data and Task Settings,relation-classification,0,9,0.5,142,0.6283185840707964,8,0.4705882352941176,1,0
144,"We follow the cross-validation setting of Chan and and , and the preprocessing and evaluation metrics of ACE05 .",Results and Discussion,Data and Task Settings,relation-classification,0,10,0.5555555555555556,143,0.6327433628318584,9,0.5294117647058824,1,0
145,SemEval-2010,Results and Discussion,,relation-classification,0,11,0.6111111111111112,144,0.6371681415929203,10,0.5882352941176471,1,0
146,Task 8 defines 9 relation types between nominals and a tenth type,Results and Discussion,SemEval-2010,relation-classification,0,12,0.6666666666666666,145,0.6415929203539823,11,0.6470588235294118,1,0
147,Other when two nouns have none of these relations .,Results and Discussion,SemEval-2010,relation-classification,0,13,0.7222222222222222,146,0.6460176991150443,12,0.7058823529411765,1,0
148,"We treat this Other type as a negative relation type , and no direction is considered .",Results and Discussion,SemEval-2010,relation-classification,0,14,0.7777777777777778,147,0.6504424778761062,13,0.7647058823529411,1,0
149,"The dataset consists of 8,000 training and 2,717 test sentences , and each sentence is annotated with a relation between two given nominals .",Results and Discussion,SemEval-2010,relation-classification,0,15,0.8333333333333334,148,0.6548672566371682,14,0.8235294117647058,1,0
150,We randomly selected 800 sentences from the training set as our development set .,Results and Discussion,SemEval-2010,relation-classification,0,16,0.8888888888888888,149,0.6592920353982301,15,0.8823529411764706,1,0
151,"We followed the official task setting , and report the official macro -averaged F1 - score ( Macro - F1 ) on the 9 relation types .",Results and Discussion,SemEval-2010,relation-classification,0,17,0.9444444444444444,150,0.6637168141592921,16,0.9411764705882352,1,0
152,"For more details of the data and task settings , please refer to the supplementary material .",Results and Discussion,SemEval-2010,relation-classification,0,18,1.0,151,0.668141592920354,17,1.0,1,0
153,Experimental Settings,,,relation-classification,0,0,0.0,152,0.672566371681416,0,0.0,1,0
154,We implemented our model using the cnn library .,Experimental Settings,Experimental Settings,relation-classification,0,1,0.0238095238095238,153,0.6769911504424779,1,0.0238095238095238,1,1
155,"We parsed the texts using the Stanford neural dependency parser 7 ( Chen and Manning , 2014 ) with the original Stanford Dependencies .",Experimental Settings,Experimental Settings,relation-classification,0,2,0.0476190476190476,154,0.6814159292035398,2,0.0476190476190476,1,1
156,"Based on preliminary tuning , we fixed embedding dimensions n w to 200 , n p , n d , n e to 25 , and dimensions of intermediate layers ( n ls , n lt of LSTM - RNNs and n he , n hr of hidden layers ) to 100 .",Experimental Settings,Experimental Settings,relation-classification,0,3,0.0714285714285714,155,0.6858407079646017,3,0.0714285714285714,1,1
157,We initialized word vectors via word2 vec trained on Wikipedia 8 and randomly initialized all other parameters .,Experimental Settings,Experimental Settings,relation-classification,0,4,0.0952380952380952,156,0.6902654867256637,4,0.0952380952380952,1,1
158,We tuned hyper - parameters using development sets for ACE05 and SemEval - 2010 Task 8 to achieve high primary ( Micro - and Macro - ) F1-scores .,Experimental Settings,Experimental Settings,relation-classification,0,5,0.119047619047619,157,0.6946902654867256,5,0.119047619047619,1,0
159,"9 For ACE04 , we directly employed the best parameters for ACE05 .",Experimental Settings,Experimental Settings,relation-classification,0,6,0.1428571428571428,158,0.6991150442477876,6,0.1428571428571428,1,0
160,The hyperparameter settings are shown in the supplementary material .,Experimental Settings,Experimental Settings,relation-classification,0,7,0.1666666666666666,159,0.7035398230088495,7,0.1666666666666666,1,0
161,For SemEval-2010,Experimental Settings,,relation-classification,0,8,0.1904761904761904,160,0.7079646017699115,8,0.1904761904761904,1,0
162,"Task 8 , we also omitted the entity detection and label embeddings since only target nominals are annotated and the task defines no entity types .",Experimental Settings,For SemEval-2010,relation-classification,0,9,0.2142857142857142,161,0.7123893805309734,9,0.2142857142857142,1,0
163,Our statistical significance results are based on the Approximate Randomization ( AR ) test .,Experimental Settings,For SemEval-2010,relation-classification,0,10,0.238095238095238,162,0.7168141592920354,10,0.238095238095238,1,0
164,End - to - end Relation Extraction Results,Experimental Settings,For SemEval-2010,relation-classification,0,11,0.2619047619047619,163,0.7212389380530974,11,0.2619047619047619,1,0
165,"Table 1 compares our model with the state - of - theart feature - based model of on final test sets , and shows that our model performs better than the state - of - the - art model .",Experimental Settings,For SemEval-2010,relation-classification,0,12,0.2857142857142857,164,0.7256637168141593,12,0.2857142857142857,1,1
166,"To analyze the contributions and effects of the various components of our end - to - end relation extraction model , we perform ablation tests on the ACE05 development set ( ) .",Experimental Settings,For SemEval-2010,relation-classification,0,13,0.3095238095238095,165,0.7300884955752213,13,0.3095238095238095,1,1
167,"The performance slightly degraded without scheduled sampling , and the performance significantly degraded when we removed entity pretraining or removed both ( p < 0.05 ) .",Experimental Settings,For SemEval-2010,relation-classification,0,14,0.3333333333333333,166,0.7345132743362832,14,0.3333333333333333,1,1
168,"This is reasonable because the model can only create relation instances when both of the entities are found and , without these enhancements , it may get too late to find some relations .",Experimental Settings,For SemEval-2010,relation-classification,0,15,0.3571428571428571,167,0.7389380530973452,15,0.3571428571428571,1,0
169,Removing label embeddings did not affect 6 https://github.com/clab/cnn,Experimental Settings,For SemEval-2010,relation-classification,0,16,0.3809523809523809,168,0.7433628318584071,16,0.3809523809523809,1,0
170,7 http://nlp.stanford.edu/software/,Experimental Settings,For SemEval-2010,relation-classification,0,17,0.4047619047619047,169,0.7477876106194691,17,0.4047619047619047,1,0
171,stanford-corenlp-full-2015-04-20.zip,Experimental Settings,For SemEval-2010,relation-classification,0,18,0.4285714285714285,170,0.7522123893805309,18,0.4285714285714285,1,0
172,8 https://dumps.wikimedia.org/enwiki/ 20150901/,Experimental Settings,For SemEval-2010,relation-classification,0,19,0.4523809523809524,171,0.7566371681415929,19,0.4523809523809524,1,0
173,9,Experimental Settings,For SemEval-2010,relation-classification,0,20,0.4761904761904761,172,0.7610619469026548,20,0.4761904761904761,1,0
174,"We did not tune the precision - recall trade - offs , but doing so can specifically improve precision further .",Experimental Settings,For SemEval-2010,relation-classification,0,21,0.5,173,0.7654867256637168,21,0.5,1,0
175,"Other work on ACE is not comparable or performs worse than the model by the entity detection performance , but this degraded the recall in relation classification .",Experimental Settings,For SemEval-2010,relation-classification,0,22,0.5238095238095238,174,0.7699115044247787,22,0.5238095238095238,1,0
176,This indicates that entity label information is helpful in detecting relations .,Experimental Settings,For SemEval-2010,relation-classification,0,23,0.5476190476190477,175,0.7743362831858407,23,0.5476190476190477,1,0
177,"We also show the performance without sharing parameters , i.e. , embedding and sequence layers , for detecting entities and relations ( ? Shared parameters ) ; we first train the entity detection model , detect entities with the model , and build a separate relation extraction model using the detected entities , i.e. , without entity detection .",Experimental Settings,For SemEval-2010,relation-classification,0,24,0.5714285714285714,176,0.7787610619469026,24,0.5714285714285714,1,0
178,This setting can be regarded as a pipeline model since two separate models are trained sequentially .,Experimental Settings,For SemEval-2010,relation-classification,0,25,0.5952380952380952,177,0.7831858407079646,25,0.5952380952380952,1,0
179,"Without the shared parameters , both the performance in entity detection and relation classification drops slightly , although the differences are not significant .",Experimental Settings,For SemEval-2010,relation-classification,0,26,0.6190476190476191,178,0.7876106194690266,26,0.6190476190476191,1,0
180,"When we removed all the enhancements , i.e. , scheduled sampling , entity pretraining , label embedding , and shared parameters , the performance is significantly worse than SP - Tree ( p < 0.01 ) , showing that these enhancements provide complementary benefits to end - to - end relation extraction .",Experimental Settings,For SemEval-2010,relation-classification,0,27,0.6428571428571429,179,0.7920353982300885,27,0.6428571428571429,1,1
181,"Next , we show the performance with different LSTM - RNN structures in .",Experimental Settings,For SemEval-2010,relation-classification,0,28,0.6666666666666666,180,0.7964601769911505,28,0.6666666666666666,1,0
182,"We first compare the three input dependency structures ( SPTree , SubTree , FullTree ) for tree - structured LSTM - RNNs .",Experimental Settings,For SemEval-2010,relation-classification,0,29,0.6904761904761905,181,0.8008849557522124,29,0.6904761904761905,1,0
183,"Performances on these three structures are almost same when we distinguish the nodes in the shortest paths from other nodes , but when we do not distinguish them ( - SP ) , the information outside of the shortest path , i.e. , FullTree ( - SP ) , significantly hurts performance ( p < 0.05 ) .",Experimental Settings,For SemEval-2010,relation-classification,0,30,0.7142857142857143,182,0.8053097345132744,30,0.7142857142857143,1,0
184,We then compare our tree - structured LSTM - RNN ( SPTree ) with the Child - Sum treestructured LSTM - RNN on the shortest path of .,Experimental Settings,For SemEval-2010,relation-classification,0,31,0.7380952380952381,183,0.8097345132743363,31,0.7380952380952381,1,0
185,"Child - Sum performs worse than our SPTree model , but not with as big of a decrease as above .",Experimental Settings,For SemEval-2010,relation-classification,0,32,0.7619047619047619,184,0.8141592920353983,32,0.7619047619047619,1,0
186,This maybe because the difference in the models appears only on nodes that have multiple children and all the nodes except for the least common node have one child .,Experimental Settings,For SemEval-2010,relation-classification,0,33,0.7857142857142857,185,0.8185840707964602,33,0.7857142857142857,1,0
187,We finally show results with two counterparts of sequence - based LSTM - RNNs using the shortest path ( last two rows in ) .,Experimental Settings,For SemEval-2010,relation-classification,0,34,0.8095238095238095,186,0.8230088495575221,34,0.8095238095238095,1,0
188,SPSeq is a bidirectional LSTM - RNN on the shortest path .,Experimental Settings,For SemEval-2010,relation-classification,0,35,0.8333333333333334,187,0.827433628318584,35,0.8333333333333334,1,0
189,The LSTM unit receives input from the sequence layer concatenated with embeddings for the surrounding dependency types and directions .,Experimental Settings,For SemEval-2010,relation-classification,0,36,0.8571428571428571,188,0.831858407079646,36,0.8571428571428571,1,0
190,We concatenate the outputs of the two RNNs for the relation candidate .,Experimental Settings,For SemEval-2010,relation-classification,0,37,0.8809523809523809,189,0.8362831858407079,37,0.8809523809523809,1,0
191,SPX u is our adaptation of the shortest path LSTM - RNN proposed by to match our sequence - layer based model .,Experimental Settings,For SemEval-2010,relation-classification,0,38,0.9047619047619048,190,0.8407079646017699,38,0.9047619047619048,1,0
192,11 This has two LSTM - RNNs for the left and right subpaths of the shortest path .,Experimental Settings,For SemEval-2010,relation-classification,0,39,0.9285714285714286,191,0.8451327433628318,39,0.9285714285714286,1,0
193,"We first calculate the max pooling of the LSTM units for each of these two RNNs , and then concatenate the outputs of the pooling for the relation candidate .",Experimental Settings,For SemEval-2010,relation-classification,0,40,0.9523809523809524,192,0.8495575221238938,40,0.9523809523809524,1,0
194,The comparison with these sequence - based LSTM - RNNs indicates that a tree - structured LSTM - RNN is comparable to sequence - based ones in representing shortest paths .,Experimental Settings,For SemEval-2010,relation-classification,0,41,0.9761904761904762,193,0.8539823008849557,41,0.9761904761904762,1,0
195,"Overall , the performance comparison of the LSTM - RNN structures in show that for end - to - end relation extraction , selecting the appropriate tree structure representation of the input ( i.e. , the shortest path ) is more important than the choice of the LSTM - RNN structure on that input ( i.e. , sequential versus tree - based ) .",Experimental Settings,For SemEval-2010,relation-classification,0,42,1.0,194,0.8584070796460177,42,1.0,1,0
196,Relation Classification Analysis Results,,,relation-classification,0,0,0.0,195,0.8628318584070797,0,0.0,1,0
197,"To thoroughly analyze the relation classification part alone , e.g. , comparing different LSTM structures , architecture components such as hidden layers and input information , and classification task settings , we use the SemEval - 2010 Task 8 .",Relation Classification Analysis Results,Relation Classification Analysis Results,relation-classification,0,1,0.0434782608695652,196,0.8672566371681416,1,0.5,1,0
198,"This dataset , often used to evaluate NN models for relation classification , annotates only relation - related nominals ( unlike ACE datasets ) , so we can focus cleanly on the relation classification part .",Relation Classification Analysis Results,Relation Classification Analysis Results,relation-classification,0,2,0.0869565217391304,197,0.8716814159292036,2,1.0,1,0
199,Settings,Relation Classification Analysis Results,,relation-classification,0,3,0.1304347826086956,198,0.8761061946902655,0,0.0,1,0
200,Macro - F1 No External Knowledge Resources Our Model ( SPTree ) 0.844 dos 0.841 0.840 + Word,Relation Classification Analysis Results,Settings,relation-classification,0,4,0.1739130434782608,199,0.8805309734513275,1,0.05,1,0
201,Net Our Model ( SPTree + WordNet ) 0.855 0.856 0.837 We first report official test set results in Table 4 .,Relation Classification Analysis Results,Settings,relation-classification,0,5,0.217391304347826,200,0.8849557522123894,2,0.1,1,0
202,"Our novel LSTM - RNN model is comparable to both the state - of - the - art CNN - based models on this task with or without external sources , i.e. , WordNet , unlike the previous best LSTM - RNN model .",Relation Classification Analysis Results,Settings,relation-classification,0,6,0.2608695652173913,201,0.8893805309734514,3,0.15,1,0
203,"Next , we compare different LSTM - RNN structures in .",Relation Classification Analysis Results,Settings,relation-classification,0,7,0.3043478260869565,202,0.8938053097345132,4,0.2,1,0
204,"As for the three input dependency structures ( SPTree , SubTree , FullTree ) , Full",Relation Classification Analysis Results,Settings,relation-classification,0,8,0.3478260869565217,203,0.8982300884955752,5,0.25,1,0
205,"Tree performs significantly worse than other structures regardless of whether or not we distinguish the nodes in the shortest paths from the other nodes , which hints that the information outside of the shortest path significantly hurts the performance ( p < 0.05 ) .",Relation Classification Analysis Results,Settings,relation-classification,0,9,0.391304347826087,204,0.9026548672566372,6,0.3,1,0
206,We also compare our treestructured LSTM - RNN ( SPTree ) with sequencebased LSTM - RNNs ( SPSeq and SPXu ) and treestructured LSTM - RNNs ( Child - Sum ) .,Relation Classification Analysis Results,Settings,relation-classification,0,10,0.4347826086956521,205,0.9070796460176992,7,0.35,1,0
207,"All these LSTM - RNNs perform slightly worse than our SP - 12 When incorporating WordNet information into our model , we prepared embeddings for WordNet hypernyms extracted by SuperSenseTagger and concatenated the embeddings to the input vector ( the concatenation of word and POS embeddings ) of the sequence LSTM .",Relation Classification Analysis Results,Settings,relation-classification,0,11,0.4782608695652174,206,0.911504424778761,8,0.4,1,0
208,We tuned the dimension of the WordNet embeddings and set it to 15 using the development dataset .,Relation Classification Analysis Results,Settings,relation-classification,0,12,0.5217391304347826,207,0.915929203539823,9,0.45,1,0
209,"0.848 produces different results on FullTree as compared to the results on ACE05 in , the trend still holds that selecting the appropriate tree structure representation of the input is more important than the choice of the LSTM - RNN structure on that input .",Relation Classification Analysis Results,Settings,relation-classification,0,13,0.5652173913043478,208,0.9203539823008848,10,0.5,1,0
210,"Finally , summarizes the contribution of several model components and training settings on SemEval relation classification .",Relation Classification Analysis Results,Settings,relation-classification,0,14,0.6086956521739131,209,0.9247787610619468,11,0.55,1,0
211,"We first remove the hidden layer by directly connecting the LSTM - RNN layers to the softmax layers , and found that this slightly degraded performance , but the difference was small .",Relation Classification Analysis Results,Settings,relation-classification,0,15,0.6521739130434783,210,0.9292035398230089,12,0.6,1,0
212,We then skip the sequence layer and directly use the word and POS embeddings for the dependency layer .,Relation Classification Analysis Results,Settings,relation-classification,0,16,0.6956521739130435,211,0.9336283185840708,13,0.65,1,0
213,"Removing the sequence layer 13 or entity - related information from the sequence layer ( ? Pair ) slightly degraded performance , and , on removing both , the performance dropped significantly ( p < 0.05 ) .",Relation Classification Analysis Results,Settings,relation-classification,0,17,0.7391304347826086,212,0.9380530973451328,14,0.7,1,0
214,This indicates that the sequence layer is necessary but the last words of nominals are almost enough for expressing the relations in this task .,Relation Classification Analysis Results,Settings,relation-classification,0,18,0.782608695652174,213,0.9424778761061948,15,0.75,1,0
215,"When we replace the Stanford neural dependency parser with the Stanford lexicalized PCFG parser ( Stanford PCFG ) , the performance slightly dropped , but the difference was small .",Relation Classification Analysis Results,Settings,relation-classification,0,19,0.8260869565217391,214,0.9469026548672568,16,0.8,1,0
216,This indicates that the selection of parsing models is not critical .,Relation Classification Analysis Results,Settings,relation-classification,0,20,0.8695652173913043,215,0.9513274336283186,17,0.85,1,0
217,"We also included WordNet , and this slightly improved the performance ( + WordNet ) , but the difference was small .",Relation Classification Analysis Results,Settings,relation-classification,0,21,0.9130434782608696,216,0.9557522123893806,18,0.9,1,0
218,"Lastly , for the generation of relation candidates , generating only leftto - right candidates slightly degraded the perfor- mance , but the difference was small and hence the creation of right - to - left candidates was not critical .",Relation Classification Analysis Results,Settings,relation-classification,0,22,0.9565217391304348,217,0.9601769911504424,19,0.95,1,0
219,"Treating the inverse relation candidate as a negative instance ( Negative sampling ) also performed comparably to other generation methods in our model , which showed a significance improvement over generating only left - to - right candidates ) .",Relation Classification Analysis Results,Settings,relation-classification,0,23,1.0,218,0.9646017699115044,20,1.0,1,0
220,Conclusion,,,relation-classification,0,0,0.0,219,0.9690265486725664,0,0.0,1,0
221,We presented a novel end - to - end relation extraction model that represents both word sequence and dependency tree structures by using bidirectional sequential and bidirectional tree - structured LSTM - RNNs .,Conclusion,Conclusion,relation-classification,0,1,0.1666666666666666,220,0.9734513274336284,1,0.1666666666666666,0,0
222,"This allowed us to represent both entities and relations in a single model , achieving gains over the state - of - the - art , feature - based system on end - to - end relation extraction ( ACE04 and ACE05 ) , and showing favorably comparable performance to recent state - of - the - art CNNbased models on nominal relation classification ( Sem Eval - 2010 Task 8 ) .",Conclusion,Conclusion,relation-classification,0,2,0.3333333333333333,221,0.9778761061946902,2,0.3333333333333333,0,0
223,Our evaluation and ablation led to three key findings .,Conclusion,Conclusion,relation-classification,0,3,0.5,222,0.9823008849557522,3,0.5,0,0
224,"First , the use of both word sequence and dependency tree structures is effective .",Conclusion,Conclusion,relation-classification,0,4,0.6666666666666666,223,0.986725663716814,4,0.6666666666666666,0,0
225,"Second , training with the shared parameters improves relation extraction accuracy , especially when employed with entity pretraining , scheduled sampling , and label embeddings .",Conclusion,Conclusion,relation-classification,0,5,0.8333333333333334,224,0.991150442477876,5,0.8333333333333334,0,0
226,"Finally , the shortest path , which has been widely used in relation classification , is also appropriate for representing tree structures in neural LSTM models .",Conclusion,Conclusion,relation-classification,0,6,1.0,225,0.995575221238938,6,1.0,0,0
1,title,,,relation-classification,1,0,0.0,0,0.0,0,0.0,1,0
2,Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme,title,title,relation-classification,1,1,0.0,1,0.0040650406504065,1,0.0,1,1
3,abstract,,,relation-classification,1,0,0.0,2,0.008130081300813,0,0.0,1,0
4,Joint extraction of entities and relations is an important task in information extraction .,abstract,abstract,relation-classification,1,1,0.2,3,0.0121951219512195,1,0.2,1,0
5,"To tackle this problem , we firstly propose a novel tagging scheme that can convert the joint extraction task to a tagging problem .",abstract,abstract,relation-classification,1,2,0.4,4,0.016260162601626,2,0.4,1,0
6,"Then , based on our tagging scheme , we study different end - toend models to extract entities and their relations directly , without identifying entities and relations separately .",abstract,abstract,relation-classification,1,3,0.6,5,0.0203252032520325,3,0.6,1,0
7,We conduct experiments on a public dataset produced by distant supervision method and the experimental results show that the tagging based methods are better than most of the existing pipelined and joint learning methods .,abstract,abstract,relation-classification,1,4,0.8,6,0.024390243902439,4,0.8,1,0
8,"What 's more , the end - to - end model proposed in this paper , achieves the best results on the public dataset .",abstract,abstract,relation-classification,1,5,1.0,7,0.0284552845528455,5,1.0,1,0
9,Introduction,,,relation-classification,1,0,0.0,8,0.032520325203252,0,0.0,1,0
10,"Joint extraction of entities and relations is to detect entity mentions and recognize their semantic relations simultaneously from unstructured text , as shows .",Introduction,Introduction,relation-classification,1,1,0.025,9,0.0365853658536585,1,0.025,1,1
11,"Different from open information extraction ( Open IE ) ) whose relation words are extracted from the given sentence , in this task , relation words are extracted from a predefined relation set which may not appear in the given sentence .",Introduction,Introduction,relation-classification,1,2,0.05,10,0.040650406504065,2,0.05,1,0
12,It is an important issue in knowledge extraction and automatic construction of knowledge base .,Introduction,Introduction,relation-classification,1,3,0.075,11,0.0447154471544715,3,0.075,1,0
13,"Traditional methods handle this task in a pipelined manner , i.e. , extracting the entities first and then recognizing their relations .",Introduction,Introduction,relation-classification,1,4,0.1,12,0.048780487804878,4,0.1,1,0
14,"This separated framework makes the task easy to deal with , and each component can be more flexible .",Introduction,Introduction,relation-classification,1,5,0.125,13,0.0528455284552845,5,0.125,1,0
15,But it neglects the relevance between these two sub - tasks and each subtask is an independent model .,Introduction,Introduction,relation-classification,1,6,0.15,14,0.056910569105691,6,0.15,1,0
16,The results of entity recognition may affect the performance of relation classification and lead to erroneous delivery .,Introduction,Introduction,relation-classification,1,7,0.175,15,0.0609756097560975,7,0.175,1,0
17,"Different from the pipelined methods , joint learning framework is to extract entities together with relations using a single model .",Introduction,Introduction,relation-classification,1,8,0.2,16,0.065040650406504,8,0.2,1,1
18,"It can effectively integrate the information of entities and relations , and it has been shown to achieve better results in this task .",Introduction,Introduction,relation-classification,1,9,0.225,17,0.0691056910569105,9,0.225,1,0
19,"However , most existing joint methods are feature - based structured systems .",Introduction,Introduction,relation-classification,1,10,0.25,18,0.073170731707317,10,0.25,1,0
20,"They need complicated feature engineering and heavily rely on the other NLP toolkits , which might also lead to error propagation .",Introduction,Introduction,relation-classification,1,11,0.275,19,0.0772357723577235,11,0.275,1,0
21,"In order to reduce the manual work in feature extraction , recently , presents a neural network - based method for the end - to - end entities and relations extraction .",Introduction,Introduction,relation-classification,1,12,0.3,20,0.08130081300813,12,0.3,1,0
22,"Although the joint models can represent both entities and relations with shared parameters in a single model , they also extract the entities and relations separately and produce redundant information .",Introduction,Introduction,relation-classification,1,13,0.325,21,0.0853658536585365,13,0.325,1,0
23,"For instance , the sentence in contains three entities : "" United States "" , "" Trump "" and "" Apple Inc "" .",Introduction,Introduction,relation-classification,1,14,0.35,22,0.089430894308943,14,0.35,1,0
24,"But only "" United States "" and "" Trump "" hold a fix relation "" Country - President "" .",Introduction,Introduction,relation-classification,1,15,0.375,23,0.0934959349593495,15,0.375,1,0
25,"Entity "" Apple Inc "" has no obvious relationship with the other entities in this sentence .",Introduction,Introduction,relation-classification,1,16,0.4,24,0.0975609756097561,16,0.4,1,0
26,"Hence , the extracted result from this sentence is { United States e 1 , Country - President r , Trump e 2 } , which called triplet here .",Introduction,Introduction,relation-classification,1,17,0.425,25,0.1016260162601626,17,0.425,1,0
27,"In this paper , we focus on the extraction of triplets that are composed of two entities and one relation between these two entities .",Introduction,Introduction,relation-classification,1,18,0.45,26,0.1056910569105691,18,0.45,1,0
28,"Therefore , we can model the triplets directly , rather than extracting the entities and relations separately .",Introduction,Introduction,relation-classification,1,19,0.475,27,0.1097560975609756,19,0.475,1,0
29,"Based on the motivations , we propose a tagging scheme accompanied with the end - to - end model to settle this problem .",Introduction,Introduction,relation-classification,1,20,0.5,28,0.1138211382113821,20,0.5,1,0
30,We design a kind of novel tags which contain the information of entities and the relationships they hold .,Introduction,Introduction,relation-classification,1,21,0.525,29,0.1178861788617886,21,0.525,1,0
31,"Based on this tagging scheme , the joint extraction of entities and relations can be transformed into a tagging problem .",Introduction,Introduction,relation-classification,1,22,0.55,30,0.1219512195121951,22,0.55,1,0
32,"In this way , we can also easily use neural networks to model the task without complicated feature engineering .",Introduction,Introduction,relation-classification,1,23,0.575,31,0.1260162601626016,23,0.575,1,0
33,"Recently , end - to - end models based on LSTM have been successfully applied to various tagging tasks :",Introduction,Introduction,relation-classification,1,24,0.6,32,0.1300813008130081,24,0.6,1,0
34,"Named Entity Recognition , CCG",Introduction,Introduction,relation-classification,1,25,0.625,33,0.1341463414634146,25,0.625,1,0
35,"Supertagging , Chunking et al .",Introduction,Introduction,relation-classification,1,26,0.65,34,0.1382113821138211,26,0.65,1,0
36,"LSTM is capable of learning long - term dependencies , which is beneficial to sequence modeling tasks .",Introduction,Introduction,relation-classification,1,27,0.675,35,0.1422764227642276,27,0.675,1,0
37,"Therefore , based on our tagging scheme , we investigate different kinds of LSTM - based end - to - end models to jointly extract the entities and relations .",Introduction,Introduction,relation-classification,1,28,0.7,36,0.1463414634146341,28,0.7,1,0
38,We also modify the decoding method by adding a biased loss to make it more suitable for our special tags .,Introduction,Introduction,relation-classification,1,29,0.725,37,0.1504065040650406,29,0.725,1,0
39,The method we proposed is a supervised learning algorithm .,Introduction,Introduction,relation-classification,1,30,0.75,38,0.1544715447154471,30,0.75,1,0
40,"In reality , however , the process of manually labeling a training set with a large number of entity and relation is too expensive and error-prone .",Introduction,Introduction,relation-classification,1,31,0.775,39,0.1585365853658536,31,0.775,1,0
41,"Therefore , we conduct experiments on a public dataset 1 which is produced by distant supervision method to validate our approach .",Introduction,Introduction,relation-classification,1,32,0.8,40,0.1626016260162601,32,0.8,1,0
42,The experimental results show that our tagging scheme is effective in this task .,Introduction,Introduction,relation-classification,1,33,0.825,41,0.1666666666666666,33,0.825,1,0
43,"In addition , our end - to - end model can achieve the best results on the public dataset .",Introduction,Introduction,relation-classification,1,34,0.85,42,0.1707317073170731,34,0.85,1,0
44,"The major contributions of this paper are : ( 1 ) A novel tagging scheme is proposed to jointly extract entities and relations , which can easily transform the extraction problem into a tagging task .",Introduction,Introduction,relation-classification,1,35,0.875,43,0.1747967479674796,35,0.875,1,0
45,"( 2 ) Based on our tagging scheme , we study different kinds of end - to - end models to settle the problem .",Introduction,Introduction,relation-classification,1,36,0.9,44,0.1788617886178861,36,0.9,1,0
46,The tagging - based methods are better than most of the existing pipelined and joint learning methods .,Introduction,Introduction,relation-classification,1,37,0.925,45,0.1829268292682926,37,0.925,1,0
47,"( 3 ) Furthermore , we also develop an end - to - 1 https://github.com/shanzhenren/Co",Introduction,Introduction,relation-classification,1,38,0.95,46,0.1869918699186991,38,0.95,1,0
48,Type end model with biased loss function to suit for the novel tags .,Introduction,Introduction,relation-classification,1,39,0.975,47,0.1910569105691057,39,0.975,1,0
49,It can enhance the association between related entities .,Introduction,Introduction,relation-classification,1,40,1.0,48,0.1951219512195122,40,1.0,1,0
50,Related Works,,,relation-classification,1,0,0.0,49,0.1991869918699187,0,0.0,1,0
51,"Entities and relations extraction is an important step to construct a knowledge base , which can be benefit for many NLP tasks .",Related Works,Related Works,relation-classification,1,1,0.0666666666666666,50,0.2032520325203252,1,0.0666666666666666,0,0
52,Two main frameworks have been widely used to solve the problem of extracting entity and their relationships .,Related Works,Related Works,relation-classification,1,2,0.1333333333333333,51,0.2073170731707317,2,0.1333333333333333,0,0
53,One is the pipelined method and the other is the joint learning method .,Related Works,Related Works,relation-classification,1,3,0.2,52,0.2113821138211382,3,0.2,0,0
54,"The pipelined method treats this task as two separated tasks , i.e. , named entity recognition ( NER ) and relation classification ( RC ) .",Related Works,Related Works,relation-classification,1,4,0.2666666666666666,53,0.2154471544715447,4,0.2666666666666666,0,0
55,"Classical NER models are linear statistical models , such as Hidden Markov Models ( HMM ) and Conditional Random Fields ( CRF ) .",Related Works,Related Works,relation-classification,1,5,0.3333333333333333,54,0.2195121951219512,5,0.3333333333333333,0,0
56,"Recently , several neural network architectures have been successfully applied to NER , which is regarded as a sequential token tagging task .",Related Works,Related Works,relation-classification,1,6,0.4,55,0.2235772357723577,6,0.4,0,0
57,Existing methods for relation classification can also be divided into handcrafted feature based methods and neural network based methods .,Related Works,Related Works,relation-classification,1,7,0.4666666666666667,56,0.2276422764227642,7,0.4666666666666667,0,0
58,While joint models extract entities and relations using a single model .,Related Works,Related Works,relation-classification,1,8,0.5333333333333333,57,0.2317073170731707,8,0.5333333333333333,0,0
59,Most of the joint methods are feature - based structured systems .,Related Works,Related Works,relation-classification,1,9,0.6,58,0.2357723577235772,9,0.6,0,0
60,"Recently , uses a LSTMbased model to extract entities and relations , which can reduce the manual work .",Related Works,Related Works,relation-classification,1,10,0.6666666666666666,59,0.2398373983739837,10,0.6666666666666666,0,0
61,"Different from the above methods , the method proposed in this paper is based on a special tagging manner , so that we can easily use end - toend model to extract results without NER and RC .",Related Works,Related Works,relation-classification,1,11,0.7333333333333333,60,0.2439024390243902,11,0.7333333333333333,0,0
62,end - to - end method is to map the input sentence into meaningful vectors and then back to produce a sequence .,Related Works,Related Works,relation-classification,1,12,0.8,61,0.2479674796747967,12,0.8,0,0
63,It is widely used in machine translation and sequence tagging tasks .,Related Works,Related Works,relation-classification,1,13,0.8666666666666667,62,0.2520325203252032,13,0.8666666666666667,0,0
64,"Most methods apply bidirectional LSTM to encode the input sentences , but the decoding methods are always different .",Related Works,Related Works,relation-classification,1,14,0.9333333333333332,63,0.2560975609756097,14,0.9333333333333332,0,0
65,"For examples , use a CRF layers to decode the tag sequence , while apply LSTM layer to produce the tag sequence .",Related Works,Related Works,relation-classification,1,15,1.0,64,0.2601626016260163,15,1.0,0,0
66,Method,,,relation-classification,1,0,0.0,65,0.2642276422764227,0,0.0,1,0
67,We propose a novel tagging scheme and an end -toend model with biased objective function to jointly extract entities and their relations .,Method,Method,relation-classification,1,1,0.0144927536231884,66,0.2682926829268293,1,0.0714285714285714,1,1
68,"In this section , we firstly introduce how to change the extraction problem to a tagging problem based on our tagging method .",Method,Method,relation-classification,1,2,0.0289855072463768,67,0.2723577235772357,2,0.1428571428571428,1,0
69,Then we detail the model we used to extract results .,Method,Method,relation-classification,1,3,0.0434782608695652,68,0.2764227642276423,3,0.2142857142857142,1,0
70,is an example of how the results are tagged .,Method,Method,relation-classification,1,4,0.0579710144927536,69,0.2804878048780488,4,0.2857142857142857,1,0
71,Each word is assigned a label that contributes to extract the results .,Method,Method,relation-classification,1,5,0.072463768115942,70,0.2845528455284553,5,0.3571428571428571,1,0
72,"Tag "" O "" represents the "" Other "" tag , which means that the corresponding word is independent of the extracted results .",Method,Method,relation-classification,1,6,0.0869565217391304,71,0.2886178861788618,6,0.4285714285714285,1,1
73,"In addition to "" O "" , the other tags consist of three parts : the word position in the entity , the relation type , and the relation role .",Method,Method,relation-classification,1,7,0.1014492753623188,72,0.2926829268292683,7,0.5,1,0
74,"We use the "" BIES "" ( Begin , Inside , End , Single ) signs to represent the position information of a word in the entity .",Method,Method,relation-classification,1,8,0.1159420289855072,73,0.2967479674796748,8,0.5714285714285714,1,1
75,"The relation type information is obtained from a predefined set of relations and the relation role information is represented by the numbers "" 1 "" and "" 2 "" .",Method,Method,relation-classification,1,9,0.1304347826086956,74,0.3008130081300813,9,0.6428571428571429,1,0
76,"An extracted result is represented by a triplet : ( Entity 1 , Relation T ype , Entity 2 ) .",Method,Method,relation-classification,1,10,0.144927536231884,75,0.3048780487804878,10,0.7142857142857143,1,0
77,""" 1 "" means that the word belongs to the first entity in the triplet , while "" 2 "" belongs to second entity that behind the relation type .",Method,Method,relation-classification,1,11,0.1594202898550724,76,0.3089430894308943,11,0.7857142857142857,1,0
78,"Thus , the total number of tags is N t = 2 * 4 * | R | + 1 , where | R | is the size of the predefined relation set .",Method,Method,relation-classification,1,12,0.1739130434782608,77,0.3130081300813008,12,0.8571428571428571,1,0
79,is an example illustrating our tagging method .,Method,Method,relation-classification,1,13,0.1884057971014492,78,0.3170731707317073,13,0.9285714285714286,1,0
80,"The input sentence contains two triplets : { United States , Country - President , Trump } and { Apple Inc , Company - Founder , Steven Paul Jobs } , where "" Country - President "" and "" Company - Founder "" are the predefined relation types .",Method,Method,relation-classification,1,14,0.2028985507246377,79,0.3211382113821138,14,1.0,1,0
81,The Tagging Scheme,Method,,relation-classification,1,15,0.217391304347826,80,0.3252032520325203,0,0.0,1,0
82,"The words "" United "" , "" States "" , "" Trump "" , "" Apple "" , "" Inc "" , "" Steven "" , "" Paul "" and "" Jobs "" are all related to the final extracted results .",Method,The Tagging Scheme,relation-classification,1,16,0.2318840579710145,81,0.3292682926829268,1,0.2,1,0
83,Thus they are tagged based on our special tags .,Method,The Tagging Scheme,relation-classification,1,17,0.2463768115942029,82,0.3333333333333333,2,0.4,1,0
84,"For example , the word of "" United "" is the first word of entity "" United States "" and is related to the relation "" Country - President "" , so its tag is "" B - CP - 1 "" .",Method,The Tagging Scheme,relation-classification,1,18,0.2608695652173913,83,0.3373983739837398,3,0.6,1,0
85,"The other entity "" Trump "" , which is corresponding to "" United States "" , is labeled as "" S - CP - 2 "" .",Method,The Tagging Scheme,relation-classification,1,19,0.2753623188405797,84,0.3414634146341463,4,0.8,1,0
86,"Besides , the other words irrelevant to the final result are labeled as "" O "" .",Method,The Tagging Scheme,relation-classification,1,20,0.2898550724637681,85,0.3455284552845528,5,1.0,1,0
87,From Tag Sequence To Extracted Results,Method,,relation-classification,1,21,0.3043478260869565,86,0.3495934959349593,0,0.0,1,0
88,"From the tag sequence in , we know that "" Trump "" and "" United States "" share the same relation type "" Country - President "" , "" Apple Inc "" and "" Steven Paul Jobs "" share the same relation type "" Company - Founder "" .",Method,From Tag Sequence To Extracted Results,relation-classification,1,22,0.3188405797101449,87,0.3536585365853658,1,0.0208333333333333,1,0
89,We combine entities with the same relation type into a triplet to get the final result .,Method,From Tag Sequence To Extracted Results,relation-classification,1,23,0.3333333333333333,88,0.3577235772357723,2,0.0416666666666666,1,0
90,"Accordingly , "" Trump "" and "" United States "" can be combined into a triplet whose relation type is "" Country - President "" .",Method,From Tag Sequence To Extracted Results,relation-classification,1,24,0.3478260869565217,89,0.3617886178861789,3,0.0625,1,0
91,"Because , the relation role of "" Trump "" is "" 2 "" and "" United States "" is "" 1 "" , the final result is { United States , Country - President , Trump } .",Method,From Tag Sequence To Extracted Results,relation-classification,1,25,0.3623188405797101,90,0.3658536585365853,4,0.0833333333333333,1,0
92,"The same applies to { Apple Inc , Company - Founder , Steven Paul Jobs } .",Method,From Tag Sequence To Extracted Results,relation-classification,1,26,0.3768115942028985,91,0.3699186991869919,5,0.1041666666666666,1,0
93,"Besides , if a sentence contains two or more triplets with the same relation type , we combine every two entities into a triplet based on the nearest principle .",Method,From Tag Sequence To Extracted Results,relation-classification,1,27,0.391304347826087,92,0.3739837398373983,6,0.125,1,0
94,"For example , if the relation type "" Country - President "" in is "" Company - Founder "" , then there will be four entities in the given sentence with the same relation type .",Method,From Tag Sequence To Extracted Results,relation-classification,1,28,0.4057971014492754,93,0.3780487804878049,7,0.1458333333333333,1,0
95,""" United States "" is closest to entity "" Trump "" and the "" Apple Inc "" is closest to "" Jobs "" , so the results will be { United States , Company - Founder , Trump } and { Apple Inc , Company - Founder , Steven Paul Jobs } .",Method,From Tag Sequence To Extracted Results,relation-classification,1,29,0.4202898550724637,94,0.3821138211382114,8,0.1666666666666666,1,0
96,"In this paper , we only consider the situation where an entity belongs to a triplet , and we leave identification of overlapping relations for future work .",Method,From Tag Sequence To Extracted Results,relation-classification,1,30,0.4347826086956521,95,0.3861788617886179,9,0.1875,1,0
97,The End - to - end Model,Method,From Tag Sequence To Extracted Results,relation-classification,1,31,0.4492753623188406,96,0.3902439024390244,10,0.2083333333333333,1,0
98,"In recent years , end - to - end model based on neural network is been widely used in sequence tagging task .",Method,From Tag Sequence To Extracted Results,relation-classification,1,32,0.463768115942029,97,0.3943089430894309,11,0.2291666666666666,1,0
99,"In this paper , we investigate an end - toend model to produce the tags sequence as shows .",Method,From Tag Sequence To Extracted Results,relation-classification,1,33,0.4782608695652174,98,0.3983739837398374,12,0.25,1,1
100,It contains a bi-directional Long Short Term Memory ( Bi - LSTM ) layer to encode the input sentence and a LSTM - based decoding layer with biased loss .,Method,From Tag Sequence To Extracted Results,relation-classification,1,34,0.4927536231884058,99,0.4024390243902439,13,0.2708333333333333,1,1
101,The biased loss can enhance the relevance of entity tags .,Method,From Tag Sequence To Extracted Results,relation-classification,1,35,0.5072463768115942,100,0.4065040650406504,14,0.2916666666666667,1,1
102,The Bi - LSTM Encoding Layer .,Method,From Tag Sequence To Extracted Results,relation-classification,1,36,0.5217391304347826,101,0.4105691056910569,15,0.3125,1,0
103,"In sequence tagging problems , the Bi - LSTM encoding layer has been shown the effectiveness to capture the semantic information of each word .",Method,From Tag Sequence To Extracted Results,relation-classification,1,37,0.5362318840579711,102,0.4146341463414634,16,0.3333333333333333,1,0
104,"It contains forward lstm layer , backward lstm layer and the concatenate layer .",Method,From Tag Sequence To Extracted Results,relation-classification,1,38,0.5507246376811594,103,0.4186991869918699,17,0.3541666666666667,1,0
105,The word embedding layer converts the word with 1 - hot representation to an embedding vector .,Method,From Tag Sequence To Extracted Results,relation-classification,1,39,0.5652173913043478,104,0.4227642276422764,18,0.375,1,0
106,"Hence , a sequence of words can be represented as : Gold standard annotation for an example sentence based on our tagging scheme , where "" CP "" is short for "" Country - President "" and "" CF "" is short for "" Company - Founder "" .",Method,From Tag Sequence To Extracted Results,relation-classification,1,40,0.5797101449275363,105,0.4268292682926829,19,0.3958333333333333,1,0
107,corresponding to the t - th word in the sentence and n is the length of the given sentence .,Method,From Tag Sequence To Extracted Results,relation-classification,1,41,0.5942028985507246,106,0.4308943089430894,20,0.4166666666666667,1,0
108,"After word embedding layer , there are two parallel LSTM layers : forward LSTM layer and backward LSTM layer .",Method,From Tag Sequence To Extracted Results,relation-classification,1,42,0.6086956521739131,107,0.4349593495934959,21,0.4375,1,0
109,"The LSTM architecture consists of a set of recurrently connected subnets , known as memory blocks .",Method,From Tag Sequence To Extracted Results,relation-classification,1,43,0.6231884057971014,108,0.4390243902439024,22,0.4583333333333333,1,0
110,Each time - step is a LSTM memory block .,Method,From Tag Sequence To Extracted Results,relation-classification,1,44,0.6376811594202898,109,0.4430894308943089,23,0.4791666666666667,1,0
111,"The LSTM memory block in Bi - LSTM encoding layer is used to compute current hidden vector ht based on the previous hidden vector h t?1 , the previous cell vector c t?1 and the current input word embedding wt .",Method,From Tag Sequence To Extracted Results,relation-classification,1,45,0.6521739130434783,110,0.4471544715447154,24,0.5,1,0
112,"It s structure diagram is shown in , and detail operations are defined as follows :",Method,From Tag Sequence To Extracted Results,relation-classification,1,46,0.6666666666666666,111,0.4512195121951219,25,0.5208333333333334,1,0
113,"where i , f and o are the input gate , forget gate and output gate respectively , b is the bias term , c is the cell memory , and W ( . ) are the parameters .",Method,From Tag Sequence To Extracted Results,relation-classification,1,47,0.6811594202898551,112,0.4552845528455284,26,0.5416666666666666,1,0
114,"For each word wt , the forward LSTM layer will encode wt by considering the contextual information from word w 1 tow t , which is marked as ? ? ht .",Method,From Tag Sequence To Extracted Results,relation-classification,1,48,0.6956521739130435,113,0.4593495934959349,27,0.5625,1,0
115,"In the similar way , the backward LSTM layer will encode wt based on the contextual information from w n tow t , which is marked as ? ? ht .",Method,From Tag Sequence To Extracted Results,relation-classification,1,49,0.7101449275362319,114,0.4634146341463415,28,0.5833333333333334,1,0
116,"Finally , we concatenate ? ?",Method,From Tag Sequence To Extracted Results,relation-classification,1,50,0.7246376811594203,115,0.4674796747967479,29,0.6041666666666666,1,0
117,ht and ? ?,Method,From Tag Sequence To Extracted Results,relation-classification,1,51,0.7391304347826086,116,0.4715447154471545,30,0.625,1,0
118,"ht to represent word t 's encoding information , denoted as",Method,From Tag Sequence To Extracted Results,relation-classification,1,52,0.7536231884057971,117,0.4756097560975609,31,0.6458333333333334,1,0
119,The LSTM Decoding Layer .,Method,From Tag Sequence To Extracted Results,relation-classification,1,53,0.7681159420289855,118,0.4796747967479675,32,0.6666666666666666,1,0
120,We also adopt a LSTM structure to produce the tag sequence .,Method,From Tag Sequence To Extracted Results,relation-classification,1,54,0.782608695652174,119,0.483739837398374,33,0.6875,1,0
121,"When detecting the tag of word wt , the inputs of decoding layer are : ht obtained from Bi - LSTM encoding layer , former predicted tag embedding T t?1 , former cell value c ( 2 ) t? 1 , and the former hidden vector in decoding layer h ( 2 ) t?1 .",Method,From Tag Sequence To Extracted Results,relation-classification,1,55,0.7971014492753623,120,0.4878048780487805,34,0.7083333333333334,1,0
122,"The structure diagram of the memory block in LSTM dis shown in , and detail operations are defined as follows :",Method,From Tag Sequence To Extracted Results,relation-classification,1,56,0.8115942028985508,121,0.491869918699187,35,0.7291666666666666,1,0
123,The final softmax layer computes normalized entity tag probabilities based on the tag predicted vector T t :,Method,From Tag Sequence To Extracted Results,relation-classification,1,57,0.8260869565217391,122,0.4959349593495935,36,0.75,1,0
124,"where W y is the softmax matrix , N t is the total number of tags .",Method,From Tag Sequence To Extracted Results,relation-classification,1,58,0.8405797101449275,123,0.5,37,0.7708333333333334,1,0
125,"Because T is similar to tag embedding and LSTM is capable of learning long - term dependencies , the decoding manner can model tag interactions .",Method,From Tag Sequence To Extracted Results,relation-classification,1,59,0.855072463768116,124,0.5040650406504065,38,0.7916666666666666,1,0
126,The Bias Objective Function .,Method,,relation-classification,1,60,0.8695652173913043,125,0.508130081300813,39,0.8125,1,0
127,We train our model to maximize the log-likelihood of the data and the optimization method we used is RM - Sprop proposed by Hinton in .,Method,The Bias Objective Function .,relation-classification,1,61,0.8840579710144928,126,0.5121951219512195,40,0.8333333333333334,1,0
128,The objective function can be defined as :,Method,The Bias Objective Function .,relation-classification,1,62,0.8985507246376812,127,0.516260162601626,41,0.8541666666666666,1,0
129,"where | D| is the size of training set , L j is the length of sentence x j , y ( j ) t is the label of word tin sentence x j and p ( j ) t is the normalized probabilities of tags which defined in Formula 15 .",Method,The Bias Objective Function .,relation-classification,1,63,0.9130434782608696,128,0.5203252032520326,42,0.875,1,0
130,"Besides , I ( O ) is a switching function to distinguish the loss of tag ' O ' and relational tags that can indicate the results .",Method,The Bias Objective Function .,relation-classification,1,64,0.927536231884058,129,0.524390243902439,43,0.8958333333333334,1,0
131,It is defined as follows :,Method,The Bias Objective Function .,relation-classification,1,65,0.9420289855072465,130,0.5284552845528455,44,0.9166666666666666,1,0
132,?,Method,The Bias Objective Function .,relation-classification,1,66,0.9565217391304348,131,0.532520325203252,45,0.9375,1,0
133,is the bias weight .,Method,The Bias Objective Function .,relation-classification,1,67,0.9710144927536232,132,0.5365853658536586,46,0.9583333333333334,1,0
134,The larger ?,Method,The Bias Objective Function .,relation-classification,1,68,0.9855072463768116,133,0.540650406504065,47,0.9791666666666666,1,0
135,"is , the greater influence of relational tags on the model .",Method,The Bias Objective Function .,relation-classification,1,69,1.0,134,0.5447154471544715,48,1.0,1,0
136,Experiments,,,relation-classification,1,0,0.0,135,0.5487804878048781,0,0.0,1,0
137,Experimental setting,,,relation-classification,1,0,0.0,136,0.5528455284552846,0,0.0,1,0
138,Dataset,Experimental setting,,relation-classification,1,1,0.1666666666666666,137,0.556910569105691,1,0.0238095238095238,1,0
139,"To evaluate the performance of our methods , we use the public dataset NYT 2 which is produced by distant supervision method .",Experimental setting,Dataset,relation-classification,1,2,0.3333333333333333,138,0.5609756097560976,2,0.0476190476190476,1,0
140,A large amount of training data can be obtained by means of distant supervision methods without manually labeling .,Experimental setting,Dataset,relation-classification,1,3,0.5,139,0.5650406504065041,3,0.0714285714285714,1,0
141,While the test set is manually labeled to ensure its quality .,Experimental setting,Dataset,relation-classification,1,4,0.6666666666666666,140,0.5691056910569106,4,0.0952380952380952,1,0
142,"In total , the training data contains 353 k triplets , and the test set contains 3 , 880 triplets .",Experimental setting,Dataset,relation-classification,1,5,0.8333333333333334,141,0.573170731707317,5,0.119047619047619,1,0
143,"Besides , the size of relation set is 24 .",Experimental setting,Dataset,relation-classification,1,6,1.0,142,0.5772357723577236,6,0.1428571428571428,1,0
144,Evaluation,,,relation-classification,1,0,0.0,143,0.5813008130081301,7,0.1666666666666666,1,0
145,"We adopt standard Precision ( Prec ) , Recall ( Rec ) and F 1 score to evaluate the results .",Evaluation,Evaluation,relation-classification,1,1,0.1428571428571428,144,0.5853658536585366,8,0.1904761904761904,1,0
146,"Different from classical methods , our method can extract triplets without knowing the information of entity types .",Evaluation,Evaluation,relation-classification,1,2,0.2857142857142857,145,0.5894308943089431,9,0.2142857142857142,1,0
147,"In other words , we did not use the label of entity types to train the model , therefore we do not need to consider the entity types in the evaluation .",Evaluation,Evaluation,relation-classification,1,3,0.4285714285714285,146,0.5934959349593496,10,0.238095238095238,1,0
148,A triplet is regarded as correct when its relation type and the head offsets of two corresponding entities are both correct .,Evaluation,Evaluation,relation-classification,1,4,0.5714285714285714,147,0.5975609756097561,11,0.2619047619047619,1,0
149,"Besides , the groundtruth relation mentions are given and "" None "" label is excluded as did .",Evaluation,Evaluation,relation-classification,1,5,0.7142857142857143,148,0.6016260162601627,12,0.2857142857142857,1,0
150,We create a validation set by randomly sampling 10 % data from test set and use the remaining data as evaluation based on ) 's suggestion .,Evaluation,Evaluation,relation-classification,1,6,0.8571428571428571,149,0.6056910569105691,13,0.3095238095238095,1,0
151,We run 10 times for each experiment then report the average results and their standard deviation as shows .,Evaluation,Evaluation,relation-classification,1,7,1.0,150,0.6097560975609756,14,0.3333333333333333,1,0
152,Hyperparameters,,,relation-classification,1,0,0.0,151,0.6138211382113821,15,0.3571428571428571,1,0
153,Our model consists of a Bi - LSTM encoding layer and a LSTM decoding layer with bias objective function .,Hyperparameters,Hyperparameters,relation-classification,1,1,0.0666666666666666,152,0.6178861788617886,16,0.3809523809523809,1,0
154,The word embeddings used in the encoding part are initialed by running word2vec 3 on NYT training corpus .,Hyperparameters,Hyperparameters,relation-classification,1,2,0.1333333333333333,153,0.6219512195121951,17,0.4047619047619047,1,1
155,The dimension of the word embeddings is d = 300 .,Hyperparameters,Hyperparameters,relation-classification,1,3,0.2,154,0.6260162601626016,18,0.4285714285714285,1,1
156,We regularize our network using dropout on embedding layer and the dropout ratio is 0.5 .,Hyperparameters,Hyperparameters,relation-classification,1,4,0.2666666666666666,155,0.6300813008130082,19,0.4523809523809524,1,1
157,The number of lstm units in encoding layer is 300 and the number in decoding layer is 600 .,Hyperparameters,Hyperparameters,relation-classification,1,5,0.3333333333333333,156,0.6341463414634146,20,0.4761904761904761,1,1
158,The bias parameter ?,Hyperparameters,Hyperparameters,relation-classification,1,6,0.4,157,0.6382113821138211,21,0.5,1,1
159,corresponding to the results in is 10 .,Hyperparameters,Hyperparameters,relation-classification,1,7,0.4666666666666667,158,0.6422764227642277,22,0.5238095238095238,1,1
160,2,Hyperparameters,Hyperparameters,relation-classification,1,8,0.5333333333333333,159,0.6463414634146342,23,0.5476190476190477,1,0
161,The dataset can be downloaded at : https://github.com/shanzhenren/CoType.,Hyperparameters,Hyperparameters,relation-classification,1,9,0.6,160,0.6504065040650406,24,0.5714285714285714,1,0
162,There are three data sets in the public resource and we only use the NYT dataset .,Hyperparameters,Hyperparameters,relation-classification,1,10,0.6666666666666666,161,0.6544715447154471,25,0.5952380952380952,1,0
163,Because more than 50 % of the data in BioInfer has overlapping relations which is beyond the scope of this paper .,Hyperparameters,Hyperparameters,relation-classification,1,11,0.7333333333333333,162,0.6585365853658537,26,0.6190476190476191,1,0
164,"As for dataset Wiki - KBP , the number of relation type in the test set is more than that of the train set , which is also not suitable fora supervised training method .",Hyperparameters,Hyperparameters,relation-classification,1,12,0.8,163,0.6626016260162602,27,0.6428571428571429,1,0
165,Details of the data can be found in is the pipelined methods and the second part ( row 4 to 6 ) is the jointly extracting methods .,Hyperparameters,Hyperparameters,relation-classification,1,13,0.8666666666666667,164,0.6666666666666666,28,0.6666666666666666,1,0
166,Our tagging methods are shown in part three ( row 7 to 9 ) .,Hyperparameters,Hyperparameters,relation-classification,1,14,0.9333333333333332,165,0.6707317073170732,29,0.6904761904761905,1,0
167,"In this part , we not only report the results of precision , recall and F1 , we also compute their standard deviation .",Hyperparameters,Hyperparameters,relation-classification,1,15,1.0,166,0.6747967479674797,30,0.7142857142857143,1,0
168,Baselines,,,relation-classification,1,0,0.0,167,0.6788617886178862,31,0.7380952380952381,1,0
169,"We compare our method with several classical triplet extraction methods , which can be divided into the following categories : the pipelined methods , the jointly extracting methods and the end - to - end methods based our tagging scheme .",Baselines,Baselines,relation-classification,1,1,0.0909090909090909,168,0.6829268292682927,32,0.7619047619047619,1,1
170,"For the pipelined methods , we follow ) 's settings :",Baselines,Baselines,relation-classification,1,2,0.1818181818181818,169,0.6869918699186992,33,0.7857142857142857,1,1
171,The NER results are obtained by CoType then several classical relation classification methods are applied to detect the relations .,Baselines,Baselines,relation-classification,1,3,0.2727272727272727,170,0.6910569105691057,34,0.8095238095238095,1,1
172,These methods are :,Baselines,Baselines,relation-classification,1,4,0.3636363636363636,171,0.6951219512195121,35,0.8333333333333334,1,1
173,"( 1 ) DS-logistic ) is a distant supervised and feature based method , which combines the advantages of supervised IE and unsupervised IE features ; ( 2 ) LINE is a network embedding method , which is suitable for arbitrary types of information networks ;",Baselines,Baselines,relation-classification,1,5,0.4545454545454545,172,0.6991869918699187,36,0.8571428571428571,1,1
174,( 3 ) FCM ) is a compositional model that combines lexicalized linguistic context and word embeddings for relation extraction .,Baselines,Baselines,relation-classification,1,6,0.5454545454545454,173,0.7032520325203252,37,0.8809523809523809,1,1
175,"The jointly extracting methods used in this paper are listed as follows : ( 4 ) DS - Joint ) is a supervised method , which jointly extracts entities and relations using structured perceptron on human - annotated dataset ; ( 5 ) MultiR is a typical distant supervised method based on multi-instance learning algorithms to combat the noisy training data ; ( 6 ) CoType ) is a domain independent framework by jointly embedding entity mentions , relation mentions , text features and type labels into meaningful representations .",Baselines,Baselines,relation-classification,1,7,0.6363636363636364,174,0.7073170731707317,38,0.9047619047619048,1,1
176,"In addition , we also compare our method with two classical end - to - end tagging models : LSTM- CRF and LSTM - LSTM .",Baselines,Baselines,relation-classification,1,8,0.7272727272727273,175,0.7113821138211383,39,0.9285714285714286,1,1
177,LSTM - CRF is proposed for entity recognition by using a bidirectional LSTM to encode input sentence and a conditional random fields to predict the entity tag sequence .,Baselines,Baselines,relation-classification,1,9,0.8181818181818182,176,0.7154471544715447,40,0.9523809523809524,1,1
178,"Different from LSTM - CRF , LSTM - LSTM uses a LSTM layer to decode the tag sequence instead of CRF .",Baselines,Baselines,relation-classification,1,10,0.9090909090909092,177,0.7195121951219512,41,0.9761904761904762,1,1
179,They are used for the first time to jointly extract entities and relations based on our tagging scheme .,Baselines,Baselines,relation-classification,1,11,1.0,178,0.7235772357723578,42,1.0,1,0
180,Experimental Results,,,relation-classification,1,0,0.0,179,0.7276422764227642,0,0.0,1,0
181,We report the results of different methods as shown in .,Experimental Results,Experimental Results,relation-classification,1,1,0.0169491525423728,180,0.7317073170731707,1,0.0588235294117647,1,0
182,"It can be seen that our method , LSTM - LSTM - Bias , outperforms all other methods in F 1 score and achieves a 3 % improvement in F 1 over the best method CoType .",Experimental Results,Experimental Results,relation-classification,1,2,0.0338983050847457,181,0.7357723577235772,2,0.1176470588235294,1,1
183,It shows the effectiveness of our proposed method .,Experimental Results,Experimental Results,relation-classification,1,3,0.0508474576271186,182,0.7398373983739838,3,0.1764705882352941,1,0
184,"Furthermore , from , we also can see that the jointly extracting methods are better than pipelined methods , and the tagging methods are better than most of the jointly extracting methods .",Experimental Results,Experimental Results,relation-classification,1,4,0.0677966101694915,183,0.7439024390243902,4,0.2352941176470588,1,0
185,It also validates the validity of our tagging scheme for the task of jointly extracting entities and relations .,Experimental Results,Experimental Results,relation-classification,1,5,0.0847457627118644,184,0.7479674796747967,5,0.2941176470588235,1,0
186,"When compared with the traditional methods , the precisions of the end - to - end models are significantly improved .",Experimental Results,Experimental Results,relation-classification,1,6,0.1016949152542373,185,0.7520325203252033,6,0.3529411764705882,1,1
187,But only LSTM - LSTM - Bias can be better to balance the precision and recall .,Experimental Results,Experimental Results,relation-classification,1,7,0.1186440677966101,186,0.7560975609756098,7,0.4117647058823529,1,0
188,The reason maybe that these end - to - end models all use a Bi - LSTM encoding input sentence and different neural networks to decode the results .,Experimental Results,Experimental Results,relation-classification,1,8,0.135593220338983,187,0.7601626016260162,8,0.4705882352941176,1,0
189,The methods based on neural networks can well fit the data .,Experimental Results,Experimental Results,relation-classification,1,9,0.1525423728813559,188,0.7642276422764228,9,0.5294117647058824,1,0
190,"Therefore , they can learn the common features of the training set well and may lead to the lower expansibility .",Experimental Results,Experimental Results,relation-classification,1,10,0.1694915254237288,189,0.7682926829268293,10,0.5882352941176471,1,0
191,We also find that the LSTM - LSTM model is better than LSTM - CRF model based on our tagging scheme .,Experimental Results,Experimental Results,relation-classification,1,11,0.1864406779661017,190,0.7723577235772358,11,0.6470588235294118,1,1
192,"Because , LSTM is capable of learning long - term dependencies and CRF is good at capturing the joint probability of the entire sequence of labels .",Experimental Results,Experimental Results,relation-classification,1,12,0.2033898305084746,191,0.7764227642276422,12,0.7058823529411765,1,0
193,The related tags may have along distance from each other .,Experimental Results,Experimental Results,relation-classification,1,13,0.2203389830508474,192,0.7804878048780488,13,0.7647058823529411,1,0
194,"Hence ,",Experimental Results,,relation-classification,1,14,0.2372881355932203,193,0.7845528455284553,14,0.8235294117647058,1,0
195,LSTM decoding manner is a little better than CRF .,Experimental Results,"Hence ,",relation-classification,1,15,0.2542372881355932,194,0.7886178861788617,15,0.8823529411764706,1,0
196,LSTM - LSTM - Bias adds a bias weight to enhance the effect of entity tags and weaken the effect of invalid tag .,Experimental Results,"Hence ,",relation-classification,1,16,0.2711864406779661,195,0.7926829268292683,16,0.9411764705882352,1,0
197,"Therefore , in this tagging scheme , our method can be better than the common LSTM - decoding methods .",Experimental Results,"Hence ,",relation-classification,1,17,0.288135593220339,196,0.7967479674796748,17,1.0,1,0
198,Analysis and Discussion,Experimental Results,,relation-classification,1,18,0.3050847457627119,197,0.8008130081300813,0,0.0,1,0
199,Error Analysis,Experimental Results,,relation-classification,1,19,0.3220338983050847,198,0.8048780487804879,0,0.0,1,0
200,"In this paper , we focus on extracting triplets composed of two entities and a relation .",Experimental Results,Error Analysis,relation-classification,1,20,0.3389830508474576,199,0.8089430894308943,1,0.0769230769230769,1,0
201,has shown the predict results of the task .,Experimental Results,Error Analysis,relation-classification,1,21,0.3559322033898305,200,0.8130081300813008,2,0.1538461538461538,1,0
202,It treats an triplet is correct only when the relation type and the head offsets of two corresponding entities are both correct .,Experimental Results,Error Analysis,relation-classification,1,22,0.3728813559322034,201,0.8170731707317073,3,0.2307692307692307,1,0
203,"In order to find out the factors that affect the results of end - to - end models , we analyze the performance on predicting each element in the triplet as shows .",Experimental Results,Error Analysis,relation-classification,1,23,0.3898305084745763,202,0.8211382113821138,4,0.3076923076923077,1,0
204,"E1 and E2 represent the performance on predicting each entity , respectively .",Experimental Results,Error Analysis,relation-classification,1,24,0.4067796610169492,203,0.8252032520325203,5,0.3846153846153846,1,0
205,"If the head offset of the first entity is correct , then the instance of E1 is correct , the same to E2 .",Experimental Results,Error Analysis,relation-classification,1,25,0.423728813559322,204,0.8292682926829268,6,0.4615384615384615,1,0
206,"Regardless of relation type , if the head offsets of two corresponding entities are both correct , the instance of ( E1 , E2 ) is correct .",Experimental Results,Error Analysis,relation-classification,1,26,0.4406779661016949,205,0.8333333333333334,7,0.5384615384615384,1,0
207,"As shown in , ( E1 , E2 ) has higher precision when compared with E1 and E2 .",Experimental Results,Error Analysis,relation-classification,1,27,0.4576271186440678,206,0.8373983739837398,8,0.6153846153846154,1,0
208,But its recall result is lower than E1 and E2 .,Experimental Results,Error Analysis,relation-classification,1,28,0.4745762711864407,207,0.8414634146341463,9,0.6923076923076923,1,0
209,It means that some of the predicted entities do not form a pair .,Experimental Results,Error Analysis,relation-classification,1,29,0.4915254237288136,208,0.8455284552845529,10,0.7692307692307693,1,0
210,"They only obtain E1 and do not find its corresponding E2 , or obtain E2 and do not find its corresponding E1 .",Experimental Results,Error Analysis,relation-classification,1,30,0.5084745762711864,209,0.8495934959349594,11,0.8461538461538461,1,0
211,"Thus it leads to the prediction of more single E and less ( E1 , E2 ) pairs .",Experimental Results,Error Analysis,relation-classification,1,31,0.5254237288135594,210,0.8536585365853658,12,0.9230769230769232,1,0
212,"Therefore , entity pair ( E1 , E2 ) has higher precision and lower recall than single E. Besides , the predicted results of ( E1 , E2 ) in have about 3 % improvement when compared predicted results in Table 1 , which means that 3 % of the test data is pre-dicted to be wrong because the relation type is predicted to be wrong .",Experimental Results,Error Analysis,relation-classification,1,32,0.5423728813559322,211,0.8577235772357723,13,1.0,1,0
213,Analysis of Biased Loss,Experimental Results,,relation-classification,1,33,0.559322033898305,212,0.8617886178861789,0,0.0,1,0
214,"Different from LSTM - CRF and LSTM - LSTM , our approach is biased towards relational labels to enhance links between entities .",Experimental Results,Analysis of Biased Loss,relation-classification,1,34,0.576271186440678,213,0.8658536585365854,1,0.2,1,0
215,"In order to further analyze the effect of the bias objective function , we visualize the ratio of predicted single entities for each end - to - end method as .",Experimental Results,Analysis of Biased Loss,relation-classification,1,35,0.5932203389830508,214,0.8699186991869918,2,0.4,1,0
216,The single entities refer to those who can not find their corresponding entities .,Experimental Results,Analysis of Biased Loss,relation-classification,1,36,0.6101694915254238,215,0.8739837398373984,3,0.6,1,0
217,"shows whether it is E1 or E2 , our method can get a relatively low ratio on the single entities .",Experimental Results,Analysis of Biased Loss,relation-classification,1,37,0.6271186440677966,216,0.8780487804878049,4,0.8,1,0
218,It means that our method can effectively associate two entities when compared LSTM - CRF and LSTM - LSTM which pay little attention to the relational tags .,Experimental Results,Analysis of Biased Loss,relation-classification,1,38,0.6440677966101694,217,0.8821138211382114,5,1.0,1,0
219,Single E1,Experimental Results,,relation-classification,1,39,0.6610169491525424,218,0.8861788617886179,0,0.0,1,0
220,"Single Besides , we also change the Bias Parameter ?",Experimental Results,Single E1,relation-classification,1,40,0.6779661016949152,219,0.8902439024390244,1,0.1666666666666666,1,0
221,"from 1 to 20 , and the predicted results are shown in .",Experimental Results,Single E1,relation-classification,1,41,0.6949152542372882,220,0.8943089430894309,2,0.3333333333333333,1,0
222,If ?,Experimental Results,Single E1,relation-classification,1,42,0.711864406779661,221,0.8983739837398373,3,0.5,1,0
223,"is too large , it will affect the accuracy of prediction and if ?",Experimental Results,Single E1,relation-classification,1,43,0.7288135593220338,222,0.902439024390244,4,0.6666666666666666,1,0
224,"is too small , the recall will decline .",Experimental Results,Single E1,relation-classification,1,44,0.7457627118644068,223,0.9065040650406504,5,0.8333333333333334,1,0
225,"When ? = 10 , LSTM - LSTM - Bias can balance the precision and recall , and can achieve the best F 1 scores .",Experimental Results,Single E1,relation-classification,1,45,0.7627118644067796,224,0.9105691056910568,6,1.0,1,0
226,Case Study,Experimental Results,,relation-classification,1,46,0.7796610169491526,225,0.9146341463414634,0,0.0,1,0
227,"In this section , we observe the prediction results of end - to - end methods , and then select several representative examples to illustrate the advantages and disadvantages of the methods as shows .",Experimental Results,Case Study,relation-classification,1,47,0.7966101694915254,226,0.91869918699187,1,0.0769230769230769,1,0
228,"Each example contains three row , the first row is the gold standard , the second and the third rows are the extracted results of model LSTM - LSTM and LSTM - LSTM - Bias respectively .",Experimental Results,Case Study,relation-classification,1,48,0.8135593220338984,227,0.9227642276422764,2,0.1538461538461538,1,0
229,"S1 represents the situation that the distance between the two interrelated entities is faraway from each other , which is more difficult to detect their relationships .",Experimental Results,Case Study,relation-classification,1,49,0.8305084745762712,228,0.926829268292683,3,0.2307692307692307,1,0
230,"When compared with LSTM - LSTM , LSTM - LSTM - Bias uses a bias objective function which enhance the relevance between entities .",Experimental Results,Case Study,relation-classification,1,50,0.847457627118644,229,0.9308943089430894,4,0.3076923076923077,1,0
231,"Therefore , in this example , LSTM - LSTM - Bias can extract two related entities , while LSTM - LSTM can only extract one entity of "" Florida "" and can not detect entity "" Panama City Beach "" .",Experimental Results,Case Study,relation-classification,1,51,0.864406779661017,230,0.934959349593496,5,0.3846153846153846,1,0
232,S2 is a negative example that shows these methods may mistakenly predict one of the entity .,Experimental Results,Case Study,relation-classification,1,52,0.8813559322033898,231,0.9390243902439024,6,0.4615384615384615,1,0
233,There are no indicative words between entities Nuremberg and Germany .,Experimental Results,Case Study,relation-classification,1,53,0.8983050847457628,232,0.943089430894309,7,0.5384615384615384,1,0
234,"Besides , the patten "" a * of * "" between Germany and M iddle Ages maybe easy to mislead the models that there exists a relation of "" Contains "" between them .",Experimental Results,Case Study,relation-classification,1,54,0.9152542372881356,233,0.9471544715447154,8,0.6153846153846154,1,0
235,The problem can be solved by adding some samples of this kind of expression patterns to the training data .,Experimental Results,Case Study,relation-classification,1,55,0.9322033898305084,234,0.951219512195122,9,0.6923076923076923,1,0
236,"S3 is a case that models can predict the entities ' head offset right , but the relational role is wrong .",Experimental Results,Case Study,relation-classification,1,56,0.9491525423728814,235,0.9552845528455284,10,0.7692307692307693,1,0
237,"LSTM - LSTM treats both "" Stephen A. Schwarzman "" and "" Blackstone Group "" as entity E1 , and can not find its corresponding E2 .",Experimental Results,Case Study,relation-classification,1,57,0.9661016949152542,236,0.959349593495935,11,0.8461538461538461,1,0
238,"Although , LSTM - LSMT - Bias can find the entities pair ( E1 , E2 ) , it reverses the roles of "" Stephen A. Schwarzman "" and "" Blackstone Group "" .",Experimental Results,Case Study,relation-classification,1,58,0.9830508474576272,237,0.9634146341463414,12,0.9230769230769232,1,0
239,"It shows that LSTM - LSTM - Bias is able to better on pre-dicting entities pair , but it remains to be improved in distinguishing the relationship between the two entities .",Experimental Results,Case Study,relation-classification,1,59,1.0,238,0.967479674796748,13,1.0,1,0
240,Conclusion,,,relation-classification,1,0,0.0,239,0.9715447154471544,0,0.0,1,0
241,"In this paper , we propose a novel tagging scheme and investigate the end - to - end models to jointly extract entities and relations .",Conclusion,Conclusion,relation-classification,1,1,0.1666666666666666,240,0.975609756097561,1,0.1666666666666666,0,0
242,The experimental results show the effectiveness of our proposed method .,Conclusion,Conclusion,relation-classification,1,2,0.3333333333333333,241,0.9796747967479674,2,0.3333333333333333,0,0
243,But it still has shortcoming on the identification of the overlapping relations .,Conclusion,Conclusion,relation-classification,1,3,0.5,242,0.983739837398374,3,0.5,0,0
244,"In the future work , we will replace the softmax function in the output layer with multiple classifier , so that a word can has multiple tags .",Conclusion,Conclusion,relation-classification,1,4,0.6666666666666666,243,0.9878048780487804,4,0.6666666666666666,0,0
245,"In this way , a word can appear in multiple triplet results , which can solve the problem of overlapping relations .",Conclusion,Conclusion,relation-classification,1,5,0.8333333333333334,244,0.991869918699187,5,0.8333333333333334,0,0
246,"Although , our model can enhance the effect of entity tags , the association between two corresponding entities still requires refinement in next works .",Conclusion,Conclusion,relation-classification,1,6,1.0,245,0.9959349593495936,6,1.0,0,0
1,title,,,relation-classification,2,0,0.0,0,0.0,0,0.0,1,0
2,Joint entity recognition and relation extraction as a multi-head selection problem,title,title,relation-classification,2,1,0.0,1,0.0033898305084745,1,0.0,1,1
3,abstract,,,relation-classification,2,0,0.0,2,0.0067796610169491,0,0.0,1,0
4,State - of - the - art models for joint entity recognition and relation extraction strongly rely on external natural language processing ( NLP ) tools such as POS ( part - of - speech ) taggers and dependency parsers .,abstract,abstract,relation-classification,2,1,0.1428571428571428,3,0.0101694915254237,1,0.1428571428571428,1,0
5,"Thus , the performance of such joint models depends on the quality of the features obtained from these NLP tools .",abstract,abstract,relation-classification,2,2,0.2857142857142857,4,0.0135593220338983,2,0.2857142857142857,1,0
6,"However , these features are not always accurate for various languages and contexts .",abstract,abstract,relation-classification,2,3,0.4285714285714285,5,0.0169491525423728,3,0.4285714285714285,1,0
7,"In this paper , we propose a joint neural model which performs entity recognition and relation extraction simultaneously , without the need of any manually extracted features or the use of any external tool .",abstract,abstract,relation-classification,2,4,0.5714285714285714,6,0.0203389830508474,4,0.5714285714285714,1,1
8,"Specifically , we model the entity recognition task using a CRF ( Conditional Random Fields ) layer and the relation extraction task as a multi-head selection problem ( i.e. , potentially identify multiple relations for each entity ) .",abstract,abstract,relation-classification,2,5,0.7142857142857143,7,0.023728813559322,5,0.7142857142857143,1,0
9,"We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) .",abstract,abstract,relation-classification,2,6,0.8571428571428571,8,0.0271186440677966,6,0.8571428571428571,1,0
10,"Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",abstract,abstract,relation-classification,2,7,1.0,9,0.0305084745762711,7,1.0,1,0
11,Introduction,,,relation-classification,2,0,0.0,10,0.0338983050847457,0,0.0,1,0
12,The goal of the entity recognition and relation extraction is to discover relational structures of entity mentions from unstructured texts .,Introduction,Introduction,relation-classification,2,1,0.032258064516129,11,0.0372881355932203,1,0.032258064516129,1,0
13,It is a central problem in information extraction since it is critical for tasks such as knowledge base population and question answering .,Introduction,Introduction,relation-classification,2,2,0.064516129032258,12,0.0406779661016949,2,0.064516129032258,1,0
14,"The problem is traditionally approached as two separate subtasks , namely ( i ) named entity recognition ( NER ) and ( ii ) relation extraction ( RE ) , in a pipeline setting .",Introduction,Introduction,relation-classification,2,3,0.0967741935483871,13,0.0440677966101694,3,0.0967741935483871,1,0
15,"The main limitations of the pipeline models are : ( i ) error propagation between the components ( i.e. , NER and RE ) and ( ii ) possible useful information from the one task is not exploited by the other ( e.g. , identifying a Works for relation might be helpful for the NER module in detecting the type of the two entities , i.e. , PER , ORG and vice versa ) .",Introduction,Introduction,relation-classification,2,4,0.1290322580645161,14,0.047457627118644,4,0.1290322580645161,1,0
16,"On the other hand , more recent studies propose to use joint models to detect entities and their relations overcoming the aforementioned issues and achieving state - of - the - art performance .",Introduction,Introduction,relation-classification,2,5,0.1612903225806451,15,0.0508474576271186,5,0.1612903225806451,1,1
17,The previous joint models heavily rely on hand - crafted features .,Introduction,Introduction,relation-classification,2,6,0.1935483870967742,16,0.0542372881355932,6,0.1935483870967742,1,0
18,"Recent advances in neural networks alleviate the issue of manual feature engineering , but some of them still depend on NLP tools ( e.g. , POS taggers , dependency parsers ) .",Introduction,Introduction,relation-classification,2,7,0.2258064516129032,17,0.0576271186440678,7,0.2258064516129032,1,0
19,propose a Recurrent Neural Network ( RNN ) - based joint model that uses a bidirectional sequential LSTM ( Long Short Term Memory ) to model the entities and a tree - LSTM that takes into account dependency tree information to model the relations between the entities .,Introduction,Introduction,relation-classification,2,8,0.2580645161290322,18,0.0610169491525423,8,0.2580645161290322,1,0
20,The dependency information is extracted using an external dependency parser .,Introduction,Introduction,relation-classification,2,9,0.2903225806451613,19,0.0644067796610169,9,0.2903225806451613,1,0
21,"Similarly , in the work of for entity and relation extraction from biomedical text , a model which also uses tree - LSTMs is applied to extract dependency information .",Introduction,Introduction,relation-classification,2,10,0.3225806451612903,20,0.0677966101694915,10,0.3225806451612903,1,0
22,"propose a method that relies on RNNs but uses a lot of hand - crafted features and additional NLP tools to extract features such as POS - tags , etc. replicate the context around the entities with Convolutional Neural Networks ( CNNs ) .",Introduction,Introduction,relation-classification,2,11,0.3548387096774194,21,0.0711864406779661,11,0.3548387096774194,1,0
23,"Note that the aforementioned works examine pairs of entities for relation extraction , rather than modeling the whole sentence directly .",Introduction,Introduction,relation-classification,2,12,0.3870967741935484,22,0.0745762711864406,12,0.3870967741935484,1,0
24,This means that relations of other pairs of entities in the same sentence - which could be helpful in deciding on the relation type fora particular pair - are not taken into account .,Introduction,Introduction,relation-classification,2,13,0.4193548387096774,23,0.0779661016949152,13,0.4193548387096774,1,0
25,"propose a neural joint model based on LSTMs where they model the whole sentence at once , but still they do not have a principled way to deal with multiple relations .",Introduction,Introduction,relation-classification,2,14,0.4516129032258064,24,0.0813559322033898,14,0.4516129032258064,1,0
26,introduce a quadratic scoring layer to model the two tasks simultaneously .,Introduction,Introduction,relation-classification,2,15,0.4838709677419355,25,0.0847457627118644,15,0.4838709677419355,1,0
27,"The limitation of this approach is that only a single relation can be assigned to a token , while the time complexity for the entity recognition task is increased compared to the standard approaches with linear complexity .",Introduction,Introduction,relation-classification,2,16,0.5161290322580645,26,0.0881355932203389,16,0.5161290322580645,1,0
28,"In this work , we focus on anew general purpose joint model that performs the two tasks of entity recognition and relation extraction simultaneously , and that can handle multiple relations together .",Introduction,Introduction,relation-classification,2,17,0.5483870967741935,27,0.0915254237288135,17,0.5483870967741935,1,1
29,"Our model achieves state - of - the - art performance in a number of different contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) without relying on any manually engineered features nor additional NLP tools .",Introduction,Introduction,relation-classification,2,18,0.5806451612903226,28,0.0949152542372881,18,0.5806451612903226,1,0
30,"In summary , our proposed model ( which will be detailed next in Section 3 ) solves several shortcomings that we identified in related works ( Section 2 ) for joint entity recognition and relation extraction : ( i ) our model does not rely on external NLP tools nor hand - crafted features , ( ii ) entities and relations within the same text fragment ( typically a sentence ) are extracted simultaneously , where ( iii ) an entity can be involved in multiple relations at once .",Introduction,Introduction,relation-classification,2,19,0.6129032258064516,29,0.0983050847457627,19,0.6129032258064516,1,0
31,"Specifically , the model of depends on dependency parsers , which perform particularly well on specific languages ( i.e. , English ) and contexts ( i.e. , news ) .",Introduction,Introduction,relation-classification,2,20,0.6451612903225806,30,0.1016949152542373,20,0.6451612903225806,1,0
32,"Yet , our ambition is to develop a model that generalizes well in various setups , therefore using only automatically extracted features that are learned during training .",Introduction,Introduction,relation-classification,2,21,0.6774193548387096,31,0.1050847457627118,21,0.6774193548387096,1,0
33,"For instance , and use exactly the same model in different contexts , i.e. , news ( ACE04 ) and biomedical data ( ADE ) , respectively .",Introduction,Introduction,relation-classification,2,22,0.7096774193548387,32,0.1084745762711864,22,0.7096774193548387,1,0
34,"Comparing our results to the ADE dataset , we obtain a 1.8 % improvement on the NER task and ? 3 % on the RE task .",Introduction,Introduction,relation-classification,2,23,0.7419354838709677,33,0.111864406779661,23,0.7419354838709677,1,0
35,"On the other hand , our model performs within a reasonable margin ( ? 0.6 % in the NER task and ? 1 % on the RE task ) on the ACE04 dataset without the use of pre-calculated features .",Introduction,Introduction,relation-classification,2,24,0.7741935483870968,34,0.1152542372881356,24,0.7741935483870968,1,0
36,This shows that the model of strongly relies on the features extracted by the dependency parsers and can not generalize well into different contexts where dependency parser features are weak .,Introduction,Introduction,relation-classification,2,25,0.8064516129032258,35,0.1186440677966101,25,0.8064516129032258,1,0
37,"Comparing to , we train our model by modeling all the entities and the relations of the sentence at once .",Introduction,Introduction,relation-classification,2,26,0.8387096774193549,36,0.1220338983050847,26,0.8387096774193549,1,0
38,This type of inference is beneficial in obtaining information about neighboring entities and relations instead of just examining a pair of entities each time .,Introduction,Introduction,relation-classification,2,27,0.8709677419354839,37,0.1254237288135593,27,0.8709677419354839,1,0
39,"Finally , we solve the underlying problem of the models proposed by and , who essentially assume classes ( i.e. , relations ) to be mutually exclusive : we solve this by phrasing the relation extraction component as a multi-label prediction problem .",Introduction,Introduction,relation-classification,2,28,0.9032258064516128,38,0.1288135593220339,28,0.9032258064516128,1,0
40,"To demonstrate the effectiveness of the proposed method , we conduct the largest experimental evaluation to date ( to the best of our knowledge ) in jointly performing both entity recognition and relation extraction ( see Section 4 and Section 5 ) , using different datasets from various domains ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) .",Introduction,Introduction,relation-classification,2,29,0.935483870967742,39,0.1322033898305084,29,0.935483870967742,1,0
41,"Specifically , we apply our method to four datasets , namely ACE04 ( news ) , Adverse Drug Events ( ADE ) , Dutch Real Estate Classifieds ( DREC ) and CoNLL'04 ( news ) .",Introduction,Introduction,relation-classification,2,30,0.967741935483871,40,0.135593220338983,30,0.967741935483871,1,0
42,"Our method outperforms all state - of - the - art methods that do not rely on any additional features or tools , while performance is very close ( or even better in the biomedical dataset ) compared to methods that do exploit hand - engineered features or NLP tools .",Introduction,Introduction,relation-classification,2,31,1.0,41,0.1389830508474576,31,1.0,1,0
43,Related work,,,relation-classification,2,0,0.0,42,0.1423728813559322,0,0.0,1,0
44,The tasks of entity recognition and relation extraction can be applied either one by one in a pipeline setting or in a joint model .,Related work,Related work,relation-classification,2,1,0.0161290322580645,43,0.1457627118644067,1,0.25,0,0
45,"In this section , we present related work for each task ( i.e. , named entity recognition and relation extraction ) as well as prior work into joint entity and relation extraction .",Related work,Related work,relation-classification,2,2,0.032258064516129,44,0.1491525423728813,2,0.5,0,0
46,1,Related work,Related work,relation-classification,2,3,0.0483870967741935,45,0.1525423728813559,3,0.75,0,0
47,"Note that another difference is that we use a CRF layer for the NER part , while Katiyar & Cardie ( 2017 ) uses a softmax and uses a quadratic scoring layer ; see further , when we discuss performance comparison results in Section 5 .",Related work,Related work,relation-classification,2,4,0.064516129032258,46,0.1559322033898305,4,1.0,0,0
48,Named entity recognition,Related work,,relation-classification,2,5,0.0806451612903225,47,0.159322033898305,0,0.0,0,0
49,"We formulate the entity identification task as a sequence labeling problem , similar to previous work on joint learning models and named entity recognition using the BIO ( Beginning , Inside , Outside ) encoding scheme .",Related work,Named entity recognition,relation-classification,2,6,0.0967741935483871,48,0.1627118644067796,1,0.0303030303030303,0,0
50,Each entity consists of multiple sequential tokens within the sentence and we should assign a tag for every token in the sentence .,Related work,Named entity recognition,relation-classification,2,7,0.1129032258064516,49,0.1661016949152542,2,0.0606060606060606,0,0
51,"That way we are able to identify the entity arguments ( start and end position ) and its type ( e.g. , ORG ) .",Related work,Named entity recognition,relation-classification,2,8,0.1290322580645161,50,0.1694915254237288,3,0.0909090909090909,0,0
52,"To do so , we assign the B - type ( beginning ) to the first token of the entity , the I - type ( inside ) to every other token within the entity and the O tag ( outside ) if a token is not part of an entity .",Related work,Named entity recognition,relation-classification,2,9,0.1451612903225806,51,0.1728813559322033,4,0.1212121212121212,0,0
53,shows an example of the BIO encoding tags assigned to the tokens of the sentence .,Related work,Named entity recognition,relation-classification,2,10,0.1612903225806451,52,0.1762711864406779,5,0.1515151515151515,0,0
54,"In the CRF layer , one can observe that we assign the B - ORG and I - ORG tags to indicate the beginning and the inside tokens of the entity "" Disease Control Center "" , respectively .",Related work,Named entity recognition,relation-classification,2,11,0.1774193548387097,53,0.1796610169491525,6,0.1818181818181818,0,0
55,"On top of the BiLSTM layer , we employ either a softmax or a CRF layer to calculate the most probable entity tag for each token .",Related work,Named entity recognition,relation-classification,2,12,0.1935483870967742,54,0.1830508474576271,7,0.2121212121212121,0,0
56,We calculate the score of each token w i for each entity tag :,Related work,Named entity recognition,relation-classification,2,13,0.2096774193548387,55,0.1864406779661017,8,0.2424242424242424,0,0
57,"where the superscript ( e ) is used for the notation of the NER task , f ( ) is an elementwise activation function ( i.e. , relu , tanh ) ,",Related work,Named entity recognition,relation-classification,2,14,0.2258064516129032,56,0.1898305084745762,9,0.2727272727272727,0,0
58,"with d as the hidden size of the LSTM , p the number of NER tags ( e.g. , B - ORG ) and",Related work,Named entity recognition,relation-classification,2,15,0.2419354838709677,57,0.1932203389830508,10,0.303030303030303,0,0
59,l the layer width .,Related work,Named entity recognition,relation-classification,2,16,0.2580645161290322,58,0.1966101694915254,11,0.3333333333333333,0,0
60,We calculate the probabilities of all the candidate tags fora give n,Related work,Named entity recognition,relation-classification,2,17,0.2741935483870967,59,0.2,12,0.3636363636363636,0,0
61,"we employ the softmax approach only for the entity classification ( EC ) task ( which is similar to NER ) where we need to predict only the entity types ( e.g. , PER ) for each token assuming boundaries are given .",Related work,Named entity recognition,relation-classification,2,18,0.2903225806451613,60,0.2033898305084746,13,0.3939393939393939,0,0
62,The CRF approach is used for the NER task which includes both entity type and boundaries recognition .,Related work,Named entity recognition,relation-classification,2,19,0.3064516129032258,61,0.2067796610169491,14,0.4242424242424242,0,0
63,"In the softmax approach , we assign entity types to tokens in a greedy way at prediction time ( i.e. , the selected tag is just the highest scoring tag overall possible set of tags ) .",Related work,Named entity recognition,relation-classification,2,20,0.3225806451612903,62,0.2101694915254237,15,0.4545454545454545,0,0
64,"Although assuming an independent tag distribution is beneficial for entity classification tasks ( e.g. , POS tagging ) , this is not the case when there are strong de-pendencies between the tags .",Related work,Named entity recognition,relation-classification,2,21,0.3387096774193548,63,0.2135593220338983,16,0.4848484848484848,0,0
65,"Specifically , in NER , the BIO tagging scheme forces several restrictions ( e.g. , B - LOC can not be followed by I - PER ) .",Related work,Named entity recognition,relation-classification,2,22,0.3548387096774194,64,0.2169491525423728,17,0.5151515151515151,0,0
66,"The softmax method allows local decisions ( i.e. , for the tag of each token w i ) even though the BiLSTM captures information about the neighboring words .",Related work,Named entity recognition,relation-classification,2,23,0.3709677419354839,65,0.2203389830508474,18,0.5454545454545454,0,0
67,"Still , the neighboring tags are not taken into account for the tag decision of a specific token .",Related work,Named entity recognition,relation-classification,2,24,0.3870967741935484,66,0.223728813559322,19,0.5757575757575758,0,0
68,"For example , in the entity "" John Smith "" , tagging "" Smith "" as PER is useful for deciding that "" John "" is B - PER .",Related work,Named entity recognition,relation-classification,2,25,0.4032258064516129,67,0.2271186440677966,20,0.6060606060606061,0,0
69,"To this end , for NER , we use a linear - chain CRF , similar to Lample et al .",Related work,Named entity recognition,relation-classification,2,26,0.4193548387096774,68,0.2305084745762712,21,0.6363636363636364,0,0
70,where an improvement of ? 1 % F 1 NER points is reported when using CRF .,Related work,Named entity recognition,relation-classification,2,27,0.4354838709677419,69,0.2338983050847457,22,0.6666666666666666,0,0
71,"In our case , with the use of CRF we also report a ? 1 % overall performance improvement as observed in",Related work,Named entity recognition,relation-classification,2,28,0.4516129032258064,70,0.2372881355932203,23,0.696969696969697,0,0
72,"is the score of the predicted tag for token w i , T is a square transition matrix in which each entry represents transition scores from one tag to another .",Related work,Named entity recognition,relation-classification,2,29,0.4677419354838709,71,0.2406779661016949,24,0.7272727272727273,0,0
73,T ?,Related work,Named entity recognition,relation-classification,2,30,0.4838709677419355,72,0.2440677966101695,25,0.7575757575757576,0,0
74,R ( p+2 ) ( p + 2 ) because y,Related work,Named entity recognition,relation-classification,2,31,0.5,73,0.247457627118644,26,0.7878787878787878,0,0
75,We apply Viterbi to obtain the tag sequence ? ( e ) with the highest score .,Related work,Named entity recognition,relation-classification,2,32,0.5161290322580645,74,0.2508474576271186,27,0.8181818181818182,0,0
76,We train both the softmax ( for the EC task ) and the CRF layer ( for NER ) by minimizing the cross - entropy loss L NER .,Related work,Named entity recognition,relation-classification,2,33,0.532258064516129,75,0.2542372881355932,28,0.8484848484848485,0,0
77,"We also use the entity tags as input to our relation extraction layer by learning label embeddings , motivated by where an improvement of 2 % F 1 is reported ( with the use of label embeddings ) .",Related work,Named entity recognition,relation-classification,2,34,0.5483870967741935,76,0.2576271186440678,29,0.8787878787878788,0,0
78,"In our case , label embeddings lead to an increase of 1 % F 1 score as reported in ( see Section 5.2 ) .",Related work,Named entity recognition,relation-classification,2,35,0.5645161290322581,77,0.2610169491525423,30,0.9090909090909092,0,0
79,"The input to the next layer is twofold : the output states of the LSTM and the learned label embedding representation , encoding the intuition that knowledge of named enti-ties can be useful for relation extraction .",Related work,Named entity recognition,relation-classification,2,36,0.5806451612903226,78,0.2644067796610169,31,0.9393939393939394,0,0
80,"During training , we use the gold entity tags , while at prediction time we use the predicted entity tags as input to the next layer .",Related work,Named entity recognition,relation-classification,2,37,0.5967741935483871,79,0.2677966101694915,32,0.9696969696969696,0,0
81,The input to the next layer is the concatenation of the hidden LSTM state hi with the label embedding g i for token w i :,Related work,Named entity recognition,relation-classification,2,38,0.6129032258064516,80,0.2711864406779661,33,1.0,0,0
82,Relation extraction,Related work,,relation-classification,2,39,0.6290322580645161,81,0.2745762711864407,0,0.0,0,0
83,We consider relation extraction as the second task of our joint model .,Related work,Relation extraction,relation-classification,2,40,0.6451612903225806,82,0.2779661016949152,1,0.1666666666666666,0,0
84,The main approaches for relation extraction rely either on hand - crafted features or neural networks .,Related work,Relation extraction,relation-classification,2,41,0.6612903225806451,83,0.2813559322033898,2,0.3333333333333333,0,0
85,"Feature - based methods focus on obtaining effective hand - crafted features , for instance defining kernel functions and designing lexical , syntactic , semantic features , etc . .",Related work,Relation extraction,relation-classification,2,42,0.6774193548387096,84,0.2847457627118644,3,0.5,0,0
86,Neural network models have been proposed to overcome the issue of manually designing hand - crafted features leading to improved performance .,Related work,Relation extraction,relation-classification,2,43,0.6935483870967742,85,0.288135593220339,4,0.6666666666666666,0,0
87,CNN - and models have been introduced to automatically extract lexical and sentence level features leading to a deeper language understanding .,Related work,Relation extraction,relation-classification,2,44,0.7096774193548387,86,0.2915254237288135,5,0.8333333333333334,0,0
88,combine CNNs and RNNs using an ensemble scheme to achieve state - of - the - art results .,Related work,Relation extraction,relation-classification,2,45,0.7258064516129032,87,0.2949152542372881,6,1.0,0,0
89,Joint entity and relation extraction,Related work,,relation-classification,2,46,0.7419354838709677,88,0.2983050847457627,0,0.0,0,0
90,Entity and relation extraction includes the task of ( i ) identifying the entities ( described in Section 2.1 ) and ( ii ) extracting the relations among them ( described in Section 2.2 ) .,Related work,Joint entity and relation extraction,relation-classification,2,47,0.7580645161290323,89,0.3016949152542373,1,0.0625,0,0
91,Feature - based joint models have been proposed to simultaneously solve the entity recognition and relation extraction ( RE ) subtasks .,Related work,Joint entity and relation extraction,relation-classification,2,48,0.7741935483870968,90,0.3050847457627119,2,0.125,0,0
92,"These methods rely on the availability of NLP tools ( e.g. , POS taggers ) or manually designed features and thus ( i ) require additional effort for the data preprocessing , ( ii ) perform poorly in different application and language settings where the NLP tools are not reliable , and ( iii ) increase the computational complexity .",Related work,Joint entity and relation extraction,relation-classification,2,49,0.7903225806451613,91,0.3084745762711864,3,0.1875,0,0
93,"In this paper , we introduce a joint neural network model to overcome the aforementioned issues and to automatically perform end - to - end relation extraction without the need of any manual feature engineering or the use of additional NLP components .",Related work,Joint entity and relation extraction,relation-classification,2,50,0.8064516129032258,92,0.311864406779661,4,0.25,0,0
94,Neural network approaches have been considered to address the problem in a joint setting ( end - to - end relation extraction ) and typically include the use of RNNs and CNNs .,Related work,Joint entity and relation extraction,relation-classification,2,51,0.8225806451612904,93,0.3152542372881356,5,0.3125,0,0
95,"Specifically , propose the use of bidirectional tree - structured RNNs to capture dependency tree information ( where parse trees are extracted using state - of - the - art dependency parsers ) which has been proven beneficial for relation extraction .",Related work,Joint entity and relation extraction,relation-classification,2,52,0.8387096774193549,94,0.3186440677966101,6,0.375,0,0
96,"apply the work of to biomedical text , reporting state - of - the - art performance for two biomedical datasets .",Related work,Joint entity and relation extraction,relation-classification,2,53,0.8548387096774194,95,0.3220338983050847,7,0.4375,0,0
97,propose the use of a lot of hand - crafted features along with RNNs .,Related work,Joint entity and relation extraction,relation-classification,2,54,0.8709677419354839,96,0.3254237288135593,8,0.5,0,0
98,"solve the entity classification task ( which is different from NER since in entity classification the boundaries of the entities are known and only the type of the entity should be predicted ) and relation extraction problems using an approximation of a global normalization objective ( i.e. , CRF ) : they replicate the context of the sentence ( left and right part of the entities ) to feed one entity pair at a time to a CNN for relation extraction .",Related work,Joint entity and relation extraction,relation-classification,2,55,0.8870967741935484,97,0.3288135593220339,9,0.5625,0,0
99,"Thus , they do not simultaneously infer other potential entities and relations within the same sentence .",Related work,Joint entity and relation extraction,relation-classification,2,56,0.9032258064516128,98,0.3322033898305084,10,0.625,0,0
100,"and The input of our model is the words of the sentence which are then represented as word vectors ( i.e. , embeddings ) .",Related work,Joint entity and relation extraction,relation-classification,2,57,0.9193548387096774,99,0.335593220338983,11,0.6875,0,0
101,The BiLSTM layer extracts a more complex representation for each word .,Related work,Joint entity and relation extraction,relation-classification,2,58,0.935483870967742,100,0.3389830508474576,12,0.75,0,0
102,Then the CRF and the sigmoid layers are able to produce the outputs for the two tasks .,Related work,Joint entity and relation extraction,relation-classification,2,59,0.9516129032258064,101,0.3423728813559322,13,0.8125,0,0
103,"The outputs for each token ( e.g. , Smith ) are : ( i ) an entity recognition label ( e.g. , I - PER ) and ( ii ) a set of tuples comprising the head tokens of the entity and the types of relations between them ( e.g. , {( Center , Works for ) , ( Atlanta , Lives in ) } ) .",Related work,Joint entity and relation extraction,relation-classification,2,60,0.967741935483871,102,0.3457627118644067,14,0.875,0,0
104,"tional complexity described by , by dividing the loss functions into a NER and a relation extraction component .",Related work,Joint entity and relation extraction,relation-classification,2,61,0.9838709677419356,103,0.3491525423728813,15,0.9375,0,0
105,"Moreover , we are able to handle multiple relations instead of just predicting single ones , as was described for the application of structured real estate advertisements of .",Related work,Joint entity and relation extraction,relation-classification,2,62,1.0,104,0.3525423728813559,16,1.0,0,0
106,Joint model,,,relation-classification,2,0,0.0,105,0.3559322033898305,0,0.0,1,0
107,"In this section , we present our multi-head joint model illustrated in .",Joint model,Joint model,relation-classification,2,1,0.015625,106,0.3593220338983051,1,0.0833333333333333,1,1
108,"The model is able to simultaneously identify the entities ( i.e. , types and boundaries ) and all the possible relations between them at once .",Joint model,Joint model,relation-classification,2,2,0.03125,107,0.3627118644067796,2,0.1666666666666666,1,0
109,We formulate the problem as a multi-head selection problem extending previous work as described in Section 2.3 .,Joint model,Joint model,relation-classification,2,3,0.046875,108,0.3661016949152542,3,0.25,1,0
110,"By multi-head , we mean that any particular entity maybe the CoNLL04 dataset is presented .",Joint model,Joint model,relation-classification,2,4,0.0625,109,0.3694915254237288,4,0.3333333333333333,1,0
111,"The input of our model is a sequence of tokens ( i.e. , words of the sentence ) which are then represented as word vectors ( i.e. , word embeddings ) .",Joint model,Joint model,relation-classification,2,5,0.078125,110,0.3728813559322034,5,0.4166666666666667,1,1
112,The BiLSTM layer is able to extract a more complex representation for each word that incorporates the context via the RNN structure .,Joint model,Joint model,relation-classification,2,6,0.09375,111,0.376271186440678,6,0.5,1,0
113,Then the CRF and the sigmoid layers are able to produce the outputs for the two tasks .,Joint model,Joint model,relation-classification,2,7,0.109375,112,0.3796610169491525,7,0.5833333333333334,1,0
114,"The outputs for each token ( e.g. , Smith ) are twofold : ( i ) an entity recognition label ( e.g. , I - PER , denoting the token is inside a named entity of type PER ) and ( ii ) a set of tuples comprising the head tokens of the entity and the types of relations between them ( e.g. , {( Center , Works for ) , ( Atlanta , Lives in ) } ) .",Joint model,Joint model,relation-classification,2,8,0.125,113,0.3830508474576271,8,0.6666666666666666,1,1
115,"Since we assume token - based encoding , we consider only the last token of the entity as head of another token , eliminating redundant relations .",Joint model,Joint model,relation-classification,2,9,0.140625,114,0.3864406779661017,9,0.75,1,0
116,"For instance , there is a Works for relation between entities "" John Smith "" and "" Disease Control Center "" .",Joint model,Joint model,relation-classification,2,10,0.15625,115,0.3898305084745763,10,0.8333333333333334,1,0
117,"Instead of connecting all tokens of the entities , we connect only "" Smith "" with "" Center "" .",Joint model,Joint model,relation-classification,2,11,0.171875,116,0.3932203389830508,11,0.9166666666666666,1,0
118,"Also , for the case of no relation , we introduce the "" N "" label and we predict the token itself as the head .",Joint model,Joint model,relation-classification,2,12,0.1875,117,0.3966101694915254,12,1.0,1,0
119,Embedding layer,Joint model,,relation-classification,2,13,0.203125,118,0.4,0,0.0,1,0
120,"Given a sentence w = w 1 , ... , w n as a sequence of tokens , the word embedding layer is responsible to map each token to a word vector ( w word2vec ) .",Joint model,Embedding layer,relation-classification,2,14,0.21875,119,0.4033898305084746,1,0.0909090909090909,1,0
121,We use pre-trained word embeddings using the Skip - Gram word2vec model .,Joint model,Embedding layer,relation-classification,2,15,0.234375,120,0.4067796610169492,2,0.1818181818181818,1,0
122,"In this work , we also use character embeddings since they are commonly applied to neural NER .",Joint model,Embedding layer,relation-classification,2,16,0.25,121,0.4101694915254237,3,0.2727272727272727,1,0
123,This type of embeddings is able to capture morphological features such as prefixes and suffixes .,Joint model,Embedding layer,relation-classification,2,17,0.265625,122,0.4135593220338983,4,0.3636363636363636,1,0
124,"For instance , in the Adverse Drug Events ( ADE ) dataset , the suffix "" toxicity "" can specify an adverse drug event entity such as "" neurotoxicity "" or "" hepatotoxicity "" and thus it is very informative .",Joint model,Embedding layer,relation-classification,2,18,0.28125,123,0.4169491525423728,5,0.4545454545454545,1,0
125,"Another example might be the Dutch suffix "" kamer "" ( "" room "" in English ) in the Dutch Real Estate Classifieds ( DREC ) dataset which is used to specify the space entities "" badkamer "" ( "" bathroom "" in English ) and "" slaapkamer "" ( "" bedroom "" in English ) .",Joint model,Embedding layer,relation-classification,2,19,0.296875,124,0.4203389830508474,6,0.5454545454545454,1,0
126,"Character - level embeddings are learned during training , similar to Ma & Hovy and .",Joint model,Embedding layer,relation-classification,2,20,0.3125,125,0.423728813559322,7,0.6363636363636364,1,0
127,"In the work of Lample et al. , character embeddings lead to a performance improvement of up to 3 % in terms of NER F 1 score .",Joint model,Embedding layer,relation-classification,2,21,0.328125,126,0.4271186440677966,8,0.7272727272727273,1,0
128,"In our work , by incorporating character embeddings , we report in an increase of ? 2 %",Joint model,Embedding layer,relation-classification,2,22,0.34375,127,0.4305084745762711,9,0.8181818181818182,1,0
129,overall F 1 scoring points .,Joint model,Embedding layer,relation-classification,2,23,0.359375,128,0.4338983050847457,10,0.9090909090909092,1,0
130,"For more details , see Section 5.2 .",Joint model,,relation-classification,2,24,0.375,129,0.4372881355932203,11,1.0,1,0
131,Bidirectional LSTM encoding layer,Joint model,,relation-classification,2,25,0.390625,130,0.4406779661016949,0,0.0,1,0
132,RNNs are commonly used in modeling sequential data and have been successfully applied in various NLP tasks .,Joint model,Bidirectional LSTM encoding layer,relation-classification,2,26,0.40625,131,0.4440677966101695,1,0.2,1,0
133,"In this work , we use multi - layer LSTMs , a specific kind of RNNs which are able to capture long term dependencies well .",Joint model,Bidirectional LSTM encoding layer,relation-classification,2,27,0.421875,132,0.447457627118644,2,0.4,1,0
134,We employ a BiLSTM which is able to encode information from left to right ( past to future ) and right to left ( future to past ) .,Joint model,Bidirectional LSTM encoding layer,relation-classification,2,28,0.4375,133,0.4508474576271186,3,0.6,1,0
135,"This way , we can combine bidirectional information for each word by concatenating the forward ( hi ) and the backward ( hi ) output at timestep i .",Joint model,Bidirectional LSTM encoding layer,relation-classification,2,29,0.453125,134,0.4542372881355932,4,0.8,1,0
136,The BiLSTM output at timestep i can be written as :,Joint model,Bidirectional LSTM encoding layer,relation-classification,2,30,0.46875,135,0.4576271186440678,5,1.0,1,0
137,Relation extraction as multi-head selection,Joint model,,relation-classification,2,31,0.484375,136,0.4610169491525424,0,0.0,1,0
138,"In this subsection , we describe the relation extraction task , formulated as a multi-head selection problem .",Joint model,Relation extraction as multi-head selection,relation-classification,2,32,0.5,137,0.4644067796610169,1,0.0303030303030303,1,0
139,"In the general formulation of our method , each token w i can have multiple heads ( i.e. , multiple relations with other tokens ) .",Joint model,Relation extraction as multi-head selection,relation-classification,2,33,0.515625,138,0.4677966101694915,2,0.0606060606060606,1,0
140,"We predict the tuple (? i ,? i ) where ?",Joint model,Relation extraction as multi-head selection,relation-classification,2,34,0.53125,139,0.4711864406779661,3,0.0909090909090909,1,0
141,i is the vector of heads and ?,Joint model,Relation extraction as multi-head selection,relation-classification,2,35,0.546875,140,0.4745762711864407,4,0.1212121212121212,1,0
142,i is the vector of the corresponding relations for each token w i .,Joint model,Relation extraction as multi-head selection,relation-classification,2,36,0.5625,141,0.4779661016949152,5,0.1515151515151515,1,0
143,"This is different for the previous standard head selection for dependency parsing method since ( i ) it is extended to predict multiple heads and ( ii ) the decisions for the heads and the relations are jointly taken ( i.e. , instead of first predicting the heads and then in a next step the relations by using an additional classifier ) .",Joint model,Relation extraction as multi-head selection,relation-classification,2,37,0.578125,142,0.4813559322033898,6,0.1818181818181818,1,0
144,"Given as input a token sequence wand a set of relation labels R , our goal is to identify for each token w i , i ?",Joint model,Relation extraction as multi-head selection,relation-classification,2,38,0.59375,143,0.4847457627118644,7,0.2121212121212121,1,0
145,"{ 0 , ... , n} the vector of the most probable heads ?",Joint model,Relation extraction as multi-head selection,relation-classification,2,39,0.609375,144,0.488135593220339,8,0.2424242424242424,1,0
146,i ? wand the vector of the most probable corresponding relation labelsr i ?,Joint model,Relation extraction as multi-head selection,relation-classification,2,40,0.625,145,0.4915254237288136,9,0.2727272727272727,1,0
147,R .,Joint model,,relation-classification,2,41,0.640625,146,0.4949152542372881,10,0.303030303030303,1,0
148,We calculate the score between tokens w i and w j given a label r k as follows :,Joint model,R .,relation-classification,2,42,0.65625,147,0.4983050847457627,11,0.3333333333333333,1,0
149,"where the superscript ( r ) is used for the notation of the relation task , f ( ) is an elementwise activation function ( i.e. , relu , tanh ) ,",Joint model,R .,relation-classification,2,43,0.671875,148,0.5016949152542373,12,0.3636363636363636,1,0
150,"is the hidden size of the LSTM , b is the size of the label embeddings and l the layer width .",Joint model,R .,relation-classification,2,44,0.6875,149,0.5050847457627119,13,0.3939393939393939,1,0
151,We define,Joint model,,relation-classification,2,45,0.703125,150,0.5084745762711864,14,0.4242424242424242,1,0
152,"to be the probability of token w j to be selected as the head of token w i with the relation label r k between them , where ?( . ) stands for the sigmoid function .",Joint model,We define,relation-classification,2,46,0.71875,151,0.511864406779661,15,0.4545454545454545,1,0
153,We minimize the cross - entropy loss L rel during training :,Joint model,We define,relation-classification,2,47,0.734375,152,0.5152542372881356,16,0.4848484848484848,1,0
154,where y i ?,Joint model,We define,relation-classification,2,48,0.75,153,0.5186440677966102,17,0.5151515151515151,1,0
155,wand r i ?,Joint model,We define,relation-classification,2,49,0.765625,154,0.5220338983050847,18,0.5454545454545454,1,0
156,R are the ground truth vectors of heads and associated relation labels of w i and m is the number of relations ( heads ) for w i .,Joint model,We define,relation-classification,2,50,0.78125,155,0.5254237288135594,19,0.5757575757575758,1,0
157,"After training , we keep the combination of heads ?",Joint model,We define,relation-classification,2,51,0.796875,156,0.5288135593220339,20,0.6060606060606061,1,0
158,i and relation labelsr i exceeding a threshold based on the estimated joint probability as defined in Eq..,Joint model,We define,relation-classification,2,52,0.8125,157,0.5322033898305085,21,0.6363636363636364,1,0
159,"Unlike previous work on joint models ( Katiyar & Cardie , 2017 ) , we are able to predict multiple relations considering the classes as independent and not mutually exclusive ( the probabilities do not necessarily sum to 1 for different classes ) .",Joint model,We define,relation-classification,2,53,0.828125,158,0.535593220338983,22,0.6666666666666666,1,0
160,"For the joint entity and relation extraction task , we calculate the final objective as L NER + L rel .",Joint model,We define,relation-classification,2,54,0.84375,159,0.5389830508474577,23,0.696969696969697,1,0
161,Edmonds ' algorithm,Joint model,,relation-classification,2,55,0.859375,160,0.5423728813559322,24,0.7272727272727273,1,0
162,Our model is able to simultaneously extract entity mentions and the relations between them .,Joint model,Edmonds ' algorithm,relation-classification,2,56,0.875,161,0.5457627118644067,25,0.7575757575757576,1,0
163,"To demonstrate the effectiveness and the general purpose nature of our model , we also test it on the recently introduced Dutch real estate classifieds ( DREC ) dataset where the entities need to form a tree structure .",Joint model,Edmonds ' algorithm,relation-classification,2,57,0.890625,162,0.5491525423728814,26,0.7878787878787878,1,0
164,"By using thresholded inference , a tree structure of relations is not guaranteed .",Joint model,Edmonds ' algorithm,relation-classification,2,58,0.90625,163,0.5525423728813559,27,0.8181818181818182,1,0
165,Thus we should enforce tree structure constraints to our model .,Joint model,Edmonds ' algorithm,relation-classification,2,59,0.921875,164,0.5559322033898305,28,0.8484848484848485,1,0
166,"To this end , we post-process the output of our system with Edmonds ' maximum spanning tree algorithm for directed graphs .",Joint model,Edmonds ' algorithm,relation-classification,2,60,0.9375,165,0.559322033898305,29,0.8787878787878788,1,0
167,"A fully connected directed graph G = ( V , E ) is constructed ,",Joint model,Edmonds ' algorithm,relation-classification,2,61,0.953125,166,0.5627118644067797,30,0.9090909090909092,1,0
168,where the vertices,Joint model,Edmonds ' algorithm,relation-classification,2,62,0.96875,167,0.5661016949152542,31,0.9393939393939394,1,0
169,V represent the last tokens of the identified entities ( as predicted by NER ) and the edges E represent the highest scoring relations with their scores as weights .,Joint model,Edmonds ' algorithm,relation-classification,2,63,0.984375,168,0.5694915254237288,32,0.9696969696969696,1,0
170,Edmonds ' algorithm is applied in cases a tree is not already formed by thresholded inference .,Joint model,Edmonds ' algorithm,relation-classification,2,64,1.0,169,0.5728813559322034,33,1.0,1,0
171,Experimental setup,,,relation-classification,2,0,0.0,170,0.576271186440678,0,0.0,1,0
172,Datasets and evaluation metrics,,,relation-classification,2,0,0.0,171,0.5796610169491525,0,0.0,1,0
173,"We conduct experiments on four datasets : ( i ) Automatic Content Extraction , ACE04 ( Dod - We follow the cross -validation setting of Li & Ji and .",Datasets and evaluation metrics,Datasets and evaluation metrics,relation-classification,2,1,0.0294117647058823,172,0.5830508474576271,1,0.0333333333333333,1,0
174,We removed DISC and did 5 - fold cross -validation on the bnews and nwire subsets ( 348 documents ) .,Datasets and evaluation metrics,Datasets and evaluation metrics,relation-classification,2,2,0.0588235294117647,173,0.5864406779661017,2,0.0666666666666666,1,0
175,We obtained the preprocessing script from Miwa 's github codebase .,Datasets and evaluation metrics,Datasets and evaluation metrics,relation-classification,2,3,0.088235294117647,174,0.5898305084745763,3,0.1,1,0
176,"We measure the performance of our system using micro F 1 scores , Precision and Recall on both entities and relations .",Datasets and evaluation metrics,Datasets and evaluation metrics,relation-classification,2,4,0.1176470588235294,175,0.5932203389830508,4,0.1333333333333333,1,0
177,We treat an entity as correct when the entity type and the region of its head are correct .,Datasets and evaluation metrics,Datasets and evaluation metrics,relation-classification,2,5,0.1470588235294117,176,0.5966101694915255,5,0.1666666666666666,1,0
178,"We treat a relation as correct when it s type and argument entities are correct , similar to and .",Datasets and evaluation metrics,Datasets and evaluation metrics,relation-classification,2,6,0.1764705882352941,177,0.6,6,0.2,1,0
179,We refer to this type of evaluation as strict .,Datasets and evaluation metrics,Datasets and evaluation metrics,relation-classification,2,7,0.2058823529411764,178,0.6033898305084746,7,0.2333333333333333,1,0
180,"We select the best hyperparameter values on a randomly selected validation set for each fold , selected from the training set ( 15 % of the data ) since there are no official train and validation splits in the work of .",Datasets and evaluation metrics,Datasets and evaluation metrics,relation-classification,2,8,0.2352941176470588,179,0.6067796610169491,8,0.2666666666666666,1,0
181,"CoNLL04 : There are four entity types in the dataset ( Location , Organization , Person ,",Datasets and evaluation metrics,Datasets and evaluation metrics,relation-classification,2,9,0.2647058823529412,180,0.6101694915254238,9,0.3,1,0
182,"and Other ) and five relation types ( Kill , Live in , Located in , OrgBased in and Work for ) .",Datasets and evaluation metrics,Datasets and evaluation metrics,relation-classification,2,10,0.2941176470588235,181,0.6135593220338983,10,0.3333333333333333,1,0
183,We use the splits defined by Gupta et al. and .,Datasets and evaluation metrics,Datasets and evaluation metrics,relation-classification,2,11,0.3235294117647059,182,0.6169491525423729,11,0.3666666666666666,1,0
184,"The dataset consists of 910 training instances , 243 for validation and 288 for testing .",Datasets and evaluation metrics,Datasets and evaluation metrics,relation-classification,2,12,0.3529411764705882,183,0.6203389830508474,12,0.4,1,0
185,We measure the performance by computing the F 1 score on the test set .,Datasets and evaluation metrics,Datasets and evaluation metrics,relation-classification,2,13,0.3823529411764705,184,0.6237288135593221,13,0.4333333333333333,1,0
186,We adopt two evaluation settings to compare to previous work .,Datasets and evaluation metrics,Datasets and evaluation metrics,relation-classification,2,14,0.4117647058823529,185,0.6271186440677966,14,0.4666666666666667,1,0
187,"Specifically , we perform an EC task assuming the entity boundaries are given similar to Gupta et al. and .",Datasets and evaluation metrics,Datasets and evaluation metrics,relation-classification,2,15,0.4411764705882353,186,0.6305084745762712,15,0.5,1,0
188,"To obtain comparable results , we omit the entity class "" Other "" when computing the EC score .",Datasets and evaluation metrics,Datasets and evaluation metrics,relation-classification,2,16,0.4705882352941176,187,0.6338983050847458,16,0.5333333333333333,1,0
189,We score a multi-token entity as correct if at least one of its comprising token types is correct assuming that the boundaries are given ; a relation is correct when the type of the relation and the argument entities are both correct .,Datasets and evaluation metrics,Datasets and evaluation metrics,relation-classification,2,17,0.5,188,0.6372881355932203,17,0.5666666666666667,1,0
190,We report macro-average F 1 scores for EC and RE to obtain comparable results to previous studies .,Datasets and evaluation metrics,Datasets and evaluation metrics,relation-classification,2,18,0.5294117647058824,189,0.6406779661016949,18,0.6,1,0
191,"Moreover , we perform actual NER evaluation instead of just EC , reporting results using the strict evaluation metric .",Datasets and evaluation metrics,Datasets and evaluation metrics,relation-classification,2,19,0.5588235294117647,190,0.6440677966101694,19,0.6333333333333333,1,0
192,We measure the performance by computing the F 1 score on the test set .,Datasets and evaluation metrics,Datasets and evaluation metrics,relation-classification,2,20,0.5882352941176471,191,0.6474576271186441,20,0.6666666666666666,1,0
193,"To compare our results with previous work , we use the boundaries evaluation setting .",Datasets and evaluation metrics,Datasets and evaluation metrics,relation-classification,2,21,0.6176470588235294,192,0.6508474576271186,21,0.7,1,0
194,"In this setting , we count an entity as correct if the boundaries of the entity are correct .",Datasets and evaluation metrics,Datasets and evaluation metrics,relation-classification,2,22,0.6470588235294118,193,0.6542372881355932,22,0.7333333333333333,1,0
195,A relation is correct when the relation is correct and the argument entities are both correct .,Datasets and evaluation metrics,Datasets and evaluation metrics,relation-classification,2,23,0.6764705882352942,194,0.6576271186440678,23,0.7666666666666667,1,0
196,"Also , we report results using the strict evaluation for future reference .",Datasets and evaluation metrics,Datasets and evaluation metrics,relation-classification,2,24,0.7058823529411765,195,0.6610169491525424,24,0.8,1,0
197,ADE : There are two types of entities ( drugs and diseases ) in this dataset and the aim of the task is to identify the types of entities and relate each drug with a disease ( adverse drug events ) .,Datasets and evaluation metrics,Datasets and evaluation metrics,relation-classification,2,25,0.7352941176470589,196,0.6644067796610169,25,0.8333333333333334,1,0
198,"There are 6,821 sentences in total and similar to previous work , we remove ? 130 relations with overlapping entities ( e.g. , "" lithium "" is a drug which is related to "" lithium intoxication "" ) .",Datasets and evaluation metrics,Datasets and evaluation metrics,relation-classification,2,26,0.7647058823529411,197,0.6677966101694915,26,0.8666666666666667,1,0
199,"Since there are no official sets , we evaluate our model using 10 - fold cross- validation where 10 % of the data was used as validation and 10 % for test set similar to .",Datasets and evaluation metrics,Datasets and evaluation metrics,relation-classification,2,27,0.7941176470588235,198,0.6711864406779661,27,0.9,1,0
200,The final results are displayed in F 1 metric as a macro -average across the folds .,Datasets and evaluation metrics,Datasets and evaluation metrics,relation-classification,2,28,0.8235294117647058,199,0.6745762711864407,28,0.9333333333333332,1,0
201,"The dataset consists of 10,652 entities and 6,682 relations .",Datasets and evaluation metrics,Datasets and evaluation metrics,relation-classification,2,29,0.8529411764705882,200,0.6779661016949152,29,0.9666666666666668,1,0
202,We report results similar to previous work on this dataset using the strict evaluation metric .,Datasets and evaluation metrics,Datasets and evaluation metrics,relation-classification,2,30,0.8823529411764706,201,0.6813559322033899,30,1.0,1,0
203,Word embeddings,Datasets and evaluation metrics,,relation-classification,2,31,0.9117647058823528,202,0.6847457627118644,0,0.0,1,0
204,"We use pre-trained word2vec embeddings used in previous work , so as to retain the same inputs for our model and to obtain comparable results that are not affected by the input embeddings .",Datasets and evaluation metrics,Word embeddings,relation-classification,2,32,0.9411764705882352,203,0.688135593220339,1,0.3333333333333333,1,0
205,"Specifically , we use the 200 - dimensional word embeddings used in the work of for the ACE04 dataset 6 trained on Wikipedia .",Datasets and evaluation metrics,Word embeddings,relation-classification,2,33,0.9705882352941176,204,0.6915254237288135,2,0.6666666666666666,1,0
206,We obtained the 50 - dimensional word embeddings used by,Datasets and evaluation metrics,Word embeddings,relation-classification,2,34,1.0,205,0.6949152542372882,3,1.0,1,0
207,Hyperparameters and implementation details,,,relation-classification,2,0,0.0,206,0.6983050847457627,0,0.0,1,0
208,We have developed our joint model by using Python with the TensorFlow machine learning library .,Hyperparameters and implementation details,Hyperparameters and implementation details,relation-classification,2,1,0.0769230769230769,207,0.7016949152542373,1,0.0769230769230769,1,1
209,"Training is performed using the Adam optimizer ( Kingma & Ba , 2015 ) with a learning rate of 10 ?3 .",Hyperparameters and implementation details,Hyperparameters and implementation details,relation-classification,2,2,0.1538461538461538,208,0.7050847457627119,2,0.1538461538461538,1,1
210,We fix the size of the LSTM to d = 64 and the layer width of the neural network to l = 64 ( both for the entity and the relation scoring layers ) .,Hyperparameters and implementation details,Hyperparameters and implementation details,relation-classification,2,3,0.2307692307692307,209,0.7084745762711865,3,0.2307692307692307,1,1
211,We use dropout to regularize our network .,Hyperparameters and implementation details,,relation-classification,2,4,0.3076923076923077,210,0.711864406779661,4,0.3076923076923077,1,1
212,Dropout is applied in the input embeddings and in between the hidden layers for both tasks .,Hyperparameters and implementation details,We use dropout to regularize our network .,relation-classification,2,5,0.3846153846153846,211,0.7152542372881356,5,0.3846153846153846,1,0
213,Different dropout rates have been applied but the best dropout values ( 0.2 to 0.4 ) for each dataset have been used .,Hyperparameters and implementation details,We use dropout to regularize our network .,relation-classification,2,6,0.4615384615384615,212,0.7186440677966102,6,0.4615384615384615,1,0
214,The hidden dimension for the characterbased LSTMs is 25 ( for each direction ) .,Hyperparameters and implementation details,We use dropout to regularize our network .,relation-classification,2,7,0.5384615384615384,213,0.7220338983050848,7,0.5384615384615384,1,1
215,We also fixed our label embeddings to be of size b = 25 for all the datasets except for CoNLL04 where the label embeddings were not beneficial and thus were not used .,Hyperparameters and implementation details,We use dropout to regularize our network .,relation-classification,2,8,0.6153846153846154,214,0.7254237288135593,8,0.6153846153846154,1,0
216,We experimented with tanh and relu activation functions ( recall that this is the function f ( ) from the model description relu activation only in the ACE04 and tanh in all other datasets .,Hyperparameters and implementation details,We use dropout to regularize our network .,relation-classification,2,9,0.6923076923076923,215,0.7288135593220338,9,0.6923076923076923,1,0
217,We employ the technique of early stopping based on the validation set .,Hyperparameters and implementation details,We use dropout to regularize our network .,relation-classification,2,10,0.7692307692307693,216,0.7322033898305085,10,0.7692307692307693,1,1
218,"In all the datasets examined in this study , we obtain the best hyperparameters after 60 to 200 epochs depending on the size of the dataset .",Hyperparameters and implementation details,We use dropout to regularize our network .,relation-classification,2,11,0.8461538461538461,217,0.735593220338983,11,0.8461538461538461,1,1
219,We select the best epoch according to the results in the validation set .,Hyperparameters and implementation details,We use dropout to regularize our network .,relation-classification,2,12,0.9230769230769232,218,0.7389830508474576,12,0.9230769230769232,1,0
220,For more details about the effect of each hyperparameter to the model performance seethe Appendix .,Hyperparameters and implementation details,We use dropout to regularize our network .,relation-classification,2,13,1.0,219,0.7423728813559322,13,1.0,1,0
221,Results and discussion,,,relation-classification,2,0,0.0,220,0.7457627118644068,0,0.0,1,0
222,Results,,,relation-classification,2,0,0.0,221,0.7491525423728813,0,0.0,1,0
223,"In , we present the results of our analysis .",Results,Results,relation-classification,2,1,0.0217391304347826,222,0.752542372881356,1,0.027027027027027,1,0
224,The first column indicates the considered dataset .,Results,,relation-classification,2,2,0.0434782608695652,223,0.7559322033898305,2,0.054054054054054,1,0
225,"In the second column , we denote the model which is applied ( i.e. , previous work and the proposed models ) .",Results,The first column indicates the considered dataset .,relation-classification,2,3,0.0652173913043478,224,0.7593220338983051,3,0.081081081081081,1,0
226,The proposed models are the following :,Results,The first column indicates the considered dataset .,relation-classification,2,4,0.0869565217391304,225,0.7627118644067796,4,0.1081081081081081,1,0
227,"( i ) multi-head is the proposed model with the CRF layer for NER and the sigmoid loss for multiple head prediction , ( ii ) multi-head +",Results,The first column indicates the considered dataset .,relation-classification,2,5,0.108695652173913,226,0.7661016949152543,5,0.1351351351351351,1,0
228,"E is the proposed model with addition of Edmonds ' algorithm to guarantee a tree - structured output for the DREC dataset , ( iii ) single - head is the proposed method but it predicts only one head per token using a softmax loss instead of a sigmoid , and ( iv ) multi-head EC is the proposed method with a softmax to predict the entity classes assuming that the boundaries are given , and the sigmoid loss for multiple head selection . ( iii ) Relaxed : we score a multi-token entity as correct if at least one of its comprising token types is correct assuming that the boundaries are given ; a relation is correct when the type of the relation and the argument entities are both correct .",Results,The first column indicates the considered dataset .,relation-classification,2,6,0.1304347826086956,227,0.7694915254237288,6,0.1621621621621621,1,0
229,"In the next three columns , we present the results for the entity identification task ( Precision , Recall , F 1 ) and then ( in the subsequent three columns ) the results of the relation extraction task ( Precision , Recall , F 1 ) .",Results,The first column indicates the considered dataset .,relation-classification,2,7,0.1521739130434782,228,0.7728813559322034,7,0.1891891891891892,1,0
230,"Finally , in the last column , we report an additional F 1 measure which is the average F 1 performance of the two subtasks .",Results,The first column indicates the considered dataset .,relation-classification,2,8,0.1739130434782608,229,0.7762711864406779,8,0.2162162162162162,1,0
231,"We mark with bold font in , the class probabilities do not necessarily sum up to one since the classes are considered independent .",Results,The first column indicates the considered dataset .,relation-classification,2,9,0.1956521739130435,230,0.7796610169491526,9,0.2432432432432432,1,0
232,"Moreover , we use a CRF - layer to model the NER task to capture dependencies between sequential tokens .",Results,The first column indicates the considered dataset .,relation-classification,2,10,0.217391304347826,231,0.7830508474576271,10,0.2702702702702703,1,0
233,"Finally , we obtain more effective word representations by using character - level embeddings .",Results,The first column indicates the considered dataset .,relation-classification,2,11,0.2391304347826087,232,0.7864406779661017,11,0.2972972972972973,1,0
234,"On the other hand , our model performs within a reasonable margin ( ? 0.5 % for the NER task and ? 1 %",Results,The first column indicates the considered dataset .,relation-classification,2,12,0.2608695652173913,233,0.7898305084745763,12,0.3243243243243243,1,0
235,"for the RE task ) compared to For the CoNLL04 dataset , there are two different evaluation settings , namely relaxed and strict .",Results,The first column indicates the considered dataset .,relation-classification,2,13,0.2826086956521739,234,0.7932203389830509,13,0.3513513513513513,1,0
236,"In the relaxed setting , we perform an EC task instead of NER assuming that the boundaries of the entities are given .",Results,The first column indicates the considered dataset .,relation-classification,2,14,0.3043478260869565,235,0.7966101694915254,14,0.3783783783783784,1,0
237,We adopt this setting to produce comparable results with previous studies ) .,Results,The first column indicates the considered dataset .,relation-classification,2,15,0.3260869565217391,236,0.8,15,0.4054054054054054,1,0
238,"Similar to , we present results of single models and no ensembles .",Results,The first column indicates the considered dataset .,relation-classification,2,16,0.3478260869565217,237,0.8033898305084746,16,0.4324324324324324,1,0
239,We observe that our model outperforms all previous models that do not rely on complex hand - crafted features by a large margin ( > 4 % for both tasks ) .,Results,The first column indicates the considered dataset .,relation-classification,2,17,0.3695652173913043,238,0.8067796610169492,17,0.4594594594594595,1,1
240,"Unlike these previous studies that consider pairs of entities to obtain the entity types and the corresponding relations , we model the whole sentence at once .",Results,The first column indicates the considered dataset .,relation-classification,2,18,0.391304347826087,239,0.8101694915254237,18,0.4864864864864865,1,0
241,"That way , our method is able to directly infer all entities and relations of a sentence and benefit from their possible interactions that can not be modeled when training is performed for each entity pair individually , one at a time .",Results,The first column indicates the considered dataset .,relation-classification,2,19,0.4130434782608695,240,0.8135593220338984,19,0.5135135135135135,1,0
242,"In the same setting , we also report the results of Gupta et al. in which they use multiple complicated hand - crafted features coming from NLP tools .",Results,The first column indicates the considered dataset .,relation-classification,2,20,0.4347826086956521,241,0.8169491525423729,20,0.5405405405405406,1,0
243,Our model performs slightly better for the EC task and within a margin of 1 % in terms of overall F 1 score .,Results,The first column indicates the considered dataset .,relation-classification,2,21,0.4565217391304347,242,0.8203389830508474,21,0.5675675675675675,1,0
244,The difference in the overall performance is due to the fact that our model uses only automatically generated features .,Results,The first column indicates the considered dataset .,relation-classification,2,22,0.4782608695652174,243,0.823728813559322,22,0.5945945945945946,1,0
245,"We also report re-sults on the same dataset conducting NER ( i.e. , predicting entity types and boundaries ) and evaluating using the strict evaluation measure , similar to .",Results,The first column indicates the considered dataset .,relation-classification,2,23,0.5,244,0.8271186440677966,23,0.6216216216216216,1,0
246,Our results are not directly comparable to the work of because we use the splits provided by .,Results,The first column indicates the considered dataset .,relation-classification,2,24,0.5217391304347826,245,0.8305084745762712,24,0.6486486486486487,1,0
247,"However , in this setting we present the results from as reference .",Results,The first column indicates the considered dataset .,relation-classification,2,25,0.5434782608695652,246,0.8338983050847457,25,0.6756756756756757,1,0
248,"We report an improvement of ? 2 % overall F 1 score , which suggests that our neural model is able to extract more informative representations compared to feature - based approaches .",Results,The first column indicates the considered dataset .,relation-classification,2,26,0.5652173913043478,247,0.8372881355932204,26,0.7027027027027027,1,0
249,"We also report results for the DREC dataset , with two different evaluation settings .",Results,The first column indicates the considered dataset .,relation-classification,2,27,0.5869565217391305,248,0.8406779661016949,27,0.7297297297297297,1,1
250,"Specifically , we use the boundaries and the strict settings .",Results,The first column indicates the considered dataset .,relation-classification,2,28,0.6086956521739131,249,0.8440677966101695,28,0.7567567567567568,1,1
251,"We transform the previous results from to the boundaries setting to make them comparable to our model since in their work , they report token - based F 1 score , which is not a common evaluation metric in relation extraction problems .",Results,The first column indicates the considered dataset .,relation-classification,2,29,0.6304347826086957,250,0.847457627118644,29,0.7837837837837838,1,0
252,"Also , in their work , they focus on identifying only the boundaries of the entities and not the types ( e.g. , Floor , Space ) .",Results,The first column indicates the considered dataset .,relation-classification,2,30,0.6521739130434783,251,0.8508474576271187,30,0.8108108108108109,1,0
253,"In the boundaries evaluation , we achieve ? 3 % improvement for both tasks .",Results,The first column indicates the considered dataset .,relation-classification,2,31,0.6739130434782609,252,0.8542372881355932,31,0.8378378378378378,1,1
254,"This is due to the fact that their quadratic scoring layer is beneficial for the RE task , yet complicates NER , which is usually modeled as a sequence labeling task .",Results,The first column indicates the considered dataset .,relation-classification,2,32,0.6956521739130435,253,0.8576271186440678,32,0.8648648648648649,1,0
255,"Moreover , we report results using the strict evaluation which is used inmost related works .",Results,The first column indicates the considered dataset .,relation-classification,2,33,0.717391304347826,254,0.8610169491525423,33,0.8918918918918919,1,0
256,"Using the prior knowledge that each entity has only one head , we can simplify our model and predict only one head each time ( i.e. , using a softmax loss ) .",Results,The first column indicates the considered dataset .,relation-classification,2,34,0.7391304347826086,255,0.864406779661017,34,0.918918918918919,1,0
257,The difference between the single and the multi-head models is marginal ( < 0.1 % for both tasks ) .,Results,The first column indicates the considered dataset .,relation-classification,2,35,0.7608695652173914,256,0.8677966101694915,35,0.945945945945946,1,0
258,"This shows that our model ( multi-head ) can adapt to various environments , even if the setting is single head ( in terms of the application , and thus also in both training and test data ) .",Results,The first column indicates the considered dataset .,relation-classification,2,36,0.782608695652174,257,0.8711864406779661,36,0.972972972972973,1,0
259,"Finally , we compare our model with previous work",Results,,relation-classification,2,37,0.8043478260869565,258,0.8745762711864407,37,1.0,1,0
260,Analysis of feature contribution,Results,,relation-classification,2,38,0.8260869565217391,259,0.8779661016949153,0,0.0,1,0
261,We conduct ablation tests on the ACE04 dataset reported in to analyze the effectiveness of the various parts of our joint model .,Results,Analysis of feature contribution,relation-classification,2,39,0.8478260869565217,260,0.8813559322033898,1,0.125,1,1
262,The performance of the RE task decreases ( ? 1 % in terms of F 1 score ) when we remove the label embeddings layer and only use the LSTM hidden states as inputs for the RE task .,Results,Analysis of feature contribution,relation-classification,2,40,0.8695652173913043,261,0.8847457627118644,2,0.25,1,1
263,"This shows that the NER labels , as expected , provide meaningful information for the RE component .",Results,Analysis of feature contribution,relation-classification,2,41,0.8913043478260869,262,0.888135593220339,3,0.375,1,0
264,Removing character embeddings also degrades the performance of both NER ( ? 1 % ) and RE ( ? 2 % ) tasks by a relatively large margin .,Results,Analysis of feature contribution,relation-classification,2,42,0.9130434782608696,263,0.8915254237288136,4,0.5,1,1
265,"This illustrates that composing words by the representation of characters is effective , and our method benefits from additional information such as capital letters , suffixes and prefixes within the token ( i.e. , its character sequences ) .",Results,Analysis of feature contribution,relation-classification,2,43,0.9347826086956522,264,0.8949152542372881,5,0.625,1,0
266,"Finally , we conduct experiments for the NER task by removing the CRF loss layer and substituting it with a softmax .",Results,Analysis of feature contribution,relation-classification,2,44,0.9565217391304348,265,0.8983050847457628,6,0.75,1,1
267,"Assuming independent distribution of labels ( i.e. , softmax ) leads to a slight decrease in the F 1 performance of the NER module and a ? 2 % decrease in the performance of the RE task .",Results,Analysis of feature contribution,relation-classification,2,45,0.9782608695652174,266,0.9016949152542372,7,0.875,1,1
268,"This happens because the CRF loss is able to capture the strong tag dependencies ( e.g. , I - LOC can not follow B - PER ) that are present in the dataset instead of just assuming that the tag decision for each token is independent from tag decisions of neighboring tokens .",Results,Analysis of feature contribution,relation-classification,2,46,1.0,267,0.905084745762712,8,1.0,1,0
269,Conclusion,,,relation-classification,2,0,0.0,268,0.9084745762711864,0,0.0,1,0
270,"In this work , we present a joint neural model to simultaneously extract entities and relations from textual data .",Conclusion,Conclusion,relation-classification,2,1,0.0384615384615384,269,0.911864406779661,1,0.0384615384615384,0,0
271,Our model comprises a CRF layer for the entity recognition task and a sigmoid layer for the relation extraction task .,Conclusion,Conclusion,relation-classification,2,2,0.0769230769230769,270,0.9152542372881356,2,0.0769230769230769,0,0
272,"Specifically , we model the relation extraction task as a multi-head selection problem since one entity can have multiple relations .",Conclusion,Conclusion,relation-classification,2,3,0.1153846153846153,271,0.91864406779661,3,0.1153846153846153,0,0
273,"Previous models on this task rely heavily on external NLP tools ( i.e. , POS taggers , dependency parsers ) .",Conclusion,Conclusion,relation-classification,2,4,0.1538461538461538,272,0.9220338983050848,4,0.1538461538461538,0,0
274,"Thus , the performance of these models is affected by the accuracy of the extracted features .",Conclusion,Conclusion,relation-classification,2,5,0.1923076923076923,273,0.9254237288135592,5,0.1923076923076923,0,0
275,"Unlike previous studies , our model produces automatically generated features rather than relying on hand - crafted ones , or existing NLP tools .",Conclusion,Conclusion,relation-classification,2,6,0.2307692307692307,274,0.928813559322034,6,0.2307692307692307,0,0
276,"Given its independence from such NLP or other feature generating tools , our approach can be easily adopted for any language and context .",Conclusion,Conclusion,relation-classification,2,7,0.2692307692307692,275,0.9322033898305084,7,0.2692307692307692,0,0
277,We demonstrate the effectiveness of our approach by conducting a large scale experimental study .,Conclusion,Conclusion,relation-classification,2,8,0.3076923076923077,276,0.9355932203389832,8,0.3076923076923077,0,0
278,Our model is able to outperform neural methods that automatically generate features while the results are marginally similar ( or sometimes better ) compared to feature - based neural network approaches .,Conclusion,Conclusion,relation-classification,2,9,0.3461538461538461,277,0.9389830508474576,9,0.3461538461538461,0,0
279,"As future work , we aim to explore the effectiveness of entity pre-training for the entity recognition module .",Conclusion,Conclusion,relation-classification,2,10,0.3846153846153846,278,0.9423728813559322,10,0.3846153846153846,0,0
280,This approach has been proven beneficial in the work of Miwa & Bansal ( 2016 ) for both the entity and the relation extraction modules .,Conclusion,Conclusion,relation-classification,2,11,0.4230769230769231,279,0.9457627118644067,11,0.4230769230769231,0,0
281,"In addition ,",Conclusion,,relation-classification,2,12,0.4615384615384615,280,0.9491525423728814,12,0.4615384615384615,0,0
282,we are planning to explore away to reduce the calculations in the quadratic relation scoring layer .,Conclusion,"In addition ,",relation-classification,2,13,0.5,281,0.952542372881356,13,0.5,0,0
283,"For instance , a straightforward way to do so is to use in the sigmoid layer only the tokens that have been identified as entities .",Conclusion,"In addition ,",relation-classification,2,14,0.5384615384615384,282,0.9559322033898304,14,0.5384615384615384,0,0
284,"Gupta , P. , . optimize only over the NER task ) , ( ii ) explore several hyperparameters of the network ( e.g. , dropout , LSTM size , character embeddings size ) , and ( iii ) report F 1 score using different word embeddings compared to the embeddings used in previous works .",Conclusion,"In addition ,",relation-classification,2,15,0.5769230769230769,283,0.9593220338983052,15,0.5769230769230769,0,0
285,"In of the main paper , we focused on comparing our model against other joint models that are able to solve the two tasks ( i.e. , NER and relation extraction ) simultaneously , mainly demonstrating superiority of phrasing the relation extraction as a multi-head selection problem ( enabling the extraction of multiple relations at once ) .",Conclusion,"In addition ,",relation-classification,2,16,0.6153846153846154,284,0.9627118644067796,16,0.6153846153846154,0,0
286,"Here , in and vice versa ) .",Conclusion,,relation-classification,2,17,0.6538461538461539,285,0.9661016949152542,17,0.6538461538461539,0,0
287,"Note that improving NER in isolation was not the objective of our multi-head model , but we rather aimed to compare our model against other joint models that solve the task of entity recognition and relation identification simultaneously .",Conclusion,"Here , in and vice versa ) .",relation-classification,2,18,0.6923076923076923,286,0.9694915254237289,18,0.6923076923076923,0,0
288,We thus did not envision to claim or achieve state - of - the - art performance in each of the individual building blocks of our joint model .,Conclusion,"Here , in and vice versa ) .",relation-classification,2,19,0.7307692307692307,287,0.9728813559322034,19,0.7307692307692307,0,0
289,"and A4 show the performance of our model on the test set for different values of the embedding dropout , LSTM layer dropout and the LSTM output dropout hyperparameters , respectively .",Conclusion,"Here , in and vice versa ) .",relation-classification,2,20,0.7692307692307693,288,0.976271186440678,20,0.7692307692307693,0,0
290,"Note that the hyperparameter values used for the results in Section 5 were obtained by tuning over the development set , and these are indicated in boldface in the tables below .",Conclusion,"Here , in and vice versa ) .",relation-classification,2,21,0.8076923076923077,289,0.9796610169491524,21,0.8076923076923077,0,0
291,We vary one hyperparameter at a time in order to assess the effect of a particular hyperparameter .,Conclusion,"Here , in and vice versa ) .",relation-classification,2,22,0.8461538461538461,290,0.9830508474576272,22,0.8461538461538461,0,0
292,"The main outcomes from these tables are twofold : ( i ) low dropout values ( e.g. , 0 , 0.1 ) lead to a performance decrease in the overall F 1 score ( see where a ? 3 % F 1 decrease is reported on the ACE04 dataset ) and ( ii ) average dropout values ( i.e. , 0.2 - 0.4 ) lead to consistently similar results .",Conclusion,"Here , in and vice versa ) .",relation-classification,2,23,0.8846153846153846,291,0.9864406779661016,23,0.8846153846153846,0,0
293,"In In the main results ( see Section 5 ) , to guarantee a fair comparison to previous work and to obtain comparable results that are not affected by the input embeddings , we use embeddings used also in prior studies .",Conclusion,"Here , in and vice versa ) .",relation-classification,2,24,0.9230769230769232,292,0.9898305084745764,24,0.9230769230769232,0,0
294,"To assess the performance of our system to input variations , we also report results using different word embeddings ( see ) ( i.e. , ; Li et al. ) on the ACE04 dataset .",Conclusion,"Here , in and vice versa ) .",relation-classification,2,25,0.9615384615384616,293,0.9932203389830508,25,0.9615384615384616,0,0
295,"Our results showcase that our model , even when using different word embeddings , is still performing better compared to other works that , like ours , do not rely on additional NLP tools .",Conclusion,"Here , in and vice versa ) .",relation-classification,2,26,1.0,294,0.9966101694915256,26,1.0,0,0
1,title,,,relation-classification,3,0,0.0,0,0.0,0,0.0,1,0
2,Adversarial training for multi-context joint entity and relation extraction,title,title,relation-classification,3,1,0.0,1,0.0072992700729927,1,0.0,1,1
3,abstract,,,relation-classification,3,0,0.0,2,0.0145985401459854,0,0.0,1,0
4,Adversarial training ( AT ) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data .,abstract,abstract,relation-classification,3,1,0.3333333333333333,3,0.0218978102189781,1,0.3333333333333333,1,0
5,We show how to use AT for the tasks of entity recognition and relation extraction .,abstract,abstract,relation-classification,3,2,0.6666666666666666,4,0.0291970802919708,2,0.6666666666666666,1,1
6,"In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations , allows improving the state - of - the - art effectiveness on several datasets in different contexts ( i.e. , news , biomedical , and real estate data ) and for different languages ( English and Dutch ) .",abstract,abstract,relation-classification,3,3,1.0,5,0.0364963503649635,3,1.0,1,1
7,Introduction,,,relation-classification,3,0,0.0,6,0.0437956204379562,0,0.0,1,0
8,"Many neural network methods have recently been exploited in various natural language processing ( NLP ) tasks , such as parsing , POS tagging , relation extraction , translation , and joint tasks .",Introduction,Introduction,relation-classification,3,1,0.1,7,0.0510948905109489,1,0.1,1,0
9,"However , observed that intentional small scale perturbations ( i.e. , adversarial examples ) to the input of such models may lead to incorrect decisions ( with high confidence ) .",Introduction,Introduction,relation-classification,3,2,0.2,8,0.0583941605839416,2,0.2,1,0
10,proposed adversarial training ( AT ) ( for image recognition ) as a regularization method which uses a mixture of clean and adversarial examples to enhance the robustness of the model .,Introduction,Introduction,relation-classification,3,3,0.3,9,0.0656934306569343,3,0.3,1,0
11,"Although AT has recently been applied in NLP tasks ( e.g. , text classification ) , this paper - to the best of our knowledge - is the first attempt investigating regularization effects of AT in a joint setting for two related tasks .",Introduction,Introduction,relation-classification,3,4,0.4,10,0.072992700729927,4,0.4,1,0
12,We start from a baseline joint model that performs the tasks of named entity recognition and relation extraction at once .,Introduction,Introduction,relation-classification,3,5,0.5,11,0.0802919708029197,5,0.5,1,0
13,"Previously proposed models ( summarized in Section 2 ) exhibit several issues that the neural network - based baseline approach ( detailed in Section 3.1 ) overcomes : ( i ) our model uses automatically extracted features without the need of external parsers nor manually extracted features ( see ; ; ) , ( ii ) all entities and the corresponding relations within the sentence are extracted at once , instead of examining one pair of entities at a time ( see ) , and ( iii ) we model relation extraction in a multi-label setting , allowing multiple relations per entity ( see ; ) .",Introduction,Introduction,relation-classification,3,6,0.6,12,0.0875912408759124,6,0.6,1,0
14,The core contribution of the paper is the use of AT as an extension in the training procedure for the joint extraction task ( Section 3.2 ) .,Introduction,Introduction,relation-classification,3,7,0.7,13,0.0948905109489051,7,0.7,1,0
15,"To evaluate the proposed AT method , we perform a large scale experimental study in this joint task ( see Section 4 ) , using datasets from different contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) .",Introduction,Introduction,relation-classification,3,8,0.8,14,0.1021897810218978,8,0.8,1,0
16,"We use a strong baseline that outperforms all previous models that rely on automatically extracted features , achieving state - of - the - art performance ( Section 5 ) .",Introduction,Introduction,relation-classification,3,9,0.9,15,0.1094890510948905,9,0.9,1,0
17,"Compared to the baseline model , applying AT during training leads to a consistent additional increase in joint extraction effectiveness .",Introduction,Introduction,relation-classification,3,10,1.0,16,0.1167883211678832,10,1.0,1,0
18,Related work,,,relation-classification,3,0,0.0,17,0.1240875912408759,0,0.0,1,0
19,Joint entity and relation extraction :,Related work,Related work,relation-classification,3,1,0.0666666666666666,18,0.1313868613138686,1,0.0666666666666666,0,0
20,Joint models that are based on manually extracted features have been proposed for performing both the named entity recognition ( NER ) and relation extraction subtasks at once .,Related work,Related work,relation-classification,3,2,0.1333333333333333,19,0.1386861313868613,2,0.1333333333333333,0,0
21,"These methods rely on the availability of NLP tools ( e.g. , POS taggers ) or manually designed features leading to additional complexity .",Related work,Related work,relation-classification,3,3,0.2,20,0.145985401459854,3,0.2,0,0
22,Neural network methods have been exploited to overcome this feature design issue and usually involve RNNs and CNNs .,Related work,Related work,relation-classification,3,4,0.2666666666666666,21,0.1532846715328467,4,0.2666666666666666,0,0
23,"Specifically , as well as apply bidirectional tree - structured RNNs for different contexts ( i.e. , news , biomedical ) to capture syntactic information ( using external dependency parsers ) .",Related work,Related work,relation-classification,3,5,0.3333333333333333,22,0.1605839416058394,5,0.3333333333333333,0,0
24,propose the use of various manually extracted features along with RNNs .,Related work,Related work,relation-classification,3,6,0.4,23,0.1678832116788321,6,0.4,0,0
25,"solve the simpler problem of entity classification ( EC , assuming entity boundaries are given ) , instead of NER , and they replicate the context around the entities , feeding entity pairs to the relation extraction layer .",Related work,Related work,relation-classification,3,7,0.4666666666666667,24,0.1751824817518248,7,0.4666666666666667,0,0
26,investigate RNNs with attention without taking into account that relation labels are not mutually exclusive .,Related work,Related work,relation-classification,3,8,0.5333333333333333,25,0.1824817518248175,8,0.5333333333333333,0,0
27,"Finally , use LSTMs in a joint model for extracting just one relation at a time , but increase the complexity of the NER part .",Related work,Related work,relation-classification,3,9,0.6,26,0.1897810218978102,9,0.6,0,0
28,Our baseline model enables simultaneous extraction of multiple relations from the same input .,Related work,Related work,relation-classification,3,10,0.6666666666666666,27,0.1970802919708029,10,0.6666666666666666,0,0
29,"Then , we further extend this strong baseline using adversarial training .",Related work,Related work,relation-classification,3,11,0.7333333333333333,28,0.2043795620437956,11,0.7333333333333333,0,0
30,Adversarial training ( AT ) has been proposed to make classifiers more robust to input perturbations in the context of image recognition .,Related work,Related work,relation-classification,3,12,0.8,29,0.2116788321167883,12,0.8,0,0
31,"In the context of NLP , several variants have been proposed for different tasks such as text classification , relation extraction and POS tagging .",Related work,Related work,relation-classification,3,13,0.8666666666666667,30,0.218978102189781,13,0.8666666666666667,0,0
32,AT is considered as a regularization method .,Related work,Related work,relation-classification,3,14,0.9333333333333332,31,0.2262773722627737,14,0.9333333333333332,0,0
33,"Unlike other regularization methods ( i.e. , dropout , word dropout ) that introduce random noise , AT generates perturbations that are variations of examples easily misclassified by the model .",Related work,Related work,relation-classification,3,15,1.0,32,0.2335766423357664,15,1.0,0,0
34,Model,,,relation-classification,3,0,0.0,33,0.2408759124087591,0,0.0,1,0
35,Joint learning as head selection,Model,Model,relation-classification,3,1,0.0222222222222222,34,0.2481751824817518,0,0.0,1,0
36,"The baseline model , described in detail in , is illustrated in .",Model,Model,relation-classification,3,2,0.0444444444444444,35,0.2554744525547445,1,0.0227272727272727,1,1
37,It aims to detect ( i ) the type and the boundaries of the entities and ( ii ) the relations between them .,Model,Model,relation-classification,3,3,0.0666666666666666,36,0.2627737226277372,2,0.0454545454545454,1,1
38,"The input is a sequence of tokens ( i.e. , sentence ) w = w 1 , ... , w n .",Model,Model,relation-classification,3,4,0.0888888888888888,37,0.2700729927007299,3,0.0681818181818181,1,1
39,"We use character level embeddings to implicitly capture morphological features ( e.g. , prefixes and suffixes ) , representing each character by a vector ( embedding ) .",Model,Model,relation-classification,3,5,0.1111111111111111,38,0.2773722627737226,4,0.0909090909090909,1,1
40,The character embeddings are fed to a bidirectional LSTM ( BiLSTM ) to obtain the character - based representation of the word .,Model,Model,relation-classification,3,6,0.1333333333333333,39,0.2846715328467153,5,0.1136363636363636,1,1
41,We also use pre-trained word embeddings .,Model,Model,relation-classification,3,7,0.1555555555555555,40,0.291970802919708,6,0.1363636363636363,1,1
42,"Word and character embeddings are concatenated to form the final token representation , which is then fed to a BiLSTM layer to extract sequential information .",Model,Model,relation-classification,3,8,0.1777777777777777,41,0.2992700729927007,7,0.1590909090909091,1,1
43,"For the NER task , we adopt the BIO ( Beginning , Inside , Outside ) encoding scheme .",Model,Model,relation-classification,3,9,0.2,42,0.3065693430656934,8,0.1818181818181818,1,1
44,"In , the B - PER tag is assigned to the beginning token of a ' person ' ( PER ) entity .",Model,Model,relation-classification,3,10,0.2222222222222222,43,0.3138686131386861,9,0.2045454545454545,1,0
45,"For the prediction of the entity tags , we use : ( i ) a softmax approach for the entity classification ( EC ) task ( assuming entity boundaries given ) or ( ii ) a CRF approach where we identify both the type and the boundaries for each entity .",Model,Model,relation-classification,3,11,0.2444444444444444,44,0.3211678832116788,10,0.2272727272727272,1,0
46,"During decoding , in the softmax setting , we greedily detect the entity types of the tokens ( i.e. , independent prediction ) .",Model,Model,relation-classification,3,12,0.2666666666666666,45,0.3284671532846715,11,0.25,1,0
47,"Although independent distribution of types is reasonable for EC tasks , this is not the case when there are strong correlations between neighboring tags .",Model,Model,relation-classification,3,13,0.2888888888888888,46,0.3357664233576642,12,0.2727272727272727,1,0
48,"For instance , the BIO encoding scheme imposes several constraints in the NER task ( e.g. , the B - PER and I - LOC tags can not be sequential ) .",Model,Model,relation-classification,3,14,0.3111111111111111,47,0.3430656934306569,13,0.2954545454545454,1,0
49,"Motivated by this intuition , we use a linear - chain CRF for the NER task .",Model,Model,relation-classification,3,15,0.3333333333333333,48,0.3503649635036496,14,0.3181818181818182,1,0
50,"For decoding , in the CRF setting , we use the Viterbi algorithm .",Model,Model,relation-classification,3,16,0.3555555555555555,49,0.3576642335766423,15,0.3409090909090909,1,0
51,"During training , for both EC ( softmax ) and NER tasks ( CRF ) , we minimize the cross - entropy loss L NER .",Model,Model,relation-classification,3,17,0.3777777777777777,50,0.364963503649635,16,0.3636363636363636,1,0
52,"The entity tags are later fed into the relation extraction layer as label embeddings ( see ) , assuming that knowledge of the entity types is beneficial in predicting the relations between the involved entities .",Model,Model,relation-classification,3,18,0.4,51,0.3722627737226277,17,0.3863636363636363,1,0
53,We model the relation extraction task as a multi-label head selection problem .,Model,Model,relation-classification,3,19,0.4222222222222222,52,0.3795620437956204,18,0.4090909090909091,1,1
54,"In our model , each word w i can be involved in multiple relations with other words .",Model,Model,relation-classification,3,20,0.4444444444444444,53,0.3868613138686131,19,0.4318181818181818,1,0
55,"For instance , in the example illustrated in , "" Smith "" could be involved not only in a Lives in relation with the token "" California "" ( head ) but also in other relations simultaneously ( e.g. , Works for , Born In with some corresponding tokens ) .",Model,Model,relation-classification,3,21,0.4666666666666667,54,0.3941605839416058,20,0.4545454545454545,1,0
56,"The goal of the task is to predict for each word w i , a vector of heads ?",Model,Model,relation-classification,3,22,0.4888888888888889,55,0.4014598540145985,21,0.4772727272727273,1,0
57,i and the vector of corresponding relationsr i .,Model,Model,relation-classification,3,23,0.5111111111111111,56,0.4087591240875912,22,0.5,1,0
58,"We compute the score s ( w j , w i , r k ) of word w j to be the head of w i given a relation label r k using a single layer neural network .",Model,Model,relation-classification,3,24,0.5333333333333333,57,0.4160583941605839,23,0.5227272727272727,1,0
59,"The corresponding probability is defined as : P ( w j , r k | w i ; ? ) = ? ( s ( w j , w i , r k ) ) , where ?( . ) is the sigmoid function .",Model,Model,relation-classification,3,25,0.5555555555555556,58,0.4233576642335766,24,0.5454545454545454,1,0
60,"During training , we minimize the cross - entropy loss L rel as :",Model,Model,relation-classification,3,26,0.5777777777777777,59,0.4306569343065693,25,0.5681818181818182,1,0
61,where m is the number of associated heads ( and thus relations ) per word w i .,Model,Model,relation-classification,3,27,0.6,60,0.437956204379562,26,0.5909090909090909,1,0
62,"During decoding , the most probable heads and relations are selected using threshold - based prediction .",Model,Model,relation-classification,3,28,0.6222222222222222,61,0.4452554744525547,27,0.6136363636363636,1,0
63,The final objective for the joint task is computed as L JOINT ( w ; ? ) = L NER + L rel where ?,Model,Model,relation-classification,3,29,0.6444444444444445,62,0.4525547445255474,28,0.6363636363636364,1,0
64,is a set of parameters .,Model,Model,relation-classification,3,30,0.6666666666666666,63,0.4598540145985401,29,0.6590909090909091,1,0
65,"In the case of multi-token entities , only the last token of the entity can serve as head of another token , to eliminate redundant relations .",Model,Model,relation-classification,3,31,0.6888888888888889,64,0.4671532846715328,30,0.6818181818181818,1,0
66,"If an entity is not involved in any relation , we predict the auxiliary "" N "" relation label and the token itself as head .",Model,Model,relation-classification,3,32,0.7111111111111111,65,0.4744525547445255,31,0.7045454545454546,1,0
67,Adversarial training ( AT ),Model,Model,relation-classification,3,33,0.7333333333333333,66,0.4817518248175182,32,0.7272727272727273,1,0
68,We exploit the idea of AT as a regularization method to make our model robust to input perturbations .,Model,Model,relation-classification,3,34,0.7555555555555555,67,0.4890510948905109,33,0.75,1,1
69,"Specifically , we generate examples which are variations of the original ones by adding some noise at the level of the concatenated word representation .",Model,Model,relation-classification,3,35,0.7777777777777778,68,0.4963503649635036,34,0.7727272727272727,1,0
70,This is similar to the concept introduced by to improve the robustness of image recognition classifiers .,Model,Model,relation-classification,3,36,0.8,69,0.5036496350364964,35,0.7954545454545454,1,0
71,We generate an adversarial example by adding the worst - case perturbation ?,Model,Model,relation-classification,3,37,0.8222222222222222,70,0.5109489051094891,36,0.8181818181818182,1,0
72,adv to the original embedding w that maximizes the loss function :,Model,Model,relation-classification,3,38,0.8444444444444444,71,0.5182481751824818,37,0.8409090909090909,1,0
73,where ?,Model,Model,relation-classification,3,39,0.8666666666666667,72,0.5255474452554745,38,0.8636363636363636,1,0
74,is a copy of the current model parameters .,Model,Model,relation-classification,3,40,0.8888888888888888,73,0.5328467153284672,39,0.8863636363636364,1,0
75,Since Eq.,Model,,relation-classification,3,41,0.9111111111111112,74,0.5401459854014599,40,0.9090909090909092,1,0
76,"( 2 ) is intractable in neural networks , we use the approximation proposed in defined as : ? adv = g/ g , with g = ? w L JOINT ( w ; ? ) , where is a small bounded norm treated as a hyperparameter .",Model,Since Eq.,relation-classification,3,42,0.9333333333333332,75,0.5474452554744526,41,0.9318181818181818,1,0
77,"Similar to , we set to be ? ?",Model,Since Eq.,relation-classification,3,43,0.9555555555555556,76,0.5547445255474452,42,0.9545454545454546,1,0
78,D ( where Dis the dimension of the embeddings ) .,Model,Since Eq.,relation-classification,3,44,0.9777777777777776,77,0.5620437956204379,43,0.9772727272727272,1,0
79,"We train on the mixture of original and adversarial examples , so the final loss is computed as : L JOINT ( w ; ? ) + L JOINT ( w + ? adv ;? ) .",Model,Since Eq.,relation-classification,3,45,1.0,78,0.5693430656934306,44,1.0,1,0
80,Experimental setup,,,relation-classification,3,0,0.0,79,0.5766423357664233,0,0.0,1,0
81,"We evaluate our models on four datasets , using the code as available from our github codebase .",Experimental setup,Experimental setup,relation-classification,3,1,0.0217391304347826,80,0.583941605839416,1,0.0217391304347826,1,1
82,1,Experimental setup,Experimental setup,relation-classification,3,2,0.0434782608695652,81,0.5912408759124088,2,0.0434782608695652,1,0
83,"Specifically , we follow the 5 - fold crossvalidation defined by for the ACE04 dataset .",Experimental setup,Experimental setup,relation-classification,3,3,0.0652173913043478,82,0.5985401459854015,3,0.0652173913043478,1,0
84,"For the CoNLL04 ) EC task ( assuming boundaries are given ) , we use the same splits as in ; .",Experimental setup,Experimental setup,relation-classification,3,4,0.0869565217391304,83,0.6058394160583942,4,0.0869565217391304,1,0
85,We also evaluate our models on the NER task similar to in the same dataset using 10 - fold cross validation .,Experimental setup,Experimental setup,relation-classification,3,5,0.108695652173913,84,0.6131386861313869,5,0.108695652173913,1,1
86,"For the Dutch Real Estate Classifieds , DREC ( Bekoulis et al. , 2017 ) dataset , we use train - test splits as in .",Experimental setup,Experimental setup,relation-classification,3,6,0.1304347826086956,85,0.6204379562043796,6,0.1304347826086956,1,0
87,"For the Adverse Drug Events , ADE , we perform 10 - fold cross -validation similar to .",Experimental setup,Experimental setup,relation-classification,3,7,0.1521739130434782,86,0.6277372262773723,7,0.1521739130434782,1,0
88,"To obtain comparable results that are not affected by the input embeddings , we use the embeddings of the previous works .",Experimental setup,Experimental setup,relation-classification,3,8,0.1739130434782608,87,0.635036496350365,8,0.1739130434782608,1,0
89,We employ early stopping in all of the experiments .,Experimental setup,Experimental setup,relation-classification,3,9,0.1956521739130435,88,0.6423357664233577,9,0.1956521739130435,1,1
90,"We use the Adam optimizer and we fix the hyperparameters ( i.e. , ? , dropout values , best epoch , learning rate ) on the validation sets .",Experimental setup,Experimental setup,relation-classification,3,10,0.217391304347826,89,0.6496350364963503,10,0.217391304347826,1,1
91,The scaling parameter ?,Experimental setup,Experimental setup,relation-classification,3,11,0.2391304347826087,90,0.656934306569343,11,0.2391304347826087,1,0
92,"is selected from { 5 e?2 , 1 e ? 2 , 1 e ? 3 , 1e?4 } .",Experimental setup,Experimental setup,relation-classification,3,12,0.2608695652173913,91,0.6642335766423357,12,0.2608695652173913,1,0
93,"Larger values of ? ( i.e. , larger perturbations ) lead to consistent performance decrease in our early experiments .",Experimental setup,Experimental setup,relation-classification,3,13,0.2826086956521739,92,0.6715328467153284,13,0.2826086956521739,1,0
94,This can be explained from the fact that adding more noise can change the content of the sentence as also reported by .,Experimental setup,Experimental setup,relation-classification,3,14,0.3043478260869565,93,0.6788321167883211,14,0.3043478260869565,1,0
95,"We use three types of evaluation , namely : ( i ) S( trict ) : we score an entity as correct if both the entity boundaries and the entity type are correct ( ACE04 , ADE , CoNLL04 , DREC ) , ( ii ) B ( oundaries ) : we score an entity as correct if only the entity boundaries are correct while the entity type is not taken into account ( DREC ) and ( iii ) R( elaxed ) : a multi-token entity is considered correct if at least one correct type is assigned to the tokens comprising the entity , assuming that the : Comparison of our method with the stateof - the - art in terms of F 1 score .",Experimental setup,Experimental setup,relation-classification,3,15,0.3260869565217391,94,0.6861313868613139,15,0.3260869565217391,1,1
96,"The proposed models are : ( i ) baseline , ( ii ) baseline EC ( predicts only entity classes ) and ( iii ) baseline ( EC ) + AT ( regularized by AT ) .",Experimental setup,Experimental setup,relation-classification,3,16,0.3478260869565217,95,0.6934306569343066,16,0.3478260869565217,1,0
97,The and symbols indicate whether the models rely on external NLP tools .,Experimental setup,Experimental setup,relation-classification,3,17,0.3695652173913043,96,0.7007299270072993,17,0.3695652173913043,1,0
98,"We include different evaluation types ( S , Rand B ) .",Experimental setup,Experimental setup,relation-classification,3,18,0.391304347826087,97,0.708029197080292,18,0.391304347826087,1,0
99,"boundaries are known ( CoNLL04 ) , to compare to previous works .",Experimental setup,Experimental setup,relation-classification,3,19,0.4130434782608695,98,0.7153284671532847,19,0.4130434782608695,1,0
100,"In all cases , a relation is considered as correct when both the relation type and the argument entities are correct .",Experimental setup,Experimental setup,relation-classification,3,20,0.4347826086956521,99,0.7226277372262774,20,0.4347826086956521,1,0
101,shows our experimental results .,Experimental setup,Experimental setup,relation-classification,3,21,0.4565217391304347,100,0.7299270072992701,21,0.4565217391304347,1,0
102,The name of the dataset is presented in the first column while the models are listed in the second column .,Experimental setup,Experimental setup,relation-classification,3,22,0.4782608695652174,101,0.7372262773722628,22,0.4782608695652174,1,0
103,"The proposed models are the following : ( i ) baseline : the baseline model shown in with the CRF layer and the sigmoid loss , ( ii ) baseline EC : the proposed model with the softmax layer for EC , ( iii ) baseline ( EC ) + AT : the baseline regularized using AT .",Experimental setup,Experimental setup,relation-classification,3,23,0.5,102,0.7445255474452555,23,0.5,1,0
104,The final three columns present the F 1 results for the two subtasks and their average performance .,Experimental setup,Experimental setup,relation-classification,3,24,0.5217391304347826,103,0.7518248175182481,24,0.5217391304347826,1,0
105,Bold values indicate the best results among models that use only automatically extracted features .,Experimental setup,Experimental setup,relation-classification,3,25,0.5434782608695652,104,0.7591240875912408,25,0.5434782608695652,1,0
106,"For ACE04 , the baseline outperforms by ? 2 % in both tasks .",Experimental setup,Experimental setup,relation-classification,3,26,0.5652173913043478,105,0.7664233576642335,26,0.5652173913043478,1,1
107,"This improvement can be explained by the use of : ( i ) multi-label head selection , ( ii ) CRF - layer and ( iii ) character level embeddings .",Experimental setup,Experimental setup,relation-classification,3,27,0.5869565217391305,106,0.7737226277372263,27,0.5869565217391305,1,0
108,"Compared to , who rely on NLP tools , the baseline performs within a reasonable margin ( less than 1 % ) on the joint task .",Experimental setup,Experimental setup,relation-classification,3,28,0.6086956521739131,107,0.781021897810219,28,0.6086956521739131,1,0
109,"On the other hand , use the same model for the ADE biomedical dataset , where we report a 2.5 % overall improvement .",Experimental setup,Experimental setup,relation-classification,3,29,0.6304347826086957,108,0.7883211678832117,29,0.6304347826086957,1,0
110,This indicates that NLP tools are not always accurate for various contexts .,Experimental setup,Experimental setup,relation-classification,3,30,0.6521739130434783,109,0.7956204379562044,30,0.6521739130434783,1,0
111,"For the CoNLL04 dataset , we use two evaluation settings .",Experimental setup,Experimental setup,relation-classification,3,31,0.6739130434782609,110,0.8029197080291971,31,0.6739130434782609,1,1
112,We use the relaxed evaluation similar to ; on the EC task .,Experimental setup,Experimental setup,relation-classification,3,32,0.6956521739130435,111,0.8102189781021898,32,0.6956521739130435,1,0
113,"The baseline model outperforms the state - of - the - art models that do not rely on manually extracted features ( > 4 % improvement for both tasks ) , since we directly model the whole sentence , instead of just considering pairs of entities .",Experimental setup,Experimental setup,relation-classification,3,33,0.717391304347826,112,0.8175182481751825,33,0.717391304347826,1,1
114,"Moreover , compared to the model of that relies on complex features , the baseline model performs within a margin of 1 % in terms of overall F 1 score .",Experimental setup,Experimental setup,relation-classification,3,34,0.7391304347826086,113,0.8248175182481752,34,0.7391304347826086,1,0
115,"We also report NER results on the same dataset and improve overall F 1 score with ? 1 % compared to , indicating that our automatically extracted features are more informative than the hand - crafted ones .",Experimental setup,Experimental setup,relation-classification,3,35,0.7608695652173914,114,0.8321167883211679,35,0.7608695652173914,1,0
116,These automatically extracted features exhibit their performance improvement mainly due to the shared LSTM layer that learns to automatically generate feature representations of entities and their corresponding relations within a single model .,Experimental setup,Experimental setup,relation-classification,3,36,0.782608695652174,115,0.8394160583941606,36,0.782608695652174,1,0
117,"For the DREC dataset , we use two evaluation methods .",Experimental setup,Experimental setup,relation-classification,3,37,0.8043478260869565,116,0.8467153284671532,37,0.8043478260869565,1,1
118,"In the boundaries evaluation , the baseline has an improvement of ? 3 % on both tasks compared to , whose quadratic scoring layer complicates NER .",Experimental setup,Experimental setup,relation-classification,3,38,0.8260869565217391,117,0.8540145985401459,38,0.8260869565217391,1,1
119,and show the effectiveness of the adversarial training on top of the baseline model .,Experimental setup,Experimental setup,relation-classification,3,39,0.8478260869565217,118,0.8613138686131386,39,0.8478260869565217,1,1
120,"In all of the experiments , AT improves the predictive performance of the baseline model in the joint setting .",Experimental setup,Experimental setup,relation-classification,3,40,0.8695652173913043,119,0.8686131386861314,40,0.8695652173913043,1,1
121,"Moreover , as seen in , the performance of the models using AT is closer to maximum even from the early training epochs .",Experimental setup,Experimental setup,relation-classification,3,41,0.8913043478260869,120,0.8759124087591241,41,0.8913043478260869,1,0
122,"Specifically , for ACE04 , there is an improvement in both tasks as well as in the overall F 1 performance ( 0.4 % ) .",Experimental setup,Experimental setup,relation-classification,3,42,0.9130434782608696,121,0.8832116788321168,42,0.9130434782608696,1,1
123,"For CoNLL04 , we note an improvement in the overall F 1 of 0.4 % for the EC and 0.8 % for the NER tasks , respectively .",Experimental setup,Experimental setup,relation-classification,3,43,0.9347826086956522,122,0.8905109489051095,43,0.9347826086956522,1,1
124,"For the DREC dataset , in both settings , there is an overall improvement of ? 1 % .",Experimental setup,Experimental setup,relation-classification,3,44,0.9565217391304348,123,0.8978102189781022,44,0.9565217391304348,1,1
125,"shows that from the first epochs , the model obtains its maximum performance on the DREC validation set .",Experimental setup,Experimental setup,relation-classification,3,45,0.9782608695652174,124,0.9051094890510948,45,0.9782608695652174,1,0
126,"Finally , for ADE , our AT model beats the baseline F 1 by 0.7 % .",Experimental setup,Experimental setup,relation-classification,3,46,1.0,125,0.9124087591240876,46,1.0,1,1
127,Results,,,relation-classification,3,0,0.0,126,0.9197080291970804,0,0.0,1,0
128,"Our results demonstrate that AT outperforms the neural baseline model consistently , considering our experiments across multiple and more diverse datasets than typical related works .",Results,Results,relation-classification,3,1,0.1666666666666666,127,0.927007299270073,1,0.1666666666666666,1,0
129,The im - provement of AT over our baseline ( depending on the dataset ) ranges from ? 0.4 % to ? 0.9 % in terms of overall F 1 score .,Results,Results,relation-classification,3,2,0.3333333333333333,128,0.9343065693430656,2,0.3333333333333333,1,0
130,"This seemingly small performance increase is mainly due to the limited performance benefit for the NER component , which is in accordance with the recent advances in NER using neural networks that report similarly small gains ( e.g. , the performance improvement in and on the CoNLL - 2003 test set is 0.01 % and 0.17 % F 1 percentage points , while in the work of , a 0.07 % F 1 improvement on CoNLL - 2000 using AT for NER is reported ) .",Results,Results,relation-classification,3,3,0.5,129,0.9416058394160584,3,0.5,1,0
131,"However , the relation extraction performance increases by ? 1 % F 1 scoring points , except for the ACE04 dataset .",Results,Results,relation-classification,3,4,0.6666666666666666,130,0.948905109489051,4,0.6666666666666666,1,0
132,"Further , as seen in , the improvement for CoNLL04 is particularly small on the evaluation set .",Results,Results,relation-classification,3,5,0.8333333333333334,131,0.9562043795620438,5,0.8333333333333334,1,0
133,"This may indicate a correlation between the dataset size and the benefit of adversarial training in the context of joint models , but this needs further investigation in future work .",Results,Results,relation-classification,3,6,1.0,132,0.9635036496350364,6,1.0,1,0
134,Conclusion,,,relation-classification,3,0,0.0,133,0.9708029197080292,0,0.0,1,0
135,We proposed to use adversarial training ( AT ) for the joint task of entity recognition and relation extraction .,Conclusion,Conclusion,relation-classification,3,1,0.3333333333333333,134,0.978102189781022,1,0.3333333333333333,0,0
136,"The contribution of this study is twofold : ( i ) investigation of the consistent effectiveness of AT as a regularization method over a multi-context baseline joint model , with ( ii ) a large scale experimental evaluation .",Conclusion,Conclusion,relation-classification,3,2,0.6666666666666666,135,0.9854014598540146,2,0.6666666666666666,0,0
137,"Experiments show that AT improves the results for each task separately , as well as the overall performance of the baseline joint model , while reaching high performance already during the first epochs of the training procedure .",Conclusion,Conclusion,relation-classification,3,3,1.0,136,0.9927007299270072,3,1.0,0,0
1,title,,,relation-classification,4,0,0.0,0,0.0,0,0.0,1,0
2,Graph Convolution over Pruned Dependency Trees Improves Relation Extraction,title,,relation-classification,4,1,0.0,1,0.0038022813688212,1,0.0,1,1
3,abstract,,,relation-classification,4,0,0.0,2,0.0076045627376425,0,0.0,1,0
4,Dependency trees help relation extraction models capture long - range relations between words .,abstract,abstract,relation-classification,4,1,0.125,3,0.0114068441064638,1,0.125,1,1
5,"However , existing dependency - based models either neglect crucial information ( e.g. , negation ) by pruning the dependency trees too aggressively , or are computationally inefficient because it is difficult to parallelize over different tree structures .",abstract,abstract,relation-classification,4,2,0.25,4,0.0152091254752851,2,0.25,1,0
6,"We propose an extension of graph convolutional networks that is tailored for relation extraction , which pools information over arbitrary dependency structures efficiently in parallel .",abstract,abstract,relation-classification,4,3,0.375,5,0.0190114068441064,3,0.375,1,0
7,"To incorporate relevant information while maximally removing irrelevant content , we further apply a novel pruning strategy to the input trees by keeping words immediately around the shortest path between the two entities among which a relation might hold .",abstract,abstract,relation-classification,4,4,0.5,6,0.0228136882129277,4,0.5,1,0
8,"The resulting model achieves state - of - the - art performance on the large - scale TACRED dataset , outperforming existing sequence and dependency - based neural models .",abstract,abstract,relation-classification,4,5,0.625,7,0.026615969581749,5,0.625,1,0
9,"We also show through detailed analysis that this model has complementary strengths to sequence models , and combining them further improves the state of the art .",abstract,abstract,relation-classification,4,6,0.75,8,0.0304182509505703,6,0.75,1,0
10,* Equal contribution .,abstract,abstract,relation-classification,4,7,0.875,9,0.0342205323193916,7,0.875,1,0
11,The order of authorship was decided by a tossed coin .,abstract,abstract,relation-classification,4,8,1.0,10,0.0380228136882129,8,1.0,1,0
12,Introduction,,,relation-classification,4,0,0.0,11,0.0418250950570342,0,0.0,1,0
13,"Relation extraction involves discerning whether a relation exists between two entities in a sentence ( often termed subject and object , respectively ) .",Introduction,Introduction,relation-classification,4,1,0.0434782608695652,12,0.0456273764258555,1,0.0434782608695652,1,0
14,"Successful relation extraction is the cornerstone of applications requiring relational understanding of unstructured text on a large scale , such as question answering , knowledge base population , and biomedical knowledge discovery .",Introduction,Introduction,relation-classification,4,2,0.0869565217391304,13,0.0494296577946768,2,0.0869565217391304,1,0
15,"Models making use of dependency parses of the input sentences , or dependency - based models , : An example modified from the TAC KBP challenge corpus .",Introduction,Introduction,relation-classification,4,3,0.1304347826086956,14,0.053231939163498,3,0.1304347826086956,1,0
16,"A subtree of the original UD dependency tree between the subject ( "" he "" ) and object ( "" Mike Cane "" ) is also shown , where the shortest dependency path between the entities is highlighted in bold .",Introduction,Introduction,relation-classification,4,4,0.1739130434782608,15,0.0570342205323193,4,0.1739130434782608,1,0
17,"Note that negation ( "" not "" ) is off the dependency path .",Introduction,Introduction,relation-classification,4,5,0.217391304347826,16,0.0608365019011406,5,0.217391304347826,1,0
18,"have proven to be very effective in relation extraction , because they capture long - range syntactic relations that are obscure from the surface form alone ( e.g. , when long clauses or complex scoping are present ) .",Introduction,Introduction,relation-classification,4,6,0.2608695652173913,17,0.0646387832699619,6,0.2608695652173913,1,1
19,Traditional feature - based models are able to represent dependency information by featurizing dependency trees as overlapping paths along the trees .,Introduction,Introduction,relation-classification,4,7,0.3043478260869565,18,0.0684410646387832,7,0.3043478260869565,1,0
20,"However , these models face the challenge of sparse feature spaces and are brittle to lexical variations .",Introduction,Introduction,relation-classification,4,8,0.3478260869565217,19,0.0722433460076045,8,0.3478260869565217,1,0
21,More recent neural models address this problem with distributed representations built from their computation graphs formed along parse trees .,Introduction,Introduction,relation-classification,4,9,0.391304347826087,20,0.0760456273764258,9,0.391304347826087,1,0
22,One common approach to leverage dependency information is to perform bottom - up or top - down computation along the parse tree or the subtree below the lowest common ancestor ( LCA ) of the entities .,Introduction,Introduction,relation-classification,4,10,0.4347826086956521,21,0.0798479087452471,10,0.4347826086956521,1,0
23,"Another popular approach , inspired by , is to reduce the parse tree to the shortest dependency path between the entities .",Introduction,Introduction,relation-classification,4,11,0.4782608695652174,22,0.0836501901140684,11,0.4782608695652174,1,0
24,"However , these models suffer from several drawbacks .",Introduction,Introduction,relation-classification,4,12,0.5217391304347826,23,0.0874524714828897,12,0.5217391304347826,1,0
25,"Neural models operating directly on parse trees are usually difficult to parallelize and thus computationally inefficient , because aligning trees for efficient batch training is usually nontrivial .",Introduction,Introduction,relation-classification,4,13,0.5652173913043478,24,0.091254752851711,13,0.5652173913043478,1,0
26,"Models based on the shortest dependency path between the subject and object are computationally more efficient , but this simplifying assumption has major limitations as well .",Introduction,Introduction,relation-classification,4,14,0.6086956521739131,25,0.0950570342205323,14,0.6086956521739131,1,0
27,"shows a real - world example where crucial information ( i.e. , negation ) would be excluded when the model is restricted to only considering the dependency path .",Introduction,Introduction,relation-classification,4,15,0.6521739130434783,26,0.0988593155893536,15,0.6521739130434783,1,0
28,"In this work , we propose a novel extension of the graph convolutional network ) that is tailored for relation extraction .",Introduction,Introduction,relation-classification,4,16,0.6956521739130435,27,0.1026615969581749,16,0.6956521739130435,1,0
29,"Our model encodes the dependency structure over the input sentence with efficient graph convolution operations , then extracts entity - centric representations to make robust relation predictions .",Introduction,Introduction,relation-classification,4,17,0.7391304347826086,28,0.1064638783269961,17,0.7391304347826086,1,1
30,"We also apply a novel path - centric pruning technique to remove irrelevant information from the tree while maximally keeping relevant content , which further improves the performance of several dependencybased models including ours .",Introduction,Introduction,relation-classification,4,18,0.782608695652174,29,0.1102661596958174,18,0.782608695652174,1,1
31,"We test our model on the popular SemEval 2010 Task 8 dataset and the more recent , larger TAC - RED dataset .",Introduction,Introduction,relation-classification,4,19,0.8260869565217391,30,0.1140684410646387,19,0.8260869565217391,1,0
32,"On both datasets , our model not only outperforms existing dependency - based neural models by a significant margin when combined with the new pruning technique , but also achieves a 10 - 100x speedup over existing tree - based models .",Introduction,Introduction,relation-classification,4,20,0.8695652173913043,31,0.11787072243346,20,0.8695652173913043,1,0
33,"On TACRED , our model further achieves the state - of - the - art performance , surpassing a competitive neural sequence model baseline .",Introduction,Introduction,relation-classification,4,21,0.9130434782608696,32,0.1216730038022813,21,0.9130434782608696,1,0
34,"This model also exhibits complementary strengths to sequence models on TACRED , and combining these two model types through simple prediction interpolation further improves the state of the art .",Introduction,Introduction,relation-classification,4,22,0.9565217391304348,33,0.1254752851711026,22,0.9565217391304348,1,0
35,"To recap , our main contributions are : ( i ) we propose a neural model for relation extraction based on graph convolutional networks , which allows it to efficiently pool information over arbitrary dependency structures ; ( ii ) we present anew pathcentric pruning technique to help dependencybased models maximally remove irrelevant information without damaging crucial content to improve their robustness ; ( iii ) we present detailed analysis on the model and the pruning technique , and show that dependency - based models have complementary strengths with sequence models .",Introduction,Introduction,relation-classification,4,23,1.0,34,0.1292775665399239,23,1.0,1,0
36,Models,,,relation-classification,4,0,0.0,35,0.1330798479087452,0,0.0,1,0
37,"In this section , we first describe graph convolutional networks ( GCNs ) over dependency tree structures , and then we introduce an architecture that uses GCNs at its core for relation extraction .",Models,Models,relation-classification,4,1,0.0151515151515151,36,0.1368821292775665,1,0.0,1,0
38,Graph Convolutional Networks over Dependency Trees,Models,,relation-classification,4,2,0.0303030303030303,37,0.1406844106463878,0,0.0,1,0
39,The graph convolutional network is an adaptation of the convolutional neural network for encoding graphs .,Models,Graph Convolutional Networks over Dependency Trees,relation-classification,4,3,0.0454545454545454,38,0.1444866920152091,1,0.0454545454545454,1,0
40,"Given a graph with n nodes , we can represent the graph structure with an n n adjacency matrix A where A ij = 1 if there is an edge going from node i to node j.",Models,Graph Convolutional Networks over Dependency Trees,relation-classification,4,4,0.0606060606060606,39,0.1482889733840304,2,0.0909090909090909,1,0
41,"In an L-layer GCN , if we denote by h ( l?1 ) i the input vector and h ( l ) i the output vector of node i at the l - th layer , a graph convolution operation can be written as",Models,Graph Convolutional Networks over Dependency Trees,relation-classification,4,5,0.0757575757575757,40,0.1520912547528517,3,0.1363636363636363,1,0
42,"where W ( l ) is a linear transformation , b ( l ) a bias term , and ?",Models,Graph Convolutional Networks over Dependency Trees,relation-classification,4,6,0.0909090909090909,41,0.155893536121673,4,0.1818181818181818,1,0
43,"a nonlinear function ( e.g. , ReLU ) .",Models,Graph Convolutional Networks over Dependency Trees,relation-classification,4,7,0.106060606060606,42,0.1596958174904943,5,0.2272727272727272,1,0
44,"Intuitively , during each graph convolution , each node gathers and summarizes information from its neighboring nodes in the graph .",Models,Graph Convolutional Networks over Dependency Trees,relation-classification,4,8,0.1212121212121212,43,0.1634980988593156,6,0.2727272727272727,1,0
45,"We adapt the graph convolution operation to model dependency trees by converting each tree into its corresponding adjacency matrix A , where A ij = 1 if there is a dependency edge between tokens i and j .",Models,Graph Convolutional Networks over Dependency Trees,relation-classification,4,9,0.1363636363636363,44,0.1673003802281368,7,0.3181818181818182,1,0
46,"However , naively applying the graph convolution operation in Equation ( 1 ) could lead to node representations with drastically different magnitudes , since the degree of a token varies a lot .",Models,Graph Convolutional Networks over Dependency Trees,relation-classification,4,10,0.1515151515151515,45,0.1711026615969581,8,0.3636363636363636,1,0
47,This could bias our sentence representation towards favoring high - degree nodes regardless of the information carried in the node ( see details in Section 2.2 ) .,Models,Graph Convolutional Networks over Dependency Trees,relation-classification,4,11,0.1666666666666666,46,0.1749049429657794,9,0.4090909090909091,1,0
48,"Furthermore , the information in h ( l?1 ) i is never carried over to hi , since nodes never connect to themselves in a dependency tree .",Models,Graph Convolutional Networks over Dependency Trees,relation-classification,4,12,0.1818181818181818,47,0.1787072243346007,10,0.4545454545454545,1,0
49,"We resolve these issues by normalizing the activations in the graph convolution before feeding it through the nonlinearity , and adding self - loops to each node in the graph :",Models,Graph Convolutional Networks over Dependency Trees,relation-classification,4,13,0.1969696969696969,48,0.182509505703422,11,0.5,1,0
50,"where = A + I with I being then n identity matrix , and d i = n j=1 ij is the degree of token i in the resulting graph . :",Models,Graph Convolutional Networks over Dependency Trees,relation-classification,4,14,0.2121212121212121,49,0.1863117870722433,12,0.5454545454545454,1,0
51,Relation extraction with a graph convolutional network .,Models,Graph Convolutional Networks over Dependency Trees,relation-classification,4,15,0.2272727272727272,50,0.1901140684410646,13,0.5909090909090909,1,0
52,"The left side shows the overall architecture , while on the right side , we only show the detailed graph convolution computation for the word "" relative "" for clarity .",Models,Graph Convolutional Networks over Dependency Trees,relation-classification,4,16,0.2424242424242424,51,0.1939163498098859,14,0.6363636363636364,1,0
53,A full unlabeled dependency parse of the sentence is also provided for reference .,Models,Graph Convolutional Networks over Dependency Trees,relation-classification,4,17,0.2575757575757575,52,0.1977186311787072,15,0.6818181818181818,1,0
54,Stacking this operation over,Models,Graph Convolutional Networks over Dependency Trees,relation-classification,4,18,0.2727272727272727,53,0.2015209125475285,16,0.7272727272727273,1,0
55,"L layers gives us a deep GCN network , where we set h Moreover , the propagation of information between tokens occurs in parallel , and the runtime does not depend on the depth of the dependency tree .",Models,Graph Convolutional Networks over Dependency Trees,relation-classification,4,19,0.2878787878787879,54,0.2053231939163498,17,0.7727272727272727,1,0
56,Note that the GCN model presented above uses the same parameters for all edges in the dependency graph .,Models,Graph Convolutional Networks over Dependency Trees,relation-classification,4,20,0.303030303030303,55,0.2091254752851711,18,0.8181818181818182,1,0
57,"We also experimented with : ( 1 ) using different transformation matrices W for topdown , bottom - up , and self - loop edges ; and ( 2 ) adding dependency relation - specific parameters for edge - wise gating , similar to .",Models,Graph Convolutional Networks over Dependency Trees,relation-classification,4,21,0.3181818181818182,56,0.2129277566539923,19,0.8636363636363636,1,0
58,"We found that modeling directions does not lead to improvement , 1 and adding edgewise gating further hurts performance .",Models,Graph Convolutional Networks over Dependency Trees,relation-classification,4,22,0.3333333333333333,57,0.2167300380228136,20,0.9090909090909092,1,0
59,"We hypothesize that this is because the presented GCN model is usually already able to capture dependency edge patterns that are informative for classifying relations , and modeling edge directions and types does not offer additional discriminative power to the network before it leads to overfitting .",Models,Graph Convolutional Networks over Dependency Trees,relation-classification,4,23,0.3484848484848485,58,0.2205323193916349,21,0.9545454545454546,1,0
60,"For example , the relations entailed by "" A 's son , B "" and "" B 's son , A "" can be readily distinguished with "" 's "" attached to different entities , even when edge directionality is not considered .",Models,Graph Convolutional Networks over Dependency Trees,relation-classification,4,24,0.3636363636363636,59,0.2243346007604562,22,1.0,1,0
61,Encoding Relations with GCN,Models,,relation-classification,4,25,0.3787878787878788,60,0.2281368821292775,0,0.0,1,0
62,We now formally define the task of relation extraction .,Models,Encoding Relations with GCN,relation-classification,4,26,0.3939393939393939,61,0.2319391634980988,1,0.0666666666666666,1,0
63,"Let X = [ x 1 , ... , x n ] denote a sentence , where xi is the i th token .",Models,Encoding Relations with GCN,relation-classification,4,27,0.4090909090909091,62,0.2357414448669201,2,0.1333333333333333,1,0
64,A subject entity and an object entity are identified and correspond to two spans in the sentence :,Models,Encoding Relations with GCN,relation-classification,4,28,0.4242424242424242,63,0.2395437262357414,3,0.2,1,0
65,.,Models,Encoding Relations with GCN,relation-classification,4,29,0.4393939393939394,64,0.2433460076045627,4,0.2666666666666666,1,0
66,"Given X , X s , and X o , the goal of relation extraction is to predict a relation r ?",Models,Encoding Relations with GCN,relation-classification,4,30,0.4545454545454545,65,0.247148288973384,5,0.3333333333333333,1,0
67,"R ( a predefined relation set ) that holds between the entities or "" no relation "" otherwise .",Models,Encoding Relations with GCN,relation-classification,4,31,0.4696969696969697,66,0.2509505703422053,6,0.4,1,0
68,"After applying an L-layer GCN over word vectors , we obtain hidden representations of each token that are directly influenced by its neighbors no more than L edges apart in the dependency tree .",Models,Encoding Relations with GCN,relation-classification,4,32,0.4848484848484848,67,0.2547528517110266,7,0.4666666666666667,1,0
69,"To make use of these word representations for relation extraction , we first obtain a sentence representation as follows ( see also left ) :",Models,Encoding Relations with GCN,relation-classification,4,33,0.5,68,0.2585551330798479,8,0.5333333333333333,1,0
70,"where h ( l ) denotes the collective hidden representations at layer l of the GCN , and f : R dn ?",Models,Encoding Relations with GCN,relation-classification,4,34,0.5151515151515151,69,0.2623574144486692,9,0.6,1,0
71,Rd is a max pooling function that maps from n output vectors to the sentence vector .,Models,Encoding Relations with GCN,relation-classification,4,35,0.5303030303030303,70,0.2661596958174905,10,0.6666666666666666,1,0
72,We also observe that information close to entity tokens in the dependency tree is often central to relation classification .,Models,Encoding Relations with GCN,relation-classification,4,36,0.5454545454545454,71,0.2699619771863117,11,0.7333333333333333,1,0
73,"Therefore , we also obtain a subject representation h s from h ( L ) as follows",Models,Encoding Relations with GCN,relation-classification,4,37,0.5606060606060606,72,0.2737642585551331,12,0.8,1,0
74,as well as an object representation ho similarly .,Models,Encoding Relations with GCN,relation-classification,4,38,0.5757575757575758,73,0.2775665399239543,13,0.8666666666666667,1,0
75,"Inspired by recent work on relational learning between entities , we obtain the final representation used for classification by concatenating the sentence and the entity representations , and feeding them through a feed - forward neural network ( FFNN ) :",Models,Encoding Relations with GCN,relation-classification,4,39,0.5909090909090909,74,0.2813688212927757,14,0.9333333333333332,1,0
76,This h final representation is then fed into a linear layer followed by a softmax operation to obtain a probability distribution over relations .,Models,Encoding Relations with GCN,relation-classification,4,40,0.6060606060606061,75,0.2851711026615969,15,1.0,1,0
77,Contextualized GCN,Models,,relation-classification,4,41,0.6212121212121212,76,0.2889733840304182,0,0.0,1,0
78,"The network architecture introduced so far learns effective representations for relation extraction , but it also leaves a few issues inadequately addressed .",Models,Contextualized GCN,relation-classification,4,42,0.6363636363636364,77,0.2927756653992395,1,0.04,1,0
79,"First , the input word vectors do not contain contextual information about word order or disambiguation .",Models,Contextualized GCN,relation-classification,4,43,0.6515151515151515,78,0.2965779467680608,2,0.08,1,0
80,"Second , the GCN highly depends on a correct parse tree to extract crucial information from the sentence ( especially when pruning is performed ) , while existing parsing algorithms produce imperfect trees in many cases .",Models,Contextualized GCN,relation-classification,4,44,0.6666666666666666,79,0.3003802281368821,3,0.12,1,0
81,"To resolve these issues , we further apply a Contextualized GCN ( C - GCN ) model , where the input word vectors are first fed into a bi-directional long short - term memory ( LSTM ) network to generate contextualized representations , which are then used ash ( 0 ) in the original model .",Models,Contextualized GCN,relation-classification,4,45,0.6818181818181818,80,0.3041825095057034,4,0.16,1,0
82,This BiL - STM contextualization layer is trained jointly with the rest of the network .,Models,Contextualized GCN,relation-classification,4,46,0.696969696969697,81,0.3079847908745247,5,0.2,1,0
83,We show empirically in Section 5 that this augmentation substantially improves the performance over the original model .,Models,Contextualized GCN,relation-classification,4,47,0.7121212121212122,82,0.311787072243346,6,0.24,1,0
84,"We note that this relation extraction model is conceptually similar to graph kernel - based models , in that it aims to utilize local dependency tree patterns to inform relation classification .",Models,Contextualized GCN,relation-classification,4,48,0.7272727272727273,83,0.3155893536121673,7,0.28,1,0
85,"Our model also incorporates crucial off - path information , which greatly improves its robustness compared to shortest dependency pathbased approaches .",Models,Contextualized GCN,relation-classification,4,49,0.7424242424242424,84,0.3193916349809886,8,0.32,1,0
86,"Compared to tree - structured models ( e.g. , Tree - LSTM",Models,Contextualized GCN,relation-classification,4,50,0.7575757575757576,85,0.3231939163498099,9,0.36,1,0
87,"( Tai et al. , 2015 ) ) , it not only is able to capture more global information through the use of pooling functions , but also achieves substantial speedup by not requiring recursive operations that are difficult to parallelize .",Models,Contextualized GCN,relation-classification,4,51,0.7727272727272727,86,0.3269961977186312,10,0.4,1,0
88,"For example , we observe that on a Titan Xp GPU , training a Tree - LSTM model over a minibatch of 50 examples takes 6.54 seconds on average , while training the original GCN model takes only 0.07 seconds , and the C - GCN model 0.08 seconds .",Models,Contextualized GCN,relation-classification,4,52,0.7878787878787878,87,0.3307984790874524,11,0.44,1,0
89,Incorporating Off - path Information with Path - centric Pruning,Models,Contextualized GCN,relation-classification,4,53,0.803030303030303,88,0.3346007604562737,12,0.48,1,0
90,"Dependency trees provide rich structures that one can exploit in relation extraction , but most of the information pertinent to relations is usually contained within the subtree rooted at the lowest common ancestor ( LCA ) of the two entities .",Models,Contextualized GCN,relation-classification,4,54,0.8181818181818182,89,0.338403041825095,13,0.52,1,0
91,Previous studies have shown that removing tokens outside this scope helps relation extraction by eliminating irrelevant information from the sentence .,Models,Contextualized GCN,relation-classification,4,55,0.8333333333333334,90,0.3422053231939163,14,0.56,1,0
92,It is therefore desirable to combine our GCN models with tree pruning strategies to further improve performance .,Models,Contextualized GCN,relation-classification,4,56,0.8484848484848485,91,0.3460076045627376,15,0.6,1,0
93,"However , pruning too aggressively ( e.g. , keeping only the dependency path ) could lead to loss of crucial information and conversely hurt robustness .",Models,Contextualized GCN,relation-classification,4,57,0.8636363636363636,92,0.3498098859315589,16,0.64,1,0
94,"For instance , the negation in is neglected when a model is restricted to only looking at the dependency path between the entities .",Models,Contextualized GCN,relation-classification,4,58,0.8787878787878788,93,0.3536121673003802,17,0.68,1,0
95,"Similarly , in the sentence "" She was diagnosed with cancer last year , and succumbed this June "" , the dependency path She?diagnosed ?",Models,Contextualized GCN,relation-classification,4,59,0.8939393939393939,94,0.3574144486692015,18,0.72,1,0
96,cancer is not sufficient to establish that cancer is the cause of death for the subject unless the conjunction dependency to succumbed is also present .,Models,Contextualized GCN,relation-classification,4,60,0.9090909090909092,95,0.3612167300380228,19,0.76,1,0
97,"Motivated by these observations , we propose path - centric pruning , a novel technique to incorporate information off the dependency path .",Models,Contextualized GCN,relation-classification,4,61,0.9242424242424242,96,0.3650190114068441,20,0.8,1,0
98,This is achieved by including tokens that are up to distance K away from the dependency path in the LCA subtree .,Models,Contextualized GCN,relation-classification,4,62,0.9393939393939394,97,0.3688212927756654,21,0.84,1,0
99,"K = 0 , corresponds to pruning the tree down to the path , K = 1 keeps all nodes that are directly attached to the path , and K = ?",Models,Contextualized GCN,relation-classification,4,63,0.9545454545454546,98,0.3726235741444867,22,0.88,1,0
100,retains the entire LCA subtree .,Models,Contextualized GCN,relation-classification,4,64,0.9696969696969696,99,0.376425855513308,23,0.92,1,0
101,"We combine this pruning strategy with our GCN model , by directly feeding the pruned trees into the graph convolutional layers .",Models,Contextualized GCN,relation-classification,4,65,0.9848484848484848,100,0.3802281368821292,24,0.96,1,0
102,"We show that pruning with K = 1 achieves the best balance between including relevant information ( e.g. , negation and conjunction ) and keeping irrelevant content out of the resulting pruned tree as much as possible .",Models,Contextualized GCN,relation-classification,4,66,1.0,101,0.3840304182509506,25,1.0,1,0
103,Related Work,,,relation-classification,4,0,0.0,102,0.3878326996197718,0,0.0,1,0
104,"At the core of fully - supervised and distantlysupervised relation extraction approaches are statistical classifiers , many of which find syntactic information beneficial .",Related Work,Related Work,relation-classification,4,1,0.0588235294117647,103,0.3916349809885932,1,0.0588235294117647,0,0
105,"For example , explored adding syntactic features to a statistical classifier and found them to be useful when sentences are long .",Related Work,Related Work,relation-classification,4,2,0.1176470588235294,104,0.3954372623574144,2,0.1176470588235294,0,0
106,"Various kernel - based approaches also leverage syntactic information to measure similarity between training and test examples to predict the relation , finding that tree - based kernels and dependency path - based kernels ( Bunescu and Mooney , 2005 ) are effective for this task .",Related Work,Related Work,relation-classification,4,3,0.1764705882352941,105,0.3992395437262357,3,0.1764705882352941,0,0
107,Recent studies have found neural models effective in relation extraction .,Related Work,Related Work,relation-classification,4,4,0.2352941176470588,106,0.403041825095057,4,0.2352941176470588,0,0
108,first applied a one - dimensional convolutional neural network ( CNN ) with manual features to encode relations .,Related Work,Related Work,relation-classification,4,5,0.2941176470588235,107,0.4068441064638783,5,0.2941176470588235,0,0
109,showed that combining a CNN with a recurrent neural network ( RNN ) through a voting scheme can further improve performance .,Related Work,Related Work,relation-classification,4,6,0.3529411764705882,108,0.4106463878326996,6,0.3529411764705882,0,0
110,and proposed to use attention mechanisms over RNN and CNN architectures for this task .,Related Work,Related Work,relation-classification,4,7,0.4117647058823529,109,0.4144486692015209,7,0.4117647058823529,0,0
111,"Apart from neural models over word sequences , incorporating dependency trees into neural models has also been shown to improve relation extraction performance by capturing long - distance relations .",Related Work,Related Work,relation-classification,4,8,0.4705882352941176,110,0.4182509505703422,8,0.4705882352941176,0,0
112,generalized the idea of dependency path kernels by applying a LSTM network over the shortest dependency path between entities .,Related Work,Related Work,relation-classification,4,9,0.5294117647058824,111,0.4220532319391635,9,0.5294117647058824,0,0
113,Liu et al. first applied a recursive network over the subtrees rooted at the words on the dependency path and then applied a CNN over the path .,Related Work,Related Work,relation-classification,4,10,0.5882352941176471,112,0.4258555133079847,10,0.5882352941176471,0,0
114,"Miwa and Bansal ( 2016 ) applied a Tree - LSTM , a generalized form of LSTM over dependency trees , in a joint entity and relation extraction setting .",Related Work,Related Work,relation-classification,4,11,0.6470588235294118,113,0.4296577946768061,11,0.6470588235294118,0,0
115,They found it to be most effective when applied to the subtree rooted at the LCA of the two entities .,Related Work,Related Work,relation-classification,4,12,0.7058823529411765,114,0.4334600760456273,12,0.7058823529411765,0,0
116,"More recently , and have shown that relatively simple neural models ( CNN and augmented LSTM , respectively ) can achieve comparable or superior performance to dependency - based models when trained on larger datasets .",Related Work,Related Work,relation-classification,4,13,0.7647058823529411,115,0.4372623574144487,13,0.7647058823529411,0,0
117,"In this paper , we study dependency - based models in depth and show that with a properly designed architecture , they can outperform and have complementary advantages to sequence models , even in a large - scale setting .",Related Work,Related Work,relation-classification,4,14,0.8235294117647058,116,0.4410646387832699,14,0.8235294117647058,0,0
118,"Finally , we note that a technique similar to pathcentric pruning has been applied to reduce the space of possible arguments in semantic role labeling .",Related Work,Related Work,relation-classification,4,15,0.8823529411764706,117,0.4448669201520912,15,0.8823529411764706,0,0
119,"The authors showed pruning words too faraway from the path between the predicate and the root to be beneficial , but reported the best pruning distance to be 10 , which almost always retains the entire tree .",Related Work,Related Work,relation-classification,4,16,0.9411764705882352,118,0.4486692015209125,16,0.9411764705882352,0,0
120,"Our method differs in that it is applied to the shortest dependency path between entities , and we show that in our technique the best pruning distance is 1 for several dependency - based relation extraction models .",Related Work,Related Work,relation-classification,4,17,1.0,119,0.4524714828897338,17,1.0,0,0
121,Experiments,,,relation-classification,4,0,0.0,120,0.4562737642585551,0,0.0,1,0
122,Baseline Models,,,relation-classification,4,0,0.0,121,0.4600760456273764,0,0.0,1,0
123,We compare our models with several competitive dependency - based and neural sequence models .,Baseline Models,Baseline Models,relation-classification,4,1,0.0769230769230769,122,0.4638783269961977,1,0.0769230769230769,1,0
124,Dependency - based models .,Baseline Models,Baseline Models,relation-classification,4,2,0.1538461538461538,123,0.467680608365019,2,0.1538461538461538,1,0
125,In our main experiments we compare with three types of dependency - based models .,Baseline Models,Baseline Models,relation-classification,4,3,0.2307692307692307,124,0.4714828897338403,3,0.2307692307692307,1,0
126,( 1 ) A logistic regression ( LR ) classifier which combines dependencybased features with other lexical features .,Baseline Models,Baseline Models,relation-classification,4,4,0.3076923076923077,125,0.4752851711026616,4,0.3076923076923077,1,1
127,"( 2 ) Shortest Dependency Path LSTM ( SDP - LSTM ) , which applies a neural sequence model on the shortest path between the subject and object entities in the dependency tree .",Baseline Models,Baseline Models,relation-classification,4,5,0.3846153846153846,126,0.4790874524714829,5,0.3846153846153846,1,1
128,"Tree - LSTM , which is a recursive model that generalizes the LSTM to arbitrary tree structures .",Baseline Models,Baseline Models,relation-classification,4,6,0.4615384615384615,127,0.4828897338403042,6,0.4615384615384615,1,1
129,"We investigate the child - sum variant of Tree - LSTM , and apply it to the dependency tree ( or part of it ) .",Baseline Models,Baseline Models,relation-classification,4,7,0.5384615384615384,128,0.4866920152091255,7,0.5384615384615384,1,0
130,"In practice , we find that modifying this model by concatenating dependency label embeddings to the input of forget gates improves its performance on relation extraction , and therefore use this variant in our experiments .",Baseline Models,Baseline Models,relation-classification,4,8,0.6153846153846154,129,0.4904942965779467,8,0.6153846153846154,1,0
131,"Earlier , our group compared and with sequence models , and we report these results ; for ( 3 ) we report results with our own implementation .",Baseline Models,Baseline Models,relation-classification,4,9,0.6923076923076923,130,0.494296577946768,9,0.6923076923076923,1,0
132,Neural sequence model .,Baseline Models,Baseline Models,relation-classification,4,10,0.7692307692307693,131,0.4980988593155893,10,0.7692307692307693,1,0
133,"Our group presented a competitive sequence model that employs a position - aware attention mechanism over LSTM outputs ( PA - LSTM ) , and showed that it outperforms several CNN and dependency - based models by a substantial margin .",Baseline Models,Baseline Models,relation-classification,4,11,0.8461538461538461,132,0.5019011406844106,11,0.8461538461538461,1,1
134,"We compare with this strong baseline , and use its open implementation in further analysis .",Baseline Models,Baseline Models,relation-classification,4,12,0.9230769230769232,133,0.5057034220532319,12,0.9230769230769232,1,0
135,3,Baseline Models,Baseline Models,relation-classification,4,13,1.0,134,0.5095057034220533,13,1.0,1,0
136,Experimental Setup,,,relation-classification,4,0,0.0,135,0.5133079847908745,0,0.0,1,0
137,We conduct experiments on two relation extraction datasets :,Experimental Setup,Experimental Setup,relation-classification,4,1,0.0238095238095238,136,0.5171102661596958,1,0.0833333333333333,1,0
138,"( 1 ) TACRED : Introduced in , TACRED contains over 106 k mention pairs drawn from the yearly TAC KBP 4 challenge .",Experimental Setup,Experimental Setup,relation-classification,4,2,0.0476190476190476,137,0.5209125475285171,2,0.1666666666666666,1,0
139,It represents 41 relation types and a special no relation class when the mention pair does not have a relation between them within these categories .,Experimental Setup,Experimental Setup,relation-classification,4,3,0.0714285714285714,138,0.5247148288973384,3,0.25,1,0
140,"Mentions in TACRED are typed , with subjects categorized into person and organization , and objects into 16 fine - grained types ( e.g. , date and location ) .",Experimental Setup,Experimental Setup,relation-classification,4,4,0.0952380952380952,139,0.5285171102661597,4,0.3333333333333333,1,0
141,We report micro-averaged F 1 scores on this dataset as is conventional .,Experimental Setup,Experimental Setup,relation-classification,4,5,0.119047619047619,140,0.532319391634981,5,0.4166666666666667,1,0
142,"For fair comparisons on the TACRED dataset , we follow the evaluation protocol used in by selecting the model with the median dev F 1 from 5 independent runs and reporting its test F 1 .",Experimental Setup,Experimental Setup,relation-classification,4,6,0.1428571428571428,141,0.5361216730038023,6,0.5,1,0
143,"We also use the same "" entity mask "" strategy where we replace each subject ( and object similarly ) entity with a special SUBJ - < NER > token .",Experimental Setup,Experimental Setup,relation-classification,4,7,0.1666666666666666,142,0.5399239543726235,7,0.5833333333333334,1,0
144,"For all models , we also adopt the "" multichannel "" strategy by concatenating the input word embeddings with POS and NER embeddings .",Experimental Setup,Experimental Setup,relation-classification,4,8,0.1904761904761904,143,0.5437262357414449,8,0.6666666666666666,1,0
145,"Traditionally , evaluation on SemEval is conducted without entity mentions masked .",Experimental Setup,Experimental Setup,relation-classification,4,9,0.2142857142857142,144,0.5475285171102662,9,0.75,1,0
146,"However , as we will discuss in Section 6.4 , this method encourages models to overfit to these mentions and fails to test their actual ability to generalize .",Experimental Setup,Experimental Setup,relation-classification,4,10,0.238095238095238,145,0.5513307984790875,10,0.8333333333333334,1,0
147,"We therefore report results with two evaluation protocols : ( 1 ) with- mention , where mentions are kept for comparison with previous work ; and ( 2 ) maskmention , where they are masked to test the generalization of our model in a more realistic setting .",Experimental Setup,Experimental Setup,relation-classification,4,11,0.2619047619047619,146,0.5551330798479087,11,0.9166666666666666,1,0
148,"Due to space limitations , we report model training details in the supplementary material .",Experimental Setup,Experimental Setup,relation-classification,4,12,0.2857142857142857,147,0.55893536121673,12,1.0,1,0
149,Results on the TACRED Dataset,Experimental Setup,,relation-classification,4,13,0.3095238095238095,148,0.5627376425855514,0,0.0,1,1
150,We present our main results on the TACRED test set in .,Experimental Setup,Results on the TACRED Dataset,relation-classification,4,14,0.3333333333333333,149,0.5665399239543726,1,0.0769230769230769,1,0
151,We observe that our GCN model Our Model ( C - GCN ) 84.8 * 76.5 * outperforms all dependency - based models by at least 1.6 F 1 .,Experimental Setup,Results on the TACRED Dataset,relation-classification,4,15,0.3571428571428571,150,0.5703422053231939,2,0.1538461538461538,1,1
152,"By using contextualized word representations , the C - GCN model further outperforms the strong PA - LSTM model by 1.3 F 1 , and achieves anew state of the art .",Experimental Setup,Results on the TACRED Dataset,relation-classification,4,16,0.3809523809523809,151,0.5741444866920152,3,0.2307692307692307,1,1
153,"In addition , we find our model improves upon other dependencybased models in both precision and recall .",Experimental Setup,Results on the TACRED Dataset,relation-classification,4,17,0.4047619047619047,152,0.5779467680608364,4,0.3076923076923077,1,1
154,"Comparing the C - GCN model with the GCN model , we find that the gain mainly comes from improved recall .",Experimental Setup,Results on the TACRED Dataset,relation-classification,4,18,0.4285714285714285,153,0.5817490494296578,5,0.3846153846153846,1,1
155,We hypothesize that this is because the C - GCN is more robust to parse errors by capturing local word patterns ( see also Section 6.2 ) .,Experimental Setup,Results on the TACRED Dataset,relation-classification,4,19,0.4523809523809524,154,0.5855513307984791,6,0.4615384615384615,1,0
156,"As we will show in Section 6.2 , we find that our GCN models have complementary strengths when compared to the PA - LSTM .",Experimental Setup,Results on the TACRED Dataset,relation-classification,4,20,0.4761904761904761,155,0.5893536121673004,7,0.5384615384615384,1,1
157,"To leverage this result , we experiment with a simple interpolation strategy to combine these models .",Experimental Setup,Results on the TACRED Dataset,relation-classification,4,21,0.5,156,0.5931558935361216,8,0.6153846153846154,1,0
158,"Given the output probabilities PG ( r|x ) from a GCN model and PS ( r|x ) from the sequence model for any relation r , we calculate the interpolated probability as",Experimental Setup,Results on the TACRED Dataset,relation-classification,4,22,0.5238095238095238,157,0.596958174904943,9,0.6923076923076923,1,0
159,where ? ?,Experimental Setup,Results on the TACRED Dataset,relation-classification,4,23,0.5476190476190477,158,0.6007604562737643,10,0.7692307692307693,1,0
160,"[ 0 , 1 ] is chosen on the dev set and set to 0.6 .",Experimental Setup,Results on the TACRED Dataset,relation-classification,4,24,0.5714285714285714,159,0.6045627376425855,11,0.8461538461538461,1,0
161,"This simple interpolation between a GCN and a PA - LSTM achieves an F 1 score of 67.1 , outperforming each model alone by at least 2.0 F 1 .",Experimental Setup,Results on the TACRED Dataset,relation-classification,4,25,0.5952380952380952,160,0.6083650190114068,12,0.9230769230769232,1,0
162,An interpolation between a C - GCN and a PA - LSTM further improves the result to 68.2 .,Experimental Setup,Results on the TACRED Dataset,relation-classification,4,26,0.6190476190476191,161,0.6121673003802282,13,1.0,1,0
163,Results on the SemEval Dataset,Experimental Setup,,relation-classification,4,27,0.6428571428571429,162,0.6159695817490495,0,0.0,1,0
164,"To study the generalizability of our proposed model , we also trained and evaluated our best C - GCN model on the SemEval test set ) .",Experimental Setup,Results on the SemEval Dataset,relation-classification,4,28,0.6666666666666666,163,0.6197718631178707,1,0.0714285714285714,1,0
165,"We find that under the conventional with- entity evaluation , our C - GCN model outperforms all existing dependency - based neural models on this sep - arate dataset .",Experimental Setup,Results on the SemEval Dataset,relation-classification,4,29,0.6904761904761905,164,0.623574144486692,2,0.1428571428571428,1,1
166,"Notably , by properly incorporating off - path information , our model outperforms the previous shortest dependency path - based model ( SDP - LSTM ) .",Experimental Setup,Results on the SemEval Dataset,relation-classification,4,30,0.7142857142857143,165,0.6273764258555133,3,0.2142857142857142,1,1
167,"Under the mask - entity evaluation , our C - GCN model also outperforms PA - LSTM by a substantial margin , suggesting its generalizability even when entities are not seen .",Experimental Setup,Results on the SemEval Dataset,relation-classification,4,31,0.7380952380952381,166,0.6311787072243346,4,0.2857142857142857,1,1
168,Effect of Path - centric,Experimental Setup,Results on the SemEval Dataset,relation-classification,4,32,0.7619047619047619,167,0.6349809885931559,5,0.3571428571428571,1,0
169,Pruning,Experimental Setup,,relation-classification,4,33,0.7857142857142857,168,0.6387832699619772,6,0.4285714285714285,1,0
170,"To show the effectiveness of path - centric pruning , we compare the two GCN models and the Tree - LSTM when the pruning distance K is varied .",Experimental Setup,Pruning,relation-classification,4,34,0.8095238095238095,169,0.6425855513307985,7,0.5,1,1
171,"We experimented with K ? { 0 , 1 , 2 , ?} on the TACRED dev set , and also include results when the full tree is used .",Experimental Setup,Pruning,relation-classification,4,35,0.8333333333333334,170,0.6463878326996197,8,0.5714285714285714,1,0
172,"As shown in , the performance of all three models peaks when K = 1 , outperforming their respective dependency path - based counterpart ( K = 0 ) .",Experimental Setup,Pruning,relation-classification,4,36,0.8571428571428571,171,0.6501901140684411,9,0.6428571428571429,1,1
173,This confirms our hypothesis in Section 3 that incorporating off - path information is crucial to relation extraction .,Experimental Setup,Pruning,relation-classification,4,37,0.8809523809523809,172,0.6539923954372624,10,0.7142857142857143,1,0
174,Miwa and Bansal ( 2016 ) reported that a Tree - LSTM achieves similar performance when the dependency path and the LCA subtree are used respectively .,Experimental Setup,Pruning,relation-classification,4,38,0.9047619047619048,173,0.6577946768060836,11,0.7857142857142857,1,0
175,"Our experiments confirm this , and further show that the result can be improved by path - centric pruning with K = 1 .",Experimental Setup,Pruning,relation-classification,4,39,0.9285714285714286,174,0.6615969581749049,12,0.8571428571428571,1,0
176,"We find that all three models are less effective when the entire dependency tree is present , indicating that including extra information hurts performance .",Experimental Setup,Pruning,relation-classification,4,40,0.9523809523809524,175,0.6653992395437263,13,0.9285714285714286,1,1
177,"Finally , we note that contextualizing the GCN makes it less sensitive to changes in the tree structures provided , presumably because the model can use word sequence information in the LSTM layer to recover any off - path information that it needs for correct relation extraction .",Experimental Setup,Pruning,relation-classification,4,41,0.9761904761904762,176,0.6692015209125475,14,1.0,1,1
178,Analysis & Discussion,Experimental Setup,,relation-classification,4,42,1.0,177,0.6730038022813688,0,0.0,1,0
179,Ablation Study,,,relation-classification,4,0,0.0,178,0.6768060836501901,0,0.0,1,0
180,"To study the contribution of each component in the C - GCN model , we ran an ablation study on the TACRED dev set ) .",Ablation Study,Ablation Study,relation-classification,4,1,0.0526315789473684,179,0.6806083650190115,1,0.0526315789473684,1,1
181,We find that : The entity representations and feedforward layers contribute 1.0 F 1 .,Ablation Study,Ablation Study,relation-classification,4,2,0.1052631578947368,180,0.6844106463878327,2,0.1052631578947368,1,1
182,"( 2 ) When we remove the dependency structure ( i.e. , setting to I ) , the score drops by 3.2 F 1 .",Ablation Study,Ablation Study,relation-classification,4,3,0.1578947368421052,181,0.688212927756654,3,0.1578947368421052,1,1
183,"( 3 ) F 1 drops by 10.3 when we remove the feedforward layers , the LSTM component and the dependency structure altogether .",Ablation Study,Ablation Study,relation-classification,4,4,0.2105263157894736,182,0.6920152091254753,4,0.2105263157894736,1,1
184,"( 4 ) Removing the pruning ( i.e. , using full trees as input ) further hurts the result by another 9.7 F 1 .",Ablation Study,Ablation Study,relation-classification,4,5,0.2631578947368421,183,0.6958174904942965,5,0.2631578947368421,1,1
185,Complementary Strengths of GCNs and PA - LSTMs,Ablation Study,Ablation Study,relation-classification,4,6,0.3157894736842105,184,0.6996197718631179,6,0.3157894736842105,1,0
186,"To understand what the GCN models are capturing and how they differ from a sequence model such as the PA - LSTM , we compared their performance :",Ablation Study,Ablation Study,relation-classification,4,7,0.3684210526315789,185,0.7034220532319392,7,0.3684210526315789,1,0
187,The three dependency edges that contribute the most to the classification of different relations in the TACRED dev set .,Ablation Study,Ablation Study,relation-classification,4,8,0.4210526315789473,186,0.7072243346007605,8,0.4210526315789473,1,0
188,"For clarity , we removed edges which 1 ) connect to common punctuation ( i.e. , commas , periods , and quotation marks ) , 2 ) connect to common prepositions ( i.e. , of , to , by ) , and 3 ) connect between tokens within the same entity .",Ablation Study,Ablation Study,relation-classification,4,9,0.4736842105263157,187,0.7110266159695817,9,0.4736842105263157,1,0
189,"We use PER , ORG for entity types of PERSON , ORGANIZATION .",Ablation Study,Ablation Study,relation-classification,4,10,0.5263157894736842,188,0.714828897338403,10,0.5263157894736842,1,0
190,"We use S - and O - to denote subject and object entities , respectively .",Ablation Study,Ablation Study,relation-classification,4,11,0.5789473684210527,189,0.7186311787072244,11,0.5789473684210527,1,0
191,We also include edges for more relations in the supplementary material .,Ablation Study,Ablation Study,relation-classification,4,12,0.631578947368421,190,0.7224334600760456,12,0.631578947368421,1,0
192,over examples in the TACRED dev set .,Ablation Study,Ablation Study,relation-classification,4,13,0.6842105263157895,191,0.7262357414448669,13,0.6842105263157895,1,0
193,"Specifically , for each model , we trained it for 5 independent runs with different seeds , and for each example we evaluated the model 's accuracy over these 5 runs .",Ablation Study,Ablation Study,relation-classification,4,14,0.7368421052631579,192,0.7300380228136882,14,0.7368421052631579,1,0
194,"For instance , if a model correctly classifies an example for 3 out of 5 times , it achieves an accuracy of 60 % on this example .",Ablation Study,Ablation Study,relation-classification,4,15,0.7894736842105263,193,0.7338403041825095,15,0.7894736842105263,1,0
195,"We observe that on 847 ( 3.7 % ) dev examples , our C - GCN model achieves an accuracy at least 60 % higher than that of the PA - LSTM , while on 629 ( 2.8 % ) examples the PA - LSTM achieves 60 % higher .",Ablation Study,Ablation Study,relation-classification,4,16,0.8421052631578947,194,0.7376425855513308,16,0.8421052631578947,1,0
196,This complementary performance explains the gain we see in when the two models are combined .,Ablation Study,Ablation Study,relation-classification,4,17,0.8947368421052632,195,0.7414448669201521,17,0.8947368421052632,1,0
197,"We further show that this difference is due to each model 's competitive advantage : dependency - based models are better at handling sentences with entities farther apart , while sequence models can better leverage local word patterns regardless of parsing quality ( see also .",Ablation Study,Ablation Study,relation-classification,4,18,0.9473684210526316,196,0.7452471482889734,18,0.9473684210526316,1,0
198,We include further analysis in the supplementary material .,Ablation Study,Ablation Study,relation-classification,4,19,1.0,197,0.7490494296577946,19,1.0,1,0
199,Understanding Model Behavior,,,relation-classification,4,0,0.0,198,0.752851711026616,0,0.0,1,0
200,"To gain more insights into the C - GCN model 's behavior , we visualized the partial dependency tree it is processing and how much each token 's final representation contributed to h sent ( ) .",Understanding Model Behavior,Understanding Model Behavior,relation-classification,4,1,0.0476190476190476,199,0.7566539923954373,1,0.1428571428571428,1,0
201,"We find that the model often focuses on the dependency path , but sometimes also incorporates offpath information to help reinforce its prediction .",Understanding Model Behavior,Understanding Model Behavior,relation-classification,4,2,0.0952380952380952,200,0.7604562737642585,2,0.2857142857142857,1,0
202,"The model also learns to ignore determiners ( e.g. , "" the "" ) as they rarely affect relation prediction .",Understanding Model Behavior,Understanding Model Behavior,relation-classification,4,3,0.1428571428571428,201,0.7642585551330798,3,0.4285714285714285,1,0
203,"To further understand what dependency edges contribute most to the classification of different relations , we scored each dependency edge by summing up the number of dimensions each of its connected nodes contributed to h sent .",Understanding Model Behavior,Understanding Model Behavior,relation-classification,4,4,0.1904761904761904,202,0.7680608365019012,4,0.5714285714285714,1,0
204,We present the top scoring edges in .,Understanding Model Behavior,Understanding Model Behavior,relation-classification,4,5,0.238095238095238,203,0.7718631178707225,5,0.7142857142857143,1,0
205,"As can be seen in the table , most of these edges are associated with indicative nouns or verbs of each relation .",Understanding Model Behavior,Understanding Model Behavior,relation-classification,4,6,0.2857142857142857,204,0.7756653992395437,6,0.8571428571428571,1,0
206,5,Understanding Model Behavior,Understanding Model Behavior,relation-classification,4,7,0.3333333333333333,205,0.779467680608365,7,1.0,1,0
207,Entity Bias in the SemEval Dataset,Understanding Model Behavior,,relation-classification,4,8,0.3809523809523809,206,0.7832699619771863,0,0.0,1,0
208,"In our study , we observed a high correlation between the entity mentions in a sentence and its relation label in the SemEval dataset .",Understanding Model Behavior,Entity Bias in the SemEval Dataset,relation-classification,4,9,0.4285714285714285,207,0.7870722433460076,1,0.0769230769230769,1,0
209,"We experimented with PA - LSTM models to analyze this dependency tree corresponding to K = 1 in path-centric pruning is shown , and the shortest dependency path is thickened .",Understanding Model Behavior,Entity Bias in the SemEval Dataset,relation-classification,4,10,0.4761904761904761,208,0.7908745247148289,2,0.1538461538461538,1,0
210,We omit edges to punctuation for clarity .,Understanding Model Behavior,Entity Bias in the SemEval Dataset,relation-classification,4,11,0.5238095238095238,209,0.7946768060836502,3,0.2307692307692307,1,0
211,"The first example shows that the C - GCN is effective at leveraging long - range dependencies while reducing noise with the help of pruning ( while the PA - LSTM predicts no relation twice , org : alternate names twice , and org : parents once in this case ) .",Understanding Model Behavior,Entity Bias in the SemEval Dataset,relation-classification,4,12,0.5714285714285714,210,0.7984790874524715,4,0.3076923076923077,1,0
212,"The second example shows that the PA - LSTM is better at leveraging the proximity of the word "" migrated "" regardless of attachment errors in the parse ( while the C - GCN is misled to predict per :country of birth three times , and no relation twice ) .",Understanding Model Behavior,Entity Bias in the SemEval Dataset,relation-classification,4,13,0.6190476190476191,211,0.8022813688212928,5,0.3846153846153846,1,0
213,phenomenon .,Understanding Model Behavior,Entity Bias in the SemEval Dataset,relation-classification,4,14,0.6666666666666666,212,0.8060836501901141,6,0.4615384615384615,1,0
214,"We started by simplifying every sentence in the SemEval training and dev sets to "" subject and object "" , where subject and object are the actual entities in the sentence .",Understanding Model Behavior,Entity Bias in the SemEval Dataset,relation-classification,4,15,0.7142857142857143,213,0.8098859315589354,7,0.5384615384615384,1,0
215,"Surprisingly , a trained PA - LSTM model on this data is able to achieve 65.1 F 1 on the dev set if Glo Ve is used to initialize word vectors , and 47.9 dev F 1 even without GloVe initialization .",Understanding Model Behavior,Entity Bias in the SemEval Dataset,relation-classification,4,16,0.7619047619047619,214,0.8136882129277566,8,0.6153846153846154,1,0
216,"To further evaluate the model in a more realistic setting , we trained one model with the original SemEval training set ( unmasked ) and one with mentions masked in the training set , following what we have done for TACRED ( masked ) .",Understanding Model Behavior,Entity Bias in the SemEval Dataset,relation-classification,4,17,0.8095238095238095,215,0.8174904942965779,9,0.6923076923076923,1,0
217,"While the unmasked model achieves a 83.6 F 1 on the original SemEval dev set , F 1 drops drastically to 62.4 if we replace dev set entity mentions with a special < UNK > token to simulate the presence of unseen entities .",Understanding Model Behavior,Entity Bias in the SemEval Dataset,relation-classification,4,18,0.8571428571428571,216,0.8212927756653993,10,0.7692307692307693,1,0
218,"In contrast , the masked model is unaffected by unseen entity mentions and achieves a stable dev F 1 of 74.7 .",Understanding Model Behavior,Entity Bias in the SemEval Dataset,relation-classification,4,19,0.9047619047619048,217,0.8250950570342205,11,0.8461538461538461,1,0
219,This suggests that models trained without entities masked generalize poorly to new examples with unseen entities .,Understanding Model Behavior,Entity Bias in the SemEval Dataset,relation-classification,4,20,0.9523809523809524,218,0.8288973384030418,12,0.9230769230769232,1,0
220,Our findings call for more careful evaluation that takes dataset biases into account in future relation extraction studies .,Understanding Model Behavior,Entity Bias in the SemEval Dataset,relation-classification,4,21,1.0,219,0.8326996197718631,13,1.0,1,0
221,Conclusion,,,relation-classification,4,0,0.0,220,0.8365019011406845,0,0.0,1,0
222,We showed the success of a neural architecture based on a graph convolutional network for relation extraction .,Conclusion,Conclusion,relation-classification,4,1,0.3333333333333333,221,0.8403041825095057,1,0.3333333333333333,0,0
223,We also proposed path - centric pruning to improve the robustness of dependencybased models by removing irrelevant content without ignoring crucial information .,Conclusion,Conclusion,relation-classification,4,2,0.6666666666666666,222,0.844106463878327,2,0.6666666666666666,0,0
224,"We showed through detailed analysis that our model has complementary strengths to sequence models , and that the proposed pruning technique can be effectively applied to other dependency - based models .",Conclusion,Conclusion,relation-classification,4,3,1.0,223,0.8479087452471483,3,1.0,0,0
225,A Experimental Details,,,relation-classification,4,0,0.0,224,0.8517110266159695,0,0.0,1,0
226,A.1 Hyperparameters TACRED,,,relation-classification,4,0,0.0,225,0.8555133079847909,1,0.0666666666666666,1,0
227,We set LSTM hidden size to 200 in all neural models .,A.1 Hyperparameters TACRED,A.1 Hyperparameters TACRED,relation-classification,4,1,0.0714285714285714,226,0.8593155893536122,2,0.1333333333333333,1,0
228,We also use hidden size 200 for the output feedforward layers in the GCN model .,A.1 Hyperparameters TACRED,A.1 Hyperparameters TACRED,relation-classification,4,2,0.1428571428571428,227,0.8631178707224335,3,0.2,1,0
229,We use 2 GCN layers and 2 feedforward ( FFNN ) layers in our experiments .,A.1 Hyperparameters TACRED,A.1 Hyperparameters TACRED,relation-classification,4,3,0.2142857142857142,228,0.8669201520912547,4,0.2666666666666666,1,0
230,We employ the ReLU function for all nonlinearities in the GCN layers and the standard max pooling operations in all pooling layers .,A.1 Hyperparameters TACRED,A.1 Hyperparameters TACRED,relation-classification,4,4,0.2857142857142857,229,0.870722433460076,5,0.3333333333333333,1,0
231,"For the Tree - LSTM model , we find a 2 - layer architecture works substantially better than the vanilla 1 - layer model , and use it in all our experiments .",A.1 Hyperparameters TACRED,A.1 Hyperparameters TACRED,relation-classification,4,5,0.3571428571428571,230,0.8745247148288974,6,0.4,1,0
232,"For both the Tree - LSTM and our models , we apply path - centric pruning with K = 1 , as we find that this generates best results for all models ( also see ) .",A.1 Hyperparameters TACRED,A.1 Hyperparameters TACRED,relation-classification,4,6,0.4285714285714285,231,0.8783269961977186,7,0.4666666666666667,1,0
233,"We use the pretrained 300 - dimensional Glo Ve vectors to initialize word embeddings , and we use embedding size of 30 for all other embeddings ( i.e. , POS , NER ) .",A.1 Hyperparameters TACRED,A.1 Hyperparameters TACRED,relation-classification,4,7,0.5,232,0.8821292775665399,8,0.5333333333333333,1,0
234,"We use the dependency parse trees , POS and NER sequences as included in the original release of the dataset , which was generated with Stanford CoreNLP .",A.1 Hyperparameters TACRED,A.1 Hyperparameters TACRED,relation-classification,4,8,0.5714285714285714,233,0.8859315589353612,9,0.6,1,0
235,For regularization we apply dropout with p = 0.5 to all LSTM layers and all but the last GCN layers .,A.1 Hyperparameters TACRED,A.1 Hyperparameters TACRED,relation-classification,4,9,0.6428571428571429,234,0.8897338403041825,10,0.6666666666666666,1,0
236,Sem Eval,A.1 Hyperparameters TACRED,,relation-classification,4,10,0.7142857142857143,235,0.8935361216730038,11,0.7333333333333333,1,0
237,We use LSTM hidden size of 100 and use 1 GCN layer for the SemEval dataset .,A.1 Hyperparameters TACRED,Sem Eval,relation-classification,4,11,0.7857142857142857,236,0.8973384030418251,12,0.8,1,0
238,"We preprocess the dataset with Stanford CoreNLP to generate the dependency parse trees , POS and NER annotations .",A.1 Hyperparameters TACRED,Sem Eval,relation-classification,4,12,0.8571428571428571,237,0.9011406844106464,13,0.8666666666666667,1,0
239,All other hyperparameters are set to be the same .,A.1 Hyperparameters TACRED,Sem Eval,relation-classification,4,13,0.9285714285714286,238,0.9049429657794676,14,0.9333333333333332,1,0
240,"For both datasets , we work with the Universal Dependencies v 1 formalism .",A.1 Hyperparameters TACRED,Sem Eval,relation-classification,4,14,1.0,239,0.908745247148289,15,1.0,1,0
241,A.2 Training,,,relation-classification,4,0,0.0,240,0.9125475285171104,0,0.0,1,0
242,For training we use Stochastic Gradient Descent with an initial learning rate of 1.0 .,A.2 Training,A.2 Training,relation-classification,4,1,0.0454545454545454,241,0.9163498098859316,1,0.0454545454545454,1,0
243,We use a cutoff of 5 for gradient clipping .,A.2 Training,A.2 Training,relation-classification,4,2,0.0909090909090909,242,0.9201520912547528,2,0.0909090909090909,1,0
244,"For GCN models , we train every model for 100 epochs on the TAC - RED dataset , and from epoch 5 we start to anneal the learning rate by a factor of 0.9 every time the F 1 score on the dev set does not increase after an epoch .",A.2 Training,A.2 Training,relation-classification,4,3,0.1363636363636363,243,0.9239543726235742,3,0.1363636363636363,1,0
245,For Tree - LSTM models we find 30 total epochs to be enough .,A.2 Training,A.2 Training,relation-classification,4,4,0.1818181818181818,244,0.9277566539923956,4,0.1818181818181818,1,0
246,"Due to the small size of the SemEval dataset , we train all models for 150 epochs , and use an initial learning rate of 0.5 with a decay rate of 0.95 .",A.2 Training,A.2 Training,relation-classification,4,5,0.2272727272727272,245,0.9315589353612168,5,0.2272727272727272,1,0
247,"In our experiments we found that the output vector h sent tends to have large magnitude , and : Aggregated 5 - run difference compared to PA - LSTM on the TACRED dev set .",A.2 Training,A.2 Training,relation-classification,4,6,0.2727272727272727,246,0.935361216730038,6,0.2727272727272727,1,0
248,"For each example , if X out of 5 GCN models predicted its label correctly and Y PA - LSTM models did , it is aggregated in the bar labeled X ?",A.2 Training,A.2 Training,relation-classification,4,7,0.3181818181818182,247,0.9391634980988594,7,0.3181818181818182,1,0
249,Y .,A.2 Training,,relation-classification,4,8,0.3636363636363636,248,0.9429657794676806,8,0.3636363636363636,1,0
250,""" 0 "" is omitted due to redundancy .",A.2 Training,Y .,relation-classification,4,9,0.4090909090909091,249,0.946768060836502,9,0.4090909090909091,1,0
251,therefore adding the following regularization term to the cross entropy loss of each example improves the results :,A.2 Training,Y .,relation-classification,4,10,0.4545454545454545,250,0.9505703422053232,10,0.4545454545454545,1,0
252,"Here , reg functions as an l 2 regularization on the learned sentence representations .",A.2 Training,Y .,relation-classification,4,11,0.5,251,0.9543726235741444,11,0.5,1,0
253,?,A.2 Training,Y .,relation-classification,4,12,0.5454545454545454,252,0.9581749049429658,12,0.5454545454545454,1,0
254,controls the regularization strength and we set ? = 0.003 .,A.2 Training,Y .,relation-classification,4,13,0.5909090909090909,253,0.9619771863117872,13,0.5909090909090909,1,0
255,We empirically found this to be more effective than applying l 2 regularization on the convolutional weights .,A.2 Training,Y .,relation-classification,4,14,0.6363636363636364,254,0.9657794676806084,14,0.6363636363636364,1,0
256,B Comparing GCN models and PA - LSTM on TACRED,A.2 Training,Y .,relation-classification,4,15,0.6818181818181818,255,0.9695817490494296,15,0.6818181818181818,1,0
257,We compared the performance of both GCN models with the PA - LSTM on the TACRED dev set .,A.2 Training,Y .,relation-classification,4,16,0.7272727272727273,256,0.973384030418251,16,0.7272727272727273,1,0
258,"To minimize randomness that is not inherent to these models , we accumulate statistics over 5 independent runs of each model , and report them in .",A.2 Training,Y .,relation-classification,4,17,0.7727272727272727,257,0.9771863117870724,17,0.7727272727272727,1,0
259,"As is shown in the figure , both GCN models capture very different examples from the PA - LSTM model .",A.2 Training,Y .,relation-classification,4,18,0.8181818181818182,258,0.9809885931558936,18,0.8181818181818182,1,0
260,"In the entire dev set of 22,631 examples , 1,450 had at least 3 more GCN models predicting the label correctly compared to the PA - LSTM , and 1,550 saw an improvement from using the PA - LSTM .",A.2 Training,Y .,relation-classification,4,19,0.8636363636363636,259,0.9847908745247148,19,0.8636363636363636,1,0
261,"The C - GCN , on the other hand , outperformed the PA - LSTM by at least 3 models on a total of 847 examples , and lost by a margin of at least 3 on another 629 examples , as reported in the main text .",A.2 Training,Y .,relation-classification,4,20,0.9090909090909092,260,0.988593155893536,20,0.9090909090909092,1,0
262,This smaller difference is also reflected in the diminished gain from ensembling with the PA - LSTM shown in .,A.2 Training,Y .,relation-classification,4,21,0.9545454545454546,261,0.9923954372623576,21,0.9545454545454546,1,0
263,We hypoth -,A.2 Training,Y .,relation-classification,4,22,1.0,262,0.9961977186311788,22,1.0,1,0
1,title,,,relation-classification,5,0,0.0,0,0.0,0,0.0,1,0
2,End - to - end neural relation extraction using deep biaffine attention,title,title,relation-classification,5,1,0.0,1,0.0088495575221238,1,0.0,1,1
3,abstract,,,relation-classification,5,0,0.0,2,0.0176991150442477,0,0.0,1,0
4,"We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features .",abstract,abstract,relation-classification,5,1,0.3333333333333333,3,0.0265486725663716,1,0.3333333333333333,1,1
5,"The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship .",abstract,abstract,relation-classification,5,2,0.6666666666666666,4,0.0353982300884955,2,0.6666666666666666,1,0
6,"On the benchmark "" relation and entity recognition "" dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .",abstract,abstract,relation-classification,5,3,1.0,5,0.0442477876106194,3,1.0,1,0
7,Introduction,,,relation-classification,5,0,0.0,6,0.0530973451327433,0,0.0,1,0
8,Extracting entities and their semantic relations from raw text is a key information extraction task .,Introduction,Introduction,relation-classification,5,1,0.037037037037037,7,0.0619469026548672,1,0.037037037037037,1,1
9,"For example , given the sentence "" David Foster is the AP 's Northwest regional reporter , based in Seattle "" in the CoNLL04 dataset , our goal is to recognize "" David Foster "" as person , "" AP "" as organization , and "" Northwest "" and "" Seattle "" as location entities , then classifiy entity pairs to extract structured information : Work For ( David Foster , AP ) , OrgBased In ( AP , Northwest ) and OrgBased In ( AP , Seattle ) .",Introduction,Introduction,relation-classification,5,2,0.074074074074074,8,0.0707964601769911,2,0.074074074074074,1,0
10,Such information is useful in many other NLP tasks .,Introduction,Introduction,relation-classification,5,3,0.1111111111111111,9,0.079646017699115,3,0.1111111111111111,1,0
11,"Especially in IR applications such as entity search , structured search and question answering , it helps provide end users with significantly better search experience .",Introduction,Introduction,relation-classification,5,4,0.1481481481481481,10,0.0884955752212389,4,0.1481481481481481,1,0
12,A common relation extraction approach is to construct pipeline systems with separate sub-systems for the two tasks of named entity recognition and relation classification .,Introduction,Introduction,relation-classification,5,5,0.1851851851851851,11,0.0973451327433628,5,0.1851851851851851,1,0
13,"More recently , end - to - end systems which jointly learn to extract entities and relations have been proposed with strong potential to obtain high performance .",Introduction,Introduction,relation-classification,5,6,0.2222222222222222,12,0.1061946902654867,6,0.2222222222222222,1,1
14,Traditional joint approaches are feature - based supervised learning methods which employ numerous syntactic and lexical features based on external NLP tools as well as knowledge base resources .,Introduction,Introduction,relation-classification,5,7,0.2592592592592592,13,0.1150442477876106,7,0.2592592592592592,1,0
15,State - of - the - art relation extraction performance has been obtained by end - to - end models based on neural networks .,Introduction,Introduction,relation-classification,5,8,0.2962962962962963,14,0.1238938053097345,8,0.2962962962962963,1,0
16,"Specifically , proposed a RNNbased model which achieved top results on the CoNLL04 dataset .",Introduction,Introduction,relation-classification,5,9,0.3333333333333333,15,0.1327433628318584,9,0.3333333333333333,1,0
17,Their approach relies on various manually extracted features .,Introduction,,relation-classification,5,10,0.3703703703703703,16,0.1415929203539823,10,0.3703703703703703,1,0
18,Other neural models employ dependency parsing - based information .,Introduction,Their approach relies on various manually extracted features .,relation-classification,5,11,0.4074074074074074,17,0.1504424778761062,11,0.4074074074074074,1,0
19,"In particular , applied bottom - up and top - down tree - structured LSTMs to model dependency paths between entities .",Introduction,Their approach relies on various manually extracted features .,relation-classification,5,12,0.4444444444444444,18,0.1592920353982301,12,0.4444444444444444,1,0
20,integrated implicit syntactic information by using latent feature representations extracted from a pre-trained BiLSTM - based dependency parser .,Introduction,Their approach relies on various manually extracted features .,relation-classification,5,13,0.4814814814814814,19,0.168141592920354,13,0.4814814814814814,1,0
21,"entity recognition , and a CNN on top of the BiLSTM for classifying relations .",Introduction,Their approach relies on various manually extracted features .,relation-classification,5,14,0.5185185185185185,20,0.1769911504424778,14,0.5185185185185185,1,0
22,"Adel and Schtze ( 2017 ) assumed that entity boundaries are given , and trained a CNN to extract context features around the entities , and using these features for entity and relation classification .",Introduction,Their approach relies on various manually extracted features .,relation-classification,5,15,0.5555555555555556,21,0.1858407079646017,15,0.5555555555555556,1,0
23,"Recently , formulated the joint entity and relation extraction problem as a directed graph and proposed a BiLSTM - and transition - based approach to generate the graph incrementally . [ 4 ] extended the multi-head selection - based joint model with adversarial training .",Introduction,Their approach relies on various manually extracted features .,relation-classification,5,16,0.5925925925925926,22,0.1946902654867256,16,0.5925925925925926,1,0
24,"In , the joint task is formulated as a sequence tagging problem , and a BiLSTM with a softmax output layer can then be used for joint prediction .",Introduction,Their approach relies on various manually extracted features .,relation-classification,5,17,0.6296296296296297,23,0.2035398230088495,17,0.6296296296296297,1,0
25,"In this paper , we present a novel end - to - end neural model for joint entity and relation extraction .",Introduction,Their approach relies on various manually extracted features .,relation-classification,5,18,0.6666666666666666,24,0.2123893805309734,18,0.6666666666666666,1,1
26,"As illustrated in , our model architecture can be viewed as a mixture of a named entity recognition ( NER ) component and a relation classification ( RC ) component .",Introduction,Their approach relies on various manually extracted features .,relation-classification,5,19,0.7037037037037037,25,0.2212389380530973,19,0.7037037037037037,1,1
27,Our NER component employs a BiLSTM - CRF architecture to predict entities from input word tokens .,Introduction,Their approach relies on various manually extracted features .,relation-classification,5,20,0.7407407407407407,26,0.2300884955752212,20,0.7407407407407407,1,1
28,"Based on both the input words and the predicted NER labels , the RC component uses another BiLSTM to learn latent features relevant for relation classification .",Introduction,Their approach relies on various manually extracted features .,relation-classification,5,21,0.7777777777777778,27,0.2389380530973451,21,0.7777777777777778,1,1
29,"In most previous neural joint models , the relation classification part relies on a common "" linear "" concatenation - based mechanism over the latent features associated with entity pairs , i.e. the latent features are first concatenated into a single feature vector which is then linearly transformed before being fed into a softmax classifier .",Introduction,Their approach relies on various manually extracted features .,relation-classification,5,22,0.8148148148148148,28,0.247787610619469,22,0.8148148148148148,1,0
30,"In contrast , our RC component takes into account second - order interactions over the latent features via a tensor .",Introduction,Their approach relies on various manually extracted features .,relation-classification,5,23,0.8518518518518519,29,0.2566371681415929,23,0.8518518518518519,1,1
31,"In particular , for relation classification we propose a novel use of the deep biaffine attention mechanism which was first introduced in dependency parsing .",Introduction,Their approach relies on various manually extracted features .,relation-classification,5,24,0.8888888888888888,30,0.2654867256637168,24,0.8888888888888888,1,1
32,"Experimental results on the benchmark "" relation and entity recognition "" dataset CoNLL04 show that our model outperforms previous models , obtaining new stateof - the - art scores .",Introduction,Their approach relies on various manually extracted features .,relation-classification,5,25,0.925925925925926,31,0.2743362831858407,25,0.925925925925926,1,0
33,"In addition , using the biaffine attention improves the performance compared to using the linear mechanism significantly .",Introduction,Their approach relies on various manually extracted features .,relation-classification,5,26,0.9629629629629628,32,0.2831858407079646,26,0.9629629629629628,1,0
34,We also provide an ablation study to investigate effects of different contributing factors in our model .,Introduction,Their approach relies on various manually extracted features .,relation-classification,5,27,1.0,33,0.2920353982300885,27,1.0,1,0
35,Our proposed model,,,relation-classification,5,0,0.0,34,0.3008849557522124,0,0.0,1,0
36,This section details our end - to - end relation extraction model .,Our proposed model,Our proposed model,relation-classification,5,1,0.0434782608695652,35,0.3097345132743362,1,0.0434782608695652,1,0
37,"Given an input sequence of n word tokens w 1 , w 2 , ... , w n , we use a vector vi to represent each i th word w i by concatenating word embedding e",Our proposed model,Our proposed model,relation-classification,5,2,0.0869565217391304,36,0.3185840707964602,2,0.0869565217391304,1,0
38,"Here , for each word type w , we use a one - layer BiLSTM ( BiLSTM char ) to learn its character - level word embedding e ( C ) w.",Our proposed model,Our proposed model,relation-classification,5,3,0.1304347826086956,37,0.3274336283185841,3,0.1304347826086956,1,0
39,Named entity recognition ( NER ) :,Our proposed model,Our proposed model,relation-classification,5,4,0.1739130434782608,38,0.336283185840708,4,0.1739130434782608,1,0
40,"The NER component feeds the sequence of vectors v 1:n with an additional context position index i into another BiLSTM ( BiLSTM NER ) to learn a "" latent "" feature vector representing the i th word token .",Our proposed model,Our proposed model,relation-classification,5,5,0.217391304347826,39,0.3451327433628318,5,0.217391304347826,1,0
41,Then the NER component performs linear transformation of each latent feature vector by using a single - layer feed - forward network ( FFNN NER ) :,Our proposed model,Our proposed model,relation-classification,5,6,0.2608695652173913,40,0.3539823008849557,6,0.2608695652173913,1,0
42,The output layer size of FFNN NER is the number of BIOLU - based NER labels .,Our proposed model,Our proposed model,relation-classification,5,7,0.3043478260869565,41,0.3628318584070796,7,0.3043478260869565,1,0
43,The NER component feeds the output vectors h 1:n into a linear - chain CRF layer for NER label prediction .,Our proposed model,Our proposed model,relation-classification,5,8,0.3478260869565217,42,0.3716814159292035,8,0.3478260869565217,1,0
44,"A cross-entropy loss L NER is computed during training , while the Viterbi algorithm is used for decoding .",Our proposed model,Our proposed model,relation-classification,5,9,0.391304347826087,43,0.3805309734513274,9,0.391304347826087,1,0
45,Our NER component thus is the BiLSTM - CRF model with additional LSTM - based character - level word embeddings .,Our proposed model,Our proposed model,relation-classification,5,10,0.4347826086956521,44,0.3893805309734513,10,0.4347826086956521,1,0
46,"Relation classification ( RC ) : Assume that t 1 , t 2 , ... , tn are NER labels predicted by the NER component for the input words .",Our proposed model,Our proposed model,relation-classification,5,11,0.4782608695652174,45,0.3982300884955752,11,0.4782608695652174,1,0
47,We represent each i th predicted label by a vector embedding e ti .,Our proposed model,Our proposed model,relation-classification,5,12,0.5217391304347826,46,0.4070796460176991,12,0.5217391304347826,1,0
48,We create a sequence of vectors x 1:n in which each x i is computed as :,Our proposed model,Our proposed model,relation-classification,5,13,0.5652173913043478,47,0.415929203539823,13,0.5652173913043478,1,0
49,"As for NER , the RC component also uses a BiLSTM ( BiLSTM RC ) to learn another set of latent feature vectors , but from the sequence x 1:n :",Our proposed model,Our proposed model,relation-classification,5,14,0.6086956521739131,48,0.4247787610619469,14,0.6086956521739131,1,0
50,The RC component further uses these latent vectors r i for relation classification .,Our proposed model,Our proposed model,relation-classification,5,15,0.6521739130434783,49,0.4336283185840708,15,0.6521739130434783,1,0
51,We propose a novel use of the deep biaffine attention mechanism for relation classification .,Our proposed model,Our proposed model,relation-classification,5,16,0.6956521739130435,50,0.4424778761061947,16,0.6956521739130435,1,0
52,"The biaffine attention mechanism was proposed for dependency parsing , helping to produce the best reported parsing performance to date .",Our proposed model,Our proposed model,relation-classification,5,17,0.7391304347826086,51,0.4513274336283185,17,0.7391304347826086,1,0
53,"First , to encode the directionality of a relation , we use two single - layer feed - forward networks to project each r i into head and tail vector representations which correspond to whether the i th word serves as the head or tail argument of the relation :",Our proposed model,Our proposed model,relation-classification,5,18,0.782608695652174,52,0.4601769911504424,18,0.782608695652174,1,0
54,"Following , our RC component incrementally constructs relation candidates using all possible combinations of the last word tokens of predicted entities , i.e. words with L or U labels .",Our proposed model,Our proposed model,relation-classification,5,19,0.8260869565217391,53,0.4690265486725664,19,0.8260869565217391,1,0
55,We assign an entity pair to a negative relation class ( NEG ) when the pair has no relation or when the predicted entities are not correct .,Our proposed model,Our proposed model,relation-classification,5,20,0.8695652173913043,54,0.4778761061946903,20,0.8695652173913043,1,0
56,"For example , for , we would have two relation candidates : NEG ( Paris , International ) and OrgBased In ( International , Paris ) .",Our proposed model,Our proposed model,relation-classification,5,21,0.9130434782608696,55,0.4867256637168141,21,0.9130434782608696,1,0
57,"Then for each head - tail candidate pair ( w j , wk ) , we apply the biaffine attention operator :",Our proposed model,Our proposed model,relation-classification,5,22,0.9565217391304348,56,0.495575221238938,22,0.9565217391304348,1,0
58,3 Experiments,Our proposed model,Our proposed model,relation-classification,5,23,1.0,57,0.504424778761062,23,1.0,1,0
59,Experimental setup,,,relation-classification,5,0,0.0,58,0.5132743362831859,0,0.0,1,0
60,Evaluation scenarios :,Experimental setup,Experimental setup,relation-classification,5,1,0.0476190476190476,59,0.5221238938053098,1,0.0476190476190476,1,0
61,We evaluate our joint model on two evaluation setup scenarios :,Experimental setup,Experimental setup,relation-classification,5,2,0.0952380952380952,60,0.5309734513274337,2,0.0952380952380952,1,0
62,( 1 ) NER&RC :,Experimental setup,Experimental setup,relation-classification,5,3,0.1428571428571428,61,0.5398230088495575,3,0.1428571428571428,1,0
63,A realistic scenario where entity boundaries are not given .,Experimental setup,Experimental setup,relation-classification,5,4,0.1904761904761904,62,0.5486725663716814,4,0.1904761904761904,1,0
64,( 2 ) EC&RC : A less realistic scenario where the entity boundaries are given ] .,Experimental setup,Experimental setup,relation-classification,5,5,0.238095238095238,63,0.5575221238938053,5,0.238095238095238,1,0
65,Thus the NER task which identifies both entity boundaries and classes reduces to the entity classification ( EC ) task .,Experimental setup,Experimental setup,relation-classification,5,6,0.2857142857142857,64,0.5663716814159292,6,0.2857142857142857,1,0
66,"Following , we encode the gold entity boundaries in the BILOU scheme .",Experimental setup,Experimental setup,relation-classification,5,7,0.3333333333333333,65,0.5752212389380531,7,0.3333333333333333,1,0
67,"Then we represent each B , I , O , L or U boundary tag as a vector embedding .",Experimental setup,Experimental setup,relation-classification,5,8,0.3809523809523809,66,0.584070796460177,8,0.3809523809523809,1,0
68,"As a result , the vector vi in Equation 1 now also includes the boundary tag embedding in addition to the word embedding and character - level word embedding .",Experimental setup,Experimental setup,relation-classification,5,9,0.4285714285714285,67,0.5929203539823009,9,0.4285714285714285,1,0
69,"Dataset : We use the benchmark "" entity and relation recognition "" dataset CoNLL04 from .",Experimental setup,Experimental setup,relation-classification,5,10,0.4761904761904761,68,0.6017699115044248,10,0.4761904761904761,1,0
70,"Following , we use the 64%/16%/20 % training / development / test presplit available from Adel and Schtze ( 2017 ) , in which the test set was previously also used by .",Experimental setup,Experimental setup,relation-classification,5,11,0.5238095238095238,69,0.6106194690265486,11,0.5238095238095238,1,0
71,Implementation :,Experimental setup,Experimental setup,relation-classification,5,12,0.5714285714285714,70,0.6194690265486725,12,0.5714285714285714,1,0
72,Our model is implemented using DYNET v 2.0 .,Experimental setup,,relation-classification,5,13,0.6190476190476191,71,0.6283185840707964,13,0.6190476190476191,1,1
73,"We optimize the objective loss using Adam , no mini-batches and run for 100 epochs .",Experimental setup,Our model is implemented using DYNET v 2.0 .,relation-classification,5,14,0.6666666666666666,72,0.6371681415929203,14,0.6666666666666666,1,1
74,We compute the average of NER / EC score and RC score after each training epoch .,Experimental setup,Our model is implemented using DYNET v 2.0 .,relation-classification,5,15,0.7142857142857143,73,0.6460176991150443,15,0.7142857142857143,1,0
75,"We choose the model with the highest average score on the development set , which is then applied to the test set for the final evaluation phase .",Experimental setup,Our model is implemented using DYNET v 2.0 .,relation-classification,5,16,0.7619047619047619,74,0.6548672566371682,16,0.7619047619047619,1,0
76,More details of the implementation as well as optimal hyper - parameters are in the Appendix .,Experimental setup,Our model is implemented using DYNET v 2.0 .,relation-classification,5,17,0.8095238095238095,75,0.6637168141592921,17,0.8095238095238095,1,0
77,Our code is available at : https : //github.com/datquocnguyen/jointRE,Experimental setup,,relation-classification,5,18,0.8571428571428571,76,0.672566371681416,18,0.8571428571428571,1,1
78,"Metric : Similar to previous works in , we use the macro -averaged F1 - score over the entity classes to score NER / EC and over the relation classes to score RC .",Experimental setup,Our code is available at : https : //github.com/datquocnguyen/jointRE,relation-classification,5,19,0.9047619047619048,77,0.6814159292035398,19,0.9047619047619048,1,0
79,More details of the metric are also in the Appendix .,Experimental setup,Our code is available at : https : //github.com/datquocnguyen/jointRE,relation-classification,5,20,0.9523809523809524,78,0.6902654867256637,20,0.9523809523809524,1,0
80,"Unlike previous neural models , we report results as mean and standard deviation of the scores over 10 runs with 10 random seeds .",Experimental setup,Our code is available at : https : //github.com/datquocnguyen/jointRE,relation-classification,5,21,1.0,79,0.6991150442477876,21,1.0,1,0
81,Main results,,,relation-classification,5,0,0.0,80,0.7079646017699115,0,0.0,1,0
82,End - to - end results :,Main results,Main results,relation-classification,5,1,0.0357142857142857,81,0.7168141592920354,1,0.0357142857142857,1,0
83,The first six rows in compare our results with previous state - of - the - art published results on the same test set .,Main results,Main results,relation-classification,5,2,0.0714285714285714,82,0.7256637168141593,2,0.0714285714285714,1,0
84,"In particular , our model obtains 2 + % absolute higher NER and RC scores ( Setup 1 ) than the BiLSTM - CRF - based multihead selection model .",Main results,Main results,relation-classification,5,3,0.1071428571428571,83,0.7345132743362832,3,0.1071428571428571,1,0
85,We also obtain 7 + % higher EC and RC scores ( Setup 2 ) than Adel and Schtze ( 2017 ) .,Main results,Main results,relation-classification,5,4,0.1428571428571428,84,0.7433628318584071,4,0.1428571428571428,1,0
86,"Note that Gupta et al. ( 2016 ) use the same test set as we do , however they report final results on a 80/0 / 20 training / development / test split rather than our 64/16 /20 , i.e. Gupta et al. ( 2016 ) use a larger training set , but producing about 1.5 % lower EC score and similar RC score against ours .",Main results,Main results,relation-classification,5,5,0.1785714285714285,85,0.7522123893805309,5,0.1785714285714285,1,0
87,"These results show that our model performs better than previous state - of - the - art models , using the same setup .",Main results,Main results,relation-classification,5,6,0.2142857142857142,86,0.7610619469026548,6,0.2142857142857142,1,0
88,"In , the last two rows present results reported in and on the dataset CoNLL04 .",Main results,Main results,relation-classification,5,7,0.25,87,0.7699115044247787,7,0.25,1,0
89,"However , these results are not comparable due to their random sampling of the test set , i.e. using different train - test splits .",Main results,Main results,relation-classification,5,8,0.2857142857142857,88,0.7787610619469026,8,0.2857142857142857,1,0
90,Both and employ additional extra features based on external NLP tools and use larger training sets than ours .,Main results,Main results,relation-classification,5,9,0.3214285714285714,89,0.7876106194690266,9,0.3214285714285714,1,0
91,"Specifically , integrate syntactic features by using a pre-trained BiLSTM - based dependency parser to extract BiLSTM - based latent feature representations for words in the input sentence , and then using these latent representations directly as part of the input embeddings in their model .",Main results,Main results,relation-classification,5,10,0.3571428571428571,90,0.7964601769911505,10,0.3571428571428571,1,0
92,We plan to extend our model with their syntactic integration approach to further improve our model performance in future work .,Main results,Main results,relation-classification,5,11,0.3928571428571428,91,0.8053097345132744,11,0.3928571428571428,1,0
93,Ablation analysis :,Main results,Main results,relation-classification,5,12,0.4285714285714285,92,0.8141592920353983,12,0.4285714285714285,1,0
94,"We provide in the results of a pipeline approach where we treat our two NER and RC components as independent networks , and train them separately .",Main results,Main results,relation-classification,5,13,0.4642857142857143,93,0.8230088495575221,13,0.4642857142857143,1,1
95,"Here , the RC network uses gold NER labels when training , and uses predicted labels produced by the NER network when decoding .",Main results,Main results,relation-classification,5,14,0.5,94,0.831858407079646,14,0.5,1,0
96,"We find that the joint approach does slightly better than the pipeline approach in relation classification , although the .",Main results,Main results,relation-classification,5,15,0.5357142857142857,95,0.8407079646017699,15,0.5357142857142857,1,1
97,"Ablation results on the development set . * and ** denote the statistically significant differences against the full results at p < 0.05 and p < 0.01 , respectively ( using the two - tailed paired t- test ) .",Main results,Main results,relation-classification,5,16,0.5714285714285714,96,0.8495575221238938,16,0.5714285714285714,1,0
98,differences are not significant .,Main results,Main results,relation-classification,5,17,0.6071428571428571,97,0.8584070796460177,17,0.6071428571428571,1,0
99,A similar observation is also found in .,Main results,,relation-classification,5,18,0.6428571428571429,98,0.8672566371681416,18,0.6428571428571429,1,0
100,"Also , in preliminary experiments , we do not find any significant difference in performance of our joint model when feeding gold NER labels instead of predicted NER labels into the RC component during training .",Main results,A similar observation is also found in .,relation-classification,5,19,0.6785714285714286,99,0.8761061946902655,19,0.6785714285714286,1,0
101,This is not surprising as the training NER score is at 99 +% .,Main results,A similar observation is also found in .,relation-classification,5,20,0.7142857142857143,100,0.8849557522123894,20,0.7142857142857143,1,0
102,also presents ablation tests over 5 factors of our joint model on the development set .,Main results,A similar observation is also found in .,relation-classification,5,21,0.75,101,0.8938053097345132,21,0.75,1,0
103,"In particular , Setup 1 performances significantly degrade by 4 + % absolutely , when not using the character - level word embeddings .",Main results,A similar observation is also found in .,relation-classification,5,22,0.7857142857142857,102,0.9026548672566372,22,0.7857142857142857,1,0
104,"The performances also decrease when using a softmax classifier for NER label prediction rather than a CRF layer ( here , the decrease is significant ) .",Main results,A similar observation is also found in .,relation-classification,5,23,0.8214285714285714,103,0.911504424778761,23,0.8214285714285714,1,0
105,"In contrast , we do not find any significant difference in Setup 2 scores when not using either the character - level embeddings or the CRF layer , clearly showing the usefulness of the given gold entity boundaries .",Main results,A similar observation is also found in .,relation-classification,5,24,0.8571428571428571,104,0.9203539823008848,24,0.8571428571428571,1,0
106,"The 3 remaining factors , including removing NER label embeddings and not taking either the Bilinear or Linear part ( in Equation 8 ) into the Biaffine attention layer , do not affect the NER / EC score .",Main results,A similar observation is also found in .,relation-classification,5,25,0.8928571428571429,105,0.9292035398230089,25,0.8928571428571429,1,0
107,"However , they significantly decrease the RC score .",Main results,,relation-classification,5,26,0.9285714285714286,106,0.9380530973451328,26,0.9285714285714286,1,0
108,"This is reasonable because those 3 factors are part of the RC component only , thus helpful in predicting relations .",Main results,"However , they significantly decrease the RC score .",relation-classification,5,27,0.9642857142857144,107,0.9469026548672568,27,0.9642857142857144,1,0
109,"More specifically , using the Biaffine attention produces about 1.5 % significant improvements to a common Linear transformation mechanism in relation classification , i.e. , "" w / o Bilinear "" results against the full results in : 65.4 % vs. 66.9 % and 72.0 % vs. 73.3 % ( although using Biaffine increases training time over using Linear by 35 % , relatively ) .",Main results,"However , they significantly decrease the RC score .",relation-classification,5,28,1.0,108,0.9557522123893806,28,1.0,1,0
110,Conclusion,,,relation-classification,5,0,0.0,109,0.9646017699115044,0,0.0,1,0
111,"In this paper , we have presented an end - to - end neural network - based relation extraction model .",Conclusion,Conclusion,relation-classification,5,1,0.3333333333333333,110,0.9734513274336284,1,0.3333333333333333,0,0
112,Our model employs a BiLSTM - CRF architecture for entity recognition and a biaffine attention mechanism for relation classification .,Conclusion,Conclusion,relation-classification,5,2,0.6666666666666666,111,0.9823008849557522,2,0.6666666666666666,0,0
113,"On the benchmark CoNLL04 dataset , our model produces new state - of - the - art performance .",Conclusion,Conclusion,relation-classification,5,3,1.0,112,0.991150442477876,3,1.0,0,0
1,title,,,relation-classification,6,0,0.0,0,0.0,0,0.0,1,0
2,Semantic Relation Classification via Bidirectional LSTM Networks with Entity - aware Attention using Latent Entity Typing,title,title,relation-classification,6,1,0.0,1,0.005524861878453,1,0.0,1,1
3,abstract,,,relation-classification,6,0,0.0,2,0.011049723756906,0,0.0,1,0
4,Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing ( NLP ) .,abstract,abstract,relation-classification,6,1,0.1666666666666666,3,0.0165745856353591,1,0.1666666666666666,1,1
5,"Most previous models for relation classification rely on the high - level lexical and syntatic features obtained by NLP tools such as WordNet , dependency parser , part - ofspeech ( POS ) tagger , and named entity recognizers ( NER ) .",abstract,abstract,relation-classification,6,2,0.3333333333333333,4,0.0220994475138121,2,0.3333333333333333,1,0
6,"In addition , state - of - the - art neural models based on attention mechanisms do not fully utilize information of entity that maybe the most crucial features for relation classification .",abstract,abstract,relation-classification,6,3,0.5,5,0.0276243093922651,3,0.5,1,0
7,"To address these issues , we propose a novel end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) method .",abstract,abstract,relation-classification,6,4,0.6666666666666666,6,0.0331491712707182,4,0.6666666666666666,1,0
8,Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET .,abstract,abstract,relation-classification,6,5,0.8333333333333334,7,0.0386740331491712,5,0.8333333333333334,1,0
9,"Experimental results on the SemEval - 2010 Task 8 , one of the most popular relation classification task , demonstrate that our model outperforms existing state - of the - art models without any high - level features .",abstract,abstract,relation-classification,6,6,1.0,8,0.0441988950276243,6,1.0,1,0
10,Introduction,,,relation-classification,6,0,0.0,9,0.0497237569060773,0,0.0,1,0
11,"Classifying semantic relations between entity pairs in sentences plays a vital role in various NLP tasks , such as information extraction , question answering and knowledge base population .",Introduction,Introduction,relation-classification,6,1,0.0625,10,0.0552486187845303,1,0.0625,1,0
12,A task of relation classification is defined as predicting a semantic relationship between two tagged entities in a sentence .,Introduction,Introduction,relation-classification,6,2,0.125,11,0.0607734806629834,2,0.125,1,1
13,"For example , given a sentence with tagged entity pair , crash and attack , this sentence is classified into the re-lation Cause - Effect ( e1 , e2 ) 1 between the entity pair like .",Introduction,Introduction,relation-classification,6,3,0.1875,12,0.0662983425414364,3,0.1875,1,0
14,"A first entity is surrounded by e 1 and / e 1 , and a second entity is surrounded by e 2 and / e 2 .",Introduction,Introduction,relation-classification,6,4,0.25,13,0.0718232044198895,4,0.25,1,0
15,"Most previous relation classification models rely heavily on high - level lexical and syntactic features obtained from NLP tools such as WordNet , dependency parser , part - of - speech ( POS ) tagger , and named entity recognizer ( NER ) .",Introduction,Introduction,relation-classification,6,5,0.3125,14,0.0773480662983425,5,0.3125,1,0
16,The classification models relying on such features suffer from propagation of implicit error of the tools and they are computationally expensive .,Introduction,Introduction,relation-classification,6,6,0.375,15,0.0828729281767955,6,0.375,1,0
17,"Recently , many studies therefore propose end - toend neural models without the high - level features .",Introduction,Introduction,relation-classification,6,7,0.4375,16,0.0883977900552486,7,0.4375,1,0
18,"Among them , attention - based models , which focus to the most important semantic information in a sentence , show state - of - the - art results in a lot of NLP tasks .",Introduction,Introduction,relation-classification,6,8,0.5,17,0.0939226519337016,8,0.5,1,0
19,"Since these models are mainly proposed for solving translation and language modeling tasks , they could not fully utilize the information of tagged entities in relation classification task .",Introduction,Introduction,relation-classification,6,9,0.5625,18,0.0994475138121546,9,0.5625,1,0
20,"However , tagged entity pairs could be powerful hints for solving relation classification task .",Introduction,Introduction,relation-classification,6,10,0.625,19,0.1049723756906077,10,0.625,1,0
21,"For example , even if we do not consider other words except the crash and attack , we intuitively know that the entity pair has a relation Cause - Effect ( e1 , e2 ) 1 better than Component - Whole ( e1 , e2 ) 1 in To address these issues , We propose a novel endto - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing ( LET ) .",Introduction,Introduction,relation-classification,6,11,0.6875,20,0.1104972375690607,11,0.6875,1,0
22,"To capture the context of sentences , We obtain word representations by self attention mechanisms and build the recurrent neural architecture with Bidirectional Long Short - Term Memory ( LSTM ) networks .",Introduction,Introduction,relation-classification,6,12,0.75,21,0.1160220994475138,12,0.75,1,0
23,Entity - aware attention focuses on the most important semantic information considering entity pairs with word positions relative to these pairs and latent types obtained by LET .,Introduction,Introduction,relation-classification,6,13,0.8125,22,0.1215469613259668,13,0.8125,1,0
24,The contributions of our work are summarized as follows :,Introduction,Introduction,relation-classification,6,14,0.875,23,0.1270718232044199,14,0.875,1,0
25,We propose an novel end - to - end recurrent neural model and an entity - aware attention mechanism with a LET which focuses to semantic information of entities and their latent types ; ( 2 ) Our model obtains 85.2 % F1 - score in SemEval- 2010 Task 8 and it outper - forms existing state - of - the - art models without any highlevel features ;,Introduction,Introduction,relation-classification,6,15,0.9375,24,0.1325966850828729,15,0.9375,1,0
26,"We show that our model is more interpretable since it 's decision making process could be visualized with self attention , entity - aware attention , and LET .",Introduction,Introduction,relation-classification,6,16,1.0,25,0.1381215469613259,16,1.0,1,0
27,Related Work,,,relation-classification,6,0,0.0,26,0.143646408839779,0,0.0,1,0
28,There are several studies for solving relation classification task .,Related Work,Related Work,relation-classification,6,1,0.0526315789473684,27,0.149171270718232,1,0.0526315789473684,0,0
29,Early methods used handcrafted features through a series of NLP tools or manually designing kernels .,Related Work,Related Work,relation-classification,6,2,0.1052631578947368,28,0.154696132596685,2,0.1052631578947368,0,0
30,"These approaches use high - level lexical and syntactic features obtained from NLP tools and manually designing kernels , but the classification models relying on such features suffer from propagation of implicit error of the tools .",Related Work,Related Work,relation-classification,6,3,0.1578947368421052,29,0.1602209944751381,3,0.1578947368421052,0,0
31,"On the other hands , deep neural networks have shown outperform previous models using handcraft features .",Related Work,Related Work,relation-classification,6,4,0.2105263157894736,30,0.1657458563535911,4,0.2105263157894736,0,0
32,"Especially , many researches tried to solve the problem based on end - to - end models using only raw sentences and pre-trained word representations learned by Skip - gram and Continuous Bag - of - Words .",Related Work,Related Work,relation-classification,6,5,0.2631578947368421,31,0.1712707182320442,5,0.2631578947368421,0,0
33,Zeng et al .,Related Work,Related Work,relation-classification,6,6,0.3157894736842105,32,0.1767955801104972,6,0.3157894736842105,0,0
34,employed a deep convolutional neural network ( CNN ) for extracting lexical and sentence level features .,Related Work,Related Work,relation-classification,6,7,0.3684210526315789,33,0.1823204419889502,7,0.3684210526315789,0,0
35,Dos Santos et al.,Related Work,Related Work,relation-classification,6,8,0.4210526315789473,34,0.1878453038674033,8,0.4210526315789473,0,0
36,proposed model for learning vector of each relation class using ranking loss to reduce the impact of artificial classes .,Related Work,Related Work,relation-classification,6,9,0.4736842105263157,35,0.1933701657458563,9,0.4736842105263157,0,0
37,Zhang and Wang used bidirectional recurrent neural network ( RNN ) to learn long - term dependency between entity pairs .,Related Work,Related Work,relation-classification,6,10,0.5263157894736842,36,0.1988950276243093,10,0.5263157894736842,0,0
38,"Fur-thermore , Zhang et al. proposed bidirectional LSTM network ( BLSTM ) utilizing position of words , POS tags , named entity information , dependency parse .",Related Work,Related Work,relation-classification,6,11,0.5789473684210527,37,0.2044198895027624,11,0.5789473684210527,0,0
39,This model resolved vanishing gradient problem appeared in RNNs by using BLSTM .,Related Work,Related Work,relation-classification,6,12,0.631578947368421,38,0.2099447513812154,12,0.631578947368421,0,0
40,"Recently , some researcher have proposed attentionbased models which can focus to the most important semantic information in a sentence .",Related Work,Related Work,relation-classification,6,13,0.6842105263157895,39,0.2154696132596685,13,0.6842105263157895,0,0
41,Zhou et al. combined attention mechanisms with BLSTM .,Related Work,Related Work,relation-classification,6,14,0.7368421052631579,40,0.2209944751381215,14,0.7368421052631579,0,0
42,Xiao and Liu split the sentence into two entities and used two attention - based BLSTM hierarchically .,Related Work,Related Work,relation-classification,6,15,0.7894736842105263,41,0.2265193370165746,15,0.7894736842105263,0,0
43,Shen and Huang proposed attention - based CNN using word level attention mechanism that is able to better determine which parts of the sentence are more influential .,Related Work,Related Work,relation-classification,6,16,0.8421052631578947,42,0.2320441988950276,16,0.8421052631578947,0,0
44,"In contrast with end - to - end model , several works proposed models utilizing the shortest dependency path ( SDP ) between entity pairs of dependency parse trees .",Related Work,Related Work,relation-classification,6,17,0.8947368421052632,43,0.2375690607734806,17,0.8947368421052632,0,0
45,SDP - LSTM model proposed by Yan et al .,Related Work,Related Work,relation-classification,6,18,0.9473684210526316,44,0.2430939226519337,18,0.9473684210526316,0,0
46,and deep recurrent neural networks ( DRNNs ) model proposed by Xu et al eliminate irrelevant words out of SDP and use neural network based on the meaningful words composing SDP .,Related Work,Related Work,relation-classification,6,19,1.0,45,0.2486187845303867,19,1.0,0,0
47,Model,,,relation-classification,6,0,0.0,46,0.2541436464088398,0,0.0,1,0
48,"In this section , we introduce a novel recurrent neural model that incorporate an entity - aware attention mechanism with a LET method in detail .",Model,Model,relation-classification,6,1,0.0131578947368421,47,0.2596685082872928,1,0.1,1,1
49,"As shown inure 2 , our model consists of four main components : Word Representation that maps each word in a sentence into vector representations ; ( 2 ) Self Attention that captures the meaning of the correlation between words based on multi-head attention ; ( 3 ) BLSTM which sequentially encodes the representations of self attention layer ; ( 4 ) Entity - aware Attention that calculates attention weights with respect to the entity pairs , word positions relative to these pairs , and their latent types obtained by LET .",Model,Model,relation-classification,6,2,0.0263157894736842,48,0.2651933701657458,2,0.2,1,1
50,"After that , the features are averaged along the time steps to produce the sentencelevel features .",Model,Model,relation-classification,6,3,0.0394736842105263,49,0.2707182320441988,3,0.3,1,0
51,Word Representation,Model,,relation-classification,6,4,0.0526315789473684,50,0.2762430939226519,4,0.4,1,0
52,Let a input sentence is denoted by,Model,Word Representation,relation-classification,6,5,0.0657894736842105,51,0.281767955801105,5,0.5,1,0
53,where n is the number of words .,Model,Word Representation,relation-classification,6,6,0.0789473684210526,52,0.287292817679558,6,0.6,1,0
54,We transform each word into vector representations by looking up word embedding matrix W word ?,Model,Word Representation,relation-classification,6,7,0.0921052631578947,53,0.292817679558011,7,0.7,1,0
55,"R dw |V | , where d w is the dimension of the vector and | V | is the size of vocabulary .",Model,Word Representation,relation-classification,6,8,0.1052631578947368,54,0.2983425414364641,8,0.8,1,0
56,"Then the word representations X = {x 1 , x 2 , ... , x n } are obtained by mapping w i , the i - th word , to a column vector xi ?",Model,Word Representation,relation-classification,6,9,0.1184210526315789,55,0.3038674033149171,9,0.9,1,0
57,R dw are fed into the next layer .,Model,Word Representation,relation-classification,6,10,0.131578947368421,56,0.3093922651933701,10,1.0,1,0
58,Self Attention,Model,,relation-classification,6,11,0.1447368421052631,57,0.3149171270718232,0,0.0,1,0
59,We can obtain the richer word representations by using self attentions .,Model,Self Attention,relation-classification,6,12,0.1578947368421052,58,0.3204419889502762,1,0.027027027027027,1,0
60,These word representations are considered the context based on correlation between words in a sentence .,Model,Self Attention,relation-classification,6,13,0.1710526315789473,59,0.3259668508287293,2,0.054054054054054,1,0
61,"The illustrates the results of the self attention in the sentence , "" the ?e1 ? pollution ? / e1 ?was caused by the ?e2 ? shipwrek ?/e2 ? "" , which is labeled Cause - Effect ( e1 , e2 ) .",Model,Self Attention,relation-classification,6,14,0.1842105263157894,60,0.3314917127071823,3,0.081081081081081,1,0
62,There are visualizations of the two heads in the multi-head attention applied for self attention .,Model,Self Attention,relation-classification,6,15,0.1973684210526315,61,0.3370165745856354,4,0.1081081081081081,1,0
63,"The color density indicates the attention values , results of Equation 3.1 , which means how much an entity focuses on each word in a sentence .",Model,Self Attention,relation-classification,6,16,0.2105263157894736,62,0.3425414364640884,5,0.1351351351351351,1,0
64,"In , the left represents the words that pollution , the first entity , focuses on and the right represents the words that shipwreck , the second entity , focuses on .",Model,Self Attention,relation-classification,6,17,0.2236842105263158,63,0.3480662983425414,6,0.1621621621621621,1,0
65,"We can recognize that the entity pair is commonly concentrated on was , caused , and each other .",Model,Self Attention,relation-classification,6,18,0.2368421052631578,64,0.3535911602209944,7,0.1891891891891892,1,0
66,"Actually , these words play the most important role in semantically predicting the Cause - Effect ( e1 , e2 ) , which is the relation class of this entity pair .",Model,Self Attention,relation-classification,6,19,0.25,65,0.3591160220994475,8,0.2162162162162162,1,0
67,"shows where the model focuses on the sentence to compute relations between entity pairs , which is the result of visualizing the alpha vectors in Equation 3.9 .",Model,Self Attention,relation-classification,6,20,0.2631578947368421,66,0.3646408839779005,9,0.2432432432432432,1,0
68,"The important words in sentence are highlighted in yellow , which means that the more clearly the color is , the more important it is .",Model,Self Attention,relation-classification,6,21,0.2763157894736842,67,0.3701657458563536,10,0.2702702702702703,1,0
69,"For example , in the first sentence , the inside is strongly highlighted , which is actually the best word representing the relation Component - whole ( e 1 , e2 ) between the given entity pair .",Model,Self Attention,relation-classification,6,22,0.2894736842105263,68,0.3756906077348066,11,0.2972972972972973,1,0
70,"As another example , in the third sentence , the highlighted assess and using represent the relation , Instrument - Agency ( e2 , e1 ) between entity pair , analysts and frequency , well .",Model,Self Attention,relation-classification,6,23,0.3026315789473684,69,0.3812154696132597,12,0.3243243243243243,1,0
71,"We can see that the using is more highlighted than the assess , because the former represents the relation better .",Model,Self Attention,relation-classification,6,24,0.3157894736842105,70,0.3867403314917127,13,0.3513513513513513,1,0
72,"visualizes latent type representation t j? { 1 , 2 } in Equation 3.12",Model,Self Attention,relation-classification,6,25,0.3289473684210526,71,0.3922651933701657,14,0.3783783783783784,1,0
73,"Since the dimensionality of representation vectors are too large to visualize , we applied the t - SNE , one of the most popular dimensionality reduction methods .",Model,Self Attention,relation-classification,6,26,0.3421052631578947,72,0.3977900552486187,15,0.4054054054054054,1,0
74,"In , the red points represent latent type vectors c i?K and the rests are latent type representations t j , where the colors of points are determined by the closest of the latent type vectors in the vector space of the original dimensionality .",Model,Self Attention,relation-classification,6,27,0.3552631578947368,73,0.4033149171270718,16,0.4324324324324324,1,0
75,The points are generally well divided and are almost uniformly distributed without being biased to one side .,Model,Self Attention,relation-classification,6,28,0.3684210526315789,74,0.4088397790055249,17,0.4594594594594595,1,0
76,summarizes the results of extracting 50 entities in close order with each latent type vector .,Model,Self Attention,relation-classification,6,29,0.3815789473684211,75,0.4143646408839779,18,0.4864864864864865,1,0
77,This allows us to roughly understand what latent types of entities are .,Model,Self Attention,relation-classification,6,30,0.3947368421052631,76,0.4198895027624309,19,0.5135135135135135,1,0
78,We use a total of three types and find that similar characteristics appear in words grouped by together .,Model,Self Attention,relation-classification,6,31,0.4078947368421052,77,0.425414364640884,20,0.5405405405405406,1,0
79,"In the type 1 , the words are related to human 's jobs and foods .",Model,Self Attention,relation-classification,6,32,0.4210526315789473,78,0.430939226519337,21,0.5675675675675675,1,0
80,"The type2 has a lot of entities related to machines and engineering like engine , woofer , and motor .",Model,Self Attention,relation-classification,6,33,0.4342105263157895,79,0.43646408839779,22,0.5945945945945946,1,0
81,"Finally , in type3 , there are many words with bad meanings related associated with disasters and :",Model,Self Attention,relation-classification,6,34,0.4473684210526316,80,0.4419889502762431,23,0.6216216216216216,1,0
82,Sets of Entities grouped by Latent Types drugs .,Model,Self Attention,relation-classification,6,35,0.4605263157894737,81,0.4475138121546961,24,0.6486486486486487,1,0
83,"As a result , each type has a set of words with similar characteristics , which can prove that LET works effectively .",Model,Self Attention,relation-classification,6,36,0.4736842105263157,82,0.4530386740331492,25,0.6756756756756757,1,0
84,Bidirectional LSTM,Model,,relation-classification,6,37,0.4868421052631579,83,0.4585635359116022,26,0.7027027027027027,1,0
85,Network,Model,,relation-classification,6,38,0.5,84,0.4640883977900552,27,0.7297297297297297,1,0
86,"For sequentially encoding the output of self attention layer , we use a BLSTM that consists of two sub LSTM networks : a forward LSTM network which encodes the context of a input sentence and a backward LSTM network which encodes that one of the reverse sentence .",Model,Network,relation-classification,6,39,0.5131578947368421,85,0.4696132596685083,28,0.7567567567567568,1,0
87,"More formally , BLSTM works as follows :",Model,Network,relation-classification,6,40,0.5263157894736842,86,0.4751381215469613,29,0.7837837837837838,1,0
88,The representation vectors M obtained from self attention layer are forwarded into to the network step by step .,Model,Network,relation-classification,6,41,0.5394736842105263,87,0.4806629834254143,30,0.8108108108108109,1,0
89,"At the time step t , the hidden state",Model,Network,relation-classification,6,42,0.5526315789473685,88,0.4861878453038674,31,0.8378378378378378,1,0
90,Entity - aware Attention Mechanism,Model,Network,relation-classification,6,43,0.5657894736842105,89,0.4917127071823204,32,0.8648648648648649,1,0
91,Although many models with attention mechanism achieved state - of - the - art performance in many NLP tasks .,Model,Network,relation-classification,6,44,0.5789473684210527,90,0.4972375690607735,33,0.8918918918918919,1,0
92,"However , for the relation classification task , these models lack of prior knowledge forgiven entity pairs , which could be powerful hints for solving the task .",Model,Network,relation-classification,6,45,0.5921052631578947,91,0.5027624309392266,34,0.918918918918919,1,0
93,Relation classification differs from sentence classification in that information about entities is given along with sentences .,Model,Network,relation-classification,6,46,0.6052631578947368,92,0.5082872928176796,35,0.945945945945946,1,0
94,We propose a novel entity - aware attention mechanism for fully utilizing informative factors in given entity pairs .,Model,Network,relation-classification,6,47,0.618421052631579,93,0.5138121546961326,36,0.972972972972973,1,0
95,"Entity - aware attention utilizes the two additional features except H = {h 1 , h 2 , ... , h n } , ( 1 ) relative position features , ( 2 ) entity features with LET , and the final sentence representation z , result of the attention , is computed as follows :",Model,Network,relation-classification,6,48,0.631578947368421,94,0.5193370165745856,37,1.0,1,0
96,Relative Position Features,Model,,relation-classification,6,49,0.6447368421052632,95,0.5248618784530387,0,0.0,1,0
97,"In relation classification , the position of each word relative to entities has been widely used for word representations .",Model,Relative Position Features,relation-classification,6,50,0.6578947368421053,96,0.5303867403314917,1,0.0833333333333333,1,0
98,"Recently , position - aware attention is published as away to use the relative position features more effectively .",Model,Relative Position Features,relation-classification,6,51,0.6710526315789473,97,0.5359116022099447,2,0.1666666666666666,1,0
99,"It is a variant of attention mechanisms , which use not only outputs of BLSTM but also the relative position features when calculating attention weights .",Model,Relative Position Features,relation-classification,6,52,0.6842105263157895,98,0.5414364640883977,3,0.25,1,0
100,We adopt this method with slightly modification as shown in Equation 3.8 .,Model,Relative Position Features,relation-classification,6,53,0.6973684210526315,99,0.5469613259668509,4,0.3333333333333333,1,0
101,"In the equation , p e 1 i ?",Model,Relative Position Features,relation-classification,6,54,0.7105263157894737,100,0.5524861878453039,5,0.4166666666666667,1,0
102,R dp and p e 2 i ?,Model,Relative Position Features,relation-classification,6,55,0.7236842105263158,101,0.5580110497237569,6,0.5,1,0
103,"R dp corresponds to the position of the i - th word relative to the first entity ( e 1 - th word ) and second entity ( e 2 - th word ) in a sentence respectively , where e j ?{ 1 , 2 } is a index of j-th entity .",Model,Relative Position Features,relation-classification,6,56,0.7368421052631579,102,0.56353591160221,7,0.5833333333333334,1,0
104,"Similar to word embeddings , the relative positions are converted to vector representations by looking up learnable embedding matrix W pos ?",Model,Relative Position Features,relation-classification,6,57,0.75,103,0.569060773480663,8,0.6666666666666666,1,0
105,"R dp ( 2L?1 ) , where d p is the dimension of the relative position vectors and L is the maximum sentence length .",Model,Relative Position Features,relation-classification,6,58,0.7631578947368421,104,0.574585635359116,9,0.75,1,0
106,"Finally , the representations of BLSTM layer take into account the context and the positional relationship with entities by concatenating hi , p e 1 i , and p e 2 i .",Model,Relative Position Features,relation-classification,6,59,0.7763157894736842,105,0.580110497237569,10,0.8333333333333334,1,0
107,The representation is linearly transformed by W H ?,Model,Relative Position Features,relation-classification,6,60,0.7894736842105263,106,0.585635359116022,11,0.9166666666666666,1,0
108,R da ( 2 d h + 2 dp ) as in the Equation 3.8 .,Model,Relative Position Features,relation-classification,6,61,0.8026315789473685,107,0.5911602209944752,12,1.0,1,0
109,Entity Features with Latent Type,Model,,relation-classification,6,62,0.8157894736842105,108,0.5966850828729282,0,0.0,1,0
110,"Since entity pairs are powerful hints for solving relation classification task , we involve the entity pairs and their types in the attention mechanism to effectively train relations between entity pairs and other words in a sentence .",Model,Entity Features with Latent Type,relation-classification,6,63,0.8289473684210527,109,0.6022099447513812,1,0.0714285714285714,1,0
111,We employ the two entity - aware features .,Model,Entity Features with Latent Type,relation-classification,6,64,0.8421052631578947,110,0.6077348066298343,2,0.1428571428571428,1,0
112,"The first is the hidden states of BLSTM corresponding to positions of entity pairs , which are high - level features representing entities .",Model,Entity Features with Latent Type,relation-classification,6,65,0.8552631578947368,111,0.6132596685082873,3,0.2142857142857142,1,0
113,These are denoted by h ei ?,Model,Entity Features with Latent Type,relation-classification,6,66,0.868421052631579,112,0.6187845303867403,4,0.2857142857142857,1,0
114,"R 2d h , where e i is index of i - th entity .",Model,Entity Features with Latent Type,relation-classification,6,67,0.881578947368421,113,0.6243093922651933,5,0.3571428571428571,1,0
115,"In addition , latent types of the entities obtained by LET , our proposed novel method , are the second one .",Model,Entity Features with Latent Type,relation-classification,6,68,0.8947368421052632,114,0.6298342541436464,6,0.4285714285714285,1,0
116,"Using types as features can be a great way to improve performance , since the types of entities alone can be inferred the approximate relations .",Model,Entity Features with Latent Type,relation-classification,6,69,0.9078947368421052,115,0.6353591160220995,7,0.5,1,0
117,"Because the annotated types are not given , we use the latent type representations by applying the LET inspired by latent topic clustering , a method for predicting latent topic of texts in question answering task .",Model,Entity Features with Latent Type,relation-classification,6,70,0.9210526315789472,116,0.6408839779005525,8,0.5714285714285714,1,0
118,The LET constructs the type representations by weighting K latent type vectors based on attention mechanisms .,Model,Entity Features with Latent Type,relation-classification,6,71,0.9342105263157896,117,0.6464088397790055,9,0.6428571428571429,1,0
119,The mathematical formulation is the follows :,Model,Entity Features with Latent Type,relation-classification,6,72,0.9473684210526316,118,0.6519337016574586,10,0.7142857142857143,1,0
120,where c i is the i - th latent type vector and K is the number of latent entity types .,Model,Entity Features with Latent Type,relation-classification,6,73,0.9605263157894736,119,0.6574585635359116,11,0.7857142857142857,1,0
121,"As a result , entity features are constructed by concatenating the hidden states corresponding entity positions and types of entity pairs .",Model,Entity Features with Latent Type,relation-classification,6,74,0.9736842105263158,120,0.6629834254143646,12,0.8571428571428571,1,0
122,"After linear transformation of the entity features , they add up with the representations of BLSTM layer as in Equation 3.8 , and the representation of sentence z ?",Model,Entity Features with Latent Type,relation-classification,6,75,0.986842105263158,121,0.6685082872928176,13,0.9285714285714286,1,0
123,R 2 d h is computed by Equations from 3.8 to 3.10 .,Model,Entity Features with Latent Type,relation-classification,6,76,1.0,122,0.6740331491712708,14,1.0,1,0
124,Classification and Training,,,relation-classification,6,0,0.0,123,0.6795580110497238,0,0.0,1,0
125,The sentence representation obtained from the entity - aware attention z is fed into a fully connected softmax layer for classification .,Classification and Training,Classification and Training,relation-classification,6,1,0.0909090909090909,124,0.6850828729281768,1,0.0909090909090909,1,0
126,"It produces the conditional probability p ( y|S , ? ) overall relation types :",Classification and Training,Classification and Training,relation-classification,6,2,0.1818181818181818,125,0.6906077348066298,2,0.1818181818181818,1,0
127,where y is a target relation class and S is the input sentence .,Classification and Training,Classification and Training,relation-classification,6,3,0.2727272727272727,126,0.6961325966850829,3,0.2727272727272727,1,0
128,The ?,Classification and Training,Classification and Training,relation-classification,6,4,0.3636363636363636,127,0.7016574585635359,4,0.3636363636363636,1,0
129,is whole learnable parameters in the whole network including,Classification and Training,Classification and Training,relation-classification,6,5,0.4545454545454545,128,0.7071823204419889,5,0.4545454545454545,1,0
130,where | R| is the number of relation classes .,Classification and Training,Classification and Training,relation-classification,6,6,0.5454545454545454,129,0.712707182320442,6,0.5454545454545454,1,0
131,"A loss function L is the cross entropy between the predictions and the ground truths , which is defined as :",Classification and Training,Classification and Training,relation-classification,6,7,0.6363636363636364,130,0.7182320441988951,7,0.6363636363636364,1,0
132,"where | D| is the size of training dataset and ( S ( i ) , y ( i ) ) is the i - th sample in the dataset .",Classification and Training,Classification and Training,relation-classification,6,8,0.7272727272727273,131,0.7237569060773481,8,0.7272727272727273,1,0
133,We minimize the loss L using AdaDelta optimizer to compute the parameters ? of our model .,Classification and Training,Classification and Training,relation-classification,6,9,0.8181818181818182,132,0.7292817679558011,9,0.8181818181818182,1,0
134,"To alleviate overfitting , we constrain the L2 regularization with the coefficient ?.",Classification and Training,Classification and Training,relation-classification,6,10,0.9090909090909092,133,0.7348066298342542,10,0.9090909090909092,1,0
135,"In addition , the dropout method is applied afterword embedding , LSTM network , and entity - aware attention to prevent co-adaptation of hidden units by randomly omitting feature detectors .",Classification and Training,Classification and Training,relation-classification,6,11,1.0,134,0.7403314917127072,11,1.0,1,0
136,Experiments,,,relation-classification,6,0,0.0,135,0.7458563535911602,0,0.0,1,0
137,Dataset and Evaluation Metrics,,,relation-classification,6,0,0.0,136,0.7513812154696132,0,0.0,1,0
138,"We evaluate our model on the SemEval - 2010 Task 8 dataset , which is an commonly used benchmark for relation classification and compare the results with the state - of - the - art models in this area .",Dataset and Evaluation Metrics,Dataset and Evaluation Metrics,relation-classification,6,1,0.2,137,0.7569060773480663,1,0.2,1,0
139,"The dataset contains 10 distinguished relations , Cause - Effect , Instrument - Agency , Product - Producer , Content - Container , Entity - Origin , Entity - Destination , Component - Whole , Member - Collection , Message - Topic , and Other .",Dataset and Evaluation Metrics,Dataset and Evaluation Metrics,relation-classification,6,2,0.4,138,0.7624309392265194,2,0.4,1,0
140,"The former 9 relations have two directions , whereas Other is not directional , so the total number of relations is 19 .",Dataset and Evaluation Metrics,Dataset and Evaluation Metrics,relation-classification,6,3,0.6,139,0.7679558011049724,3,0.6,1,0
141,"There are 10,717 annotated sentences which consist of 8,000 samples for training and 2,717 samples for testing .",Dataset and Evaluation Metrics,Dataset and Evaluation Metrics,relation-classification,6,4,0.8,140,0.7734806629834254,4,0.8,1,0
142,"We adopt the official evaluation metric of SemEval - 2010 Task 8 , which is based on the macro -averaged F1 - score ( excluding Other ) , and takes into consideration the directionality .",Dataset and Evaluation Metrics,Dataset and Evaluation Metrics,relation-classification,6,5,1.0,141,0.7790055248618785,5,1.0,1,0
143,Implementation Details,,,relation-classification,6,0,0.0,142,0.7845303867403315,0,0.0,1,0
144,We tune the hyperparameters for our model on the development set randomly sampled 800 sentences for validation .,Implementation Details,Implementation Details,relation-classification,6,1,0.0714285714285714,143,0.7900552486187845,1,0.5,1,0
145,The best hyperparameters in our proposed model are shown in following .,Implementation Details,Implementation Details,relation-classification,6,2,0.1428571428571428,144,0.7955801104972375,2,1.0,1,0
146,Hyperparameter,Implementation Details,,relation-classification,6,3,0.2142857142857142,145,0.8011049723756906,0,0.0,1,0
147,"Description Value We use pre-trained weights of the publicly available Glo Ve model to initialize word embeddings in our model , and other weights are randomly initialized from zero-mean Gaussian distribution .",Implementation Details,Hyperparameter,relation-classification,6,4,0.2857142857142857,146,0.8066298342541437,1,0.0909090909090909,1,0
148,compares our Entity - aware Attention LSTM model with state - of - theart models on this relation classification dataset .,Implementation Details,Hyperparameter,relation-classification,6,5,0.3571428571428571,147,0.8121546961325967,2,0.1818181818181818,1,0
149,"We divide the models into three groups , Non-Neural Model , SDP - based Model , and End - to - End Model .",Implementation Details,Hyperparameter,relation-classification,6,6,0.4285714285714285,148,0.8176795580110497,3,0.2727272727272727,1,0
150,"First , the SVM , Non-Neural Model , was top of the SemEval - 2010 task , during the official competition period .",Implementation Details,Hyperparameter,relation-classification,6,7,0.5,149,0.8232044198895028,4,0.3636363636363636,1,0
151,They used many handcraft feature and SVM classifier .,Implementation Details,Hyperparameter,relation-classification,6,8,0.5714285714285714,150,0.8287292817679558,5,0.4545454545454545,1,0
152,"As a result , they achieved an F1-score of 82.2 % .",Implementation Details,Hyperparameter,relation-classification,6,9,0.6428571428571429,151,0.8342541436464088,6,0.5454545454545454,1,0
153,"The second is SDP - based Model such as MVRNN , FCM , DepNN , de pLCNN + NS , SDP - LSTM , and DRNNs .",Implementation Details,Hyperparameter,relation-classification,6,10,0.7142857142857143,152,0.8397790055248618,7,0.6363636363636364,1,0
154,The SDP is reasonable features for detecting semantic structure of sentences .,Implementation Details,Hyperparameter,relation-classification,6,11,0.7857142857142857,153,0.8453038674033149,8,0.7272727272727273,1,0
155,"Actually , the SDP - based models show high performance , but SDP may not always be accurate and the parsing time is exponentially increased by long sentences .",Implementation Details,Hyperparameter,relation-classification,6,12,0.8571428571428571,154,0.850828729281768,9,0.8181818181818182,1,0
156,The last model is End - to - End Model automatically learned internal representations can occur between the original inputs and the final outputs in deep learning .,Implementation Details,Hyperparameter,relation-classification,6,13,0.9285714285714286,155,0.856353591160221,10,0.9090909090909092,1,0
157,"There are CNN - based models such as CNN , CR - CNN , and Attention - CNN and RNN - based models such as BLSTM , Attention - BLSTM , and Hierarchical - BLSTM ( Hier - BLSTM ) for this task .",Implementation Details,Hyperparameter,relation-classification,6,14,1.0,156,0.861878453038674,11,1.0,1,0
158,Experimental Results,,,relation-classification,6,0,0.0,157,0.8674033149171271,0,0.0,1,0
159,Model F1,,,relation-classification,6,0,0.0,158,0.8729281767955801,0,0.0,1,0
160,"Non Our proposed model achieves an F1-score of 85.2 % which outperforms all competing state - of - theart approaches except depLCNN + NS , DRNNs , and Attention - CNN .",Model F1,Model F1,relation-classification,6,1,0.0714285714285714,159,0.8784530386740331,1,0.2,1,1
161,"However , they rely on high - level lexical features such as WordNet , dependency parse trees , POS tags , and NER tags from NLP tools .",Model F1,Model F1,relation-classification,6,2,0.1428571428571428,160,0.8839779005524862,2,0.4,1,0
162,The experimental results show that the LET is effective for relation classification .,Model F1,Model F1,relation-classification,6,3,0.2142857142857142,161,0.8895027624309392,3,0.6,1,0
163,The LET improve a performance of 0.5 % than the model not applied it .,Model F1,Model F1,relation-classification,6,4,0.2857142857142857,162,0.8950276243093923,4,0.8,1,0
164,The model showed the best performance with three types .,Model F1,Model F1,relation-classification,6,5,0.3571428571428571,163,0.9005524861878453,5,1.0,1,0
165,Visualization,Model F1,,relation-classification,6,6,0.4285714285714285,164,0.9060773480662984,0,0.0,1,0
166,There are three different visualization to demonstrate that our model is more interpretable .,Model F1,Visualization,relation-classification,6,7,0.5,165,0.9116022099447514,1,0.1428571428571428,1,0
167,"First , the visualization of self attention shows where each word focus on parts of a sentence .",Model F1,Visualization,relation-classification,6,8,0.5714285714285714,166,0.9171270718232044,2,0.2857142857142857,1,0
168,"By showing the words that the entity pair attends , we can find the words that well represent the relation between them .",Model F1,Visualization,relation-classification,6,9,0.6428571428571429,167,0.9226519337016574,3,0.4285714285714285,1,0
169,"Next , the entity - aware attention visualization shows where the model pays attend to a sentence .",Model F1,Visualization,relation-classification,6,10,0.7142857142857143,168,0.9281767955801103,4,0.5714285714285714,1,0
170,"This visualization result highlights important words in a sentence , which are usually important keywords for classification .",Model F1,Visualization,relation-classification,6,11,0.7857142857142857,169,0.9337016574585636,5,0.7142857142857143,1,0
171,"Finally , we visualize representation of type in LET by using t- SNE , a method for dimensionality reduction , and group the whole entities in the dataset by the its latent types .",Model F1,Visualization,relation-classification,6,12,0.8571428571428571,170,0.9392265193370166,6,0.8571428571428571,1,0
172,Entity - aware Attention,Model F1,Visualization,relation-classification,6,13,0.9285714285714286,171,0.9447513812154696,7,1.0,1,0
173,Latent Entity Type,Model F1,,relation-classification,6,14,1.0,172,0.9502762430939228,0,0.0,1,0
174,Conclusion,,,relation-classification,6,0,0.0,173,0.9558011049723756,0,0.0,1,0
175,"In this paper , we proposed entity - aware attention mechanism with latent entity typing and a novel end - to - end recurrent neural model which incorporates this mechanism for relation classification .",Conclusion,Conclusion,relation-classification,6,1,0.1428571428571428,174,0.9613259668508288,1,0.1428571428571428,0,0
176,Our model achieves 85.2 % F1 - score in SemEval- 2010,Conclusion,Conclusion,relation-classification,6,2,0.2857142857142857,175,0.9668508287292816,2,0.2857142857142857,0,0
177,Task 8 using only raw sentence and word embeddings without any high - level features from NLP tools and it outperforms existing state - of - the - art methods .,Conclusion,Conclusion,relation-classification,6,3,0.4285714285714285,176,0.9723756906077348,3,0.4285714285714285,0,0
178,"In addition , our three visualizations of attention mechanisms applied to the model demonstrate that our model is more interpretable than previous models .",Conclusion,Conclusion,relation-classification,6,4,0.5714285714285714,177,0.9779005524861878,4,0.5714285714285714,0,0
179,We expect our model to be extended not only the relation classification task but also other tasks that entity plays an important role .,Conclusion,Conclusion,relation-classification,6,5,0.7142857142857143,178,0.9834254143646408,5,0.7142857142857143,0,0
180,"Especially , latent entity typing can be effectively applied to sequence modeling task using entity information without NER .",Conclusion,Conclusion,relation-classification,6,6,0.8571428571428571,179,0.988950276243094,6,0.8571428571428571,0,0
181,"In the future , we will propose anew method in question answering or knowledge base population based on relations between entities extracted from our model .",Conclusion,Conclusion,relation-classification,6,7,1.0,180,0.994475138121547,7,1.0,0,0
1,title,,,relation-classification,7,0,0.0,0,0.0,0,0.0,1,0
2,Data and text mining BioBERT : a pre-trained biomedical language representation model for biomedical text mining,title,title,relation-classification,7,1,0.0,1,0.0050251256281407,1,0.0,1,1
3,abstract,,,relation-classification,7,0,0.0,2,0.0100502512562814,0,0.0,1,0
4,Motivation :,abstract,abstract,relation-classification,7,1,0.0833333333333333,3,0.0150753768844221,1,0.0833333333333333,1,0
5,Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows .,abstract,abstract,relation-classification,7,2,0.1666666666666666,4,0.0201005025125628,2,0.1666666666666666,1,0
6,"With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models .",abstract,abstract,relation-classification,7,3,0.25,5,0.0251256281407035,3,0.25,1,1
7,"However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora .",abstract,abstract,relation-classification,7,4,0.3333333333333333,6,0.0301507537688442,4,0.3333333333333333,1,1
8,"In this article , we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora .",abstract,abstract,relation-classification,7,5,0.4166666666666667,7,0.0351758793969849,5,0.4166666666666667,1,1
9,Results :,abstract,abstract,relation-classification,7,6,0.5,8,0.0402010050251256,6,0.5,1,0
10,"We introduce BioBERT ( Bidirectional Encoder Representations from Transformers for Biomedical Text Mining ) , which is a domain - specific language representation model pre-trained on large - scale biomedical corpora .",abstract,abstract,relation-classification,7,7,0.5833333333333334,9,0.0452261306532663,7,0.5833333333333334,1,0
11,"With almost the same architecture across tasks , BioBERT largely outperforms BERT and previous state - of - the - art models in a variety of biomedical text mining tasks when pre-trained on biomedical corpora .",abstract,abstract,relation-classification,7,8,0.6666666666666666,10,0.050251256281407,8,0.6666666666666666,1,0
12,"While BERT obtains performance comparable to that of previous state - of - the - art models , BioBERT significantly outperforms them on the following three representative biomedical text mining tasks : biomedical named entity recognition ( 0.62 % F1 score improvement ) , biomedical relation extraction ( 2.80 % F1 score improvement ) and biomedical question answering ( 12.24 % MRR improvement ) .",abstract,abstract,relation-classification,7,9,0.75,11,0.0552763819095477,9,0.75,1,0
13,Our analysis results show that pre-training BERT on biomedical corpora helps it to understand complex biomedical texts .,abstract,abstract,relation-classification,7,10,0.8333333333333334,12,0.0603015075376884,10,0.8333333333333334,1,0
14,Availability and implementation :,abstract,abstract,relation-classification,7,11,0.9166666666666666,13,0.0653266331658291,11,0.9166666666666666,1,0
15,"We make the pre-trained weights of BioBERT freely available at https://github. com/naver/biobert-pretrained , and the source code for fine - tuning",abstract,abstract,relation-classification,7,12,1.0,14,0.0703517587939698,12,1.0,1,1
16,Introduction,,,relation-classification,7,0,0.0,15,0.0753768844221105,0,0.0,1,0
17,The volume of biomedical literature continues to rapidly increase .,Introduction,Introduction,relation-classification,7,1,0.0625,16,0.0804020100502512,1,0.0625,1,0
18,"On average , more than 3000 new articles are published everyday in peer-reviewed journals , excluding pre-prints and technical reports such as clinical trial reports in various archives .",Introduction,Introduction,relation-classification,7,2,0.125,17,0.0854271356783919,2,0.125,1,0
19,PubMed alone has a total of 29M articles as of January 2019 .,Introduction,Introduction,relation-classification,7,3,0.1875,18,0.0904522613065326,3,0.1875,1,0
20,Reports containing valuable information about new discoveries and new insights are continuously added to the already overwhelming amount of literature .,Introduction,Introduction,relation-classification,7,4,0.25,19,0.0954773869346733,4,0.25,1,0
21,"Consequently , there is increasingly more demand for accurate biomedical text mining tools for extracting information from the literature .",Introduction,Introduction,relation-classification,7,5,0.3125,20,0.100502512562814,5,0.3125,1,0
22,Recent progress of biomedical text mining models was made possible by the advancements of deep learning techniques used in natural language processing ( NLP ) .,Introduction,Introduction,relation-classification,7,6,0.375,21,0.1055276381909547,6,0.375,1,0
23,"For instance , Long Short - Term Memory ( LSTM ) and Conditional Random Field ( CRF ) have greatly improved performance in biomedical named entity recognition ( NER ) over the last few years .",Introduction,Introduction,relation-classification,7,7,0.4375,22,0.1105527638190954,7,0.4375,1,0
24,Other deep learning based models have made improvements in biomedical text mining tasks such as relation extraction ( RE ) and question answering ( QA ) .,Introduction,Introduction,relation-classification,7,8,0.5,23,0.1155778894472361,8,0.5,1,0
25,"However , directly applying state - of - the - art NLP methodologies to biomedical text mining has limitations .",Introduction,Introduction,relation-classification,7,9,0.5625,24,0.1206030150753768,9,0.5625,1,0
26,"First , as recent word representation models such as Word2 Vec , ELMo and BERT are trained and tested mainly on datasets containing general domain texts ( e.g. Wikipedia ) , it is difficult to estimate their performance on datasets containing biomedical texts .",Introduction,Introduction,relation-classification,7,10,0.625,25,0.1256281407035175,10,0.625,1,0
27,"Also , the word distributions of general and biomedical corpora are quite different , which can often be a problem for biomedical text mining models .",Introduction,Introduction,relation-classification,7,11,0.6875,26,0.1306532663316583,11,0.6875,1,1
28,"As a result , recent models in biomedical text mining rely largely on adapted versions of word representations .",Introduction,Introduction,relation-classification,7,12,0.75,27,0.135678391959799,12,0.75,1,0
29,"In this study , we hypothesize that current state - of - the - art word representation models such as BERT need to be trained on biomedical corpora to be effective in biomedical text mining tasks .",Introduction,Introduction,relation-classification,7,13,0.8125,28,0.1407035175879397,13,0.8125,1,0
30,"Previously , Word2 Vec , which is one of the most widely known context independent word representation models , was trained on biomedical corpora which contain terms and expressions that are usually not included in a general domain corpus .",Introduction,Introduction,relation-classification,7,14,0.875,29,0.1457286432160804,14,0.875,1,0
31,"While ELMo and BERT have proven the effectiveness of contextualized word representations , they can not obtain high performance on biomedical corpora because they are pre-trained on only general domain corpora .",Introduction,Introduction,relation-classification,7,15,0.9375,30,0.1507537688442211,15,0.9375,1,0
32,"As BERT achieves very strong results on various NLP tasks while using almost the same structure across the tasks , adapting BERT for the biomedical domain could potentially benefit numerous biomedical NLP researches .",Introduction,Introduction,relation-classification,7,16,1.0,31,0.1557788944723618,16,1.0,1,0
33,Approach,,,relation-classification,7,0,0.0,32,0.1608040201005025,0,0.0,1,0
34,"In this article , we introduce BioBERT , which is a pre-trained language representation model for the biomedical domain .",Approach,Approach,relation-classification,7,1,0.0769230769230769,33,0.1658291457286432,1,0.0769230769230769,1,1
35,The overall process of pre-training and fine - tuning BioBERT is illustrated in .,Approach,Approach,relation-classification,7,2,0.1538461538461538,34,0.1708542713567839,2,0.1538461538461538,1,0
36,"First , we initialize BioBERT with weights from BERT , which was pretrained on general domain corpora ( English Wikipedia and Books Corpus ) .",Approach,Approach,relation-classification,7,3,0.2307692307692307,35,0.1758793969849246,3,0.2307692307692307,1,1
37,"Then , BioBERT is pre-trained on biomedical domain corpora ( PubMed abstracts and PMC full - text articles ) .",Approach,Approach,relation-classification,7,4,0.3076923076923077,36,0.1809045226130653,4,0.3076923076923077,1,1
38,"To show the effectiveness of our approach in biomedical text mining , BioBERT is fine - tuned and evaluated on three popular biomedical text mining tasks .",Approach,Approach,relation-classification,7,5,0.3846153846153846,37,0.185929648241206,5,0.3846153846153846,1,1
39,"We test various pre-training strategies with different combinations and sizes of general domain corpora and biomedical corpora , and analyze the effect of each corpus on pre-training .",Approach,Approach,relation-classification,7,6,0.4615384615384615,38,0.1909547738693467,6,0.4615384615384615,1,0
40,We also provide in - depth analyses of BERT and BioBERT to show the necessity of our pre-training strategies .,Approach,Approach,relation-classification,7,7,0.5384615384615384,39,0.1959798994974874,7,0.5384615384615384,1,0
41,The contributions of our paper are as follows :,Approach,Approach,relation-classification,7,8,0.6153846153846154,40,0.2010050251256281,8,0.6153846153846154,1,0
42,BioBERT is the first domain - specific BERT based model pretrained on biomedical corpora for 23 days on eight NVIDIA V100 GPUs .,Approach,Approach,relation-classification,7,9,0.6923076923076923,41,0.2060301507537688,9,0.6923076923076923,1,0
43,We show that pre-training BERT on biomedical corpora largely improves its performance .,Approach,Approach,relation-classification,7,10,0.7692307692307693,42,0.2110552763819095,10,0.7692307692307693,1,0
44,"BioBERT obtains higher F 1 scores in biomedical NER ( 0.62 ) and biomedical RE ( 2.80 ) , and a higher MRR score ( 12.24 ) in biomedical QA than the current state - of the - art models .",Approach,Approach,relation-classification,7,11,0.8461538461538461,43,0.2160804020100502,11,0.8461538461538461,1,0
45,"Compared with most previous biomedical text mining models that are mainly focused on a single task such as NER or QA , our model BioBERT achieves state - of - the - art performance on various biomedical text mining tasks , while requiring only minimal architectural modifications .",Approach,Approach,relation-classification,7,12,0.9230769230769232,44,0.2211055276381909,12,0.9230769230769232,1,0
46,"We make our pre-processed datasets , the pre-trained weights of BioBERT and the source code for fine - tuning BioBERT publicly available .",Approach,Approach,relation-classification,7,13,1.0,45,0.2261306532663316,13,1.0,1,0
47,Materials and methods,,,relation-classification,7,0,0.0,46,0.2311557788944723,0,0.0,1,0
48,BioBERT basically has the same structure as BERT .,Materials and methods,Materials and methods,relation-classification,7,1,0.0204081632653061,47,0.236180904522613,1,0.0769230769230769,1,0
49,"We briefly discuss the recently proposed BERT , and then we describe in detail the pre-training and fine - tuning process of BioBERT .",Materials and methods,Materials and methods,relation-classification,7,2,0.0408163265306122,48,0.2412060301507537,2,0.1538461538461538,1,0
50,BERT : bidirectional encoder representations from transformers,Materials and methods,Materials and methods,relation-classification,7,3,0.0612244897959183,49,0.2462311557788944,3,0.2307692307692307,1,0
51,Learning word representations from a large amount of unannotated text is a long - established method .,Materials and methods,Materials and methods,relation-classification,7,4,0.0816326530612244,50,0.2512562814070351,4,0.3076923076923077,1,0
52,"While previous models ( e.g. Word2 Vec , GloVe ) focused on learning context independent word representations , recent works have focused on learning context dependent word representations .",Materials and methods,Materials and methods,relation-classification,7,5,0.1020408163265306,51,0.2562814070351759,5,0.3846153846153846,1,0
53,"For instance , ELMo uses a bidirectional language model , while uses machine translation to embed context information into word representations .",Materials and methods,Materials and methods,relation-classification,7,6,0.1224489795918367,52,0.2613065326633166,6,0.4615384615384615,1,0
54,BERT is a contextualized word representation model that is based on a masked language model and pretrained using bidirectional transformers .,Materials and methods,Materials and methods,relation-classification,7,7,0.1428571428571428,53,0.2663316582914573,7,0.5384615384615384,1,0
55,"Due to the nature of language modeling where future words can not be seen , previous language models were limited to a combination of two unidirectional language models ( i.e. left - to - right and right - toleft ) .",Materials and methods,Materials and methods,relation-classification,7,8,0.1632653061224489,54,0.271356783919598,8,0.6153846153846154,1,0
56,"BERT uses a masked language model that predicts randomly masked words in a sequence , and hence can be used for learning bidirectional representations .",Materials and methods,Materials and methods,relation-classification,7,9,0.1836734693877551,55,0.2763819095477386,9,0.6923076923076923,1,0
57,"Also , it obtains state - of - the - art performance on most NLP tasks , while requiring minimal task - specific architectural modification .",Materials and methods,Materials and methods,relation-classification,7,10,0.2040816326530612,56,0.2814070351758794,10,0.7692307692307693,1,0
58,"According to the authors of BERT , incorporating information from bidirectional representations , rather than unidirectional representations , is crucial for representing words in natural language .",Materials and methods,Materials and methods,relation-classification,7,11,0.2244897959183673,57,0.2864321608040201,11,0.8461538461538461,1,0
59,We hypothesize that such bidirectional representations are also critical in biomedical text mining as complex relationships between biomedical terms often exist in a biomedical corpus .,Materials and methods,Materials and methods,relation-classification,7,12,0.2448979591836734,58,0.2914572864321608,12,0.9230769230769232,1,0
60,"Due to the space limitations , we refer readers to fora more detailed description of BERT .",Materials and methods,Materials and methods,relation-classification,7,13,0.2653061224489796,59,0.2964824120603015,13,1.0,1,0
61,Pre-training BioBERT,Materials and methods,Materials and methods,relation-classification,7,14,0.2857142857142857,60,0.3015075376884422,0,0.0,1,0
62,"As a general purpose language representation model , BERT was pretrained on English Wikipedia and Books Corpus .",Materials and methods,Materials and methods,relation-classification,7,15,0.3061224489795918,61,0.3065326633165829,1,0.0769230769230769,1,0
63,"However , biomedical domain texts contain a considerable number of domain - specific .",Materials and methods,Materials and methods,relation-classification,7,16,0.3265306122448979,62,0.3115577889447236,2,0.1538461538461538,1,0
64,"Overview of the pre-training and fine - tuning of BioBERT proper nouns ( e.g. BRCA1 , c.248T > C ) and terms ( e.g. transcriptional , antimicrobial ) , which are understood mostly by biomedical researchers .",Materials and methods,Materials and methods,relation-classification,7,17,0.3469387755102041,63,0.3165829145728643,3,0.2307692307692307,1,0
65,"As a result , NLP models designed for general purpose language understanding often obtains poor performance in biomedical text mining tasks .",Materials and methods,Materials and methods,relation-classification,7,18,0.3673469387755102,64,0.321608040201005,4,0.3076923076923077,1,0
66,"In this work , we pre-train BioBERT on PubMed abstracts ( PubMed ) and PubMed Central full - text articles ( PMC ) .",Materials and methods,Materials and methods,relation-classification,7,19,0.3877551020408163,65,0.3266331658291457,5,0.3846153846153846,1,0
67,"The text corpora used for pre-training of BioBERT are listed in , and the tested combinations of text corpora are listed in .",Materials and methods,Materials and methods,relation-classification,7,20,0.4081632653061224,66,0.3316582914572864,6,0.4615384615384615,1,0
68,"For computational efficiency , whenever the Wiki Books corpora were used for pre-training , we initialized BioBERT with the pre-trained BERT model provided by .",Materials and methods,Materials and methods,relation-classification,7,21,0.4285714285714285,67,0.3366834170854271,7,0.5384615384615384,1,0
69,We define BioBERT as a language representation model whose pre-training corpora includes biomedical corpora ( e.g. BioBERT ( PubMed ) ) .,Materials and methods,Materials and methods,relation-classification,7,22,0.4489795918367347,68,0.3417085427135678,8,0.6153846153846154,1,0
70,"For tokenization , BioBERT uses WordPiece tokenization , which mitigates the out - of - vocabulary issue .",Materials and methods,Materials and methods,relation-classification,7,23,0.4693877551020408,69,0.3467336683417085,9,0.6923076923076923,1,0
71,"With WordPiece tokenization , any new words can be represented by frequent subwords ( e.g. Immunoglobulin >",Materials and methods,Materials and methods,relation-classification,7,24,0.4897959183673469,70,0.3517587939698492,10,0.7692307692307693,1,0
72,I ##mm ##uno ##g ##lo # #bul # #in ) .,Materials and methods,Materials and methods,relation-classification,7,25,0.5102040816326531,71,0.3567839195979899,11,0.8461538461538461,1,0
73,We found that using cased vocabulary ( not lowercasing ) results in slightly better performances in downstream tasks .,Materials and methods,Materials and methods,relation-classification,7,26,0.5306122448979592,72,0.3618090452261306,12,0.9230769230769232,1,0
74,"Although we could have constructed new WordPiece vocabulary based on biomedical corpora , we used the original vocabulary of BERT BASE for the following reasons : ( i ) compatibility of BioBERT with BERT , which allows BERT pre-trained on general domain corpora to be re-used , and makes it easier to interchangeably use existing models based on BERT and BioBERT and ( ii ) any new words may still be represented and fine - tuned for the biomedical domain using the original WordPiece vocabulary of BERT .",Materials and methods,Materials and methods,relation-classification,7,27,0.5510204081632653,73,0.3668341708542713,13,1.0,1,0
75,Fine-tuning BioBERT,Materials and methods,Materials and methods,relation-classification,7,28,0.5714285714285714,74,0.371859296482412,0,0.0,1,0
76,"With minimal architectural modification , BioBERT can be applied to various downstream text mining tasks .",Materials and methods,Materials and methods,relation-classification,7,29,0.5918367346938775,75,0.3768844221105528,1,0.0476190476190476,1,0
77,"We fine - tune BioBERT on the following three representative biomedical text mining tasks : NER , RE and QA .",Materials and methods,Materials and methods,relation-classification,7,30,0.6122448979591837,76,0.3819095477386934,2,0.0952380952380952,1,0
78,"Named entity recognition is one of the most fundamental biomedical text mining tasks , which involves recognizing numerous domain - specific proper nouns in a biomedical corpus .",Materials and methods,Materials and methods,relation-classification,7,31,0.6326530612244898,77,0.3869346733668342,3,0.1428571428571428,1,0
79,"While most previous works were built upon different combinations of LSTMs and CRFs , BERT has a simple architecture based on bidirectional transformers .",Materials and methods,Materials and methods,relation-classification,7,32,0.6530612244897959,78,0.3919597989949748,4,0.1904761904761904,1,0
80,BERT uses a single output layer based on the representations from its last layer to compute only token level BIO2 probabilities .,Materials and methods,Materials and methods,relation-classification,7,33,0.673469387755102,79,0.3969849246231156,5,0.238095238095238,1,0
81,"Note that while previous works in biomedical NER often used word embeddings trained on PubMed or PMC corpora , BioBERT directly learns WordPiece embeddings during pre-training and fine - tuning .",Materials and methods,Materials and methods,relation-classification,7,34,0.6938775510204082,80,0.4020100502512563,6,0.2857142857142857,1,0
82,"For the evaluation metrics of NER , we used entity level precision , recall and F1 score .",Materials and methods,Materials and methods,relation-classification,7,35,0.7142857142857143,81,0.4070351758793969,7,0.3333333333333333,1,0
83,Relation extraction is a task of classifying relations of named entities in a biomedical corpus .,Materials and methods,Materials and methods,relation-classification,7,36,0.7346938775510204,82,0.4120603015075377,8,0.3809523809523809,1,0
84,"We utilized the sentence classifier of the original version of BERT , which uses a [ CLS ] token for the classification of relations .",Materials and methods,Materials and methods,relation-classification,7,37,0.7551020408163265,83,0.4170854271356783,9,0.4285714285714285,1,0
85,Sentence classification is performed using a single output layer based on a [ CLS ] token representation from BERT .,Materials and methods,Materials and methods,relation-classification,7,38,0.7755102040816326,84,0.4221105527638191,10,0.4761904761904761,1,0
86,We anonymized target named entities in a sentence using pre-defined tags such as @ GENE $ or @DISEASE $ .,Materials and methods,Materials and methods,relation-classification,7,39,0.7959183673469388,85,0.4271356783919598,11,0.5238095238095238,1,0
87,"For instance , a sentence with two target entities ( gene and disease in this case ) is represented as "" Serine at position 986 of @GENE $ maybe an independent genetic predictor of angiographic @DISEASE $ . """,Materials and methods,Materials and methods,relation-classification,7,40,0.8163265306122449,86,0.4321608040201005,12,0.5714285714285714,1,0
88,"The precision , recall and F 1 scores on the RE task are reported .",Materials and methods,Materials and methods,relation-classification,7,41,0.8367346938775511,87,0.4371859296482412,13,0.6190476190476191,1,0
89,Question answering is a task of answering questions posed in natural language given related passages .,Materials and methods,Materials and methods,relation-classification,7,42,0.8571428571428571,88,0.4422110552763819,14,0.6666666666666666,1,0
90,"To fine - tune BioBERT for QA , we used the same BERT architecture used for SQuAD .",Materials and methods,Materials and methods,relation-classification,7,43,0.8775510204081632,89,0.4472361809045226,15,0.7142857142857143,1,0
91,We used the BioASQ factoid datasets because their format is similar to that of SQuAD .,Materials and methods,Materials and methods,relation-classification,7,44,0.8979591836734694,90,0.4522613065326633,16,0.7619047619047619,1,0
92,Token level probabilities for the start / end location of answer phrases are computed using a single output layer .,Materials and methods,Materials and methods,relation-classification,7,45,0.9183673469387756,91,0.457286432160804,17,0.8095238095238095,1,0
93,"However , we observed that about 30 % of the BioASQ factoid questions were unanswerable in an extractive QA setting as the exact answers did not appear in the given passages .",Materials and methods,Materials and methods,relation-classification,7,46,0.9387755102040816,92,0.4623115577889447,18,0.8571428571428571,1,0
94,"Like , we excluded the samples with unanswerable questions from the training sets .",Materials and methods,Materials and methods,relation-classification,7,47,0.9591836734693876,93,0.4673366834170854,19,0.9047619047619048,1,0
95,"Also , we used the same pre-training process of , which uses SQuAD , and it largely improved the performance of both BERT and BioBERT .",Materials and methods,Materials and methods,relation-classification,7,48,0.979591836734694,94,0.4723618090452261,20,0.9523809523809524,1,0
96,"We used the following evaluation metrics from BioASQ : strict accuracy , lenient accuracy and mean reciprocal rank .",Materials and methods,Materials and methods,relation-classification,7,49,1.0,95,0.4773869346733668,21,1.0,1,0
97,Results,,,relation-classification,7,0,0.0,96,0.4824120603015075,0,0.0,1,0
98,Datasets,Results,,relation-classification,7,1,0.05,97,0.4874371859296482,0,0.0,1,0
99,The statistics of biomedical NER datasets are listed in .,Results,Datasets,relation-classification,7,2,0.1,98,0.4924623115577889,1,0.0526315789473684,1,0
100,"We used the pre-processed versions of all the NER datasets provided by except the 2010 i 2 b2 / VA , JNLPBA and Species - 800 datasets .",Results,Datasets,relation-classification,7,3,0.15,99,0.4974874371859296,2,0.1052631578947368,1,0
101,The pre-processed NCBI Disease dataset has fewer annotations than the original dataset due to the removal of duplicate articles from its training set .,Results,Datasets,relation-classification,7,4,0.2,100,0.5025125628140703,3,0.1578947368421052,1,0
102,We used the CoNLL format ( https :// github.com/spyysalo/standoff2conll ) for pre-processing the 2010 i 2b2 / VA and JNLPBA datasets .,Results,Datasets,relation-classification,7,5,0.25,101,0.507537688442211,4,0.2105263157894736,1,0
103,The Species - 800 dataset was preprocessed and split based on the dataset of Pyysalo ( https://github. com/spyysalo/s800 ) .,Results,Datasets,relation-classification,7,6,0.3,102,0.5125628140703518,5,0.2631578947368421,1,0
104,"We did not use alternate annotations for the BC2 GM dataset , and all NER evaluations are based on entity - level exact matches .",Results,Datasets,relation-classification,7,7,0.35,103,0.5175879396984925,6,0.3157894736842105,1,0
105,"Note that although there are several other recently introduced high quality biomedical NER datasets , we use datasets that are frequently used by many biomedical NLP researchers , which makes it much easier to compare our work with theirs .",Results,Datasets,relation-classification,7,8,0.4,104,0.5226130653266332,7,0.3684210526315789,1,0
106,The RE datasets contain gene - disease relations and protein - chemical relations ) .,Results,Datasets,relation-classification,7,9,0.45,105,0.5276381909547738,8,0.4210526315789473,1,0
107,Pre-processed GAD and EU - ADR datasets are available with our provided codes .,Results,Datasets,relation-classification,7,10,0.5,106,0.5326633165829145,9,0.4736842105263157,1,0
108,"For the CHEMPROT dataset , we used the same pre-processing procedure described in .",Results,Datasets,relation-classification,7,11,0.55,107,0.5376884422110553,10,0.5263157894736842,1,0
109,"We used the BioASQ factoid datasets , which can be converted into the same format as the SQuAD dataset ) .",Results,Datasets,relation-classification,7,12,0.6,108,0.542713567839196,11,0.5789473684210527,1,0
110,We used full abstracts ( PMIDs ) and related questions and answers provided by the BioASQ organizers .,Results,Datasets,relation-classification,7,13,0.65,109,0.5477386934673367,12,0.631578947368421,1,0
111,We have made the pre-processed BioASQ datasets publicly available .,Results,Datasets,relation-classification,7,14,0.7,110,0.5527638190954773,13,0.6842105263157895,1,0
112,"For all the datasets , we used the same dataset splits used in previous works ) fora fair evaluation ; however , the splits of LINAAEUS and Species - 800 could not be found from and maybe different .",Results,Datasets,relation-classification,7,15,0.75,111,0.5577889447236181,14,0.7368421052631579,1,0
113,"Like previous work , we reported the performance of 10 - fold cross-validation on datasets that do not have separate test sets ( e.g. GAD , EU - ADR ) .",Results,Datasets,relation-classification,7,16,0.8,112,0.5628140703517588,15,0.7894736842105263,1,0
114,We compare BERT and BioBERT with the current state - of - theart models and report their scores .,Results,Datasets,relation-classification,7,17,0.85,113,0.5678391959798995,16,0.8421052631578947,1,0
115,Note that the state - of - the - art models each have a different architecture and training procedure .,Results,Datasets,relation-classification,7,18,0.9,114,0.5728643216080402,17,0.8947368421052632,1,0
116,"For instance , the state - of - the - art model by trained on the JNLPBA dataset is based on multiple Bi - LSTM CRF models with character level CNNs , while the state - of - the - art model by trained on the LINNAEUS dataset uses a Bi - LSTM CRF model with character level LSTMs and is additionally trained on silver - standard datasets .",Results,Datasets,relation-classification,7,19,0.95,115,0.5778894472361809,18,0.9473684210526316,1,0
117,"On the other hand , BERT and BioBERT have exactly the same structure , and use only the gold standard datasets and not any additional datasets .",Results,Datasets,relation-classification,7,20,1.0,116,0.5829145728643216,19,1.0,1,0
118,Experimental setups,,,relation-classification,7,0,0.0,117,0.5879396984924623,0,0.0,1,0
119,We used the BERT BASE model pre-trained on English Wikipedia and Books Corpus for 1 M steps .,Experimental setups,Experimental setups,relation-classification,7,1,0.0625,118,0.592964824120603,1,0.0625,1,1
120,BioBERT v1.0 ( PubMed PMC ) is the version of BioBERT ( PubMed PMC ) trained for 470 K steps .,Experimental setups,Experimental setups,relation-classification,7,2,0.125,119,0.5979899497487438,2,0.125,1,1
121,"When using both the PubMed and PMC corpora , we found that 200K and 270K pre-training steps were optimal for PubMed and PMC , respectively .",Experimental setups,Experimental setups,relation-classification,7,3,0.1875,120,0.6030150753768844,3,0.1875,1,0
122,"We also used the ablated versions of BioBERT v1.0 , which were pre-trained on only PubMed for 200 K steps ( Bio BERT v1.0 ( PubMed ) ) and PMC for 270K steps ( Bio BERT v1.0 ( PMC ) ) .",Experimental setups,Experimental setups,relation-classification,7,4,0.25,121,0.6080402010050251,4,0.25,1,0
123,"After our initial release of BioBERT v 1.0 , we pre-trained BioBERT on PubMed for 1 M steps , and we refer to this version as BioBERT v 1.1 ( PubMed ) .",Experimental setups,Experimental setups,relation-classification,7,5,0.3125,122,0.6130653266331658,5,0.3125,1,0
124,Other hyper - parameters such as batch size and learning rate scheduling for pre-training BioBERT are the same as those for pre-training BERT unless stated otherwise .,Experimental setups,Experimental setups,relation-classification,7,6,0.375,123,0.6180904522613065,6,0.375,1,0
125,"We pre-trained BioBERT using Naver Smart Machine Learning ( NSML ) , which is utilized for large - scale experiments that need to be run on several GPUs .",Experimental setups,Experimental setups,relation-classification,7,7,0.4375,124,0.6231155778894473,7,0.4375,1,0
126,We used eight NVIDIA V100 ( 32GB ) GPUs for the pre-training .,Experimental setups,Experimental setups,relation-classification,7,8,0.5,125,0.628140703517588,8,0.5,1,1
127,"The maximum sequence length was fixed to 512 and the mini-batch size was set to 192 , resulting in 98 304 words per iteration .",Experimental setups,Experimental setups,relation-classification,7,9,0.5625,126,0.6331658291457286,9,0.5625,1,1
128,It takes more than 10 days to pre-train BioBERT v 1.0 ( PubMed PMC ) nearly 23 days for BioBERT v 1.1 ( PubMed ) in this setting .,Experimental setups,Experimental setups,relation-classification,7,10,0.625,127,0.6381909547738693,10,0.625,1,0
129,"Despite our best efforts to use BERT LARGE , we used only BERT BASE due to the computational complexity of BERT LARGE .",Experimental setups,Experimental setups,relation-classification,7,11,0.6875,128,0.6432160804020101,11,0.6875,1,0
130,We used a single NVIDIA Titan Xp ( 12GB ) GPU to fine - tune BioBERT on each task .,Experimental setups,Experimental setups,relation-classification,7,12,0.75,129,0.6482412060301508,12,0.75,1,1
131,Note that the fine - tuning process is more computationally efficient than pre-training BioBERT .,Experimental setups,Experimental setups,relation-classification,7,13,0.8125,130,0.6532663316582915,13,0.8125,1,0
132,"For finetuning , a batch size of 10 , 16 , 32 or 64 was selected , and a learning rate of 5e5 , 3e5 or 1e5 was selected .",Experimental setups,Experimental setups,relation-classification,7,14,0.875,131,0.6582914572864321,14,0.875,1,0
133,Fine - tuning BioBERT on QA and RE tasks took less than an hour as the size of the training data is much smaller than that of the training data used by .,Experimental setups,Experimental setups,relation-classification,7,15,0.9375,132,0.6633165829145728,15,0.9375,1,0
134,"On the other hand , it takes more than 20 epochs for BioBERT to reach its highest performance on the NER datasets .",Experimental setups,Experimental setups,relation-classification,7,16,1.0,133,0.6683417085427136,16,1.0,1,0
135,Experimental results,,,relation-classification,7,0,0.0,134,0.6733668341708543,0,0.0,1,0
136,The results of NER are shown in .,Experimental results,Experimental results,relation-classification,7,1,0.032258064516129,135,0.678391959798995,1,0.0714285714285714,1,1
137,"First , we observe that BERT , which was pre-trained on only the general domain corpus is quite effective , but the micro averaged F 1 score of BERT was lower ( 2.01 lower ) than that of the state - of - the - art models .",Experimental results,Experimental results,relation-classification,7,2,0.064516129032258,136,0.6834170854271356,2,0.1428571428571428,1,0
138,"On the other hand , BioBERT achieves higher scores than BERT on all the datasets .",Experimental results,Experimental results,relation-classification,7,3,0.0967741935483871,137,0.6884422110552764,3,0.2142857142857142,1,1
139,"BioBERT outperformed the state - of - the - art models on six out of nine datasets , and BioBERT v 1.1 ( PubMed ) outperformed the state - of - the - art models by 0.62 in terms of micro averaged F1 score .",Experimental results,Experimental results,relation-classification,7,4,0.1290322580645161,138,0.6934673366834171,4,0.2857142857142857,1,1
140,"The relatively low scores on the LINNAEUS dataset can be attributed to the following : ( i ) the lack of a silver - standard dataset for training previous state - of - the - art models and ( ii ) different training / test set splits used in previous work , which were unavailable .",Experimental results,Experimental results,relation-classification,7,5,0.1612903225806451,139,0.6984924623115578,5,0.3571428571428571,1,0
141,The RE results of each model are shown in .,Experimental results,Experimental results,relation-classification,7,6,0.1935483870967742,140,0.7035175879396985,6,0.4285714285714285,1,1
142,"BERT achieved better performance than the state - of - the - art model on the CHEMPROT dataset , which demonstrates its effectiveness in RE .",Experimental results,Experimental results,relation-classification,7,7,0.2258064516129032,141,0.7085427135678392,7,0.5,1,0
143,"On average ( micro ) , BioBERT v1.0 ( PubMed ) obtained a higher F1 score ( 2.80 higher ) than the state - of - the - art models .",Experimental results,Experimental results,relation-classification,7,8,0.2580645161290322,142,0.7135678391959799,8,0.5714285714285714,1,1
144,"Also , BioBERT achieved the highest F 1 scores on 2 out of 3 biomedical datasets .",Experimental results,Experimental results,relation-classification,7,9,0.2903225806451613,143,0.7185929648241206,9,0.6428571428571429,1,1
145,The QA results are shown in .,Experimental results,Experimental results,relation-classification,7,10,0.3225806451612903,144,0.7236180904522613,10,0.7142857142857143,1,0
146,We micro averaged the best scores of the state - of - the - art models from each batch .,Experimental results,Experimental results,relation-classification,7,11,0.3548387096774194,145,0.7286432160804021,11,0.7857142857142857,1,0
147,BERT obtained a higher micro averaged MRR score ( 7.0 higher ) than the state - of - the - art models .,Experimental results,Experimental results,relation-classification,7,12,0.3870967741935484,146,0.7336683417085427,12,0.8571428571428571,1,0
148,"All versions of BioBERT significantly outperformed BERT and the state - of - the - art models , and in particular , BioBERT v1.1 ( PubMed ) obtained a strict accuracy of 38.77 , a lenient accuracy of 53.81 and a mean reciprocal rank score of 44.77 , all of which were micro averaged .",Experimental results,Experimental results,relation-classification,7,13,0.4193548387096774,147,0.7386934673366834,13,0.9285714285714286,1,0
149,"On all the biomedical QA datasets , BioBERT achieved new state - of - the - art performance in terms of MRR .",Experimental results,Experimental results,relation-classification,7,14,0.4516129032258064,148,0.7437185929648241,14,1.0,1,1
150,Discussion,Experimental results,,relation-classification,7,15,0.4838709677419355,149,0.7487437185929648,0,0.0,1,0
151,We used additional corpora of different sizes for pre-training and investigated their effect on performance .,Experimental results,Discussion,relation-classification,7,16,0.5161290322580645,150,0.7537688442211056,1,0.0625,1,0
152,"For BioBERT v1.0 ( PubMed ) , we set the number of pre-training steps to 200K and varied the size of the PubMed corpus .",Experimental results,Discussion,relation-classification,7,17,0.5483870967741935,151,0.7587939698492462,2,0.125,1,0
153,"shows that the performance of BioBERT v 1.0 ( PubMed ) on three NER datasets ( NCBI Disease , BC2GM , BC4CHEMD ) changes in relation to the size of the PubMed corpus .",Experimental results,Discussion,relation-classification,7,18,0.5806451612903226,152,0.7638190954773869,3,0.1875,1,0
154,"Pre-training on 1 billion words is quite effective , and the performance on each dataset mostly improves until 4.5 billion words .",Experimental results,Discussion,relation-classification,7,19,0.6129032258064516,153,0.7688442211055276,4,0.25,1,0
155,We also saved the pre-trained weights from BioBERT v 1.0 ( PubMed ) at different pre-training steps to measure how the number of pre-training steps affects its performance on fine - tuning tasks .,Experimental results,Discussion,relation-classification,7,20,0.6451612903225806,154,0.7738693467336684,5,0.3125,1,0
156,shows the performance changes of BioBERT v 1.0 ( PubMed ) on the same three NER datasets in relation to the number of pre-training steps .,Experimental results,Discussion,relation-classification,7,21,0.6774193548387096,155,0.7788944723618091,6,0.375,1,0
157,The results clearly show that the performance on each dataset improves as the number of pre-training steps increases .,Experimental results,Discussion,relation-classification,7,22,0.7096774193548387,156,0.7839195979899497,7,0.4375,1,0
158,"Finally , shows the absolute performance improvements of BioBERT v 1.0 ( PubMed PMC ) over BERT on all 15 datasets .",Experimental results,Discussion,relation-classification,7,23,0.7419354838709677,157,0.7889447236180904,8,0.5,1,0
159,"F1 scores were used for NER / RE , and MRR scores were used for QA .",Experimental results,Discussion,relation-classification,7,24,0.7741935483870968,158,0.7939698492462312,9,0.5625,1,0
160,BioBERT significantly improves performance on most of the datasets .,Experimental results,Discussion,relation-classification,7,25,0.8064516129032258,159,0.7989949748743719,10,0.625,1,0
161,"As shown in , we sampled predictions from BERT and BioBERT v 1.1 ( PubMed ) to seethe effect of pre-training on downstream tasks .",Experimental results,Discussion,relation-classification,7,26,0.8387096774193549,160,0.8040201005025126,11,0.6875,1,0
162,BioBERT can recognize biomedical named entities that BERT can not and can find the exact boundaries of named Disease 19 665 BC5 CDR Disease 12 694 BC5CDR Drug / Chem. 15 411 BC4CHEMD Drug / Chem. 79 842 BC2 GM Gene / Protein 20 703 JNLPBA Gene / Protein 35 460 LINNAEUS Species 4077 Species - 800 Species 3708,Experimental results,Discussion,relation-classification,7,27,0.8709677419354839,161,0.8090452261306532,12,0.75,1,0
163,"Note Note : For the CHEMPROT dataset , the number of relations in the training , validation and test sets was summed . 486 150 BioASQ 6 b-factoid 618 161",Experimental results,Discussion,relation-classification,7,28,0.9032258064516128,162,0.8140703517587939,13,0.8125,1,0
164,entities .,Experimental results,Discussion,relation-classification,7,29,0.935483870967742,163,0.8190954773869347,14,0.875,1,0
165,"While BERT often gives incorrect answers to simple biomedical questions , BioBERT provides correct answers to such questions .",Experimental results,Discussion,relation-classification,7,30,0.967741935483871,164,0.8241206030150754,15,0.9375,1,0
166,"Also , BioBERT can provide longer named entities as answers .",Experimental results,Discussion,relation-classification,7,31,1.0,165,0.8291457286432161,16,1.0,1,0
167,Conclusion,,,relation-classification,7,0,0.0,166,0.8341708542713567,0,0.0,1,0
168,"In this article , we introduced BioBERT , which is a pre-trained language representation model for biomedical text mining .",Conclusion,Conclusion,relation-classification,7,1,0.03125,167,0.8391959798994975,1,0.0714285714285714,0,0
169,We showed that pre-training BERT on biomedical corpora is crucial in applying it to the biomedical domain .,Conclusion,Conclusion,relation-classification,7,2,0.0625,168,0.8442211055276382,2,0.1428571428571428,0,0
170,"Requiring minimal task - specific architectural modification , BioBERT outperforms previous models on biomedical text mining tasks such as NER , RE and QA .",Conclusion,Conclusion,relation-classification,7,3,0.09375,169,0.8492462311557789,3,0.2142857142857142,0,0
171,"The pre-released version of BioBERT ( January 2019 ) has already been shown to be very effective in many biomedical text mining tasks such as NER for clinical notes , human phenotype - gene RE and clinical temporal RE .",Conclusion,Conclusion,relation-classification,7,4,0.125,170,0.8542713567839196,4,0.2857142857142857,0,0
172,The following updated versions of BioBERT will be available to the bioNLP community : ( i ) BioBERT BASE and BioBERT LARGE trained on only PubMed abstracts without initialization from the existing BERT model and ( ii ) BioBERT BASE and BioBERT LARGE trained on domain - specific vocabulary based on WordPiece .,Conclusion,Conclusion,relation-classification,7,5,0.15625,171,0.8592964824120602,5,0.3571428571428571,0,0
173,"Notes : Precision ( P ) , Recall ( R ) and F1 ( F ) scores on each dataset are reported .",Conclusion,Conclusion,relation-classification,7,6,0.1875,172,0.864321608040201,6,0.4285714285714285,0,0
174,"The best scores are in bold , and the second best scores are underlined .",Conclusion,Conclusion,relation-classification,7,7,0.21875,173,0.8693467336683417,7,0.5,0,0
175,"We list the scores of the state - of - the - art ( SOTA ) models on different datasets as follows : scores of Xu et al . Notes : Precision ( P ) , Recall ( R ) and F1 ( F ) scores on each dataset are reported .",Conclusion,Conclusion,relation-classification,7,8,0.25,174,0.8743718592964824,8,0.5714285714285714,0,0
176,"The best scores are in bold , and the second best scores are underlined .",Conclusion,Conclusion,relation-classification,7,9,0.28125,175,0.8793969849246231,9,0.6428571428571429,0,0
177,"The scores on GAD and EU - ADR were obtained from , and the scores on CHEMPROT were obtained from .",Conclusion,Conclusion,relation-classification,7,10,0.3125,176,0.8844221105527639,10,0.7142857142857143,0,0
178,"Notes : Strict Accuracy ( S ) , Lenient Accuracy ( L ) and Mean Reciprocal Rank ( M ) scores on each dataset are reported .",Conclusion,Conclusion,relation-classification,7,11,0.34375,177,0.8894472361809045,11,0.7857142857142857,0,0
179,"The best scores are in bold , and the second best scores are underlined .",Conclusion,Conclusion,relation-classification,7,12,0.375,178,0.8944723618090452,12,0.8571428571428571,0,0
180,The best BioASQ 4 b / 5 b / 6 b scores were obtained from the BioASQ leaderboard ( http://participants-area.bioasq.org ) .,Conclusion,Conclusion,relation-classification,7,13,0.40625,179,0.8994974874371859,13,0.9285714285714286,0,0
181,"BERT . . . a case of oral penicillin anaphylaxis is described , and the terminology . . .",Conclusion,Conclusion,relation-classification,7,14,0.4375,180,0.9045226130653268,14,1.0,0,0
182,BioBERT,Conclusion,,relation-classification,7,15,0.46875,181,0.9095477386934674,0,0.0,0,0
183,Corynebacterium minutissimum is the bacteria that leads to cutaneous eruptions of erythrasma . . .,Conclusion,BioBERT,relation-classification,7,16,0.5,182,0.914572864321608,1,0.0,0,0
184,BC2GM,Conclusion,,relation-classification,7,17,0.53125,183,0.9195979899497488,0,0.0,0,0
185,BERT,Conclusion,,relation-classification,7,18,0.5625,184,0.9246231155778896,1,0.5,0,0
186,"Like the DMA , but unlike all other mammalian class II A genes , the zebrafish gene codes for two cysteine residues . . .",Conclusion,BERT,relation-classification,7,19,0.59375,185,0.9296482412060302,2,1.0,0,0
187,QA,Conclusion,,relation-classification,7,20,0.625,186,0.9346733668341708,0,0.0,0,0
188,BioASQ 6 b - factoid,Conclusion,QA,relation-classification,7,21,0.65625,187,0.9396984924623116,1,0.0909090909090909,0,0
189,Q : Which type of urinary incontinence is diagnosed with the Q tip test ?,Conclusion,QA,relation-classification,7,22,0.6875,188,0.9447236180904522,2,0.1818181818181818,0,0
190,BERT,Conclusion,,relation-classification,7,23,0.71875,189,0.949748743718593,3,0.2727272727272727,0,0
191,A total of 25 women affected by clinical stress urinary incontinence ( SUI ) were enrolled .,Conclusion,BERT,relation-classification,7,24,0.75,190,0.9547738693467336,4,0.3636363636363636,0,0
192,After undergoing ( . . .),Conclusion,BERT,relation-classification,7,25,0.78125,191,0.9597989949748744,5,0.4545454545454545,0,0
193,"Q-tip test , . . .",Conclusion,BERT,relation-classification,7,26,0.8125,192,0.964824120603015,6,0.5454545454545454,0,0
194,Q : Which bacteria causes erythrasma ?,Conclusion,BERT,relation-classification,7,27,0.84375,193,0.9698492462311558,7,0.6363636363636364,0,0
195,BERT,Conclusion,,relation-classification,7,28,0.875,194,0.9748743718592964,8,0.7272727272727273,0,0
196,Corynebacterium minutissimum is the bacteria that leads to cutaneous eruptions of erythrasma . . .,Conclusion,BERT,relation-classification,7,29,0.90625,195,0.9798994974874372,9,0.8181818181818182,0,0
197,Note :,Conclusion,BERT,relation-classification,7,30,0.9375,196,0.984924623115578,10,0.9090909090909092,0,0
198,Predicted named entities for NER and predicted answers for QA are in bold .,Conclusion,BERT,relation-classification,7,31,0.96875,197,0.9899497487437184,11,1.0,0,0
199,Funding,Conclusion,,relation-classification,7,32,1.0,198,0.9949748743718592,0,0.0,0,0
1,title,,,relation-classification,8,0,0.0,0,0.0,0,0.0,1,0
2,Extracting Multiple - Relations in One - Pass with Pre-Trained Transformers,title,title,relation-classification,8,1,0.0,1,0.0072992700729927,1,0.0,1,1
3,abstract,,,relation-classification,8,0,0.0,2,0.0145985401459854,0,0.0,1,0
4,The state - of - the - art solutions for extracting multiple entity - relations from an input paragraph always require a multiple - pass encoding on the input .,abstract,abstract,relation-classification,8,1,0.2,3,0.0218978102189781,1,0.2,1,1
5,"This paper proposes anew solution that can complete the multiple entityrelations extraction task with only one - pass encoding on the input corpus , and achieve anew state - of - the - art accuracy performance , as demonstrated in the ACE 2005 benchmark .",abstract,abstract,relation-classification,8,2,0.4,4,0.0291970802919708,2,0.4,1,1
6,Our solution is built on top of the pre-trained self - attentive models ( Transformer ) .,abstract,abstract,relation-classification,8,3,0.6,5,0.0364963503649635,3,0.6,1,0
7,"Since our method uses a single - pass to compute all relations at once , it scales to larger datasets easily ; which makes it more usable in real - world applications .",abstract,abstract,relation-classification,8,4,0.8,6,0.0437956204379562,4,0.8,1,0
8,1,abstract,abstract,relation-classification,8,5,1.0,7,0.0510948905109489,5,1.0,1,0
9,Introduction,,,relation-classification,8,0,0.0,8,0.0583941605839416,0,0.0,1,0
10,Relation extraction ( RE ) aims to find the semantic relation between a pair of entity mentions from an input paragraph .,Introduction,Introduction,relation-classification,8,1,0.0909090909090909,9,0.0656934306569343,1,0.0909090909090909,1,1
11,"A solution to this task is essential for many downstream NLP applications such as automatic knowledge - base completion , knowledge base question answering , and symbolic approaches for visual question answering , etc .",Introduction,Introduction,relation-classification,8,2,0.1818181818181818,10,0.072992700729927,2,0.1818181818181818,1,0
12,One particular type of the RE task is multiplerelations extraction ( MRE ) that aims to recognize relations of multiple pairs of entity mentions from an input paragraph .,Introduction,Introduction,relation-classification,8,3,0.2727272727272727,11,0.0802919708029197,3,0.2727272727272727,1,1
13,"Because in real - world applications , whose input paragraphs dominantly contain multiple pairs of entities , an efficient and effective solution for MRE has more important and more practical implications .",Introduction,Introduction,relation-classification,8,4,0.3636363636363636,12,0.0875912408759124,4,0.3636363636363636,1,0
14,"However , nearly all existing approaches for MRE tasks 2014 ; adopt some variations of the singlerelation extraction ( SRE ) approach , which treats each pair of entity mentions as an independent instance , and requires multiple passes of encoding for the multiple pairs of entities .",Introduction,Introduction,relation-classification,8,5,0.4545454545454545,13,0.0948905109489051,5,0.4545454545454545,1,0
15,"The drawback of this approach is obvious - it is computationally expensive and this issue becomes more severe when the input paragraph is large , making this solution impossible to implement when the encoding step involves deep models .",Introduction,Introduction,relation-classification,8,6,0.5454545454545454,14,0.1021897810218978,6,0.5454545454545454,1,0
16,"This work presents a solution that can resolve the inefficient multiple - passes issue of existing solutions for MRE by encoding the input only once , which significantly increases the efficiency and scalability .",Introduction,Introduction,relation-classification,8,7,0.6363636363636364,15,0.1094890510948905,7,0.6363636363636364,1,0
17,"Specifically , the proposed solution is built on top of the existing transformer - based , pretrained general - purposed language encoders .",Introduction,Introduction,relation-classification,8,8,0.7272727272727273,16,0.1167883211678832,8,0.7272727272727273,1,0
18,"In this paper we use Bidirectional Encoder Representations from Transformers ( BERT ) as the transformer - based encoder , but this solution is not limited to using BERT alone .",Introduction,Introduction,relation-classification,8,9,0.8181818181818182,17,0.1240875912408759,9,0.8181818181818182,1,0
19,The two novel modifications to the original BERT architecture are : ( 1 ) we introduce a structured prediction layer for predicting multiple relations for different entity pairs ; and ( 2 ) we make the selfattention layers aware of the positions of all en-tities in the input paragraph .,Introduction,Introduction,relation-classification,8,10,0.9090909090909092,18,0.1313868613138686,10,0.9090909090909092,1,0
20,"To the best of our knowledge , this work is the first promising solution that can solve MRE tasks with such high efficiency ( encoding the input in one - pass ) and effectiveness ( achieve anew state - of - the - art performance ) , as proved on the ACE 2005 benchmark .",Introduction,Introduction,relation-classification,8,11,1.0,19,0.1386861313868613,11,1.0,1,0
21,Background,,,relation-classification,8,0,0.0,20,0.145985401459854,0,0.0,1,0
22,MRE is an important task as it is an essential prior step for many downstream tasks such as automatic knowledge - base completion and questionanswering .,Background,Background,relation-classification,8,1,0.1666666666666666,21,0.1532846715328467,1,0.1666666666666666,0,0
23,Popular MRE benchmarks include ACE and ERE .,Background,,relation-classification,8,2,0.3333333333333333,22,0.1605839416058394,2,0.3333333333333333,0,0
24,"In MRE , given as a text paragraph x = {x 1 , . . . , x N } and M mentions e = {e 1 , . . . , e M } as input , the goal is to predict the relation r ij for each mention pair ( e i , e j ) either belongs to one class of a list of pre-defined relations R or falls into a special class NA indicating no relation .",Background,Popular MRE benchmarks include ACE and ERE .,relation-classification,8,3,0.5,23,0.1678832116788321,3,0.5,0,0
25,"This paper uses "" entity mention "" , "" mention "" and "" entity "" interchangeably .",Background,Popular MRE benchmarks include ACE and ERE .,relation-classification,8,4,0.6666666666666666,24,0.1751824817518248,4,0.6666666666666666,0,0
26,"Existing MRE approaches are based on either feature and model architecture selection techniques , or domain adaptations approaches .",Background,Popular MRE benchmarks include ACE and ERE .,relation-classification,8,5,0.8333333333333334,25,0.1824817518248175,5,0.8333333333333334,0,0
27,"But these approaches require multiple passes of encoding over the paragraph , as they treat a MRE task as multiple passes of a SRE task .",Background,Popular MRE benchmarks include ACE and ERE .,relation-classification,8,6,1.0,26,0.1897810218978102,6,1.0,0,0
28,Proposed Approach,,,relation-classification,8,0,0.0,27,0.1970802919708029,0,0.0,1,0
29,This section describes the proposed one - pass encoding MRE solution .,Proposed Approach,Proposed Approach,relation-classification,8,1,0.0294117647058823,28,0.2043795620437956,1,0.25,1,1
30,"The solution is built upon BERT with a structured prediction layer to enable BERT to predict multiple relations with onepass encoding , and an entity - aware self - attention mechanism to infuse the relational information with regard to multiple entities at each layer of hidden states .",Proposed Approach,Proposed Approach,relation-classification,8,2,0.0588235294117647,29,0.2116788321167883,2,0.5,1,1
31,The framework is illustrated in 1 .,Proposed Approach,,relation-classification,8,3,0.088235294117647,30,0.218978102189781,3,0.75,1,0
32,"It is worth mentioning that our solution can easily use other transformer - based encoders besides BERT , e.g..",Proposed Approach,The framework is illustrated in 1 .,relation-classification,8,4,0.1176470588235294,31,0.2262773722627737,4,1.0,1,0
33,Structured Prediction with BERT for MRE,Proposed Approach,The framework is illustrated in 1 .,relation-classification,8,5,0.1470588235294117,32,0.2335766423357664,0,0.0,1,0
34,The BERT model has been successfully applied to various NLP tasks .,Proposed Approach,The framework is illustrated in 1 .,relation-classification,8,6,0.1764705882352941,33,0.2408759124087591,1,0.0344827586206896,1,0
35,"However , the final prediction layers used in the original model is not applicable to MRE tasks .",Proposed Approach,The framework is illustrated in 1 .,relation-classification,8,7,0.2058823529411764,34,0.2481751824817518,2,0.0689655172413793,1,0
36,The MRE task essentially requires to perform edge predictions over a graph with entities as nodes .,Proposed Approach,The framework is illustrated in 1 .,relation-classification,8,8,0.2352941176470588,35,0.2554744525547445,3,0.1034482758620689,1,0
37,"Inspired by , we propose that we can first encode the input paragraph using BERT .",Proposed Approach,The framework is illustrated in 1 .,relation-classification,8,9,0.2647058823529412,36,0.2627737226277372,4,0.1379310344827586,1,0
38,"Thus , the representation fora pair of entity mentions ( e i , e j ) can be denoted as oi and o j respectively .",Proposed Approach,The framework is illustrated in 1 .,relation-classification,8,10,0.2941176470588235,37,0.2700729927007299,5,0.1724137931034483,1,0
39,"In the case of a mention e i consist of multiple hidden states ( due to the byte pair encoding ) , oi is aggregated via average - pooling over the hidden states of the corresponding tokens in the last BERT layer .",Proposed Approach,The framework is illustrated in 1 .,relation-classification,8,11,0.3235294117647059,38,0.2773722627737226,6,0.2068965517241379,1,0
40,"We then concatenate oi and o j denoted as [ o i : o j ] , and pass it to a linear classifier 2 to predict the relation",Proposed Approach,The framework is illustrated in 1 .,relation-classification,8,12,0.3529411764705882,39,0.2846715328467153,7,0.2413793103448276,1,0
41,where W L ?,Proposed Approach,The framework is illustrated in 1 .,relation-classification,8,13,0.3823529411764705,40,0.291970802919708,8,0.2758620689655172,1,0
42,R 2 dzl .,Proposed Approach,,relation-classification,8,14,0.4117647058823529,41,0.2992700729927007,9,0.3103448275862069,1,0
43,"dz is the dimension of BERT embedding at each token position , and l is the number of relation labels .",Proposed Approach,R 2 dzl .,relation-classification,8,15,0.4411764705882353,42,0.3065693430656934,10,0.3448275862068966,1,0
44,Entity - Aware Self - Attention based on Relative Distance,Proposed Approach,R 2 dzl .,relation-classification,8,16,0.4705882352941176,43,0.3138686131386861,11,0.3793103448275862,1,0
45,This section describes how we encode multiplerelations information into the model .,Proposed Approach,R 2 dzl .,relation-classification,8,17,0.5,44,0.3211678832116788,12,0.4137931034482758,1,0
46,The key concept is to use the relative distances between words and entities to encode the positional information for each entity .,Proposed Approach,R 2 dzl .,relation-classification,8,18,0.5294117647058824,45,0.3284671532846715,13,0.4482758620689655,1,0
47,This information is propagated through different layers via attention computations .,Proposed Approach,R 2 dzl .,relation-classification,8,19,0.5588235294117647,46,0.3357664233576642,14,0.4827586206896552,1,0
48,"Following , for each pair of word tokens ( x i , x j ) with the input representations from the previous layer ash i and h j , we extend the computation of self - attention z i as :",Proposed Approach,R 2 dzl .,relation-classification,8,20,0.5882352941176471,47,0.3430656934306569,15,0.5172413793103449,1,0
49,"are the parameters of the model , and dz is the dimension of the output from the self - attention layer .",Proposed Approach,R 2 dzl .,relation-classification,8,21,0.6176470588235294,48,0.3503649635036496,16,0.5517241379310345,1,0
50,"Compared to standard BERT 's self - attention , a V ij , a K ij ?",Proposed Approach,R 2 dzl .,relation-classification,8,22,0.6470588235294118,49,0.3576642335766423,17,0.5862068965517241,1,0
51,"R dz are extra , which could be viewed as the edge representation between the input element x i and x j .",Proposed Approach,R 2 dzl .,relation-classification,8,23,0.6764705882352942,50,0.364963503649635,18,0.6206896551724138,1,0
52,"Specifically , we devise a V ij and a K ij to encourage each token to be aware of the relative distance to different entity mentions , and vice versa .",Proposed Approach,R 2 dzl .,relation-classification,8,24,0.7058823529411765,51,0.3722627737226277,19,0.6551724137931034,1,0
53,"Adapted from , we argue that the relative distance information will not help if the distance is beyond a certain threshold .",Proposed Approach,R 2 dzl .,relation-classification,8,25,0.7352941176470589,52,0.3795620437956204,20,0.6896551724137931,1,0
54,Hence we first define the distance function as :,Proposed Approach,R 2 dzl .,relation-classification,8,26,0.7647058823529411,53,0.3868613138686131,21,0.7241379310344828,1,0
55,"This distance definition clips all distances to a region [?k , k].",Proposed Approach,R 2 dzl .,relation-classification,8,27,0.7941176470588235,54,0.3941605839416058,22,0.7586206896551724,1,0
56,k is a hyper - parameter to be tuned on the development set .,Proposed Approach,R 2 dzl .,relation-classification,8,28,0.8235294117647058,55,0.4014598540145985,23,0.7931034482758621,1,0
57,We can now define a V ij and a K ij formally as :,Proposed Approach,R 2 dzl .,relation-classification,8,29,0.8529411764705882,56,0.4087591240875912,24,0.8275862068965517,1,0
58,"As defined above , if either token xi or x j belongs to an entity , we will introduce a relative positional representation according to their distance .",Proposed Approach,R 2 dzl .,relation-classification,8,30,0.8823529411764706,57,0.4160583941605839,25,0.8620689655172413,1,0
59,The distance is defined in an entity - centric way as we always compute the distance from the entity mention to the other token .,Proposed Approach,R 2 dzl .,relation-classification,8,31,0.9117647058823528,58,0.4233576642335766,26,0.896551724137931,1,0
60,"If neither xi nor x j are entity mentions , we explicitly assign a zero vector to a K ij and a V ij .",Proposed Approach,R 2 dzl .,relation-classification,8,32,0.9411764705882352,59,0.4306569343065693,27,0.9310344827586208,1,0
61,"When both xi and x j are inside entity mentions , we take the distance as d ( i , j ) to make row - wise attention computation coherent as depicted in .",Proposed Approach,R 2 dzl .,relation-classification,8,33,0.9705882352941176,60,0.437956204379562,28,0.9655172413793104,1,0
62,"During the model fine - tuning , the newly introduced parameters {w K ?k , ... , w K k } and {w V ?k , ... , w V k } are trained from scratch .",Proposed Approach,R 2 dzl .,relation-classification,8,34,1.0,61,0.4452554744525547,29,1.0,1,0
63,Experiments,,,relation-classification,8,0,0.0,62,0.4525547445255474,0,0.0,1,0
64,"We demonstrate the advantage of our method on a popular MRE benchmark , ACE 2005 , and a more recent MRE benchmark , SemEval 2018 Task 7 .",Experiments,Experiments,relation-classification,8,1,0.0909090909090909,63,0.4598540145985401,1,0.5,1,0
65,"We also evaluate on a commonly used SRE benchmark SemEval 2010 task 8 , and achieve state - of - the - art performance .",Experiments,Experiments,relation-classification,8,2,0.1818181818181818,64,0.4671532846715328,2,1.0,1,0
66,Settings,Experiments,,relation-classification,8,3,0.2727272727272727,65,0.4744525547445255,0,0.0,1,0
67,"Data For ACE 2005 , we adopt the multi-domain setting and split the data following : we train on the union of news domain ( nw and bn ) , tune hyperparameters on half of the broadcast conversation ( bc ) domain , and evaluate on the remainder of broadcast conversation ( bc ) , the telephone speech ( cts ) , usenet newsgroups ( un ) , and weblogs ( wl ) domains .",Experiments,Settings,relation-classification,8,4,0.3636363636363636,66,0.4817518248175182,1,0.125,1,0
68,For Se-m,Experiments,,relation-classification,8,5,0.4545454545454545,67,0.4890510948905109,2,0.25,1,0
69,Eval 2018,Experiments,,relation-classification,8,6,0.5454545454545454,68,0.4963503649635036,3,0.375,1,0
70,"Task 7 , we evaluate on its sub-task 1.1 .",Experiments,Eval 2018,relation-classification,8,7,0.6363636363636364,69,0.5036496350364964,4,0.5,1,0
71,We use the same data split in the shared task .,Experiments,Eval 2018,relation-classification,8,8,0.7272727272727273,70,0.5109489051094891,5,0.625,1,0
72,The passages in this task is usually much longer compared to ACE .,Experiments,Eval 2018,relation-classification,8,9,0.8181818181818182,71,0.5182481751824818,6,0.75,1,0
73,"Therefore we adopt the following pre-processing step - for the entity pair in each relation , we assume the tokens related to their relation labeling are always within a range from the fifth token ahead of the pair to the fifth token after it .",Experiments,Eval 2018,relation-classification,8,10,0.9090909090909092,72,0.5255474452554745,7,0.875,1,0
74,"Therefore , the tokens in the original passage that are not covered by the range of ANY input relations , will be removed from the input .",Experiments,Eval 2018,relation-classification,8,11,1.0,73,0.5328467153284672,8,1.0,1,0
75,Methods,,,relation-classification,8,0,0.0,74,0.5401459854014599,0,0.0,1,0
76,"We compare our solution with previous works that predict a single relation per pass , our model that predicts single relation per pass for MRE , and with the following naive modifications of BERT that could achieve MRE in one - pass .",Methods,Methods,relation-classification,8,1,0.125,75,0.5474452554744526,1,0.125,1,1
77,"BERT SP : BERT with structured prediction only , which includes proposed improvement in 3.1 .",Methods,Methods,relation-classification,8,2,0.25,76,0.5547445255474452,2,0.25,1,1
78,"Entity - Aware BERT SP : our full model , which includes both improvements in 3.1 and 3.2 .",Methods,Methods,relation-classification,8,3,0.375,77,0.5620437956204379,3,0.375,1,1
79,BERT SP with position embedding on the final attention layer .,Methods,Methods,relation-classification,8,4,0.5,78,0.5693430656934306,4,0.5,1,1
80,This is a more straightforward way to achieve MRE in one - pass derived from previous works using position embeddings .,Methods,Methods,relation-classification,8,5,0.625,79,0.5766423357664233,5,0.625,1,0
81,"In this method , the BERT model encode the paragraph to the last attention - layer .",Methods,Methods,relation-classification,8,6,0.75,80,0.583941605839416,6,0.75,1,1
82,"Then , for each entity pair , it takes the hidden states , adds the relative position embeddings corresponding to the target entities , and finally makes the relation prediction for this pair .",Methods,Methods,relation-classification,8,7,0.875,81,0.5912408759124088,7,0.875,1,1
83,"BERT SP with entity indicators on input layer : it replaces our structured attention layer , and adds indicators of entities ( transformed to embeddings )",Methods,Methods,relation-classification,8,8,1.0,82,0.5985401459854015,8,1.0,1,0
84,Results on ACE 2005,,,relation-classification,8,0,0.0,83,0.6058394160583942,0,0.0,1,0
85,Main Results gives the overall results on ACE 2005 .,Results on ACE 2005,Results on ACE 2005,relation-classification,8,1,0.0526315789473684,84,0.6131386861313869,1,0.04,1,0
86,The first observation is that our model architecture achieves much better results compared to the previous state - of - the - art methods .,Results on ACE 2005,Results on ACE 2005,relation-classification,8,2,0.1052631578947368,85,0.6204379562043796,2,0.08,1,1
87,"Note that our method was not designed for domain adaptation , it still outperforms those methods with domain adaptation .",Results on ACE 2005,Results on ACE 2005,relation-classification,8,3,0.1578947368421052,86,0.6277372262773723,3,0.12,1,0
88,This result further demonstrates its effectiveness .,Results on ACE 2005,,relation-classification,8,4,0.2105263157894736,87,0.635036496350365,4,0.16,1,0
89,"Among all the BERT - based approaches , finetuning the off - the - shelf BERT does not give a satisfying result , because the sentence embeddings can not distinguish different entity pairs .",Results on ACE 2005,This result further demonstrates its effectiveness .,relation-classification,8,5,0.2631578947368421,88,0.6423357664233577,5,0.2,1,0
90,"The simpler version of our approach , BERT SP , can successfully adapt the pre-trained BERT to the MRE task , and achieves comparable performance at the 3 Note the usage of relative position embeddings does notwork for one - pass MRE , since each word corresponds to a varying number of position embedding vectors .",Results on ACE 2005,This result further demonstrates its effectiveness .,relation-classification,8,6,0.3157894736842105,89,0.6496350364963503,6,0.24,1,0
91,Summing up the vectors confuses this information .,Results on ACE 2005,,relation-classification,8,7,0.3684210526315789,90,0.656934306569343,7,0.28,1,0
92,"It works for the singlerelation per pass setting , but the performance lags behind using only indicators of the two target entities .",Results on ACE 2005,Summing up the vectors confuses this information .,relation-classification,8,8,0.4210526315789473,91,0.6642335766423357,8,0.32,1,0
93,prior state - of - the - art level of the methods without domain adaptation .,Results on ACE 2005,Summing up the vectors confuses this information .,relation-classification,8,9,0.4736842105263157,92,0.6715328467153284,9,0.36,1,0
94,"Our full model , with the structured fine - tuning of attention layers , brings further improvement of about 5.5 % , in the MRE one - pass setting , and achieves anew state - of - the - art performance when compared to the methods with domain adaptation .",Results on ACE 2005,Summing up the vectors confuses this information .,relation-classification,8,10,0.5263157894736842,93,0.6788321167883211,10,0.4,1,1
95,It also beats the other two methods on BERT in Multi- Relation per Pass .,Results on ACE 2005,Summing up the vectors confuses this information .,relation-classification,8,11,0.5789473684210527,94,0.6861313868613139,11,0.44,1,0
96,Performance Gap between MRE in One - Pass and Multi - Pass,Results on ACE 2005,Summing up the vectors confuses this information .,relation-classification,8,12,0.631578947368421,95,0.6934306569343066,12,0.48,1,0
97,The MRE - in - one - pass models can also be used to train and test with one entity pair per pass ( Single - Relation per Pass results in ) .,Results on ACE 2005,Summing up the vectors confuses this information .,relation-classification,8,13,0.6842105263157895,96,0.7007299270072993,13,0.52,1,0
98,"Therefore , we compare the same methods when applied to the multi-relation and singlerelation settings .",Results on ACE 2005,Summing up the vectors confuses this information .,relation-classification,8,14,0.7368421052631579,97,0.708029197080292,14,0.56,1,0
99,"For BERT SP with entity indicators on inputs , it is expected to perform slightly better in the single - relation setting , because of the mixture of information from multiple pairs .",Results on ACE 2005,Summing up the vectors confuses this information .,relation-classification,8,15,0.7894736842105263,98,0.7153284671532847,15,0.6,1,0
100,A 2 % gap is observed as expected .,Results on ACE 2005,,relation-classification,8,16,0.8421052631578947,99,0.7226277372262774,16,0.64,1,0
101,"By comparison , our full model has a much smaller performance gap between two different settings ( and no consistent performance drop over different domains ) .",Results on ACE 2005,A 2 % gap is observed as expected .,relation-classification,8,17,0.8947368421052632,100,0.7299270072992701,17,0.68,1,0
102,The BERT SP is not expected to have a gap as shown in the table .,Results on ACE 2005,A 2 % gap is observed as expected .,relation-classification,8,18,0.9473684210526316,101,0.7372262773722628,18,0.72,1,0
103,"For BERT SP with position embeddings on the final attention layer , we train the model in the single - relation setting and test with two different settings , so the results are the same .",Results on ACE 2005,A 2 % gap is observed as expected .,relation-classification,8,19,1.0,102,0.7445255474452555,19,0.76,1,0
104,Training and Inference Time,,,relation-classification,8,0,0.0,103,0.7518248175182481,20,0.8,1,0
105,"Through our experiment , 4 we verify that the full model with MRE is significantly faster compared to all other methods for both training and inference .",Training and Inference Time,Training and Inference Time,relation-classification,8,1,0.1111111111111111,104,0.7591240875912408,21,0.84,1,0
106,The training time for full model with MRE is 3.5 x faster than it with SRE .,Training and Inference Time,Training and Inference Time,relation-classification,8,2,0.2222222222222222,105,0.7664233576642335,22,0.88,1,0
107,"As for inference speed , the former could reach 126 relation per second compared the later at 23 relation per second .",Training and Inference Time,Training and Inference Time,relation-classification,8,3,0.3333333333333333,106,0.7737226277372263,23,0.92,1,0
108,"It is also much faster when compared to the second best performing approach , BERT SP w / pos-emb on final attlayer , which is at 76 relation per second , as it runs the last layer for every entity pair .",Training and Inference Time,Training and Inference Time,relation-classification,8,4,0.4444444444444444,107,0.781021897810219,24,0.96,1,0
109,"evaluates the usage of different prediction layers , including replacing our linear layer in Eq .",Training and Inference Time,Training and Inference Time,relation-classification,8,5,0.5555555555555556,108,0.7883211678832117,25,1.0,1,0
110,Prediction Module Selection,Training and Inference Time,Training and Inference Time,relation-classification,8,6,0.6666666666666666,109,0.7956204379562044,0,0.0,1,0
111,( 1 ) with MLP or Biaff .,Training and Inference Time,Training and Inference Time,relation-classification,8,7,0.7777777777777778,110,0.8029197080291971,1,0.0769230769230769,1,0
112,Results show that the usage of the linear predictor gives better results .,Training and Inference Time,Training and Inference Time,relation-classification,8,8,0.8888888888888888,111,0.8102189781021898,2,0.1538461538461538,1,0
113,This is consistent with the motivation of the pre-trained encoders : by unsupervised pre-training the encoders are expected to be sufficiently powerful thus adding more complex layers on top does not improve the capacity but leads to more free parameters and higher risk of over-fitting .,Training and Inference Time,Training and Inference Time,relation-classification,8,9,1.0,112,0.8175182481751825,3,0.2307692307692307,1,0
114,Results on SemEval 2018,,,relation-classification,8,0,0.0,113,0.8248175182481752,4,0.3076923076923077,1,0
115,Task 7,Results on SemEval 2018,,relation-classification,8,1,0.1428571428571428,114,0.8321167883211679,5,0.3846153846153846,1,0
116,The results on SemEval 2018 Task 7 are shown in .,Results on SemEval 2018,Task 7,relation-classification,8,2,0.2857142857142857,115,0.8394160583941606,6,0.4615384615384615,1,1
117,"Our Entity - Aware BERT SP gives comparable results to the top - ranked system in the shared task , with slightly lower Macro - F1 , which is the official metric of the task , and slightly higher Micro - F1 .",Results on SemEval 2018,Task 7,relation-classification,8,3,0.4285714285714285,116,0.8467153284671532,7,0.5384615384615384,1,1
118,"When predicting multiple relations in one - pass , we have 0.9 % drop on Macro - F1 , but a further 0.8 % improvement on Micro - F1 .",Results on SemEval 2018,Task 7,relation-classification,8,4,0.5714285714285714,117,0.8540145985401459,8,0.6153846153846154,1,1
119,"Note that the system ( Rotsztejn et al. , 2018 ) integrates many techniques like feature - engineering , model combination , pretraining embeddings on in - domain data , and artificial data generation , while our model is almost a direct adaption from the ACE architecture .",Results on SemEval 2018,Task 7,relation-classification,8,5,0.7142857142857143,118,0.8613138686131386,9,0.6923076923076923,1,0
120,"On the other hand , compared to the top singlemodel result , which makes use of additional word and entity embeddings pretrained on in - domain data , our methods demonstrate clear advantage as a single model .",Results on SemEval 2018,Task 7,relation-classification,8,6,0.8571428571428571,119,0.8686131386861314,10,0.7692307692307693,1,1
121,Additional SRE,Results on SemEval 2018,,relation-classification,8,7,1.0,120,0.8759124087591241,11,0.8461538461538461,1,0
122,Results,,,relation-classification,8,0,0.0,121,0.8832116788321168,12,0.9230769230769232,1,0
123,"We conduct additional experiments on the relation classification task , SemEval 2010 Task 8 , to com -",Results,Results,relation-classification,8,1,0.0,122,0.8905109489051095,13,1.0,1,0
124,Method,,,relation-classification,8,0,0.0,123,0.8978102189781022,0,0.0,1,0
125,Averaged F1 Macro Micro,Method,Method,relation-classification,8,1,0.125,124,0.9051094890510948,1,0.125,1,0
126,Top 3 in the Shared Task 81.7 82.8 78.9 - 76 . pare with models developed on this benchmark .,Method,Method,relation-classification,8,2,0.25,125,0.9124087591240876,2,0.25,1,0
127,"From the results in , our proposed techniques also outperforms the state - of - the - art on this single - relation benchmark .",Method,Method,relation-classification,8,3,0.375,126,0.9197080291970804,3,0.375,1,0
128,"On this single relation task , the out - of - box BERT achieves a reasonable result after finetuning .",Method,Method,relation-classification,8,4,0.5,127,0.927007299270073,4,0.5,1,0
129,"Adding the entity - aware attention gives about 8 % improvement , due to the availability of the entity information during encoding .",Method,Method,relation-classification,8,5,0.625,128,0.9343065693430656,5,0.625,1,0
130,"Adding structured prediction layer to BERT ( i.e. , BERT SP ) also leads to a similar amount of improvement .",Method,Method,relation-classification,8,6,0.75,129,0.9416058394160584,6,0.75,1,0
131,"However , the gap between BERT SP method with and without entity - aware attention is small .",Method,Method,relation-classification,8,7,0.875,130,0.948905109489051,7,0.875,1,0
132,"This is likely because of the bias of data distribution : the assumption that only two target entities exist , makes the two techniques have similar effects .",Method,Method,relation-classification,8,8,1.0,131,0.9562043795620438,8,1.0,1,0
133,Conclusion,,,relation-classification,8,0,0.0,132,0.9635036496350364,0,0.0,1,0
134,"In summary , we propose a first - of - its - kind solution that can simultaneously extract multiple relations with one - pass encoding of an input paragraph for MRE tasks .",Conclusion,Conclusion,relation-classification,8,1,0.25,133,0.9708029197080292,1,0.25,0,0
135,"With the proposed structured prediction and entity - aware self - attention layers on top of BERT , we achieve anew state - of - the - art results with high efficiency on the ACE 2005 benchmark .",Conclusion,Conclusion,relation-classification,8,2,0.5,134,0.978102189781022,2,0.5,0,0
136,"Our idea of encoding a passage regarding multiple entities has potentially broader applications beyond relation extraction , e.g. , entity - centric passage encoding in question answering .",Conclusion,Conclusion,relation-classification,8,3,0.75,135,0.9854014598540146,3,0.75,0,0
137,"In the future work , we will explore the usage of this method with other applications .",Conclusion,Conclusion,relation-classification,8,4,1.0,136,0.9927007299270072,4,1.0,0,0
1,title,,,relation-classification,9,0,0.0,0,0.0,0,0.0,1,0
2,SCIBERT : A Pretrained Language Model for Scientific Text,title,,relation-classification,9,1,0.0,1,0.0068027210884353,1,0.0,1,1
3,abstract,,,relation-classification,9,0,0.0,2,0.0136054421768707,0,0.0,1,0
4,Obtaining large - scale annotated data for NLP tasks in the scientific domain is challenging and expensive .,abstract,abstract,relation-classification,9,1,0.1666666666666666,3,0.0204081632653061,1,0.1666666666666666,1,1
5,"We release SCIBERT , a pretrained language model based on BERT ( Devlin et al. , 2019 ) to address the lack of high - quality , large - scale labeled scientific data .",abstract,abstract,relation-classification,9,2,0.3333333333333333,4,0.0272108843537414,2,0.3333333333333333,1,1
6,SCIBERT leverages unsupervised pretraining on a large multi-domain corpus of scientific publications to improve performance on downstream scientific NLP tasks .,abstract,abstract,relation-classification,9,3,0.5,5,0.0340136054421768,3,0.5,1,0
7,"We evaluate on a suite of tasks including sequence tagging , sentence classification and dependency parsing , with datasets from a variety of scientific domains .",abstract,abstract,relation-classification,9,4,0.6666666666666666,6,0.0408163265306122,4,0.6666666666666666,1,0
8,We demonstrate statistically significant improvements over BERT and achieve new state - of - theart results on several of these tasks .,abstract,abstract,relation-classification,9,5,0.8333333333333334,7,0.0476190476190476,5,0.8333333333333334,1,0
9,The code and pretrained models are available at https://github.com/allenai/scibert/.,abstract,abstract,relation-classification,9,6,1.0,8,0.0544217687074829,6,1.0,1,1
10,Introduction,,,relation-classification,9,0,0.0,9,0.0612244897959183,0,0.0,1,0
11,The exponential increase in the volume of scientific publications in the past decades has made NLP an essential tool for large - scale knowledge extraction and machine reading of these documents .,Introduction,Introduction,relation-classification,9,1,0.0833333333333333,10,0.0680272108843537,1,0.0833333333333333,1,0
12,"Recent progress in NLP has been driven by the adoption of deep neural models , but training such models often requires large amounts of labeled data .",Introduction,Introduction,relation-classification,9,2,0.1666666666666666,11,0.0748299319727891,2,0.1666666666666666,1,0
13,"In general domains , large - scale training data is often possible to obtain through crowdsourcing , but in scientific domains , annotated data is difficult and expensive to collect due to the expertise required for quality annotation .",Introduction,Introduction,relation-classification,9,3,0.25,12,0.0816326530612244,3,0.25,1,1
14,"As shown through ELMo , and BERT , unsupervised pretraining of language models on large corpora significantly improves performance on many NLP tasks .",Introduction,Introduction,relation-classification,9,4,0.3333333333333333,13,0.0884353741496598,4,0.3333333333333333,1,0
15,These models return contextualized embeddings for each token which can be passed into minimal task - specific neural architectures .,Introduction,Introduction,relation-classification,9,5,0.4166666666666667,14,0.0952380952380952,5,0.4166666666666667,1,0
16,"Leveraging the success of unsupervised pretraining has become especially important especially when task - specific annotations are difficult to obtain , like in scientific NLP .",Introduction,Introduction,relation-classification,9,6,0.5,15,0.1020408163265306,6,0.5,1,0
17,"Yet while both BERT and ELMo have released pretrained models , they are still trained on general domain corpora such as news articles and Wikipedia .",Introduction,Introduction,relation-classification,9,7,0.5833333333333334,16,0.1088435374149659,7,0.5833333333333334,1,0
18,"In this work , we make the following contributions :",Introduction,Introduction,relation-classification,9,8,0.6666666666666666,17,0.1156462585034013,8,0.6666666666666666,1,0
19,"( i ) We release SCIBERT , anew resource demonstrated to improve performance on a range of NLP tasks in the scientific domain .",Introduction,Introduction,relation-classification,9,9,0.75,18,0.1224489795918367,9,0.75,1,0
20,SCIBERT is a pretrained language model based on BERT but trained on a large corpus of scientific text .,Introduction,Introduction,relation-classification,9,10,0.8333333333333334,19,0.1292517006802721,10,0.8333333333333334,1,0
21,"( ii ) We perform extensive experimentation to investigate the performance of finetuning versus task - specific architectures atop frozen embeddings , and the effect of having an in - domain vocabulary .",Introduction,Introduction,relation-classification,9,11,0.9166666666666666,20,0.1360544217687075,11,0.9166666666666666,1,0
22,"( iii ) We evaluate SCIBERT on a suite of tasks in the scientific domain , and achieve new state - of the - art ( SOTA ) results on many of these tasks .",Introduction,Introduction,relation-classification,9,12,1.0,21,0.1428571428571428,12,1.0,1,0
23,Methods,,,relation-classification,9,0,0.0,22,0.1496598639455782,0,0.0,1,0
24,Background,,,relation-classification,9,0,0.0,23,0.1564625850340136,1,0.1,1,0
25,The BERT model architecture is based on a multilayer bidirectional Transformer .,Background,Background,relation-classification,9,1,0.0125,24,0.1632653061224489,2,0.2,0,0
26,"Instead of the traditional left - to - right language modeling objective , BERT is trained on two tasks : predicting randomly masked tokens and predicting whether two sentences follow each other .",Background,Background,relation-classification,9,2,0.025,25,0.1700680272108843,3,0.3,0,0
27,SCIB - ERT follows the same architecture as BERT but is instead pretrained on scientific text .,Background,Background,relation-classification,9,3,0.0375,26,0.1768707482993197,4,0.4,0,1
28,Vocabulary BERT uses WordPiece for unsupervised tokenization of the input text .,Background,Background,relation-classification,9,4,0.05,27,0.1836734693877551,5,0.5,0,0
29,The vocabulary is built such that it contains the most frequently used words or subword units .,Background,Background,relation-classification,9,5,0.0625,28,0.1904761904761904,6,0.6,0,0
30,We refer to the original vocabulary released with BERT as BASEVOCAB .,Background,Background,relation-classification,9,6,0.075,29,0.1972789115646258,7,0.7,0,0
31,"We construct SCIVOCAB , anew WordPiece vocabulary on our scientific corpus using the Sen - tencePiece 1 library .",Background,Background,relation-classification,9,7,0.0875,30,0.2040816326530612,8,0.8,0,1
32,We produce both cased and uncased vocabularies and set the vocabulary size to 30 K to match the size of BASEVOCAB .,Background,Background,relation-classification,9,8,0.1,31,0.2108843537414966,9,0.9,0,0
33,"The resulting token overlap between BASEVOCAB and SCIVOCAB is 42 % , illustrating a substantial difference in frequently used words between scientific and general domain texts .",Background,Background,relation-classification,9,9,0.1125,32,0.2176870748299319,10,1.0,0,0
34,Corpus,Background,,relation-classification,9,10,0.125,33,0.2244897959183673,0,0.0,0,0
35,We train SCIBERT on a random sample of 1.14 M papers from Semantic Scholar .,Background,Corpus,relation-classification,9,11,0.1375,34,0.2312925170068027,1,0.1666666666666666,0,1
36,This corpus consists of 18 % papers from the computer science domain and 82 % from the broad biomedical domain .,Background,Corpus,relation-classification,9,12,0.15,35,0.238095238095238,2,0.3333333333333333,0,1
37,"We use the full text of the papers , not just the abstracts .",Background,Corpus,relation-classification,9,13,0.1625,36,0.2448979591836734,3,0.5,0,0
38,"The average paper length is 154 sentences ( 2,769 tokens ) resulting in a corpus size of 3.17B tokens , similar to the 3.3B tokens on which BERT was trained .",Background,Corpus,relation-classification,9,14,0.175,37,0.2517006802721088,4,0.6666666666666666,0,0
39,"We split sentences using Scispa Cy , 2 which is optimized for scientific text .",Background,Corpus,relation-classification,9,15,0.1875,38,0.2585034013605442,5,0.8333333333333334,0,0
40,3 Experimental Setup,Background,Corpus,relation-classification,9,16,0.2,39,0.2653061224489796,6,1.0,0,0
41,Tasks,Background,,relation-classification,9,17,0.2125,40,0.272108843537415,0,0.0,0,0
42,We experiment on the following core NLP tasks :,Background,Tasks,relation-classification,9,18,0.225,41,0.2789115646258503,1,0.1111111111111111,0,0
43,1 .,Background,Tasks,relation-classification,9,19,0.2375,42,0.2857142857142857,2,0.2222222222222222,0,0
44,Named Entity Recognition ( NER ),Background,Tasks,relation-classification,9,20,0.25,43,0.2925170068027211,3,0.3333333333333333,0,1
45,2 . PICO Extraction ( PICO ),Background,Tasks,relation-classification,9,21,0.2625,44,0.2993197278911564,4,0.4444444444444444,0,1
46,3 . Text Classification ( CLS ),Background,Tasks,relation-classification,9,22,0.275,45,0.3061224489795918,5,0.5555555555555556,0,1
47,4 . Relation Classification ( REL ),Background,Tasks,relation-classification,9,23,0.2875,46,0.3129251700680272,6,0.6666666666666666,0,1
48,5 . Dependency Parsing ( DEP ) ,Background,Tasks,relation-classification,9,24,0.3,47,0.3197278911564626,7,0.7777777777777778,0,1
49,"PICO , like NER , is a sequence labeling task where the model extracts spans describing the Participants , Interventions , Comparisons , and Outcomes in a clinical trial paper .",Background,Tasks,relation-classification,9,25,0.3125,48,0.3265306122448979,8,0.8888888888888888,0,0
50,"REL is a special case of text classification where the model predicts the type of relation expressed between two entities , which are encapsulated in the sentence by inserted special tokens .",Background,Tasks,relation-classification,9,26,0.325,49,0.3333333333333333,9,1.0,0,0
51,Datasets,Background,,relation-classification,9,27,0.3375,50,0.3401360544217687,0,0.0,0,0
52,"For brevity , we only describe the newer datasets here , and refer the reader to the references in Table 1 for the older datasets .",Background,Datasets,relation-classification,9,28,0.35,51,0.3469387755102041,1,0.0833333333333333,0,0
53,EBM - NLP annotates PICO spans in clinical trial abstracts .,Background,Datasets,relation-classification,9,29,0.3625,52,0.3537414965986394,2,0.1666666666666666,0,0
54,SciERC annotates entities and relations from computer science ab - 1 https://github.com/google/sentencepiece,Background,Datasets,relation-classification,9,30,0.375,53,0.3605442176870748,3,0.25,0,0
55,"2 https://github.com/allenai/SciSpaCy stracts . ACL - ARC and Sci -Cite assign intent labels ( e.g. Comparison , Extension , etc. ) to sentences from scientific papers that cite other papers .",Background,Datasets,relation-classification,9,31,0.3875,54,0.3673469387755102,4,0.3333333333333333,0,0
56,The Paper Field dataset is built from the Microsoft Academic Graph 3 and maps paper titles to one of 7 fields of study .,Background,Datasets,relation-classification,9,32,0.4,55,0.3741496598639456,5,0.4166666666666667,0,0
57,"Each field of study ( i.e. geography , politics , economics , business , sociology , medicine , and psychology ) has approximately 12 K training examples .",Background,Datasets,relation-classification,9,33,0.4125,56,0.3809523809523809,6,0.5,0,0
58,Pretrained BERT,Background,,relation-classification,9,34,0.425,57,0.3877551020408163,7,0.5833333333333334,0,0
59,Variants,Background,,relation-classification,9,35,0.4375,58,0.3945578231292517,8,0.6666666666666666,0,0
60,BERT - Base,Background,Variants,relation-classification,9,36,0.45,59,0.4013605442176871,9,0.75,0,0
61,We use the pretrained weights for BERT - Base released with the original BERT code .,Background,Variants,relation-classification,9,37,0.4625,60,0.4081632653061224,10,0.8333333333333334,0,0
62,The vocabulary is BASE - VOCAB .,Background,Variants,relation-classification,9,38,0.475,61,0.4149659863945578,11,0.9166666666666666,0,0
63,We evaluate both cased and uncased versions of this model .,Background,Variants,relation-classification,9,39,0.4875,62,0.4217687074829932,12,1.0,0,0
64,SCIBERT,Background,,relation-classification,9,40,0.5,63,0.4285714285714285,0,0.0,0,0
65,We use the original BERT code to train SCIBERT on our corpus with the same configuration and size as BERT - Base .,Background,SCIBERT,relation-classification,9,41,0.5125,64,0.4353741496598639,1,0.0625,0,0
66,We train 4 different versions of SCIBERT : ( i ) cased or uncased and ( ii ) BASEVOCAB or SCIVOCAB .,Background,SCIBERT,relation-classification,9,42,0.525,65,0.4421768707482993,2,0.125,0,0
67,The two models that use BASEVOCAB are finetuned from the corresponding BERT - Base models .,Background,SCIBERT,relation-classification,9,43,0.5375,66,0.4489795918367347,3,0.1875,0,0
68,The other two models that use the new SCIVOCAB are trained from scratch .,Background,SCIBERT,relation-classification,9,44,0.55,67,0.4557823129251701,4,0.25,0,0
69,Pretraining BERT for long sentences can be slow .,Background,SCIBERT,relation-classification,9,45,0.5625,68,0.4625850340136054,5,0.3125,0,0
70,"Following the original BERT code , we set a maximum sentence length of 128 tokens , and train the model until the training loss stops decreasing .",Background,SCIBERT,relation-classification,9,46,0.575,69,0.4693877551020408,6,0.375,0,0
71,We then continue training the model allowing sentence lengths up to 512 tokens .,Background,SCIBERT,relation-classification,9,47,0.5875,70,0.4761904761904761,7,0.4375,0,0
72,We use a single TPU v 3 with 8 cores .,Background,SCIBERT,relation-classification,9,48,0.6,71,0.4829931972789115,8,0.5,0,0
73,"Training the SCIVOCAB models from scratch on our corpus takes 1 week 5 ( 5 days with max length 128 , then 2 days with max length 512 ) .",Background,SCIBERT,relation-classification,9,49,0.6125,72,0.4897959183673469,9,0.5625,0,0
74,The BASEVOCAB models take 2 fewer days of training because they are n't trained from scratch .,Background,SCIBERT,relation-classification,9,50,0.625,73,0.4965986394557823,10,0.625,0,0
75,All pretrained BERT models are converted to be compatible with PyTorch using the pytorchtransformers library .,Background,SCIBERT,relation-classification,9,51,0.6375,74,0.5034013605442177,11,0.6875,0,0
76,All our models ( Sections 3.4 and 3.5 ) are implemented in PyTorch using AllenNLP .,Background,SCIBERT,relation-classification,9,52,0.65,75,0.5102040816326531,12,0.75,0,0
77,Casing,Background,,relation-classification,9,53,0.6625,76,0.5170068027210885,13,0.8125,0,0
78,We follow in using the cased models for NER and the uncased models for all other tasks .,Background,Casing,relation-classification,9,54,0.675,77,0.5238095238095238,14,0.875,0,0
79,We also use the cased models for parsing .,Background,Casing,relation-classification,9,55,0.6875,78,0.5306122448979592,15,0.9375,0,0
80,Some light experimentation showed that the uncased models perform slightly better ( even sometimes on NER ) than cased models .,Background,Casing,relation-classification,9,56,0.7,79,0.5374149659863946,16,1.0,0,0
81,Finetuning BERT,Background,,relation-classification,9,57,0.7125,80,0.54421768707483,0,0.0,0,0
82,"We mostly follow the same architecture , optimization , and hyperparameter choices used in .",Background,Finetuning BERT,relation-classification,9,58,0.725,81,0.5510204081632653,1,0.0434782608695652,0,0
83,"For text classification ( i.e. CLS and REL ) , we feed the final BERT vector for the [ CLS ] token into a linear classification layer .",Background,Finetuning BERT,relation-classification,9,59,0.7375,82,0.5578231292517006,2,0.0869565217391304,0,0
84,"For sequence labeling ( i.e. NER and PICO ) , we feed the final BERT vector for each token into a linear classification layer with softmax output .",Background,Finetuning BERT,relation-classification,9,60,0.75,83,0.564625850340136,3,0.1304347826086956,0,0
85,"We differ slightly in using an additional conditional random field , which made evaluation easier by guaranteeing well - formed entities .",Background,Finetuning BERT,relation-classification,9,61,0.7625,84,0.5714285714285714,4,0.1739130434782608,0,0
86,"For DEP , we use the model from with dependency tag and arc embeddings of size 100 and biaffine matrix attention over BERT vectors instead of stacked BiLSTMs .",Background,Finetuning BERT,relation-classification,9,62,0.775,85,0.5782312925170068,5,0.217391304347826,0,0
87,"In all settings , we apply a dropout of 0.1 and optimize cross entropy loss using Adam .",Background,Finetuning BERT,relation-classification,9,63,0.7875,86,0.5850340136054422,6,0.2608695652173913,0,0
88,"We finetune for 2 to 5 epochs using a batch size of 32 and a learning rate of 5 e - 6 , 1 e - 5 , 2 e - 5 , or 5 e - 5 with a slanted triangular schedule which is equivalent to the linear warmup followed by linear decay .",Background,Finetuning BERT,relation-classification,9,64,0.8,87,0.5918367346938775,7,0.3043478260869565,0,0
89,"For each dataset and BERT variant , we pick the best learning rate and number of epochs on the development set and report the corresponding test results .",Background,Finetuning BERT,relation-classification,9,65,0.8125,88,0.5986394557823129,8,0.3478260869565217,0,0
90,We found the setting that works best across most datasets and models is 2 or 4 epochs and a learning rate of 2 e - 5 .,Background,Finetuning BERT,relation-classification,9,66,0.825,89,0.6054421768707483,9,0.391304347826087,0,0
91,"While task - dependent , optimal hyperparameters for each task are often the same across BERT variants .",Background,Finetuning BERT,relation-classification,9,67,0.8375,90,0.6122448979591837,10,0.4347826086956521,0,0
92,Frozen BERT,Background,,relation-classification,9,68,0.85,91,0.6190476190476191,11,0.4782608695652174,0,0
93,Embeddings,Background,,relation-classification,9,69,0.8625,92,0.6258503401360545,12,0.5217391304347826,0,0
94,"We also explore the usage of BERT as pretrained contextualized word embeddings , like ELMo ) , by training simple task - specific models atop frozen BERT embeddings .",Background,Embeddings,relation-classification,9,70,0.875,93,0.6326530612244898,13,0.5652173913043478,0,0
95,"For text classification , we feed each sentence of BERT vectors into a 2 - layer BiLSTM of size 200 and apply a multilayer perceptron ( with hidden size 200 ) on the concatenated first and last BiLSTM vectors .",Background,Embeddings,relation-classification,9,71,0.8875,94,0.6394557823129252,14,0.6086956521739131,0,0
96,"For sequence labeling , we use the same BiLSTM layers and use a conditional random field to guarantee well - formed predictions .",Background,Embeddings,relation-classification,9,72,0.9,95,0.6462585034013606,15,0.6521739130434783,0,0
97,"For DEP , we use the full model from with dependency tag and arc embeddings of size 100 and the same BiLSTM setup as other tasks .",Background,Embeddings,relation-classification,9,73,0.9125,96,0.6530612244897959,16,0.6956521739130435,0,0
98,We did not find changing the depth or size of the BiLSTMs to significantly impact results .,Background,Embeddings,relation-classification,9,74,0.925,97,0.6598639455782312,17,0.7391304347826086,0,0
99,"We optimize cross entropy loss using Adam , but holding BERT weights frozen and applying a dropout of 0.5 .",Background,Embeddings,relation-classification,9,75,0.9375,98,0.6666666666666666,18,0.782608695652174,0,0
100,We train with early stopping on the development set ( patience of 10 ) using a batch size of 32 and a learning rate of 0.001 .,Background,Embeddings,relation-classification,9,76,0.95,99,0.673469387755102,19,0.8260869565217391,0,0
101,"We did not perform extensive hyperparameter search , but while optimal hyperparameters are going to be task - dependent , some light experimentation showed these settings work fairly well across most tasks and BERT variants .",Background,Embeddings,relation-classification,9,77,0.9625,100,0.6802721088435374,20,0.8695652173913043,0,0
102,summarizes the experimental results .,Background,Embeddings,relation-classification,9,78,0.975,101,0.6870748299319728,21,0.9130434782608696,0,0
103,We observe that SCIBERT outperforms BERT - Base on scientific tasks ( + 2.11 F1 with finetuning and + 2.43 F1 without ),Background,Embeddings,relation-classification,9,79,0.9875,102,0.6938775510204082,22,0.9565217391304348,0,1
104,8 . We also achieve new SOTA results on many of these tasks using SCIBERT .,Background,Embeddings,relation-classification,9,80,1.0,103,0.7006802721088435,23,1.0,0,0
105,Results,,,relation-classification,9,0,0.0,104,0.7074829931972789,0,0.0,1,0
106,Biomedical Domain,Results,,relation-classification,9,1,0.032258064516129,105,0.7142857142857143,0,0.0,1,0
107,We observe that SCIBERT outperforms BERT - Base on biomedical tasks ( + 1.92 F1 with finetuning and + 3.59 F1 without ) .,Results,Biomedical Domain,relation-classification,9,2,0.064516129032258,106,0.7210884353741497,1,0.0909090909090909,1,1
108,"In addition , SCIB - ERT achieves new SOTA results on BC5 CDR and ChemProt , and EBM - NLP .",Results,Biomedical Domain,relation-classification,9,3,0.0967741935483871,107,0.7278911564625851,2,0.1818181818181818,1,1
109,SCIBERT performs slightly worse than SOTA on 3 datasets .,Results,Biomedical Domain,relation-classification,9,4,0.1290322580645161,108,0.7346938775510204,3,0.2727272727272727,1,0
110,The SOTA model for JNLPBA is a BiLSTM - CRF ensemble trained on multiple NER datasets not just JNLPBA .,Results,Biomedical Domain,relation-classification,9,5,0.1612903225806451,109,0.7414965986394558,4,0.3636363636363636,1,0
111,"The SOTA model for NCBI - disease is BIOBERT , which is BERT - Base finetuned on 18B tokens from biomedical papers .",Results,Biomedical Domain,relation-classification,9,6,0.1935483870967742,110,0.7482993197278912,5,0.4545454545454545,1,0
112,"The SOTA result for GENIA is in Nguyen and Verspoor ( 2019 ) which uses the model from with partof - speech ( POS ) features , which we do not use .",Results,Biomedical Domain,relation-classification,9,7,0.2258064516129032,111,0.7551020408163265,6,0.5454545454545454,1,0
113,"In , we compare SCIBERT results with reported BIOBERT results on the subset of datasets included in .",Results,Biomedical Domain,relation-classification,9,8,0.2580645161290322,112,0.7619047619047619,7,0.6363636363636364,1,0
114,"Interesting , SCIBERT outperforms BIOBERT results on 7 The SOTA paper did not report a single score .",Results,Biomedical Domain,relation-classification,9,9,0.2903225806451613,113,0.7687074829931972,8,0.7272727272727273,1,0
115,We compute the average of the reported results for each class weighted by number of examples in each class .,Results,Biomedical Domain,relation-classification,9,10,0.3225806451612903,114,0.7755102040816326,9,0.8181818181818182,1,0
116,"8 Forrest of this paper , all results reported in this manner are averaged over datasets excluding UAS for DEP since we already include LAS .",Results,Biomedical Domain,relation-classification,9,11,0.3548387096774194,115,0.782312925170068,10,0.9090909090909092,1,0
117,"BC5 CDR and ChemProt , and performs similarly on JNLPBA despite being trained on a substantially smaller biomedical corpus .",Results,Biomedical Domain,relation-classification,9,12,0.3870967741935484,116,0.7891156462585034,11,1.0,1,0
118,Computer Science Domain,Results,,relation-classification,9,13,0.4193548387096774,117,0.7959183673469388,0,0.0,1,0
119,We observe that SCIBERT outperforms BERT - Base on computer science tasks ( + 3.55 F1 with finetuning and + 1.13 F1 without ) .,Results,Computer Science Domain,relation-classification,9,14,0.4516129032258064,118,0.8027210884353742,1,0.3333333333333333,1,1
120,"In addition , SCIBERT achieves new SOTA results on ACL - ARC , and the NER part of SciERC .",Results,Computer Science Domain,relation-classification,9,15,0.4838709677419355,119,0.8095238095238095,2,0.6666666666666666,1,1
121,"For relations in Sci - ERC , our results are not comparable with those in because we are performing relation classification given gold entities , while they perform joint entity and relation extraction .",Results,Computer Science Domain,relation-classification,9,16,0.5161290322580645,120,0.8163265306122449,3,1.0,1,0
122,Multiple Domains,Results,,relation-classification,9,17,0.5483870967741935,121,0.8231292517006803,0,0.0,1,0
123,We observe that SCIBERT outperforms BERT - Base on the multidomain tasks ( + 0.49 F1 with finetuning and + 0.93 F1 without ) .,Results,Multiple Domains,relation-classification,9,18,0.5806451612903226,122,0.8299319727891157,1,0.3333333333333333,1,1
124,"In addition , SCIBERT outperforms the SOTA on Sci - Cite .",Results,Multiple Domains,relation-classification,9,19,0.6129032258064516,123,0.8367346938775511,2,0.6666666666666666,1,1
125,No prior published SOTA results exist for the Paper Field dataset .,Results,Multiple Domains,relation-classification,9,20,0.6451612903225806,124,0.8435374149659864,3,1.0,1,0
126,Discussion,Results,,relation-classification,9,21,0.6774193548387096,125,0.8503401360544217,0,0.0,1,0
127,Effect of Finetuning,Results,,relation-classification,9,22,0.7096774193548387,126,0.8571428571428571,0,0.0,1,0
128,"We observe improved results via BERT finetuning rather than task - specific architectures atop frozen embeddings ( + 3.25 F1 with SCIBERT and + 3.58 with BERT - Base , on average ) .",Results,Effect of Finetuning,relation-classification,9,23,0.7419354838709677,127,0.8639455782312925,1,0.3333333333333333,1,0
129,"For each scientific domain , we observe the largest effects of finetuning on the computer science ( + 5.59 F1 with SCIB - ERT and + 3.17 F1 with BERT - Base ) and biomedical tasks ( + 2.94 F1 with SCIBERT and + 4.61 F1 with BERT - Base ) , and the smallest effect on multidomain tasks ( + 0.7 F1 with SCIBERT and + 1.14 F1 with BERT - Base ) .",Results,Effect of Finetuning,relation-classification,9,24,0.7741935483870968,128,0.8707482993197279,2,0.6666666666666666,1,0
130,"On every dataset except BC5 CDR and SciCite , BERT - Base with finetuning outperforms ( or performs similarly to ) a model using frozen SCIBERT embeddings .",Results,Effect of Finetuning,relation-classification,9,25,0.8064516129032258,129,0.8775510204081632,3,1.0,1,0
131,Effect of SCIVOCAB,Results,,relation-classification,9,26,0.8387096774193549,130,0.8843537414965986,0,0.0,1,0
132,We assess the importance of an in - domain scientific vocabulary by repeating the finetuning experiments for SCIBERT with BASEVOCAB .,Results,Effect of SCIVOCAB,relation-classification,9,27,0.8709677419354839,131,0.891156462585034,1,0.2,1,0
133,We find the optimal hyperparameters for SCIBERT - BASEVOCAB often coincide with those of SCIB - ERT - SCIVOCAB .,Results,Effect of SCIVOCAB,relation-classification,9,28,0.9032258064516128,132,0.8979591836734694,2,0.4,1,0
134,"Averaged across datasets , we observe + 0.60 F1 when using SCIVOCAB .",Results,Effect of SCIVOCAB,relation-classification,9,29,0.935483870967742,133,0.9047619047619048,3,0.6,1,0
135,"For each scientific do - main , we observe + 0.76 F1 for biomedical tasks , + 0.61 F1 for computer science tasks , and + 0.11 F1 for multidomain tasks .",Results,Effect of SCIVOCAB,relation-classification,9,30,0.967741935483871,134,0.91156462585034,4,0.8,1,0
136,"Given the disjoint vocabularies ( Section 2 ) and the magnitude of improvement over BERT - Base ( Section 4 ) , we suspect that while an in - domain vocabulary is helpful , SCIBERT benefits most from the scientific corpus pretraining .",Results,Effect of SCIVOCAB,relation-classification,9,31,1.0,135,0.9183673469387756,5,1.0,1,0
137,Related Work,,,relation-classification,9,0,0.0,136,0.9251700680272108,0,0.0,1,0
138,Recent work on domain adaptation of BERT includes BIOBERT and CLIN - ICALBERT .,Related Work,Related Work,relation-classification,9,1,0.25,137,0.9319727891156464,1,0.25,0,0
139,"BIOBERT is trained on PubMed abstracts and PMC full text articles , and CLIN - ICALBERT is trained on clinical text from the MIMIC - III database .",Related Work,Related Work,relation-classification,9,2,0.5,138,0.9387755102040816,2,0.5,0,0
140,"In contrast , SCIBERT is trained on the full text of 1.14 M biomedical and computer science papers from the Semantic Scholar corpus .",Related Work,Related Work,relation-classification,9,3,0.75,139,0.9455782312925172,3,0.75,0,0
141,"Furthermore , SCIBERT uses an in - domain vocabulary ( SCIVOCAB ) while the other abovementioned models use the original BERT vocabulary ( BASEVOCAB ) .",Related Work,Related Work,relation-classification,9,4,1.0,140,0.9523809523809524,4,1.0,0,0
142,Conclusion and Future Work,,,relation-classification,9,0,0.0,141,0.9591836734693876,0,0.0,1,0
143,"We released SCIBERT , a pretrained language model for scientific text based on BERT .",Conclusion and Future Work,Conclusion and Future Work,relation-classification,9,1,0.2,142,0.9659863945578232,1,0.2,0,0
144,We evaluated SCIBERT on a suite of tasks and datasets from scientific domains .,Conclusion and Future Work,Conclusion and Future Work,relation-classification,9,2,0.4,143,0.9727891156462584,2,0.4,0,0
145,"SCIBERT significantly outperformed BERT - Base and achieves new SOTA results on several of these tasks , even compared to some reported BIOBERT ) results on biomedical tasks .",Conclusion and Future Work,Conclusion and Future Work,relation-classification,9,3,0.6,144,0.979591836734694,3,0.6,0,0
146,"For future work , we will release aversion of SCIBERT analogous to BERT - Large , as well as experiment with different proportions of papers from each domain .",Conclusion and Future Work,Conclusion and Future Work,relation-classification,9,4,0.8,145,0.9863945578231292,4,0.8,0,0
147,"Because these language models are costly to train , we aim to build a single resource that 's useful across multiple domains .",Conclusion and Future Work,Conclusion and Future Work,relation-classification,9,5,1.0,146,0.9931972789115646,5,1.0,0,0
1,title,,,text-classification,0,0,0.0,0,0.0,0,0.0,1,0
2,Character - level Convolutional Networks for Text Classification,title,,text-classification,0,1,0.5,1,0.0044052863436123,1,0.5,1,1
3,*,title,Character - level Convolutional Networks for Text Classification,text-classification,0,2,1.0,2,0.0088105726872246,2,1.0,1,0
4,abstract,,,text-classification,0,0,0.0,3,0.013215859030837,0,0.0,1,0
5,This article offers an empirical exploration on the use of character - level convolutional networks ( ConvNets ) for text classification .,abstract,abstract,text-classification,0,1,0.1428571428571428,4,0.0176211453744493,1,0.1428571428571428,1,0
6,We constructed several largescale datasets to show that character - level convolutional networks could achieve state - of - the - art or competitive results .,abstract,abstract,text-classification,0,2,0.2857142857142857,5,0.0220264317180616,2,0.2857142857142857,1,0
7,"Comparisons are offered against traditional models such as bag of words , n-grams and their TFIDF variants , and deep learning models such as word - based ConvNets and recurrent neural networks .",abstract,abstract,text-classification,0,3,0.4285714285714285,6,0.026431718061674,3,0.4285714285714285,1,0
8,There are also related works that use character - level features for language processing .,abstract,abstract,text-classification,0,4,0.5714285714285714,7,0.0308370044052863,4,0.5714285714285714,1,0
9,"These include using character - level n-grams with linear classifiers [ 15 ] , and incorporating character - level features to ConvNets [ 28 ] [ 29 ] .",abstract,abstract,text-classification,0,5,0.7142857142857143,8,0.0352422907488986,5,0.7142857142857143,1,0
10,"In particular , these ConvNet approaches use words as a basis , in which character - level features extracted at word [ 28 ] or word n-gram [ 29 ] level form a distributed representation .",abstract,abstract,text-classification,0,6,0.8571428571428571,9,0.039647577092511,6,0.8571428571428571,1,0
11,Improvements for part - of - speech tagging and information retrieval were observed .,abstract,abstract,text-classification,0,7,1.0,10,0.0440528634361233,7,1.0,1,0
12,Introduction,,,text-classification,0,0,0.0,11,0.0484581497797356,0,0.0,1,0
13,"Text classification is a classic topic for natural language processing , in which one needs to assign predefined categories to free - text documents .",Introduction,Introduction,text-classification,0,1,0.0204081632653061,12,0.052863436123348,1,0.0555555555555555,1,1
14,The range of text classification research goes from designing the best features to choosing the best possible machine learning classifiers .,Introduction,Introduction,text-classification,0,2,0.0408163265306122,13,0.0572687224669603,2,0.1111111111111111,1,0
15,"To date , almost all techniques of text classification are based on words , in which simple statistics of some ordered word combinations ( such as n-grams ) usually perform the best .",Introduction,Introduction,text-classification,0,3,0.0612244897959183,14,0.0616740088105726,3,0.1666666666666666,1,0
16,"On the other hand , many researchers have found convolutional networks ( ConvNets ) are useful in extracting information from raw signals , ranging from computer vision applications to speech recognition and others .",Introduction,Introduction,text-classification,0,4,0.0816326530612244,15,0.066079295154185,4,0.2222222222222222,1,0
17,"In particular , time - delay networks used in the early days of deep learning research are essentially convolutional networks that model sequential data .",Introduction,Introduction,text-classification,0,5,0.1020408163265306,16,0.0704845814977973,5,0.2777777777777778,1,0
18,"In this article we explore treating text as a kind of raw signal at character level , and applying temporal ( one-dimensional ) ConvNets to it .",Introduction,Introduction,text-classification,0,6,0.1224489795918367,17,0.0748898678414096,6,0.3333333333333333,1,1
19,For this article we only used a classification task as away to exemplify ConvNets ' ability to understand texts .,Introduction,Introduction,text-classification,0,7,0.1428571428571428,18,0.079295154185022,7,0.3888888888888889,1,1
20,"Historically we know that ConvNets usually require large - scale datasets to work , therefore we also build several of them .",Introduction,Introduction,text-classification,0,8,0.1632653061224489,19,0.0837004405286343,8,0.4444444444444444,1,0
21,An extensive set of comparisons is offered with traditional models and other deep learning models .,Introduction,Introduction,text-classification,0,9,0.1836734693877551,20,0.0881057268722467,9,0.5,1,0
22,Applying convolutional networks to text classification or natural language processing at large was explored in literature .,Introduction,Introduction,text-classification,0,10,0.2040816326530612,21,0.092511013215859,10,0.5555555555555556,1,0
23,"It has been shown that ConvNets can be directly applied to distributed or discrete embedding of words , without any knowledge on the syntactic or semantic structures of a language .",Introduction,Introduction,text-classification,0,11,0.2244897959183673,22,0.0969162995594713,11,0.6111111111111112,1,0
24,These approaches have been proven to be competitive to traditional models .,Introduction,Introduction,text-classification,0,12,0.2448979591836734,23,0.1013215859030837,12,0.6666666666666666,1,0
25,from previous research that ConvNets do not require the knowledge about the syntactic or semantic structure of a language .,Introduction,Introduction,text-classification,0,13,0.2653061224489796,24,0.105726872246696,13,0.7222222222222222,1,0
26,"This simplification of engineering could be crucial fora single system that can work for different languages , since characters always constitute a necessary construct regardless of whether segmentation into words is possible .",Introduction,Introduction,text-classification,0,14,0.2857142857142857,25,0.1101321585903083,14,0.7777777777777778,1,0
27,Working on only characters also has the advantage that abnormal character combinations such as misspellings and emoticons maybe naturally learnt .,Introduction,Introduction,text-classification,0,15,0.3061224489795918,26,0.1145374449339207,15,0.8333333333333334,1,0
28,Character - level Convolutional Networks,Introduction,Introduction,text-classification,0,16,0.3265306122448979,27,0.118942731277533,16,0.8888888888888888,1,0
29,"In this section , we introduce the design of character - level ConvNets for text classification .",Introduction,Introduction,text-classification,0,17,0.3469387755102041,28,0.1233480176211453,17,0.9444444444444444,1,0
30,"The design is modular , where the gradients are obtained by back - propagation to perform optimization .",Introduction,Introduction,text-classification,0,18,0.3673469387755102,29,0.1277533039647577,18,1.0,1,0
31,Key Modules,Introduction,,text-classification,0,19,0.3877551020408163,30,0.13215859030837,0,0.0,1,0
32,"The main component is the temporal convolutional module , which simply computes a 1 - D convolution .",Introduction,Key Modules,text-classification,0,20,0.4081632653061224,31,0.1365638766519823,1,0.0434782608695652,1,0
33,Suppose we have a discrete input function g ( x ) ?,Introduction,Key Modules,text-classification,0,21,0.4285714285714285,32,0.1409691629955947,2,0.0869565217391304,1,0
34,"[ 1 , l ] ?",Introduction,Key Modules,text-classification,0,22,0.4489795918367347,33,0.145374449339207,3,0.1304347826086956,1,0
35,Rand a discrete kernel function,Introduction,Key Modules,text-classification,0,23,0.4693877551020408,34,0.1497797356828193,4,0.1739130434782608,1,0
36,where c = k ? d + 1 is an offset constant .,Introduction,Key Modules,text-classification,0,24,0.4897959183673469,35,0.1541850220264317,5,0.217391304347826,1,0
37,"Just as in traditional convolutional networks in vision , the module is parameterized by a set of such kernel functions f ij ( x ) ( i = 1 , 2 , . . . , m and j = 1 , 2 , . . . , n) which we call weights , on a set of inputs g i ( x ) and outputs h j ( y ) .",Introduction,Key Modules,text-classification,0,25,0.5102040816326531,36,0.158590308370044,6,0.2608695652173913,1,0
38,"We call each g i ( or h j ) input ( or output ) features , and m ( or n) input ( or output ) feature size .",Introduction,Key Modules,text-classification,0,26,0.5306122448979592,37,0.1629955947136563,7,0.3043478260869565,1,0
39,The outputs h j ( y ) is obtained by a sum over i of the convolutions between g i ( x ) and f ij ( x ) .,Introduction,Key Modules,text-classification,0,27,0.5510204081632653,38,0.1674008810572687,8,0.3478260869565217,1,0
40,One key module that helped us to train deeper models is temporal max - pooling .,Introduction,Key Modules,text-classification,0,28,0.5714285714285714,39,0.171806167400881,9,0.391304347826087,1,0
41,It is the 1 - D version of the max - pooling module used in computer vision .,Introduction,Key Modules,text-classification,0,29,0.5918367346938775,40,0.1762114537444934,10,0.4347826086956521,1,0
42,Given a discrete input function g ( x ) ?,Introduction,Key Modules,text-classification,0,30,0.6122448979591837,41,0.1806167400881057,11,0.4782608695652174,1,0
43,"[ 1 , l ] ?",Introduction,Key Modules,text-classification,0,31,0.6326530612244898,42,0.185022026431718,12,0.5217391304347826,1,0
44,"R , the max - pooling function h ( y ) ?",Introduction,Key Modules,text-classification,0,32,0.6530612244897959,43,0.1894273127753304,13,0.5652173913043478,1,0
45,"[ 1 , ( l ? k) / d + 1 ] ?",Introduction,Key Modules,text-classification,0,33,0.673469387755102,44,0.1938325991189427,14,0.6086956521739131,1,0
46,R of g ( x ) is defined as,Introduction,Key Modules,text-classification,0,34,0.6938775510204082,45,0.198237885462555,15,0.6521739130434783,1,0
47,where c = k ? d + 1 is an offset constant .,Introduction,Key Modules,text-classification,0,35,0.7142857142857143,46,0.2026431718061674,16,0.6956521739130435,1,0
48,"This very pooling module enabled us to train ConvNets deeper than 6 layers , where all others fail .",Introduction,Key Modules,text-classification,0,36,0.7346938775510204,47,0.2070484581497797,17,0.7391304347826086,1,0
49,The analysis by might shed some light on this .,Introduction,Key Modules,text-classification,0,37,0.7551020408163265,48,0.211453744493392,18,0.782608695652174,1,0
50,"The non-linearity used in our model is the rectifier or thresholding function h ( x ) = max {0 , x} , which makes our convolutional layers similar to rectified linear units ( ReLUs ) .",Introduction,Key Modules,text-classification,0,38,0.7755102040816326,49,0.2158590308370044,19,0.8260869565217391,1,0
51,"The algorithm used is stochastic gradient descent ( SGD ) with a minibatch of size 128 , using momentum 0.9 and initial step size 0.01 which is halved every 3 epoches for 10 times .",Introduction,Key Modules,text-classification,0,39,0.7959183673469388,50,0.2202643171806167,20,0.8695652173913043,1,0
52,Each epoch takes a fixed number of random training samples uniformly sampled across classes .,Introduction,Key Modules,text-classification,0,40,0.8163265306122449,51,0.224669603524229,21,0.9130434782608696,1,0
53,This number will later be detailed for each dataset sparately .,Introduction,Key Modules,text-classification,0,41,0.8367346938775511,52,0.2290748898678414,22,0.9565217391304348,1,0
54,The implementation is done using Torch 7 .,Introduction,Key Modules,text-classification,0,42,0.8571428571428571,53,0.2334801762114537,23,1.0,1,0
55,Character quantization,Introduction,,text-classification,0,43,0.8775510204081632,54,0.237885462555066,0,0.0,1,0
56,Our models accept a sequence of encoded characters as input .,Introduction,Character quantization,text-classification,0,44,0.8979591836734694,55,0.2422907488986784,1,0.1666666666666666,1,0
57,"The encoding is done by prescribing an alphabet of size m for the input language , and then quantize each character using 1 - of - m encoding ( or "" one - hot "" encoding ) .",Introduction,Character quantization,text-classification,0,45,0.9183673469387756,56,0.2466960352422907,2,0.3333333333333333,1,0
58,"Then , the sequence of characters is transformed to a sequence of such m sized vectors with fixed length l 0 .",Introduction,Character quantization,text-classification,0,46,0.9387755102040816,57,0.2511013215859031,3,0.5,1,0
59,"Any character exceeding length l 0 is ignored , and any characters that are not in the alphabet including blank characters are quantized as all - zero vectors .",Introduction,Character quantization,text-classification,0,47,0.9591836734693876,58,0.2555066079295154,4,0.6666666666666666,1,0
60,"The character quantization order is backward so that the latest reading on characters is always placed near the begin of the output , making it easy for fully connected layers to associate weights with the latest reading .",Introduction,Character quantization,text-classification,0,48,0.979591836734694,59,0.2599118942731278,5,0.8333333333333334,1,0
61,Later we also compare with models that use a different alphabet in which we distinguish between upper-case and lower - case letters .,Introduction,Character quantization,text-classification,0,49,1.0,60,0.2643171806167401,6,1.0,1,0
62,Model Design,,,text-classification,0,0,0.0,61,0.2687224669603524,0,0.0,1,0
63,We designed 2 ConvNets - one large and one small .,Model Design,Model Design,text-classification,0,1,0.027027027027027,62,0.2731277533039647,1,0.3333333333333333,1,0
64,They are both 9 layers deep with 6 convolutional layers and 3 fully - connected layers .,Model Design,Model Design,text-classification,0,2,0.054054054054054,63,0.2775330396475771,2,0.6666666666666666,1,0
65,gives an illustration .,Model Design,Model Design,text-classification,0,3,0.081081081081081,64,0.2819383259911894,3,1.0,1,0
66,Some Text,Model Design,,text-classification,0,4,0.1081081081081081,65,0.2863436123348017,0,0.0,1,0
67,Convolutions,Model Design,,text-classification,0,5,0.1351351351351351,66,0.2907488986784141,0,0.0,1,0
68,Max - pooling Length Feature Quantization ...,Model Design,Convolutions,text-classification,0,6,0.1621621621621621,67,0.2951541850220264,1,0.0555555555555555,1,0
69,Conv. and Pool. layers,Model Design,,text-classification,0,7,0.1891891891891892,68,0.2995594713656387,2,0.1111111111111111,1,0
70,Fully - connected :,Model Design,Conv. and Pool. layers,text-classification,0,8,0.2162162162162162,69,0.3039647577092511,3,0.1666666666666666,1,0
71,Illustration of our model,Model Design,Conv. and Pool. layers,text-classification,0,9,0.2432432432432432,70,0.3083700440528634,4,0.2222222222222222,1,0
72,"The input have number of features equal to 70 due to our character quantization method , and the input feature length is 1014 .",Model Design,Conv. and Pool. layers,text-classification,0,10,0.2702702702702703,71,0.3127753303964757,5,0.2777777777777778,1,0
73,It seems that 1014 characters could already capture most of the texts of interest .,Model Design,Conv. and Pool. layers,text-classification,0,11,0.2972972972972973,72,0.3171806167400881,6,0.3333333333333333,1,0
74,We also insert 2 dropout modules in between the 3 fully - connected layers to regularize .,Model Design,Conv. and Pool. layers,text-classification,0,12,0.3243243243243243,73,0.3215859030837004,7,0.3888888888888889,1,0
75,They have dropout probability of 0.5 .,Model Design,Conv. and Pool. layers,text-classification,0,13,0.3513513513513513,74,0.3259911894273127,8,0.4444444444444444,1,0
76,"lists the configurations for convolutional layers , and table 2 lists the configurations for fully - connected ( linear ) layers .",Model Design,Conv. and Pool. layers,text-classification,0,14,0.3783783783783784,75,0.3303964757709251,9,0.5,1,0
77,We initialize the weights using a Gaussian distribution .,Model Design,Conv. and Pool. layers,text-classification,0,15,0.4054054054054054,76,0.3348017621145374,10,0.5555555555555556,1,0
78,"The mean and standard deviation used for initializing the large model is ( 0 , 0.02 ) and small model ( 0 , 0.05 ) . :",Model Design,Conv. and Pool. layers,text-classification,0,16,0.4324324324324324,77,0.3392070484581498,11,0.6111111111111112,1,0
79,Fully - connected layers used in our experiments .,Model Design,Conv. and Pool. layers,text-classification,0,17,0.4594594594594595,78,0.3436123348017621,12,0.6666666666666666,1,0
80,The number of output units for the last layer is determined by the problem .,Model Design,Conv. and Pool. layers,text-classification,0,18,0.4864864864864865,79,0.3480176211453744,13,0.7222222222222222,1,0
81,"For example , fora 10 - class classification problem it will be 10 .",Model Design,Conv. and Pool. layers,text-classification,0,19,0.5135135135135135,80,0.3524229074889868,14,0.7777777777777778,1,0
82,Depends on the problem,Model Design,,text-classification,0,20,0.5405405405405406,81,0.3568281938325991,15,0.8333333333333334,1,0
83,"For different problems the input lengths maybe different ( for example in our case l 0 = 1014 ) , and so are the frame lengths .",Model Design,Depends on the problem,text-classification,0,21,0.5675675675675675,82,0.3612334801762114,16,0.8888888888888888,1,0
84,"From our model design , it is easy to know that given input length l 0 , the output frame length after the last convolutional layer ( but before any of the fully - connected layers ) isl 6 = ( l 0 ? 96 ) / 27 .",Model Design,Depends on the problem,text-classification,0,22,0.5945945945945946,83,0.3656387665198238,17,0.9444444444444444,1,0
85,This number multiplied with the frame size at layer 6 will give the input dimension the first fully - connected layer accepts .,Model Design,Depends on the problem,text-classification,0,23,0.6216216216216216,84,0.3700440528634361,18,1.0,1,0
86,Layer Output Units Large Output Units,Model Design,,text-classification,0,24,0.6486486486486487,85,0.3744493392070485,0,0.0,1,0
87,Data Augmentation using Thesaurus,Model Design,,text-classification,0,25,0.6756756756756757,86,0.3788546255506608,0,0.0,1,0
88,Many researchers have found that appropriate data augmentation techniques are useful for controlling generalization error for deep learning models .,Model Design,Data Augmentation using Thesaurus,text-classification,0,26,0.7027027027027027,87,0.3832599118942731,1,0.0833333333333333,1,0
89,These techniques usually work well when we could find appropriate invariance properties that the model should possess .,Model Design,Data Augmentation using Thesaurus,text-classification,0,27,0.7297297297297297,88,0.3876651982378855,2,0.1666666666666666,1,0
90,"In terms of texts , it is not reasonable to augment the data using signal transformations as done in image or speech recognition , because the exact order of characters may form rigorous syntactic and semantic meaning .",Model Design,Data Augmentation using Thesaurus,text-classification,0,28,0.7567567567567568,89,0.3920704845814978,3,0.25,1,0
91,"Therefore , the best way to do data augmentation would have been using human rephrases of sentences , but this is unrealistic and expensive due the large volume of samples in our datasets .",Model Design,Data Augmentation using Thesaurus,text-classification,0,29,0.7837837837837838,90,0.3964757709251101,4,0.3333333333333333,1,0
92,"As a result , the most natural choice in data augmentation for us is to replace words or phrases with their synonyms .",Model Design,Data Augmentation using Thesaurus,text-classification,0,30,0.8108108108108109,91,0.4008810572687225,5,0.4166666666666667,1,0
93,"We experimented data augmentation by using an English thesaurus , which is obtained from the mytheas component used in LibreOffice 1 project .",Model Design,Data Augmentation using Thesaurus,text-classification,0,31,0.8378378378378378,92,0.4052863436123348,6,0.5,1,0
94,"That thesaurus in turn was obtained from Word - Net , where every synonym to a word or phrase is ranked by the semantic closeness to the most frequently seen meaning .",Model Design,Data Augmentation using Thesaurus,text-classification,0,32,0.8648648648648649,93,0.4096916299559471,7,0.5833333333333334,1,0
95,"To decide on how many words to replace , we extract all replaceable words from the given text and randomly chooser of them to be replaced .",Model Design,Data Augmentation using Thesaurus,text-classification,0,33,0.8918918918918919,94,0.4140969162995594,8,0.6666666666666666,1,0
96,The probability of number r is determined by a geometric distribution with parameter pin which P [ r ] ? pr .,Model Design,Data Augmentation using Thesaurus,text-classification,0,34,0.918918918918919,95,0.4185022026431718,9,0.75,1,0
97,The index s of the synonym chosen given a word is also determined by a another geometric distribution in which P [ s ] ? q s .,Model Design,Data Augmentation using Thesaurus,text-classification,0,35,0.945945945945946,96,0.4229074889867841,10,0.8333333333333334,1,0
98,"This way , the probability of a synonym chosen becomes smaller when it moves distant from the most frequently seen meaning .",Model Design,Data Augmentation using Thesaurus,text-classification,0,36,0.972972972972973,97,0.4273127753303964,11,0.9166666666666666,1,0
99,We will report the results using this new data augmentation technique with p = 0.5 and q = 0.5 .,Model Design,Data Augmentation using Thesaurus,text-classification,0,37,1.0,98,0.4317180616740088,12,1.0,1,0
100,Comparison Models,,,text-classification,0,0,0.0,99,0.4361233480176211,0,0.0,1,0
101,"To offer fair comparisons to competitive models , we conducted a series of experiments with both traditional and deep learning methods .",Comparison Models,Comparison Models,text-classification,0,1,0.5,100,0.4405286343612334,1,0.5,1,0
102,"We tried our best to choose models that can provide comparable and competitive results , and the results are reported faithfully without any model selection .",Comparison Models,Comparison Models,text-classification,0,2,1.0,101,0.4449339207048458,2,1.0,1,0
103,Traditional Methods,,,text-classification,0,0,0.0,102,0.4493392070484581,0,0.0,1,0
104,We refer to traditional methods as those that using a hand - crafted feature extractor and a linear classifier .,Traditional Methods,Traditional Methods,text-classification,0,1,0.0588235294117647,103,0.4537444933920704,1,0.0588235294117647,1,0
105,The classifier used is a multinomial logistic regression in all these models .,Traditional Methods,Traditional Methods,text-classification,0,2,0.1176470588235294,104,0.4581497797356828,2,0.1176470588235294,1,0
106,Bag - of - words and its TFIDF .,Traditional Methods,Traditional Methods,text-classification,0,3,0.1764705882352941,105,0.4625550660792951,3,0.1764705882352941,1,0
107,"For each dataset , the bag - of - words model is constructed by selecting 50,000 most frequent words from the training subset .",Traditional Methods,Traditional Methods,text-classification,0,4,0.2352941176470588,106,0.4669603524229075,4,0.2352941176470588,1,0
108,"For the normal bag - of - words , we use the counts of each word as the features .",Traditional Methods,Traditional Methods,text-classification,0,5,0.2941176470588235,107,0.4713656387665198,5,0.2941176470588235,1,0
109,"For the TFIDF ( term-frequency inverse - document - frequency ) version , we use the counts as the term-frequency .",Traditional Methods,Traditional Methods,text-classification,0,6,0.3529411764705882,108,0.4757709251101321,6,0.3529411764705882,1,0
110,The inverse document frequency is the logarithm of the division between total number of samples and number of samples with the word in the training subset .,Traditional Methods,Traditional Methods,text-classification,0,7,0.4117647058823529,109,0.4801762114537445,7,0.4117647058823529,1,0
111,The features are normalized by dividing the largest feature value .,Traditional Methods,Traditional Methods,text-classification,0,8,0.4705882352941176,110,0.4845814977973568,8,0.4705882352941176,1,0
112,Bag - of - ngrams and its TFIDF .,Traditional Methods,Traditional Methods,text-classification,0,9,0.5294117647058824,111,0.4889867841409692,9,0.5294117647058824,1,0
113,"The bag - of - ngrams models are constructed by selecting the 500,000 most frequent n-grams ( up to 5 - grams ) from the training subset for each dataset .",Traditional Methods,Traditional Methods,text-classification,0,10,0.5882352941176471,112,0.4933920704845815,10,0.5882352941176471,1,0
114,The feature values are computed the same way as in the bag - of - words model .,Traditional Methods,Traditional Methods,text-classification,0,11,0.6470588235294118,113,0.4977973568281938,11,0.6470588235294118,1,0
115,Bag - of - means on word embedding .,Traditional Methods,Traditional Methods,text-classification,0,12,0.7058823529411765,114,0.5022026431718062,12,0.7058823529411765,1,0
116,"We also have an experimental model that uses k-means on word2vec learnt from the training subset of each dataset , and then use these learnt means as representatives of the clustered words .",Traditional Methods,Traditional Methods,text-classification,0,13,0.7647058823529411,115,0.5066079295154186,13,0.7647058823529411,1,0
117,We take into consideration all the words that appeared more than 5 times in the training subset .,Traditional Methods,Traditional Methods,text-classification,0,14,0.8235294117647058,116,0.5110132158590308,14,0.8235294117647058,1,0
118,The dimension of the embedding is 300 .,Traditional Methods,Traditional Methods,text-classification,0,15,0.8823529411764706,117,0.5154185022026432,15,0.8823529411764706,1,0
119,The bag - of - means features are computed the same way as in the bag - of - words model .,Traditional Methods,Traditional Methods,text-classification,0,16,0.9411764705882352,118,0.5198237885462555,16,0.9411764705882352,1,0
120,The number of means is 5000 .,Traditional Methods,Traditional Methods,text-classification,0,17,1.0,119,0.5242290748898678,17,1.0,1,0
121,Deep Learning Methods,,,text-classification,0,0,0.0,120,0.5286343612334802,0,0.0,1,0
122,Recently deep learning methods have started to be applied to text classification .,Deep Learning Methods,Deep Learning Methods,text-classification,0,1,0.01,121,0.5330396475770925,1,0.0625,1,0
123,"We choose two simple and representative models for comparison , in which one is word - based ConvNet and the other a simple long - short term memory ( LSTM ) recurrent neural network model .",Deep Learning Methods,Deep Learning Methods,text-classification,0,2,0.02,122,0.5374449339207048,2,0.125,1,0
124,Word - based ConvNets .,Deep Learning Methods,Deep Learning Methods,text-classification,0,3,0.03,123,0.5418502202643172,3,0.1875,1,0
125,"Among the large number of recent works on word - based ConvNets for text classification , one of the differences is the choice of using pretrained or end - to - end learned word representations .",Deep Learning Methods,Deep Learning Methods,text-classification,0,4,0.04,124,0.5462555066079295,4,0.25,1,0
126,We offer comparisons with both using the pretrained word2vec embedding and using lookup tables .,Deep Learning Methods,Deep Learning Methods,text-classification,0,5,0.05,125,0.5506607929515418,5,0.3125,1,0
127,"The embedding size is 300 in both cases , in the same way as our bagof - means model .",Deep Learning Methods,Deep Learning Methods,text-classification,0,6,0.06,126,0.5550660792951542,6,0.375,1,0
128,"To ensure fair comparison , the models for each case are of the same size as our character - level ConvNets , in terms of both the number of layers and each layer 's output size .",Deep Learning Methods,Deep Learning Methods,text-classification,0,7,0.07,127,0.5594713656387665,7,0.4375,1,0
129,Experiments using a thesaurus for data augmentation are also conducted .,Deep Learning Methods,Deep Learning Methods,text-classification,0,8,0.08,128,0.5638766519823789,8,0.5,1,0
130,LSTM LSTM LSTM ... : long - short term memory,Deep Learning Methods,Deep Learning Methods,text-classification,0,9,0.09,129,0.5682819383259912,9,0.5625,1,0
131,Long - short term memory .,Deep Learning Methods,Deep Learning Methods,text-classification,0,10,0.1,130,0.5726872246696035,10,0.625,1,0
132,"We also offer a comparison with a recurrent neural network model , namely long - short term memory ( LSTM ) .",Deep Learning Methods,Deep Learning Methods,text-classification,0,11,0.11,131,0.5770925110132159,11,0.6875,1,0
133,"The LSTM model used in our case is word - based , using pretrained word2vec embedding of size 300 as in previous models .",Deep Learning Methods,Deep Learning Methods,text-classification,0,12,0.12,132,0.5814977973568282,12,0.75,1,0
134,"The model is formed by taking mean of the outputs of all LSTM cells to form a feature vector , and then using multinomial logistic regression on this feature vector .",Deep Learning Methods,Deep Learning Methods,text-classification,0,13,0.13,133,0.5859030837004405,13,0.8125,1,0
135,The output dimension is 512 .,Deep Learning Methods,Deep Learning Methods,text-classification,0,14,0.14,134,0.5903083700440529,14,0.875,1,0
136,"The variant of LSTM we used is the common "" vanilla "" architecture [ 8 ] .",Deep Learning Methods,Deep Learning Methods,text-classification,0,15,0.15,135,0.5947136563876652,15,0.9375,1,0
137,We also used gradient clipping in which the gradient norm is limited to 5 . gives an illustration .,Deep Learning Methods,Deep Learning Methods,text-classification,0,16,0.16,136,0.5991189427312775,16,1.0,1,0
138,Mean,Deep Learning Methods,,text-classification,0,17,0.17,137,0.6035242290748899,0,0.0,1,0
139,Choice of Alphabet,Deep Learning Methods,,text-classification,0,18,0.18,138,0.6079295154185022,0,0.0,1,0
140,"For the alphabet of English , one apparent choice is whether to distinguish between upper-case and lower - case letters .",Deep Learning Methods,Choice of Alphabet,text-classification,0,19,0.19,139,0.6123348017621145,1,0.0121951219512195,1,0
141,We report experiments on this choice and observed that it usually ( but not always ) gives worse results when such distinction is made .,Deep Learning Methods,Choice of Alphabet,text-classification,0,20,0.2,140,0.6167400881057269,2,0.024390243902439,1,0
142,"One possible explanation might be that semantics do not change with different letter cases , therefore there is a benefit of regularization .",Deep Learning Methods,Choice of Alphabet,text-classification,0,21,0.21,141,0.6211453744493393,3,0.0365853658536585,1,0
143,Large - scale Datasets and Results,Deep Learning Methods,,text-classification,0,22,0.22,142,0.6255506607929515,4,0.048780487804878,1,0
144,"Previous research on ConvNets in different areas has shown that they usually work well with largescale datasets , especially when the model takes in low - level raw features like characters in our case .",Deep Learning Methods,Large - scale Datasets and Results,text-classification,0,23,0.23,143,0.6299559471365639,5,0.0609756097560975,1,0
145,"However , most open datasets for text classification are quite small , and large - scale datasets are splitted with a significantly smaller training set than testing .",Deep Learning Methods,Large - scale Datasets and Results,text-classification,0,24,0.24,144,0.6343612334801763,6,0.073170731707317,1,0
146,"Therefore , instead of confusing our community more by using them , we built several large - scale datasets for our experiments , ranging from hundreds of thousands to several millions of samples .",Deep Learning Methods,Large - scale Datasets and Results,text-classification,0,25,0.25,145,0.6387665198237885,7,0.0853658536585365,1,0
147,is a summary .,Deep Learning Methods,Large - scale Datasets and Results,text-classification,0,26,0.26,146,0.6431718061674009,8,0.0975609756097561,1,0
148,Sogou news corpus .,Deep Learning Methods,Large - scale Datasets and Results,text-classification,0,27,0.27,147,0.6475770925110133,9,0.1097560975609756,1,0
149,"This dataset is a combination of the Sogo u CA and Sogo uCS news corpora , containing in total 2,909,551 news articles in various topic channels .",Deep Learning Methods,Large - scale Datasets and Results,text-classification,0,28,0.28,148,0.6519823788546255,10,0.1219512195121951,1,0
150,"We then labeled each piece of news using its URL , by manually classifying the their domain names .",Deep Learning Methods,Large - scale Datasets and Results,text-classification,0,29,0.29,149,0.6563876651982379,11,0.1341463414634146,1,0
151,This gives us a large corpus of news articles labeled with their categories .,Deep Learning Methods,Large - scale Datasets and Results,text-classification,0,30,0.3,150,0.6607929515418502,12,0.1463414634146341,1,0
152,There area large number categories but most of them contain only few articles .,Deep Learning Methods,Large - scale Datasets and Results,text-classification,0,31,0.31,151,0.6651982378854625,13,0.1585365853658536,1,0
153,"We choose 5 categories - "" sports "" , "" finance "" , "" entertainment "" , "" automobile "" and "" technology "" .",Deep Learning Methods,Large - scale Datasets and Results,text-classification,0,32,0.32,152,0.6696035242290749,14,0.1707317073170731,1,0
154,"The number of training samples selected for each class is 90,000 and testing 12,000 .",Deep Learning Methods,Large - scale Datasets and Results,text-classification,0,33,0.33,153,0.6740088105726872,15,0.1829268292682926,1,0
155,"Although this is a dataset in Chinese , we used pypinyin package combined with jieba Chinese segmentation system to produce Pinyin - a phonetic romanization of Chinese .",Deep Learning Methods,Large - scale Datasets and Results,text-classification,0,34,0.34,154,0.6784140969162996,16,0.1951219512195122,1,0
156,The models for English can then be applied to this dataset without change .,Deep Learning Methods,Large - scale Datasets and Results,text-classification,0,35,0.35,155,0.6828193832599119,17,0.2073170731707317,1,0
157,The fields used are title and content . :,Deep Learning Methods,Large - scale Datasets and Results,text-classification,0,36,0.36,156,0.6872246696035242,18,0.2195121951219512,1,0
158,Testing errors of all the models .,Deep Learning Methods,Large - scale Datasets and Results,text-classification,0,37,0.37,157,0.6916299559471366,19,0.2317073170731707,1,0
159,Numbers are in percentage .,Deep Learning Methods,Large - scale Datasets and Results,text-classification,0,38,0.38,158,0.6960352422907489,20,0.2439024390243902,1,0
160,""" Lg "" stands for "" large "" and "" Sm "" stands for "" small "" .",Deep Learning Methods,Large - scale Datasets and Results,text-classification,0,39,0.39,159,0.7004405286343612,21,0.2560975609756097,1,0
161,""" w2 v "" is an abbreviation for "" word2vec "" , and "" Lk "" for "" lookup DBPedia ontology dataset .",Deep Learning Methods,Large - scale Datasets and Results,text-classification,0,40,0.4,160,0.7048458149779736,22,0.2682926829268293,1,0
162,DBpedia is a crowd - sourced community effort to extract structured information from Wikipedia .,Deep Learning Methods,Large - scale Datasets and Results,text-classification,0,41,0.41,161,0.7092511013215859,23,0.2804878048780488,1,0
163,The DBpedia ontology dataset is constructed by picking 14 nonoverlapping classes from DBpedia 2014 .,Deep Learning Methods,Large - scale Datasets and Results,text-classification,0,42,0.42,162,0.7136563876651982,24,0.2926829268292683,1,0
164,"From each of these 14 ontology classes , we randomly choose 40,000 training samples and 5,000 testing samples .",Deep Learning Methods,Large - scale Datasets and Results,text-classification,0,43,0.43,163,0.7180616740088106,25,0.3048780487804878,1,0
165,The fields we used for this dataset contain title and abstract of each Wikipedia article .,Deep Learning Methods,Large - scale Datasets and Results,text-classification,0,44,0.44,164,0.7224669603524229,26,0.3170731707317073,1,0
166,Yelp reviews .,Deep Learning Methods,Large - scale Datasets and Results,text-classification,0,45,0.45,165,0.7268722466960352,27,0.3292682926829268,1,0
167,The Yelp reviews dataset is obtained from the Yelp Dataset Challenge in 2015 .,Deep Learning Methods,Large - scale Datasets and Results,text-classification,0,46,0.46,166,0.7312775330396476,28,0.3414634146341463,1,0
168,"This dataset contains 1,569,264 samples that have review texts .",Deep Learning Methods,Large - scale Datasets and Results,text-classification,0,47,0.47,167,0.73568281938326,29,0.3536585365853658,1,0
169,"Two classification tasks are constructed from this dataset - one predicting full number of stars the user has given , and the other predicting a polarity label by considering stars 1 and 2 negative , and 3 and 4 positive .",Deep Learning Methods,Large - scale Datasets and Results,text-classification,0,48,0.48,168,0.7400881057268722,30,0.3658536585365853,1,0
170,"The full dataset has 130,000 training samples and 10,000 testing samples in each star , and the polarity dataset has 280,000 training samples and 19,000 test samples in each polarity .",Deep Learning Methods,Large - scale Datasets and Results,text-classification,0,49,0.49,169,0.7444933920704846,31,0.3780487804878049,1,0
171,Yahoo!,Deep Learning Methods,,text-classification,0,50,0.5,170,0.748898678414097,32,0.3902439024390244,1,0
172,Answers dataset .,Deep Learning Methods,Yahoo!,text-classification,0,51,0.51,171,0.7533039647577092,33,0.4024390243902439,1,0
173,We obtained Yahoo!,Deep Learning Methods,,text-classification,0,52,0.52,172,0.7577092511013216,34,0.4146341463414634,1,0
174,Answers Comprehensive Questions and Answers version 1.0 dataset through the Yahoo !,Deep Learning Methods,We obtained Yahoo!,text-classification,0,53,0.53,173,0.762114537444934,35,0.4268292682926829,1,0
175,Webscope program .,Deep Learning Methods,We obtained Yahoo!,text-classification,0,54,0.54,174,0.7665198237885462,36,0.4390243902439024,1,0
176,"The corpus contains 4,483,032 questions and their answers .",Deep Learning Methods,We obtained Yahoo!,text-classification,0,55,0.55,175,0.7709251101321586,37,0.4512195121951219,1,0
177,We constructed a topic classification dataset from this corpus using 10 largest main categories .,Deep Learning Methods,We obtained Yahoo!,text-classification,0,56,0.56,176,0.775330396475771,38,0.4634146341463415,1,0
178,"Each class contains 140,000 training samples and 5,000 testing samples .",Deep Learning Methods,We obtained Yahoo!,text-classification,0,57,0.57,177,0.7797356828193832,39,0.4756097560975609,1,0
179,"The fields we used include question title , question content and best answer .",Deep Learning Methods,We obtained Yahoo!,text-classification,0,58,0.58,178,0.7841409691629956,40,0.4878048780487805,1,0
180,Amazon reviews .,Deep Learning Methods,We obtained Yahoo!,text-classification,0,59,0.59,179,0.788546255506608,41,0.5,1,0
181,"We obtained an Amazon review dataset from the Stanford Network Analysis Project ( SNAP ) , which spans 18 years with 34,686,770 reviews from 6,643,669 users on 2,441,053 products .",Deep Learning Methods,We obtained Yahoo!,text-classification,0,60,0.6,180,0.7929515418502202,42,0.5121951219512195,1,0
182,"Similarly to the Yelp review dataset , we also constructed 2 datasets - one full score prediction and another polarity prediction .",Deep Learning Methods,We obtained Yahoo!,text-classification,0,61,0.61,181,0.7973568281938326,43,0.524390243902439,1,0
183,"The full dataset contains 600,000 training samples and 130,000 testing samples in each class , whereas the polarity dataset contains 1,800,000 training samples and 200,000 testing samples in each polarity sentiment .",Deep Learning Methods,We obtained Yahoo!,text-classification,0,62,0.62,182,0.801762114537445,44,0.5365853658536586,1,0
184,The fields used are review title and review content .,Deep Learning Methods,We obtained Yahoo!,text-classification,0,63,0.63,183,0.8061674008810573,45,0.5487804878048781,1,0
185,lists all the testing errors we obtained from these datasets for all the applicable models .,Deep Learning Methods,We obtained Yahoo!,text-classification,0,64,0.64,184,0.8105726872246696,46,0.5609756097560976,1,0
186,"Note that since we do not have a Chinese thesaurus , the Sogou News dataset does not have any results using thesaurus augmentation .",Deep Learning Methods,We obtained Yahoo!,text-classification,0,65,0.65,185,0.8149779735682819,47,0.573170731707317,1,0
187,We labeled the best result in blue and worse result in red .,Deep Learning Methods,We obtained Yahoo!,text-classification,0,66,0.66,186,0.8193832599118943,48,0.5853658536585366,1,0
188,Figure 3 : Relative errors with comparison models,Deep Learning Methods,We obtained Yahoo!,text-classification,0,67,0.67,187,0.8237885462555066,49,0.5975609756097561,1,0
189,"To understand the results in table 4 further , we offer some empirical analysis in this section .",Deep Learning Methods,We obtained Yahoo!,text-classification,0,68,0.68,188,0.8281938325991189,50,0.6097560975609756,1,0
190,"To facilitate our analysis , we present the relative errors in with respect to comparison models .",Deep Learning Methods,We obtained Yahoo!,text-classification,0,69,0.69,189,0.8325991189427313,51,0.6219512195121951,1,0
191,"Each of these plots is computed by taking the difference between errors on comparison model and our character - level ConvNet model , then divided by the comparison model error .",Deep Learning Methods,We obtained Yahoo!,text-classification,0,70,0.7,190,0.8370044052863436,52,0.6341463414634146,1,0
192,All ConvNets in the figure are the large models with thesaurus augmentation respectively .,Deep Learning Methods,We obtained Yahoo!,text-classification,0,71,0.71,191,0.8414096916299559,53,0.6463414634146342,1,0
193,Character - level,Deep Learning Methods,We obtained Yahoo!,text-classification,0,72,0.72,192,0.8458149779735683,54,0.6585365853658537,1,0
194,ConvNet is an effective method .,Deep Learning Methods,We obtained Yahoo!,text-classification,0,73,0.73,193,0.8502202643171806,55,0.6707317073170732,1,0
195,The most important conclusion from our experiments is that character - level ConvNets could work for text classification without the need for words .,Deep Learning Methods,We obtained Yahoo!,text-classification,0,74,0.74,194,0.8546255506607929,56,0.6829268292682927,1,1
196,This is a strong indication that language could also bethought of as a signal no different from any other kind .,Deep Learning Methods,We obtained Yahoo!,text-classification,0,75,0.75,195,0.8590308370044053,57,0.6951219512195121,1,0
197,shows 12 random first - layer patches learnt by one of our character - level ConvNets for DBPedia dataset .,Deep Learning Methods,We obtained Yahoo!,text-classification,0,76,0.76,196,0.8634361233480177,58,0.7073170731707317,1,0
198,Dataset size forms a dichotomy between traditional and ConvNets models .,Deep Learning Methods,We obtained Yahoo!,text-classification,0,77,0.77,197,0.8678414096916299,59,0.7195121951219512,1,0
199,The most obvious trend coming from all the plots in is that the larger datasets tend to perform better .,Deep Learning Methods,We obtained Yahoo!,text-classification,0,78,0.78,198,0.8722466960352423,60,0.7317073170731707,1,1
200,"Traditional methods like n-grams TFIDF remain strong candidates for dataset of size up to several hundreds of thousands , and only until the dataset goes to the scale of several millions do we observe that character - level ConvNets start to do better .",Deep Learning Methods,We obtained Yahoo!,text-classification,0,79,0.79,199,0.8766519823788547,61,0.7439024390243902,1,0
201,Conv Nets may work well for user - generated data .,Deep Learning Methods,We obtained Yahoo!,text-classification,0,80,0.8,200,0.8810572687224669,62,0.7560975609756098,1,0
202,User- generated data vary in the degree of how well the texts are curated .,Deep Learning Methods,We obtained Yahoo!,text-classification,0,81,0.81,201,0.8854625550660793,63,0.7682926829268293,1,0
203,"For example , in our million scale datasets , Amazon reviews tend to be raw user-inputs , whereas users might be extra careful in their writings on Yahoo !",Deep Learning Methods,We obtained Yahoo!,text-classification,0,82,0.82,202,0.8898678414096917,64,0.7804878048780488,1,0
204,Answers .,Deep Learning Methods,,text-classification,0,83,0.83,203,0.8942731277533039,65,0.7926829268292683,1,0
205,"Plots comparing word - based deep models ( figures 3 c , 3 d and 3 e ) show that character - level ConvNets work better for less curated user - generated texts .",Deep Learning Methods,Answers .,text-classification,0,84,0.84,204,0.8986784140969163,66,0.8048780487804879,1,0
206,This property suggests that ConvNets may have better applicability to real - world scenarios .,Deep Learning Methods,Answers .,text-classification,0,85,0.85,205,0.9030837004405288,67,0.8170731707317073,1,0
207,"However , further analysis is needed to validate the hypothesis that ConvNets are truly good at identifying exotic character combinations such as misspellings and emoticons , as our experiments alone do not show any explicit evidence .",Deep Learning Methods,Answers .,text-classification,0,86,0.86,206,0.9074889867841408,68,0.8292682926829268,1,1
208,Choice of alphabet makes a difference .,Deep Learning Methods,Answers .,text-classification,0,87,0.87,207,0.9118942731277532,69,0.8414634146341463,1,0
209,shows that changing the alphabet by distinguishing between uppercase and lowercase letters could make a difference .,Deep Learning Methods,Answers .,text-classification,0,88,0.88,208,0.9162995594713657,70,0.8536585365853658,1,0
210,"For million - scale datasets , it seems that not making such distinction usually works better .",Deep Learning Methods,Answers .,text-classification,0,89,0.89,209,0.920704845814978,71,0.8658536585365854,1,0
211,"One possible explanation is that there is a regularization effect , but this is to be validated .",Deep Learning Methods,Answers .,text-classification,0,90,0.9,210,0.9251101321585904,72,0.8780487804878049,1,0
212,Semantics of tasks may not matter .,Deep Learning Methods,Answers .,text-classification,0,91,0.91,211,0.9295154185022028,73,0.8902439024390244,1,0
213,Our datasets consist of two kinds of tasks : sentiment analysis ( Yelp and Amazon reviews ) and topic classification ( all others ) .,Deep Learning Methods,Answers .,text-classification,0,92,0.92,212,0.933920704845815,74,0.902439024390244,1,0
214,This dichotomy in task semantics does not seem to play a role in deciding which method is better .,Deep Learning Methods,Answers .,text-classification,0,93,0.93,213,0.9383259911894272,75,0.9146341463414634,1,0
215,Bag - of - means is a misuse of word2vec .,Deep Learning Methods,Answers .,text-classification,0,94,0.94,214,0.9427312775330396,76,0.926829268292683,1,0
216,One of the most obvious facts one could observe from table 4 and figure 3 a is that the bag - of - means model performs worse in every case .,Deep Learning Methods,Answers .,text-classification,0,95,0.95,215,0.947136563876652,77,0.9390243902439024,1,0
217,"Comparing with traditional models , this suggests such a simple use of a distributed word representation may not give us an advantage to text classification .",Deep Learning Methods,Answers .,text-classification,0,96,0.96,216,0.9515418502202644,78,0.951219512195122,1,1
218,"However , our experiments does not speak for any other language processing tasks or use of word2vec in any other way .",Deep Learning Methods,Answers .,text-classification,0,97,0.97,217,0.9559471365638766,79,0.9634146341463414,1,0
219,There is no free lunch .,Deep Learning Methods,Answers .,text-classification,0,98,0.98,218,0.960352422907489,80,0.975609756097561,1,0
220,Our experiments once again verifies that there is not a single machine learning model that can work for all kinds of datasets .,Deep Learning Methods,Answers .,text-classification,0,99,0.99,219,0.9647577092511012,81,0.9878048780487804,1,0
221,The factors discussed in this section could all play a role in deciding which method is the best for some specific application .,Deep Learning Methods,Answers .,text-classification,0,100,1.0,220,0.9691629955947136,82,1.0,1,0
222,Conclusion and Outlook,,,text-classification,0,0,0.0,221,0.973568281938326,0,0.0,1,0
223,This article offers an empirical study on character - level convolutional networks for text classification .,Conclusion and Outlook,Conclusion and Outlook,text-classification,0,1,0.2,222,0.9779735682819384,1,0.2,0,0
224,We compared with a large number of traditional and deep learning models using several largescale datasets .,Conclusion and Outlook,Conclusion and Outlook,text-classification,0,2,0.4,223,0.9823788546255506,2,0.4,0,0
225,"On one hand , analysis shows that character - level ConvNet is an effective method .",Conclusion and Outlook,Conclusion and Outlook,text-classification,0,3,0.6,224,0.986784140969163,3,0.6,0,0
226,"On the other hand , how well our model performs in comparisons depends on many factors , such as dataset size , whether the texts are curated and choice of alphabet .",Conclusion and Outlook,Conclusion and Outlook,text-classification,0,4,0.8,225,0.9911894273127754,4,0.8,0,0
227,"In the future , we hope to apply character - level ConvNets fora broader range of language processing tasks especially when structured outputs are needed .",Conclusion and Outlook,Conclusion and Outlook,text-classification,0,5,1.0,226,0.9955947136563876,5,1.0,0,0
1,title,,,text-classification,1,0,0.0,0,0.0,0,0.0,1,0
2,Supervised and Semi- Supervised Text Categorization using LSTM for Region Embeddings,title,title,text-classification,1,1,0.0,1,0.00390625,1,0.0,1,1
3,abstract,,,text-classification,1,0,0.0,2,0.0078125,0,0.0,1,0
4,"One - hot CNN ( convolutional neural network ) has been shown to be effective for text categorization ( Johnson & Zhang , 2015a ; b ) .",abstract,abstract,text-classification,1,1,0.125,3,0.01171875,1,0.125,1,1
5,We view it as a special case of a general framework which jointly trains a linear model with a non-linear feature generator consisting of ' text region embedding + pooling ' .,abstract,abstract,text-classification,1,2,0.25,4,0.015625,2,0.25,1,0
6,"Under this framework , we explore a more sophisticated region embedding method using Long Short - Term Memory ( LSTM ) .",abstract,abstract,text-classification,1,3,0.375,5,0.01953125,3,0.375,1,0
7,"LSTM can embed text regions of variable ( and possibly large ) sizes , whereas the region size needs to be fixed in a CNN .",abstract,abstract,text-classification,1,4,0.5,6,0.0234375,4,0.5,1,0
8,We seek effective and efficient use of LSTM for this purpose in the supervised and semi-supervised settings .,abstract,abstract,text-classification,1,5,0.625,7,0.02734375,5,0.625,1,0
9,The best results were obtained by combining region embeddings in the form of LSTM and convolution layers trained on unlabeled data .,abstract,abstract,text-classification,1,6,0.75,8,0.03125,6,0.75,1,0
10,"The results indicate that on this task , embeddings of text regions , which can convey complex concepts , are more useful than embeddings of single words in isolation .",abstract,abstract,text-classification,1,7,0.875,9,0.03515625,7,0.875,1,0
11,We report performances exceeding the previous best results on four benchmark datasets .,abstract,abstract,text-classification,1,8,1.0,10,0.0390625,8,1.0,1,0
12,Introduction,,,text-classification,1,0,0.0,11,0.04296875,0,0.0,1,0
13,"Text categorization is the task of assigning labels to documents written in a natural language , and it has numerous real - world applications including sentiment analysis as well as traditional topic assignment tasks .",Introduction,Introduction,text-classification,1,1,0.009009009009009,12,0.046875,1,0.0294117647058823,1,0
14,"The state - of - the art methods for text categorization had long been linear predictors ( e.g. , SVM with a linear kernel ) with either bag - ofword or bag - of - n- gram vectors ( hereafter bow ) as input , e.g. , .",Introduction,Introduction,text-classification,1,2,0.018018018018018,13,0.05078125,2,0.0588235294117647,1,0
15,"This , however , A convolutional neural network ( CNN ) ) is a feedforward neural network with convolution layers interleaved with pooling layers , originally developed for image processing .",Introduction,Introduction,text-classification,1,3,0.027027027027027,14,0.0546875,3,0.088235294117647,1,0
16,"In its convolution layer , a small region of data ( e.g. , a small square of image ) at every location is converted to a low-dimensional vector with information relevant to the task being preserved , which we loosely term ' embedding ' .",Introduction,Introduction,text-classification,1,4,0.036036036036036,15,0.05859375,4,0.1176470588235294,1,0
17,"The embedding function is shared among all the locations , so that useful features can be detected irrespective of their locations .",Introduction,Introduction,text-classification,1,5,0.045045045045045,16,0.0625,5,0.1470588235294117,1,0
18,"In its simplest form , onehot CNN works as follows .",Introduction,Introduction,text-classification,1,6,0.054054054054054,17,0.06640625,6,0.1764705882352941,1,0
19,"A document is represented as a sequence of one - hot vectors ( each of which indicates a word by the position of a 1 ) ; a convolution layer converts small regions of the document ( e.g. , "" I love it "" ) to low-dimensional vectors at every location ( embedding of text regions ) ; a pooling layer aggregates the region embedding results to a document vector by taking componentwise maximum or average ; and the top layer classifies a document vector with a linear model .",Introduction,Introduction,text-classification,1,7,0.063063063063063,18,0.0703125,7,0.2058823529411764,1,0
20,The onehot CNN and its semi-supervised extension were shown to be superior to a number of previous methods .,Introduction,Introduction,text-classification,1,8,0.072072072072072,19,0.07421875,8,0.2352941176470588,1,0
21,"In this work , we consider a more general framework ( subsuming one - hot CNN ) which jointly trains a feature generator and a linear model , where the feature generator consists of ' region embedding + pooling ' .",Introduction,Introduction,text-classification,1,9,0.081081081081081,20,0.078125,9,0.2647058823529412,1,1
22,"The specific region embedding function of one - hot CNN takes the simple form v ( x ) = max (0 , Wx + b ) ,",Introduction,Introduction,text-classification,1,10,0.09009009009009,21,0.08203125,10,0.2941176470588235,1,0
23,"where x is a concatenation of one - hot vectors ( therefore , ' one - hot ' in the name ) of the words in the - th region ( of a fixed size ) , and the weight matrix W and the bias vector b need to be trained .",Introduction,Introduction,text-classification,1,11,0.0990990990990991,22,0.0859375,11,0.3235294117647059,1,0
24,"It is simple and fast to compute , and considering its simplicity , the method works surprisingly well if the region size is appropriately set .",Introduction,Introduction,text-classification,1,12,0.1081081081081081,23,0.08984375,12,0.3529411764705882,1,0
25,"However , there are also potential shortcomings .",Introduction,,text-classification,1,13,0.1171171171171171,24,0.09375,13,0.3823529411764705,1,0
26,"The region size must be fixed , which may not be optimal as the size of relevant regions may vary .",Introduction,"However , there are also potential shortcomings .",text-classification,1,14,0.1261261261261261,25,0.09765625,14,0.4117647058823529,1,0
27,"Practically , the region size can not be very large as the number of parameters to be learned ( components of W ) depends on it .",Introduction,"However , there are also potential shortcomings .",text-classification,1,15,0.1351351351351351,26,0.1015625,15,0.4411764705882353,1,0
28,JZ15 proposed variations to alleviate these issues .,Introduction,,text-classification,1,16,0.1441441441441441,27,0.10546875,16,0.4705882352941176,1,0
29,"For example , a bow - input variation allows x above to be a bow vector of the region .",Introduction,JZ15 proposed variations to alleviate these issues .,text-classification,1,17,0.1531531531531531,28,0.109375,17,0.5,1,0
30,"This enables a larger region , but at the expense of losing word order in the region and so its use maybe limited .",Introduction,JZ15 proposed variations to alleviate these issues .,text-classification,1,18,0.1621621621621621,29,0.11328125,18,0.5294117647058824,1,0
31,"In this work , we build on the general framework of ' region embedding + pooling ' and explore a more sophisticated region embedding via Long Short - Term Memory ( LSTM ) , seeking to overcome the shortcomings above , in the supervised and semi-supervised settings .",Introduction,JZ15 proposed variations to alleviate these issues .,text-classification,1,19,0.1711711711711711,30,0.1171875,19,0.5588235294117647,1,1
32,LSTM ) is a recurrent neural network .,Introduction,,text-classification,1,20,0.1801801801801801,31,0.12109375,20,0.5882352941176471,1,0
33,"In its typical applications to text , an LSTM takes words in a sequence one by one ; i.e. , at time t , it takes as input the t- th word and the output from time t ?",Introduction,LSTM ) is a recurrent neural network .,text-classification,1,21,0.1891891891891892,32,0.125,21,0.6176470588235294,1,0
34,"1 . Therefore , the output from each time step can be regarded as the embedding of the sequence of words that have been seen so far ( or a relevant part of it ) .",Introduction,LSTM ) is a recurrent neural network .,text-classification,1,22,0.1981981981981982,33,0.12890625,22,0.6470588235294118,1,0
35,It is designed to enable learning of dependencies over larger time lags than feasible with traditional recurrent networks .,Introduction,LSTM ) is a recurrent neural network .,text-classification,1,23,0.2072072072072072,34,0.1328125,23,0.6764705882352942,1,0
36,"That is , an LSTM can be used to embed text regions of variable ( and possibly large ) sizes .",Introduction,LSTM ) is a recurrent neural network .,text-classification,1,24,0.2162162162162162,35,0.13671875,24,0.7058823529411765,1,0
37,"We pursue the best use of LSTM for our purpose , and then compare the resulting model with the previous best methods including one - hot CNN and previous LSTM .",Introduction,LSTM ) is a recurrent neural network .,text-classification,1,25,0.2252252252252252,36,0.140625,25,0.7352941176470589,1,1
38,"Our strategy is to simplify the model as much as possible , including elimination of a word embedding layer routinely used to produce input to LSTM .",Introduction,LSTM ) is a recurrent neural network .,text-classification,1,26,0.2342342342342342,37,0.14453125,26,0.7647058823529411,1,1
39,Our findings are threefold .,Introduction,,text-classification,1,27,0.2432432432432432,38,0.1484375,27,0.7941176470588235,1,0
40,"First , in the supervised setting , our simplification strategy leads to higher accuracy and faster training than previous LSTM .",Introduction,Our findings are threefold .,text-classification,1,28,0.2522522522522522,39,0.15234375,28,0.8235294117647058,1,0
41,"Second , accuracy can be further improved by training LSTMs on unlabeled data for learning useful region embeddings and using them to produce additional input .",Introduction,Our findings are threefold .,text-classification,1,29,0.2612612612612612,40,0.15625,29,0.8529411764705882,1,0
42,"Third , both our LSTM models and one - hot CNN strongly outperform other methods including previous LSTM .",Introduction,Our findings are threefold .,text-classification,1,30,0.2702702702702703,41,0.16015625,30,0.8823529411764706,1,0
43,"The best results are obtained by combining the two types of region embeddings ( LSTM embed - dings and CNN embeddings ) trained on unlabeled data , indicating that their strengths are complementary .",Introduction,Our findings are threefold .,text-classification,1,31,0.2792792792792792,42,0.1640625,31,0.9117647058823528,1,0
44,"Overall , our results show that for text categorization , embeddings of text regions , which can convey higher - level concepts than single words in isolation , are useful , and that useful region embeddings can be learned without going through word embedding learning .",Introduction,Our findings are threefold .,text-classification,1,32,0.2882882882882883,43,0.16796875,32,0.9411764705882352,1,0
45,We report performances exceeding the previous best results on four benchmark datasets .,Introduction,Our findings are threefold .,text-classification,1,33,0.2972972972972973,44,0.171875,33,0.9705882352941176,1,0
46,Our code and experimental details are available at http://riejohnson.com/cnn download.html .,Introduction,Our findings are threefold .,text-classification,1,34,0.3063063063063063,45,0.17578125,34,1.0,1,0
47,Preliminary,Introduction,,text-classification,1,35,0.3153153153153153,46,0.1796875,0,0.0,1,0
48,"On text , LSTM has been used for labeling or generating words .",Introduction,Preliminary,text-classification,1,36,0.3243243243243243,47,0.18359375,1,0.0357142857142857,1,0
49,"It has been also used for representing short sentences mostly for sentiment analysis , and some of them rely on syntactic parse trees ; see e.g. , .",Introduction,Preliminary,text-classification,1,37,0.3333333333333333,48,0.1875,2,0.0714285714285714,1,0
50,"Unlike these studies , this work as well as JZ15 focuses on classifying general full - length documents without any special linguistic knowledge .",Introduction,Preliminary,text-classification,1,38,0.3423423423423423,49,0.19140625,3,0.1071428571428571,1,0
51,"Similarly , DL15 applied LSTM to categorizing general full - length documents .",Introduction,Preliminary,text-classification,1,39,0.3513513513513513,50,0.1953125,4,0.1428571428571428,1,0
52,"Therefore , our empirical comparisons will focus on DL15 and JZ15 , both of which reported new state of the art results .",Introduction,Preliminary,text-classification,1,40,0.3603603603603603,51,0.19921875,5,0.1785714285714285,1,0
53,"Let us first introduce the general LSTM formulation , and then briefly describe DL15 's model as it illustrates the challenges in using LSTMs for this task .",Introduction,Preliminary,text-classification,1,41,0.3693693693693693,52,0.203125,6,0.2142857142857142,1,0
54,LSTM,Introduction,,text-classification,1,42,0.3783783783783784,53,0.20703125,7,0.25,1,0
55,"While several variations exist , we base our work on the following LSTM formulation , which was used in , e.g. ,",Introduction,LSTM,text-classification,1,43,0.3873873873873873,54,0.2109375,8,0.2857142857142857,1,0
56,where denotes element - wise multiplication and ?,Introduction,LSTM,text-classification,1,44,0.3963963963963964,55,0.21484375,9,0.3214285714285714,1,0
57,"is an element - wise squash function to make the gating values in [ 0 , 1 ] .",Introduction,LSTM,text-classification,1,45,0.4054054054054054,56,0.21875,10,0.3571428571428571,1,0
58,We fix ? to sigmoid .,Introduction,LSTM,text-classification,1,46,0.4144144144144144,57,0.22265625,11,0.3928571428571428,1,0
59,x t ?,Introduction,LSTM,text-classification,1,47,0.4234234234234234,58,0.2265625,12,0.4285714285714285,1,0
60,"Rd is the input from the lower layer at time step t , where d would be , for example , size of vocabulary if the input was a one - hot vector representing a word , or the dimensionality of word vector if the lower layer was a word embedding layer .",Introduction,LSTM,text-classification,1,48,0.4324324324324324,59,0.23046875,13,0.4642857142857143,1,0
61,"With q LSTM units , the dimensionality of the weight matrices and bias vectors , which need to be trained , are W ( ) ? R qd , U ( ) ? R qq , and b ( ) ?",Introduction,LSTM,text-classification,1,49,0.4414414414414414,60,0.234375,14,0.5,1,0
62,R q for all types .,Introduction,,text-classification,1,50,0.4504504504504504,61,0.23828125,15,0.5357142857142857,1,0
63,"The centerpiece of LSTM is the memory cells ct , designed to counteract the risk of vanishing / exploding gradients , thus enabling learning of dependencies over larger time lags than feasible with traditional recurrent networks .",Introduction,R q for all types .,text-classification,1,51,0.4594594594594595,62,0.2421875,16,0.5714285714285714,1,0
64,The forget gate ft is for resetting the memory cells .,Introduction,R q for all types .,text-classification,1,52,0.4684684684684684,63,0.24609375,17,0.6071428571428571,1,0
65,The input gate it and output gate o t control the input and output of the memory cells .,Introduction,R q for all types .,text-classification,1,53,0.4774774774774775,64,0.25,18,0.6428571428571429,1,0
66,Word - vector LSTM ( wv - LSTM ) [ DL15 ] DL15 's application of LSTM to text categorization is straightforward .,Introduction,R q for all types .,text-classification,1,54,0.4864864864864865,65,0.25390625,19,0.6785714285714286,1,0
67,"As illustrated in , for each document , the output of the LSTM layer is the output of the last time step ( corresponding to the last word of the document ) , which represents the whole document ( document embedding ) .",Introduction,R q for all types .,text-classification,1,55,0.4954954954954955,66,0.2578125,20,0.7142857142857143,1,0
68,"Like many other studies of LSTM on text , words are first converted to low - dimensional dense word vectors via a word embedding layer ; therefore , we call it word - vector LSTM or wv - LSTM .",Introduction,R q for all types .,text-classification,1,56,0.5045045045045045,67,0.26171875,21,0.75,1,0
69,DL15 observed that wv - LSTM underperformed linear predictors and its training was unstable .,Introduction,R q for all types .,text-classification,1,57,0.5135135135135135,68,0.265625,22,0.7857142857142857,1,0
70,This was attributed to the fact that documents are long .,Introduction,R q for all types .,text-classification,1,58,0.5225225225225225,69,0.26953125,23,0.8214285714285714,1,0
71,"In addition , we found that training and testing of wv - LSTM is time / resource consuming .",Introduction,R q for all types .,text-classification,1,59,0.5315315315315315,70,0.2734375,24,0.8571428571428571,1,0
72,"To put it into perspective , using a GPU , one epoch of wv - LSTM training takes nearly 20 times longer than that of one - hot CNN training even though it achieves poorer accuracy ( the first two rows of ) .",Introduction,R q for all types .,text-classification,1,60,0.5405405405405406,71,0.27734375,25,0.8928571428571429,1,0
73,"This is due to the sequential nature of LSTM , i.e. , computation at time t requires the output of time t ? 1 , whereas modern computation depends on parallelization for speedup .",Introduction,R q for all types .,text-classification,1,61,0.5495495495495496,72,0.28125,26,0.9285714285714286,1,0
74,"Documents in a mini-batch can be processed in parallel , but the variability of document lengths reduces the degree of parallelization 1 .",Introduction,R q for all types .,text-classification,1,62,0.5585585585585585,73,0.28515625,27,0.9642857142857144,1,0
75,It was shown in DL15 that training becomes stable and accuracy improves drastically when LSTM and the word embedding layer are jointly pre-trained with either the language model learning objective ( predicting the next word ) or autoencoder objective ( memorizing the document ) .,Introduction,R q for all types .,text-classification,1,63,0.5675675675675675,74,0.2890625,28,1.0,1,0
76,Supervised LSTM for text categorization,Introduction,,text-classification,1,64,0.5765765765765766,75,0.29296875,0,0.0,1,0
77,"Within the framework of ' region embedding + pooling ' for text categorization , we seek effective and efficient use of LSTM as an alternative region embedding method .",Introduction,Supervised LSTM for text categorization,text-classification,1,65,0.5855855855855856,76,0.296875,1,0.25,1,0
78,"This section focuses on an end - to - end supervised setting so that there is no additional data ( e.g. , unlabeled data ) or additional algorithm ( e.g. , for learning a word embedding ) .",Introduction,Supervised LSTM for text categorization,text-classification,1,66,0.5945945945945946,77,0.30078125,2,0.5,1,0
79,Our general strategy is to simplify the model as much as possible .,Introduction,Supervised LSTM for text categorization,text-classification,1,67,0.6036036036036037,78,0.3046875,3,0.75,1,0
80,"We start with elimination of the word embedding layer so that one - hot vectors are directly fed to LSTM , which we call one - hot LSTM in short .",Introduction,Supervised LSTM for text categorization,text-classification,1,68,0.6126126126126126,79,0.30859375,4,1.0,1,0
81,Elimination of the word embedding layer,Introduction,,text-classification,1,69,0.6216216216216216,80,0.3125,0,0.0,1,0
82,Facts : A word embedding is a linear operation that can be written as Vx t with x t being a one - hot vector and columns of V being word vectors .,Introduction,Elimination of the word embedding layer,text-classification,1,70,0.6306306306306306,81,0.31640625,1,0.1111111111111111,1,0
83,"Therefore , by replacing the LSTM weights W ( ) with W ( ) V and removing the word embedding layer , a word - vector LSTM can be turned into a one - hot LSTM without changing the model behavior .",Introduction,Elimination of the word embedding layer,text-classification,1,71,0.6396396396396397,82,0.3203125,2,0.2222222222222222,1,0
84,"Thus , word - vector LSTM is not more expressive than one - hot LSTM ; rather , a merit , if any , of training with a word embedding layer would be through imposing restrictions ( e.g. , a low - rank V makes a less expressive model ) to achieve good prior / regularization effects .",Introduction,Elimination of the word embedding layer,text-classification,1,72,0.6486486486486487,83,0.32421875,3,0.3333333333333333,1,0
85,"In the end - to - end supervised setting , a word embedding matrix V would need to be initialized randomly and trained as part of the model .",Introduction,Elimination of the word embedding layer,text-classification,1,73,0.6576576576576577,84,0.328125,4,0.4444444444444444,1,0
86,"In the preliminary experiments under our framework , we were unable to improve accuracy over one - hot LSTM by inclusion of such a randomly initialized word embedding layer ; i.e. , random vectors failed to provide good prior effects .",Introduction,Elimination of the word embedding layer,text-classification,1,74,0.6666666666666666,85,0.33203125,5,0.5555555555555556,1,0
87,"Instead , demerits were evident - more meta-parameters to tune , poor accuracy with lowdimensional word vectors , and slow training / testing with high - dimensional word vectors as they are dense .",Introduction,Elimination of the word embedding layer,text-classification,1,75,0.6756756756756757,86,0.3359375,6,0.6666666666666666,1,0
88,"If a word embedding is appropriately pre-trained with unlabeled data , its inclusion is a form of semi-supervised learning and could be useful .",Introduction,Elimination of the word embedding layer,text-classification,1,76,0.6846846846846847,87,0.33984375,7,0.7777777777777778,1,0
89,"We will show later , however , that this type of approach falls behind our approach of learning region embeddings through training one - hot LSTM on unlabeled data .",Introduction,Elimination of the word embedding layer,text-classification,1,77,0.6936936936936937,88,0.34375,8,0.8888888888888888,1,0
90,"Altogether , elimination of the word embedding layer was found to be useful ; thus , we base our work on one - hot LSTM .",Introduction,Elimination of the word embedding layer,text-classification,1,78,0.7027027027027027,89,0.34765625,9,1.0,1,0
91,More simplifications,Introduction,,text-classification,1,79,0.7117117117117117,90,0.3515625,0,0.0,1,0
92,We introduce four more useful modifications to wv - LSTM that lead to higher accuracy or faster training .,Introduction,More simplifications,text-classification,1,80,0.7207207207207207,91,0.35546875,1,0.0140845070422535,1,0
93,Pooling : simplifying sub - problems,Introduction,,text-classification,1,81,0.7297297297297297,92,0.359375,2,0.028169014084507,1,0
94,Our framework of ' region embedding + pooling ' has a simplification effect as follows .,Introduction,Pooling : simplifying sub - problems,text-classification,1,82,0.7387387387387387,93,0.36328125,3,0.0422535211267605,1,0
95,"In wv - LSTM , the sub-problem that LSTM needs to solve is to represent the entire document by one vector ( document embedding ) .",Introduction,Pooling : simplifying sub - problems,text-classification,1,83,0.7477477477477478,94,0.3671875,4,0.056338028169014,1,0
96,We make this easy by changing it to detecting regions of text ( of arbitrary sizes ) that are relevant to the task and representing them by vectors ( region embedding ) .,Introduction,Pooling : simplifying sub - problems,text-classification,1,84,0.7567567567567568,95,0.37109375,5,0.0704225352112676,1,0
97,"As illustrated in , we let the LSTM layer emit vectors ht at each time step , and let pooling aggregate them into a document vector .",Introduction,Pooling : simplifying sub - problems,text-classification,1,85,0.7657657657657657,96,0.375,6,0.0845070422535211,1,0
98,"With wv - LSTM , LSTM has to remember relevant information until it gets to the end of the document even if relevant information was observed 10 K words away .",Introduction,Pooling : simplifying sub - problems,text-classification,1,86,0.7747747747747747,97,0.37890625,7,0.0985915492957746,1,0
99,The task of our LSTM is easier as it is allowed to forget old things via the forget gate and can focus on representing the concepts conveyed by smaller segments such as phrases or sentences .,Introduction,Pooling : simplifying sub - problems,text-classification,1,87,0.7837837837837838,98,0.3828125,8,0.1126760563380281,1,0
100,A related architecture appears in the Deep Learning Tutorials 2 though it uses a word embedding .,Introduction,Pooling : simplifying sub - problems,text-classification,1,88,0.7927927927927928,99,0.38671875,9,0.1267605633802817,1,0
101,"Another related work is , which combined pooling with non -LSTM recurrent networks and a word embedding .",Introduction,Pooling : simplifying sub - problems,text-classification,1,89,0.8018018018018018,100,0.390625,10,0.1408450704225352,1,0
102,Chopping for speeding up training,Introduction,,text-classification,1,90,0.8108108108108109,101,0.39453125,11,0.1549295774647887,1,0
103,"In addition to simplifying the sub-problem , pooling has the merit of enabling faster training via chopping .",Introduction,Chopping for speeding up training,text-classification,1,91,0.8198198198198198,102,0.3984375,12,0.1690140845070422,1,0
104,"Since we set the goal of LSTM to embedding text regions instead of documents , it is no longer crucial to go through the document from the beginning to the end sequentially .",Introduction,Chopping for speeding up training,text-classification,1,92,0.8288288288288288,103,0.40234375,13,0.1830985915492957,1,0
105,"At the time of training , we can chop each document into segments of a fixed length that is sufficiently long ( e.g. , 50 or 100 ) and process all the segments in a mini batch in parallel as if these segments were individual documents .",Introduction,Chopping for speeding up training,text-classification,1,93,0.8378378378378378,104,0.40625,14,0.1971830985915492,1,0
106,( Note that this is done only in the LSTM layer and pooling is done over the entire document . ),Introduction,Chopping for speeding up training,text-classification,1,94,0.8468468468468469,105,0.41015625,15,0.2112676056338028,1,0
107,We perform testing without chopping .,Introduction,,text-classification,1,95,0.8558558558558559,106,0.4140625,16,0.2253521126760563,1,0
108,"That is , we train LSTM with approximations of sequences for speedup and test with real sequences for better accuracy .",Introduction,We perform testing without chopping .,text-classification,1,96,0.8648648648648649,107,0.41796875,17,0.2394366197183098,1,0
109,"There is a risk of chopping important phrases ( e.g. , "" do n't | like it "" ) , and this can be easily avoided by having segments slightly overlap .",Introduction,We perform testing without chopping .,text-classification,1,97,0.8738738738738738,108,0.421875,18,0.2535211267605634,1,0
110,"However , we found that gains from overlapping segments tend to be small and so our experiments reported below were done without overlapping .",Introduction,We perform testing without chopping .,text-classification,1,98,0.8828828828828829,109,0.42578125,19,0.2676056338028169,1,0
111,Removing the input / output gates,Introduction,,text-classification,1,99,0.8918918918918919,110,0.4296875,20,0.2816901408450704,1,0
112,"We found that when LSTM is followed by pooling , the presence of input and output gates typically does not improve accuracy , while removing them nearly halves the time and memory required for training and testing .",Introduction,Removing the input / output gates,text-classification,1,100,0.9009009009009008,111,0.43359375,21,0.2957746478873239,1,0
113,"It is intuitive , in particular , that pooling can make the output gate unnecessary ; the role of the output gate is to prevent undesirable information from entering the output ht , and such irrelevant information can be filtered out by max - pooling .",Introduction,Removing the input / output gates,text-classification,1,101,0.90990990990991,112,0.4375,22,0.3098591549295774,1,0
114,"Without the input and output gates , the LSTM formulation can be simplified to :",Introduction,Removing the input / output gates,text-classification,1,102,0.918918918918919,113,0.44140625,23,0.323943661971831,1,0
115,( 2 ),Introduction,Removing the input / output gates,text-classification,1,103,0.927927927927928,114,0.4453125,24,0.3380281690140845,1,0
116,This is equivalent to fixing it and o t to all ones .,Introduction,Removing the input / output gates,text-classification,1,104,0.9369369369369368,115,0.44921875,25,0.352112676056338,1,0
117,"It is in spirit similar to Gated Recurrent Units but simpler , having fewer gates .",Introduction,Removing the input / output gates,text-classification,1,105,0.945945945945946,116,0.453125,26,0.3661971830985915,1,0
118,Bidirectional LSTM for better accuracy,Introduction,,text-classification,1,106,0.954954954954955,117,0.45703125,27,0.380281690140845,1,0
119,The changes from wv - LSTM above substantially reduce the time and 2 http://deeplearning.net/tutorial/lstm.html,Introduction,Bidirectional LSTM for better accuracy,text-classification,1,107,0.963963963963964,118,0.4609375,28,0.3943661971830985,1,0
120,One - hot vectors memory required for training and make it practical to add one more layer of LSTM going in the opposite direction for accuracy improvement .,Introduction,Bidirectional LSTM for better accuracy,text-classification,1,108,0.972972972972973,119,0.46484375,29,0.4084507042253521,1,0
121,"As shown in , we concatenate the output of a forward LSTM ( left to right ) and a backward LSTM ( right to left ) , which is referred to as bidirectional LSTM in the literature .",Introduction,Bidirectional LSTM for better accuracy,text-classification,1,109,0.981981981981982,120,0.46875,30,0.4225352112676056,1,0
122,"The resulting model is a one - hot bidirectional LSTM with pooling , and we abbreviate it to oh - 2 LSTMp .",Introduction,Bidirectional LSTM for better accuracy,text-classification,1,110,0.990990990990991,121,0.47265625,31,0.4366197183098591,1,0
123,"shows how much accuracy and / or training speed can be improved by elimination of the word embedding layer , pooling , chopping , removing the input / output gates , and adding the backward LSTM .",Introduction,Bidirectional LSTM for better accuracy,text-classification,1,111,1.0,122,0.4765625,32,0.4507042253521127,1,0
124,Experiments ( supervised ),,,text-classification,1,0,0.0,123,0.48046875,33,0.4647887323943662,1,0
125,"We used four datasets : IMDB , Elec , RCV1 ( second - level topics ) , and 20 - newsgroup ( 20 NG ) 3 , to facilitate direct comparison with JZ15 and DL15 .",Experiments ( supervised ),Experiments ( supervised ),text-classification,1,1,0.0142857142857142,124,0.484375,34,0.4788732394366197,1,0
126,The first three were used in JZ15 .,Experiments ( supervised ),,text-classification,1,2,0.0285714285714285,125,0.48828125,35,0.4929577464788732,1,0
127,IMDB and 20 NG were used in DL15 .,Experiments ( supervised ),,text-classification,1,3,0.0428571428571428,126,0.4921875,36,0.5070422535211268,1,0
128,The datasets are summarized in .,Experiments ( supervised ),,text-classification,1,4,0.0571428571428571,127,0.49609375,37,0.5211267605633803,1,0
129,The data was converted to lower - case letters .,Experiments ( supervised ),The datasets are summarized in .,text-classification,1,5,0.0714285714285714,128,0.5,38,0.5352112676056338,1,0
130,"In the neural network experiments , vocabulary was reduced to the most frequent 30 K words of the training data to reduce computational burden ; square loss was minimized with dropout applied to the input to the top layer ; weights were initialized by the .",Experiments ( supervised ),The datasets are summarized in .,text-classification,1,6,0.0857142857142857,129,0.50390625,39,0.5492957746478874,1,0
131,"Data. "" avg "" / "" max "" : the average / maximum length of documents ( #words ) of the training / test data .",Experiments ( supervised ),The datasets are summarized in .,text-classification,1,7,0.1,130,0.5078125,40,0.5633802816901409,1,0
132,"IMDB and Elec are for sentiment classification ( positive vs. negative ) of movie reviews and Amazon electronics product reviews , respectively .",Experiments ( supervised ),The datasets are summarized in .,text-classification,1,8,0.1142857142857142,131,0.51171875,41,0.5774647887323944,1,0
133,"RCV1 ( second - level topics only ) and 20 NG are for topic categorization of Reuters news articles and newsgroup messages , respectively .",Experiments ( supervised ),The datasets are summarized in .,text-classification,1,9,0.1285714285714285,132,0.515625,42,0.5915492957746479,1,0
134,zero mean and standard deviation 0.01 .,Experiments ( supervised ),The datasets are summarized in .,text-classification,1,10,0.1428571428571428,133,0.51953125,43,0.6056338028169014,1,0
135,Optimization was done with SGD with mini-batch size 50 or 100 with momentum or optionally rmsprop for acceleration .,Experiments ( supervised ),The datasets are summarized in .,text-classification,1,11,0.1571428571428571,134,0.5234375,44,0.6197183098591549,1,0
136,"Hyper parameters such as learning rates were chosen based on the performance on the development data , which was a held - out portion of the training data , and training was redone using all the training data with the chosen parameters .",Experiments ( supervised ),The datasets are summarized in .,text-classification,1,12,0.1714285714285714,135,0.52734375,45,0.6338028169014085,1,0
137,"We used the same pooling method as used in JZ15 , which parameterizes the number of pooling regions so that pooling is done fork non-overlapping regions of equal size , and the resulting k vectors are concatenated to make one vector per document .",Experiments ( supervised ),The datasets are summarized in .,text-classification,1,13,0.1857142857142857,136,0.53125,46,0.647887323943662,1,0
138,"The pooling settings chosen based on the performance on the development data are the same as JZ15a , which are max - pooling with k= 1 on IMDB and Elec and average - pooling with k=10 on RCV1 ; on 20 NG , max - pooling with k = 10 was chosen .",Experiments ( supervised ),The datasets are summarized in .,text-classification,1,14,0.2,137,0.53515625,47,0.6619718309859155,1,0
139,"Comparing the two types of LSTM in , we see that our one - hot bidirectional LSTM with pooling ( oh - 2 LSTMp ) outperforms word - vector LSTM ( wv - LSTM ) on all the datasets , confirming the effectiveness of our approach .",Experiments ( supervised ),The datasets are summarized in .,text-classification,1,15,0.2142857142857142,138,0.5390625,48,0.676056338028169,1,1
140,Now we review the non -LSTM baseline methods .,Experiments ( supervised ),,text-classification,1,16,0.2285714285714285,139,0.54296875,49,0.6901408450704225,1,0
141,The last row of shows the best one - hot CNN results within the constraints above .,Experiments ( supervised ),Now we review the non -LSTM baseline methods .,text-classification,1,17,0.2428571428571428,140,0.546875,50,0.704225352112676,1,0
142,"They were obtained by bow - CNN ( whose input to the embedding function is a bow vector of the region ) with region size 20 on RCV1 , and seq -CNN ( with the regular concatenation input ) with region size 3 on the others .",Experiments ( supervised ),Now we review the non -LSTM baseline methods .,text-classification,1,18,0.2571428571428571,141,0.55078125,51,0.7183098591549296,1,0
143,"In , on three out of the four datasets , oh - 2 LSTMp outperforms SVM and the CNN .",Experiments ( supervised ),Now we review the non -LSTM baseline methods .,text-classification,1,19,0.2714285714285714,142,0.5546875,52,0.7323943661971831,1,1
144,"However , on RCV1 , it underperforms both .",Experiments ( supervised ),,text-classification,1,20,0.2857142857142857,143,0.55859375,53,0.7464788732394366,1,0
145,We conjecture that this is because strict word order is not very useful on RCV1 .,Experiments ( supervised ),"However , on RCV1 , it underperforms both .",text-classification,1,21,0.3,144,0.5625,54,0.7605633802816901,1,0
146,This point can also be observed in the SVM and CNN performances .,Experiments ( supervised ),"However , on RCV1 , it underperforms both .",text-classification,1,22,0.3142857142857143,145,0.56640625,55,0.7746478873239436,1,0
147,"Only on RCV1 , n-gram SVM is no better than bag - of - word SVM , and only on RCV1 , bow - CNN outperforms seq-CNN .",Experiments ( supervised ),"However , on RCV1 , it underperforms both .",text-classification,1,23,0.3285714285714285,146,0.5703125,56,0.7887323943661971,1,0
148,"That is , on RCV1 , bags of words in a window of 20 at every location are more useful than words in strict order .",Experiments ( supervised ),"However , on RCV1 , it underperforms both .",text-classification,1,24,0.3428571428571428,147,0.57421875,57,0.8028169014084507,1,0
149,This is presumably because the former can more easily cover variability of expressions indicative of topics .,Experiments ( supervised ),"However , on RCV1 , it underperforms both .",text-classification,1,25,0.3571428571428571,148,0.578125,58,0.8169014084507042,1,0
150,"Thus , LSTM , which does not have an ability to put words into bags , loses to bow - CNN .",Experiments ( supervised ),"However , on RCV1 , it underperforms both .",text-classification,1,26,0.3714285714285714,149,0.58203125,59,0.8309859154929577,1,0
151,More on one - hot CNN vs. one - hot LSTM LSTM can embed regions of variable ( and possibly large ) sizes whereas CNN requires the region size to be fixed .,Experiments ( supervised ),"However , on RCV1 , it underperforms both .",text-classification,1,27,0.3857142857142857,150,0.5859375,60,0.8450704225352113,1,0
152,We attribute to this fact the small improvements of oh - 2 LSTMp over oh - CNN in .,Experiments ( supervised ),"However , on RCV1 , it underperforms both .",text-classification,1,28,0.4,151,0.58984375,61,0.8591549295774648,1,0
153,"However , this shortcoming of CNN can be alleviated by having multiple convolution layers with distinct region sizes .",Experiments ( supervised ),"However , on RCV1 , it underperforms both .",text-classification,1,29,0.4142857142857143,152,0.59375,62,0.8732394366197183,1,0
154,We show in the table above that one - hot CNNs with two layers ( of 1000 feature maps each ) with two different region sizes 4 rival oh - 2 LST Mp .,Experiments ( supervised ),"However , on RCV1 , it underperforms both .",text-classification,1,30,0.4285714285714285,153,0.59765625,63,0.8873239436619719,1,0
155,"Although these models are larger than those in , training / testing is still faster than the LSTM models due to simplicity of the region embeddings .",Experiments ( supervised ),"However , on RCV1 , it underperforms both .",text-classification,1,31,0.4428571428571428,154,0.6015625,64,0.9014084507042254,1,0
156,"By comparison , the strength of LSTM to embed larger regions appears not to be a big contributor here .",Experiments ( supervised ),"However , on RCV1 , it underperforms both .",text-classification,1,32,0.4571428571428571,155,0.60546875,65,0.9154929577464788,1,0
157,This maybe because the amount of training data is not sufficient enough to learn the relevance of longer word sequences .,Experiments ( supervised ),"However , on RCV1 , it underperforms both .",text-classification,1,33,0.4714285714285714,156,0.609375,66,0.9295774647887324,1,0
158,"Overall , one - hot CNN works surprising well considering its simplicity , and this observation motivates the idea of combining the two types of region embeddings , discussed later .",Experiments ( supervised ),"However , on RCV1 , it underperforms both .",text-classification,1,34,0.4857142857142857,157,0.61328125,67,0.943661971830986,1,0
159,Comparison with the previous best results on 20 NG,Experiments ( supervised ),,text-classification,1,35,0.5,158,0.6171875,68,0.9577464788732394,1,0
160,"The previous best performance on 20NG is 15.3 ( not shown in the table ) of DL15 , obtained by pre-training wv - LSTM of 1024 units with labeled training data .",Experiments ( supervised ),Comparison with the previous best results on 20 NG,text-classification,1,36,0.5142857142857142,159,0.62109375,69,0.971830985915493,1,0
161,"Our oh - 2 LSTMp achieved 13.32 , which is 2 % better .",Experiments ( supervised ),Comparison with the previous best results on 20 NG,text-classification,1,37,0.5285714285714286,160,0.625,70,0.9859154929577464,1,0
162,"The previous best results on the other datasets use unlabeled data , and we will review them with our semi-supervised results .",Experiments ( supervised ),Comparison with the previous best results on 20 NG,text-classification,1,38,0.5428571428571428,161,0.62890625,71,1.0,1,0
163,Semi-supervised LSTM,Experiments ( supervised ),Comparison with the previous best results on 20 NG,text-classification,1,39,0.5571428571428572,162,0.6328125,0,0.0,1,0
164,"To exploit unlabeled data as an additional resource , we use a non-linear extension of two - view feature learning , whose linear version appeared in our earlier work .",Experiments ( supervised ),Comparison with the previous best results on 20 NG,text-classification,1,40,0.5714285714285714,163,0.63671875,1,0.0625,1,0
165,This was used in JZ15 b to learn from unlabeled data a region embedding embodied by a convolution layer .,Experiments ( supervised ),Comparison with the previous best results on 20 NG,text-classification,1,41,0.5857142857142857,164,0.640625,2,0.125,1,0
166,In this work we use it to learn a region embedding embodied by a one - hot LSTM .,Experiments ( supervised ),Comparison with the previous best results on 20 NG,text-classification,1,42,0.6,165,0.64453125,3,0.1875,1,0
167,Let us start with a brief review of non-linear two - view feature learning .,Experiments ( supervised ),Comparison with the previous best results on 20 NG,text-classification,1,43,0.6142857142857143,166,0.6484375,4,0.25,1,0
168,Two - view embedding ( tv-embedding ) [ JZ15 b ],Experiments ( supervised ),Comparison with the previous best results on 20 NG,text-classification,1,44,0.6285714285714286,167,0.65234375,5,0.3125,1,0
169,A rough sketch is as follows .,Experiments ( supervised ),,text-classification,1,45,0.6428571428571429,168,0.65625,6,0.375,1,0
170,Consider two views of the input .,Experiments ( supervised ),,text-classification,1,46,0.6571428571428571,169,0.66015625,7,0.4375,1,0
171,An embedding is called a tv-embedding if the embedded view is as good as the original view for the purpose of predicting the other view .,Experiments ( supervised ),Consider two views of the input .,text-classification,1,47,0.6714285714285714,170,0.6640625,8,0.5,1,0
172,"If the two views and the labels ( classification targets ) are related to one another only through some hidden states , then the tv-embedded view is as good as the original view for the purpose of classification .",Experiments ( supervised ),Consider two views of the input .,text-classification,1,48,0.6857142857142857,171,0.66796875,9,0.5625,1,0
173,Such an embedding is useful provided that its dimensionality is much lower than the original view .,Experiments ( supervised ),Consider two views of the input .,text-classification,1,49,0.7,172,0.671875,10,0.625,1,0
174,JZ15 b applied this idea by regarding text regions embedded by the convolution layer as one view and their surrounding context as the other view and training a tv-embedding ( embodied by a convolution layer ) on unlabeled data .,Experiments ( supervised ),Consider two views of the input .,text-classification,1,50,0.7142857142857143,173,0.67578125,11,0.6875,1,0
175,"The obtained tv-embeddings were used to produce additional input to a supervised region embedding of one - hot CNN , resulting in higher accuracy .",Experiments ( supervised ),Consider two views of the input .,text-classification,1,51,0.7285714285714285,174,0.6796875,12,0.75,1,0
176,"we consider the following two views : the words we have already seen in the document ( view - 1 ) , and the next few words ( view - 2 ) .",Experiments ( supervised ),Consider two views of the input .,text-classification,1,52,0.7428571428571429,175,0.68359375,13,0.8125,1,0
177,The task of tv-embedding learning is to predict view - 2 based on view - 1 .,Experiments ( supervised ),Consider two views of the input .,text-classification,1,53,0.7571428571428571,176,0.6875,14,0.875,1,0
178,"We train one - hot LSTMs in both directions , as in , on unlabeled data .",Experiments ( supervised ),Consider two views of the input .,text-classification,1,54,0.7714285714285715,177,0.69140625,15,0.9375,1,0
179,"For this purpose , we use the input and output gates as well as the forget gate as we found them to be useful .",Experiments ( supervised ),Consider two views of the input .,text-classification,1,55,0.7857142857142857,178,0.6953125,16,1.0,1,0
180,Learning LSTM tv-embeddings,Experiments ( supervised ),,text-classification,1,56,0.8,179,0.69921875,0,0.0,1,0
181,The theory of tv-embedding says that the region embeddings obtained in this way are useful for the task of interest if the two views are related to each other through the concepts relevant to the task .,Experiments ( supervised ),Learning LSTM tv-embeddings,text-classification,1,57,0.8142857142857143,180,0.703125,1,0.1666666666666666,1,0
182,"To reduce undesirable relations between the views such as syntactic relations , JZ15 b performed vocabulary control to remove function words from ( and only from ) the vocabulary of the target view , which we found useful also for LSTM .",Experiments ( supervised ),Learning LSTM tv-embeddings,text-classification,1,58,0.8285714285714286,181,0.70703125,2,0.3333333333333333,1,0
183,We use the tv-embeddings obtained from unlabeled data to produce additional input to LSTM by replacing and ( 3 ) by the following :,Experiments ( supervised ),Learning LSTM tv-embeddings,text-classification,1,59,0.8428571428571429,182,0.7109375,3,0.5,1,0
184,) .,Experiments ( supervised ),Learning LSTM tv-embeddings,text-classification,1,60,0.8571428571428571,183,0.71484375,4,0.6666666666666666,1,0
185,"x j t is the output of a tv-embedding ( an LSTM trained with unlabeled data ) indexed by j at time step t , and S is a set of tv-embeddings which contains the two LSTMs going forward and backward as in .",Experiments ( supervised ),Learning LSTM tv-embeddings,text-classification,1,61,0.8714285714285714,184,0.71875,5,0.8333333333333334,1,0
186,"Although it is possible to fine - tune the tv-embeddings with labeled data , for simplicity and faster training , we fixed them in our experiments .",Experiments ( supervised ),Learning LSTM tv-embeddings,text-classification,1,62,0.8857142857142857,185,0.72265625,6,1.0,1,0
187,Combining LSTM tv-embeddings and CNN tv-embeddings,Experiments ( supervised ),,text-classification,1,63,0.9,186,0.7265625,0,0.0,1,0
188,"It is easy to see that the set S above can be expanded with any tv-embeddings , not only those in the form of LSTM ( LSTM tv-embeddings ) but also with the tv-embeddings in the form of convolution layers ( CNN tv-embeddings ) such as those obtained in JZ15 b .",Experiments ( supervised ),Combining LSTM tv-embeddings and CNN tv-embeddings,text-classification,1,64,0.9142857142857144,187,0.73046875,1,0.1428571428571428,1,0
189,"Similarly , it is possible to use LSTM tv-embeddings to produce additional input to CNN .",Experiments ( supervised ),Combining LSTM tv-embeddings and CNN tv-embeddings,text-classification,1,65,0.9285714285714286,188,0.734375,2,0.2857142857142857,1,0
190,"While both LSTM tv-embeddings and CNN tv-embeddings are region embeddings , their formulations are very different from each other ; therefore , we expect that they complement each other and bring further performance improvements when combined .",Experiments ( supervised ),Combining LSTM tv-embeddings and CNN tv-embeddings,text-classification,1,66,0.9428571428571428,189,0.73828125,3,0.4285714285714285,1,0
191,We will empirically confirm these conjectures in the experiments below .,Experiments ( supervised ),Combining LSTM tv-embeddings and CNN tv-embeddings,text-classification,1,67,0.9571428571428572,190,0.7421875,4,0.5714285714285714,1,0
192,Note that being able to naturally combine several tv-embeddings is a strength of 2100 - dim LSTM tv-embed .,Experiments ( supervised ),Combining LSTM tv-embeddings and CNN tv-embeddings,text-classification,1,68,0.9714285714285714,191,0.74609375,5,0.7142857142857143,1,0
193,6.66 6.08 9.24 5 oh -CNN 1200 - dim CNN tv-embed .,Experiments ( supervised ),Combining LSTM tv-embeddings and CNN tv-embeddings,text-classification,1,69,0.9857142857142858,192,0.75,6,0.8571428571428571,1,0
194,"6.81 6.57 7.97 our framework , which uses unlabeled data to produce additional input to LSTM instead of pre-training .",Experiments ( supervised ),Combining LSTM tv-embeddings and CNN tv-embeddings,text-classification,1,70,1.0,193,0.75390625,7,1.0,1,0
195,Semi-supervised experiments,,,text-classification,1,0,0.0,194,0.7578125,0,0.0,1,0
196,"We used IMDB , Elec , and RCV1 for our semi-supervised experiments ; 20 NG was excluded due to the absence of standard unlabeled data .",Semi-supervised experiments,Semi-supervised experiments,text-classification,1,1,0.0196078431372549,195,0.76171875,1,0.0357142857142857,1,0
197,summarizes the unlabeled data .,Semi-supervised experiments,Semi-supervised experiments,text-classification,1,2,0.0392156862745098,196,0.765625,2,0.0714285714285714,1,0
198,"To experiment with LSTM tv-embeddings , we trained two LSTMs ( forward and backward ) with 100 units each on unlabeled data .",Semi-supervised experiments,Semi-supervised experiments,text-classification,1,3,0.0588235294117647,197,0.76953125,3,0.1071428571428571,1,0
199,The training objective was to predict the next k words where k was set to 20 for RCV1 and 5 for others .,Semi-supervised experiments,Semi-supervised experiments,text-classification,1,4,0.0784313725490196,198,0.7734375,4,0.1428571428571428,1,0
200,"Similar to JZ15 b , we minimized weighted square",Semi-supervised experiments,,text-classification,1,5,0.0980392156862745,199,0.77734375,5,0.1785714285714285,1,0
201,") 2 where i goes through the time steps , z represents the next k words by a bow vector , and p is the model output ; ?",Semi-supervised experiments,"Similar to JZ15 b , we minimized weighted square",text-classification,1,6,0.1176470588235294,200,0.78125,6,0.2142857142857142,1,0
202,"i , j were set to achieve negative sampling effect for speed - up ; vocabulary control was performed for reducing undesirable relations between views , which sets the vocabulary of the target ( i.e. , the k words ) to the 30 K most frequent words excluding function words ( or stop words on RCV1 ) .",Semi-supervised experiments,"Similar to JZ15 b , we minimized weighted square",text-classification,1,7,0.1372549019607843,201,0.78515625,7,0.25,1,0
203,Other details followed the supervised experiments .,Semi-supervised experiments,,text-classification,1,8,0.1568627450980392,202,0.7890625,8,0.2857142857142857,1,0
204,"Our semi-supervised one - hot bidirectional LSTM with pooling ( oh - 2 LSTM p ) in row # 4 of used the two LSTM tv-embeddings trained on unlabeled data as described above , to produce additional input to one - hot LSTMs in two directions ( 500 units each ) .",Semi-supervised experiments,Other details followed the supervised experiments .,text-classification,1,9,0.1764705882352941,203,0.79296875,9,0.3214285714285714,1,0
205,"Compared with the supervised oh - 2 LSTMp , clear performance improvements were obtained on all the datasets , thus , confirming the effectiveness of our approach .",Semi-supervised experiments,Other details followed the supervised experiments .,text-classification,1,10,0.196078431372549,204,0.796875,10,0.3571428571428571,1,0
206,We review the semi-supervised performance of wv - LSTMs ) .,Semi-supervised experiments,Other details followed the supervised experiments .,text-classification,1,11,0.2156862745098039,205,0.80078125,11,0.3928571428571428,1,0
207,"In DL15 the model consisted of a word embedding layer of 512 dimensions , an LSTM layer with 1024 units , and 30 hidden units on top of the LSTM layer ; the word embedding layer and the LSTM were pre-trained with unlabeled data and were fine - tuned with labeled data ; pre-training used either the language model objective or autoencoder objective .",Semi-supervised experiments,Other details followed the supervised experiments .,text-classification,1,12,0.2352941176470588,206,0.8046875,12,0.4285714285714285,1,0
208,"The error rate on IMDB is from DL15 , and those on Elec and RCV1 are our best effort to perform pre-training with the language model objective .",Semi-supervised experiments,Other details followed the supervised experiments .,text-classification,1,13,0.2549019607843137,207,0.80859375,13,0.4642857142857143,1,0
209,"We used the same configuration on Elec as DL15 ; however , on RCV1 , which has 55 classes , 30 hidden units turned out to be too few and we changed it to 1000 .",Semi-supervised experiments,Other details followed the supervised experiments .,text-classification,1,14,0.2745098039215686,208,0.8125,14,0.5,1,0
210,"Although the pre-trained wv - LSTM clearly outperformed the supervised wv - LSTM , it underperformed the models with region tv-embeddings .",Semi-supervised experiments,Other details followed the supervised experiments .,text-classification,1,15,0.2941176470588235,209,0.81640625,15,0.5357142857142857,1,0
211,"Previous studies on LSTM for text often convert words into pre-trained word vectors , and word2vec is a popular choice for this purpose .",Semi-supervised experiments,Other details followed the supervised experiments .,text-classification,1,16,0.3137254901960784,210,0.8203125,16,0.5714285714285714,1,0
212,"Therefore , we tested wv - 2 LSTMp ( word - vector bidirectional LSTM with pooling ) , whose only difference from oh - 2 LSTMp is that the input to the LSTM layers is the pre-trained word vectors .",Semi-supervised experiments,Other details followed the supervised experiments .,text-classification,1,17,0.3333333333333333,211,0.82421875,17,0.6071428571428571,1,1
213,The word vectors were optionally updated ( finetuned ) during training .,Semi-supervised experiments,Other details followed the supervised experiments .,text-classification,1,18,0.3529411764705882,212,0.828125,18,0.6428571428571429,1,0
214,Two types of word vectors were tested .,Semi-supervised experiments,,text-classification,1,19,0.3725490196078431,213,0.83203125,19,0.6785714285714286,1,0
215,The Google News word vectors were trained by word2vec on a huge ( 100 billion - word ) news corpus and are provided publicly .,Semi-supervised experiments,Two types of word vectors were tested .,text-classification,1,20,0.392156862745098,214,0.8359375,20,0.7142857142857143,1,0
216,"On our tasks , wv - 2 LSTMp using the Google News vectors ( row # 2 ) performed relatively poorly .",Semi-supervised experiments,Two types of word vectors were tested .,text-classification,1,21,0.4117647058823529,215,0.83984375,21,0.75,1,0
217,"When word2vec was trained with the domain unlabeled data , better results were observed after we scaled word vectors appropriately ) .",Semi-supervised experiments,Two types of word vectors were tested .,text-classification,1,22,0.4313725490196078,216,0.84375,22,0.7857142857142857,1,0
218,"Still , it underperformed the models with region tv - embeddings ( row # 4 , 5 ) , which used the same domain unlabeled data .",Semi-supervised experiments,Two types of word vectors were tested .,text-classification,1,23,0.4509803921568627,217,0.84765625,23,0.8214285714285714,1,0
219,"We attribute the superiority of the models with tv-embeddings to the fact that they learn , from unlabeled data , embeddings of text regions , which can convey higher - level concepts than single words in isolation .",Semi-supervised experiments,Two types of word vectors were tested .,text-classification,1,24,0.4705882352941176,218,0.8515625,24,0.8571428571428571,1,0
220,"Now we review the performance of one - hot CNN with one 200 - dim CNN tv-embedding row # 5 ) , which is comparable with our LSTM with two 100 - dim LSTM tv-embeddings ( row # 4 ) in terms of the dimensionality of tv-embeddings .",Semi-supervised experiments,Two types of word vectors were tested .,text-classification,1,25,0.4901960784313725,219,0.85546875,25,0.8928571428571429,1,1
221,The LSTM ( row # 4 ) rivals or outperforms the CNN ( row # 5 ) on IMDB / Elec but underperforms it on RCV1 .,Semi-supervised experiments,Two types of word vectors were tested .,text-classification,1,26,0.5098039215686274,220,0.859375,26,0.9285714285714286,1,0
222,"Increasing the dimensionality of LSTM tvembeddings from 100 to 300 on RCV1 , we obtain 8.62 , but it still does not reach 7.97 of the CNN .",Semi-supervised experiments,Two types of word vectors were tested .,text-classification,1,27,0.5294117647058824,221,0.86328125,27,0.9642857142857144,1,0
223,"As discussed earlier , we attribute the superiority of one - hot CNN on RCV1 to its unique way of representing parts of documents via bow input .",Semi-supervised experiments,Two types of word vectors were tested .,text-classification,1,28,0.5490196078431373,222,0.8671875,28,1.0,1,0
224,Experiments combining CNN tv-embeddings and LSTM tv-embeddings,Semi-supervised experiments,,text-classification,1,29,0.5686274509803921,223,0.87109375,0,0.0,1,0
225,In Section 3.3 we noted that LSTM tv-embeddings and CNN tv-embeddings can be naturally combined .,Semi-supervised experiments,Experiments combining CNN tv-embeddings and LSTM tv-embeddings,text-classification,1,30,0.5882352941176471,224,0.875,1,0.0666666666666666,1,0
226,We experimented with this idea in the following two settings ..,Semi-supervised experiments,Experiments combining CNN tv-embeddings and LSTM tv-embeddings,text-classification,1,31,0.6078431372549019,225,0.87890625,2,0.1333333333333333,1,0
227,Comparison with previous best results .,Semi-supervised experiments,,text-classification,1,32,0.6274509803921569,226,0.8828125,3,0.2,1,0
228,Error rates ( % ) .,Semi-supervised experiments,,text-classification,1,33,0.6470588235294118,227,0.88671875,4,0.2666666666666666,1,0
229,""" U "" : Was unlabeled data used ?",Semi-supervised experiments,Error rates ( % ) .,text-classification,1,34,0.6666666666666666,228,0.890625,5,0.3333333333333333,1,0
230,""" Co - tr. optimized "" : co-training using oh - CNN as abase learner with parameters ( e.g. , when to stop ) optimized on the test data ; it demonstrates the difficulty of exploiting unlabeled data on these tasks .",Semi-supervised experiments,Error rates ( % ) .,text-classification,1,35,0.6862745098039216,229,0.89453125,6,0.4,1,0
231,"In one setting , oh - 2 LSTMp takes additional input from five embeddings : two LSTM tv-embeddings used in and three CNN tv-embeddings from JZ15 b obtained by three distinct combinations of training objectives and input representations , which are publicly provided .",Semi-supervised experiments,Error rates ( % ) .,text-classification,1,36,0.7058823529411765,230,0.8984375,7,0.4666666666666667,1,0
232,"These CNN tv-embeddings were trained to be applied to text regions of size k at every location taking bow input , where k is 5 on IMDB / Elec and 20 on RCV1 .",Semi-supervised experiments,Error rates ( % ) .,text-classification,1,37,0.7254901960784313,231,0.90234375,8,0.5333333333333333,1,0
233,"We connect each of the CNN tv-embeddings to an LSTM by aligning the centers of the regions of the former with the LSTM time steps ; e.g. , the CNN tv-embedding result on the first five words is passed to the LSTM at the time step on the third word .",Semi-supervised experiments,Error rates ( % ) .,text-classification,1,38,0.7450980392156863,232,0.90625,9,0.6,1,0
234,"In the second setting , we trained one - hot CNN with these five types of tv-embeddings by replacing ( 1 ) max ( 0 , Wx + b ) by max ( 0 , Wx + j W ( j ) x j + b ) where x j is the output of the j - th tv-embedding with the same alignment as above .",Semi-supervised experiments,Error rates ( % ) .,text-classification,1,39,0.7647058823529411,233,0.91015625,10,0.6666666666666666,1,0
235,Rows 3 - 4 of show the results of these two types of models .,Semi-supervised experiments,Error rates ( % ) .,text-classification,1,40,0.7843137254901961,234,0.9140625,11,0.7333333333333333,1,0
236,"For comparison , we also show the results of the LSTM with LSTM tv-embeddings only ( row# 1 ) and the CNN with CNN tv-embeddings only ( row # 2 ) .",Semi-supervised experiments,Error rates ( % ) .,text-classification,1,41,0.803921568627451,235,0.91796875,12,0.8,1,0
237,"To seethe effects of combination , compare row# 3 with row# 1 , and compare row # 4 with row # 2 .",Semi-supervised experiments,Error rates ( % ) .,text-classification,1,42,0.8235294117647058,236,0.921875,13,0.8666666666666667,1,0
238,"For example , adding the CNN tv-embeddings to the LSTM of row# 1 , the error rate on IMDB improved from 6.66 to 5.94 , and adding the LSTM tv-embeddings to the CNN of row # 2 , the error rate on RCV1 improved from 7.71 to 7.15 .",Semi-supervised experiments,Error rates ( % ) .,text-classification,1,43,0.8431372549019608,237,0.92578125,14,0.9333333333333332,1,0
239,"The results indicate that , as expected , LSTM tv-embeddings and CNN tv-embeddings complement each other and improve performance when combined .",Semi-supervised experiments,Error rates ( % ) .,text-classification,1,44,0.8627450980392157,238,0.9296875,15,1.0,1,0
240,Comparison with the previous best results,Semi-supervised experiments,,text-classification,1,45,0.8823529411764706,239,0.93359375,0,0.0,1,0
241,The previous best results in the literature are shown in Table 7 .,Semi-supervised experiments,Comparison with the previous best results,text-classification,1,46,0.9019607843137256,240,0.9375,1,0.1666666666666666,1,0
242,"More results of previous semi-supervised models can be found in JZ15b , all of which clearly underperform the semi-supervised one - hot CNN of .",Semi-supervised experiments,Comparison with the previous best results,text-classification,1,47,0.9215686274509804,241,0.94140625,2,0.3333333333333333,1,0
243,"The best supervised results on IMDB / Elec of JZ15a are in the first row , obtained by integrating a document embedding layer into one - hot CNN .",Semi-supervised experiments,Comparison with the previous best results,text-classification,1,48,0.9411764705882352,242,0.9453125,3,0.5,1,0
244,"Many more of the previous results on IMDB can be found in , all of which are over 10 % except for 8.78 by bi-gram NBSVM .",Semi-supervised experiments,Comparison with the previous best results,text-classification,1,49,0.9607843137254902,243,0.94921875,4,0.6666666666666666,1,0
245,7.42 by paragraph vectors ) and 6.51 by JZ15 b were considered to be large improvements .,Semi-supervised experiments,Comparison with the previous best results,text-classification,1,50,0.9803921568627452,244,0.953125,5,0.8333333333333334,1,0
246,"As shown in the last row of , our new model further improved it to 5.94 ; also on Elec and RCV1 , our best models exceeded the previous best results .",Semi-supervised experiments,Comparison with the previous best results,text-classification,1,51,1.0,245,0.95703125,6,1.0,1,0
247,Conclusion,,,text-classification,1,0,0.0,246,0.9609375,0,0.0,1,0
248,"Within the general framework of ' region embedding + pooling ' for text categorization , we explored region embeddings via one - hot LSTM .",Conclusion,Conclusion,text-classification,1,1,0.1111111111111111,247,0.96484375,1,0.1111111111111111,0,0
249,"The region embedding of onehot LSTM rivaled or outperformed that of the state - of - the art one - hot CNN , proving its effectiveness .",Conclusion,Conclusion,text-classification,1,2,0.2222222222222222,248,0.96875,2,0.2222222222222222,0,0
250,We also found that the models with either one of these two types of region embedding strongly outperformed other methods including previous LSTM .,Conclusion,Conclusion,text-classification,1,3,0.3333333333333333,249,0.97265625,3,0.3333333333333333,0,0
251,"The best results were obtained by combining the two types of region embedding trained on unlabeled data , suggesting that their strengths are complementary .",Conclusion,Conclusion,text-classification,1,4,0.4444444444444444,250,0.9765625,4,0.4444444444444444,0,0
252,"As a result , we reported substantial improvements over the previous best results on benchmark datasets .",Conclusion,Conclusion,text-classification,1,5,0.5555555555555556,251,0.98046875,5,0.5555555555555556,0,0
253,"At a high level , our results indicate the following .",Conclusion,Conclusion,text-classification,1,6,0.6666666666666666,252,0.984375,6,0.6666666666666666,0,0
254,"First , on this task , embeddings of text regions , which can convey higher - level concepts , are more useful than embeddings of single words in isolation .",Conclusion,Conclusion,text-classification,1,7,0.7777777777777778,253,0.98828125,7,0.7777777777777778,0,0
255,"Second , useful region embeddings can be learned by working with one - hot vectors directly , either on labeled data or unlabeled data .",Conclusion,Conclusion,text-classification,1,8,0.8888888888888888,254,0.9921875,8,0.8888888888888888,0,0
256,"Finally , a promising future direction might be to seek , under this framework , new region embedding methods with complementary benefits .",Conclusion,Conclusion,text-classification,1,9,1.0,255,0.99609375,9,1.0,0,0
1,title,,,text-classification,2,0,0.0,0,0.0,0,0.0,1,0
2,Bag of Tricks for Efficient Text Classification,title,title,text-classification,2,1,0.0,1,0.010752688172043,1,0.0,1,1
3,abstract,,,text-classification,2,0,0.0,2,0.021505376344086,0,0.0,1,0
4,This paper explores a simple and efficient baseline for text classification .,abstract,abstract,text-classification,2,1,0.3333333333333333,3,0.032258064516129,1,0.3333333333333333,1,1
5,"Our experiments show that our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy , and many orders of magnitude faster for training and evaluation .",abstract,abstract,text-classification,2,2,0.6666666666666666,4,0.043010752688172,2,0.6666666666666666,1,0
6,"We can train fastText on more than one billion words in less than ten minutes using a standard multicore CPU , and classify half a million sentences among 312K classes in less than a minute .",abstract,abstract,text-classification,2,3,1.0,5,0.053763440860215,3,1.0,1,0
7,Introduction,,,text-classification,2,0,0.0,6,0.064516129032258,0,0.0,1,0
8,"Text classification is an important task in Natural Language Processing with many applications , such as web search , information retrieval , ranking and document classification .",Introduction,Introduction,text-classification,2,1,0.1111111111111111,7,0.075268817204301,1,0.1111111111111111,1,0
9,"Recently , models based on neural networks have become increasingly popular .",Introduction,Introduction,text-classification,2,2,0.2222222222222222,8,0.086021505376344,2,0.2222222222222222,1,0
10,"While these models achieve very good performance in practice , they tend to be relatively slow both at train and test time , limiting their use on very large datasets .",Introduction,Introduction,text-classification,2,3,0.3333333333333333,9,0.0967741935483871,3,0.3333333333333333,1,0
11,"Meanwhile , linear classifiers are often considered as strong baselines for text classification problems .",Introduction,Introduction,text-classification,2,4,0.4444444444444444,10,0.1075268817204301,4,0.4444444444444444,1,0
12,"Despite their simplicity , they often obtain stateof - the - art performances if the right features are used .",Introduction,Introduction,text-classification,2,5,0.5555555555555556,11,0.1182795698924731,5,0.5555555555555556,1,0
13,They also have the potential to scale to very large corpus .,Introduction,Introduction,text-classification,2,6,0.6666666666666666,12,0.1290322580645161,6,0.6666666666666666,1,0
14,"In this work , we explore ways to scale these baselines to very large corpus with a large output space , in the context of text classification .",Introduction,Introduction,text-classification,2,7,0.7777777777777778,13,0.1397849462365591,7,0.7777777777777778,1,0
15,"Inspired by the recent work in efficient word representation learning , we show that linear models with a rank constraint and a fast loss approximation can train on a billion words within ten minutes , while achieving performance on par with the state - of - the - art .",Introduction,Introduction,text-classification,2,8,0.8888888888888888,14,0.1505376344086021,8,0.8888888888888888,1,0
16,"We evaluate the quality of our approach fastText 1 on two different tasks , namely tag prediction and sentiment analysis .",Introduction,Introduction,text-classification,2,9,1.0,15,0.1612903225806451,9,1.0,1,0
17,Model architecture,,,text-classification,2,0,0.0,16,0.1720430107526881,0,0.0,1,0
18,"A simple and efficient baseline for sentence classification is to represent sentences as bag of words ( BoW ) and train a linear classifier , e.g. , a logistic regression or an SVM .",Model architecture,Model architecture,text-classification,2,1,0.0333333333333333,17,0.1827956989247312,1,0.0769230769230769,1,0
19,"However , linear classifiers do not share parameters among features and classes .",Model architecture,Model architecture,text-classification,2,2,0.0666666666666666,18,0.1935483870967742,2,0.1538461538461538,1,0
20,This possibly limits their generalization in the context of large output space where some classes have very few examples .,Model architecture,Model architecture,text-classification,2,3,0.1,19,0.2043010752688172,3,0.2307692307692307,1,0
21,Common solutions to this problem are to factorize the linear classifier into low rank matrices or to use multilayer neural networks .,Model architecture,Model architecture,text-classification,2,4,0.1333333333333333,20,0.2150537634408602,4,0.3076923076923077,1,0
22,shows a simple linear model with rank constraint .,Model architecture,Model architecture,text-classification,2,5,0.1666666666666666,21,0.2258064516129032,5,0.3846153846153846,1,1
23,The first weight matrix A is a look - up table over the words .,Model architecture,Model architecture,text-classification,2,6,0.2,22,0.2365591397849462,6,0.4615384615384615,1,1
24,"The word representations are then averaged into a text representation , which is in turn fed to a linear classifier .",Model architecture,Model architecture,text-classification,2,7,0.2333333333333333,23,0.2473118279569892,7,0.5384615384615384,1,1
25,The text representa - tion is an hidden variable which can be potentially be reused .,Model architecture,Model architecture,text-classification,2,8,0.2666666666666666,24,0.2580645161290322,8,0.6153846153846154,1,0
26,"This architecture is similar to the cbow model of , where the middle word is replaced by a label .",Model architecture,Model architecture,text-classification,2,9,0.3,25,0.2688172043010752,9,0.6923076923076923,1,0
27,We use the softmax function f to compute the probability distribution over the predefined classes .,Model architecture,Model architecture,text-classification,2,10,0.3333333333333333,26,0.2795698924731182,10,0.7692307692307693,1,1
28,"For a set of N documents , this leads to minimizing the negative loglikelihood over the classes :",Model architecture,Model architecture,text-classification,2,11,0.3666666666666666,27,0.2903225806451613,11,0.8461538461538461,1,0
29,"where x n is the normalized bag of features of the nth document , y n the label , A and B the weight matrices .",Model architecture,Model architecture,text-classification,2,12,0.4,28,0.3010752688172043,12,0.9230769230769232,1,0
30,This model is trained asynchronously on multiple CPUs using stochastic gradient descent and a linearly decaying learning rate .,Model architecture,Model architecture,text-classification,2,13,0.4333333333333333,29,0.3118279569892473,13,1.0,1,0
31,Hierarchical softmax,Model architecture,,text-classification,2,14,0.4666666666666667,30,0.3225806451612903,0,0.0,1,0
32,"When the number of classes is large , computing the linear classifier is computationally expensive .",Model architecture,Hierarchical softmax,text-classification,2,15,0.5,31,0.3333333333333333,1,0.0625,1,0
33,"More precisely , the computational complexity is O ( kh ) where k is the number of classes and h the dimension of the text representation .",Model architecture,Hierarchical softmax,text-classification,2,16,0.5333333333333333,32,0.3440860215053763,2,0.125,1,0
34,"In order to improve our running time , we use a hierarchical softmax ) based on the Huffman coding tree .",Model architecture,Hierarchical softmax,text-classification,2,17,0.5666666666666667,33,0.3548387096774194,3,0.1875,1,1
35,"During training , the computational complexity drops to O ( h log 2 ( k ) ) .",Model architecture,Hierarchical softmax,text-classification,2,18,0.6,34,0.3655913978494624,4,0.25,1,0
36,The hierarchical softmax is also advantageous attest time when searching for the most likely class .,Model architecture,Hierarchical softmax,text-classification,2,19,0.6333333333333333,35,0.3763440860215054,5,0.3125,1,0
37,Each node is associated with a probability that is the probability of the path from the root to that node .,Model architecture,Hierarchical softmax,text-classification,2,20,0.6666666666666666,36,0.3870967741935484,6,0.375,1,0
38,"If the node is at depth l + 1 with parents n 1 , . . . , n l , it s probability is",Model architecture,Hierarchical softmax,text-classification,2,21,0.7,37,0.3978494623655914,7,0.4375,1,0
39,This means that the probability of anode is always lower than the one of its parent .,Model architecture,Hierarchical softmax,text-classification,2,22,0.7333333333333333,38,0.4086021505376344,8,0.5,1,0
40,Exploring the tree with a depth first search and tracking the maximum probability among the leaves allows us to discard any branch associated with a small probability .,Model architecture,Hierarchical softmax,text-classification,2,23,0.7666666666666667,39,0.4193548387096774,9,0.5625,1,0
41,"In practice , we observe a reduction of the complexity to O ( h log 2 ( k ) ) attest time .",Model architecture,Hierarchical softmax,text-classification,2,24,0.8,40,0.4301075268817204,10,0.625,1,0
42,"This approach is further extended to compute the T - top targets at the cost of O ( log ( T ) ) , using a binary heap .",Model architecture,Hierarchical softmax,text-classification,2,25,0.8333333333333334,41,0.4408602150537634,11,0.6875,1,0
43,N - gram features,Model architecture,,text-classification,2,26,0.8666666666666667,42,0.4516129032258064,12,0.75,1,0
44,Bag of words is invariant to word order but taking explicitly this order into account is often computationally very expensive .,Model architecture,N - gram features,text-classification,2,27,0.9,43,0.4623655913978494,13,0.8125,1,0
45,"Instead , we use a bag of n-grams as additional features to capture some partial information about the local word order .",Model architecture,N - gram features,text-classification,2,28,0.9333333333333332,44,0.4731182795698925,14,0.875,1,1
46,This is very efficient in practice while achieving comparable results to methods that explicitly use the order .,Model architecture,N - gram features,text-classification,2,29,0.9666666666666668,45,0.4838709677419355,15,0.9375,1,1
47,"We maintain a fast and memory efficient mapping of the n-grams by using the hashing trick with the same hashing function as in and 10M bins if we only used bigrams , and 100M otherwise .",Model architecture,N - gram features,text-classification,2,30,1.0,46,0.4946236559139785,16,1.0,1,1
48,Experiments,,,text-classification,2,0,0.0,47,0.5053763440860215,0,0.0,1,0
49,We evaluate fastText on two different tasks .,Experiments,,text-classification,2,1,0.2,48,0.5161290322580645,1,0.25,1,0
50,"First , we compare it to existing text classifers on the problem of sentiment analysis .",Experiments,We evaluate fastText on two different tasks .,text-classification,2,2,0.4,49,0.5268817204301075,2,0.5,1,0
51,"Then , we evaluate its capacity to scale to large output space on a tag prediction dataset .",Experiments,We evaluate fastText on two different tasks .,text-classification,2,3,0.6,50,0.5376344086021505,3,0.75,1,0
52,"Note that our model could be implemented with the Vowpal Wabbit library , 2 but we observe in practice , that our tailored implementation is at least 2 - 5 faster .",Experiments,We evaluate fastText on two different tasks .,text-classification,2,4,0.8,51,0.5483870967741935,4,1.0,1,0
53,Sentiment analysis,Experiments,,text-classification,2,5,1.0,52,0.5591397849462365,0,0.0,1,0
54,Datasets and baselines .,,,text-classification,2,0,0.0,53,0.5698924731182796,1,0.0666666666666666,1,0
55,We employ the same 8 datasets and evaluation protocol of .,Datasets and baselines .,Datasets and baselines .,text-classification,2,1,0.3333333333333333,54,0.5806451612903226,2,0.1333333333333333,1,0
56,We report the n-grams and TFIDF baselines from We also compare to following their evaluation protocol .,Datasets and baselines .,Datasets and baselines .,text-classification,2,2,0.6666666666666666,55,0.5913978494623656,3,0.2,1,0
57,We report their main baselines as well as their two approaches based on recurrent networks ( Conv - GRNN and LSTM - GRNN ) .,Datasets and baselines .,Datasets and baselines .,text-classification,2,3,1.0,56,0.6021505376344086,4,0.2666666666666666,1,0
58,Results .,,,text-classification,2,0,0.0,57,0.6129032258064516,5,0.3333333333333333,1,0
59,We present the results in .,Results .,,text-classification,2,1,0.0909090909090909,58,0.6236559139784946,6,0.4,1,0
60,"We use 10 hidden units and run fastText for 5 epochs with a learning rate selected on a validation set from { 0.05 , 0.1 , 0.25 , 0.5 } .",Results .,We present the results in .,text-classification,2,2,0.1818181818181818,59,0.6344086021505376,7,0.4666666666666667,1,1
61,"On this task , adding bigram information improves the performance by 1 - 4 % .",Results .,We present the results in .,text-classification,2,3,0.2727272727272727,60,0.6451612903225806,8,0.5333333333333333,1,1
62,"Overall our accuracy is slightly better than char - CNN and char - CRNN and , a bit worse than VDCNN .",Results .,We present the results in .,text-classification,2,4,0.3636363636363636,61,0.6559139784946236,9,0.6,1,1
63,"Note that we can increase the accuracy slightly by using more n-grams , for example with trigrams , the performance on Sogou goes up to 97.1 % .",Results .,We present the results in .,text-classification,2,5,0.4545454545454545,62,0.6666666666666666,10,0.6666666666666666,1,1
64,"Finally , shows that our method is competitive with the methods presented in .",Results .,We present the results in .,text-classification,2,6,0.5454545454545454,63,0.6774193548387096,11,0.7333333333333333,1,0
65,We tune the hyperparameters on the validation set and observe that using n-grams up to 5 leads to the best performance .,Results .,We present the results in .,text-classification,2,7,0.6363636363636364,64,0.6881720430107527,12,0.8,1,0
66,"Unlike , fastText does not use pre-trained word embeddings , which can be explained the 1 % difference inaccuracy .",Results .,We present the results in .,text-classification,2,8,0.7272727272727273,65,0.6989247311827957,13,0.8666666666666667,1,0
67,We show a few correct and incorrect tag predictions .,Results .,We present the results in .,text-classification,2,9,0.8181818181818182,66,0.7096774193548387,14,0.9333333333333332,1,0
68,"up compared to neural network based methods increases with the size of the dataset , going up to at least a 15,000 speed - up .",Results .,We present the results in .,text-classification,2,10,0.9090909090909092,67,0.7204301075268817,15,1.0,1,0
69,Tag prediction,Results .,,text-classification,2,11,1.0,68,0.7311827956989247,0,0.0,1,0
70,Dataset and baselines .,,,text-classification,2,0,0.0,69,0.7419354838709677,1,0.0555555555555555,1,0
71,"To test scalability of our approach , further evaluation is carried on the YFCC100M dataset which consists of almost 100M images with captions , titles and tags .",Dataset and baselines .,Dataset and baselines .,text-classification,2,1,0.0588235294117647,70,0.7526881720430108,2,0.1111111111111111,1,0
72,We focus on predicting the tags according to the title and caption ( we do not use the images ) .,Dataset and baselines .,Dataset and baselines .,text-classification,2,2,0.1176470588235294,71,0.7634408602150538,3,0.1666666666666666,1,0
73,"We remove the words and tags occurring less than 100 times and split the data into a train , validation and test set .",Dataset and baselines .,Dataset and baselines .,text-classification,2,3,0.1764705882352941,72,0.7741935483870968,4,0.2222222222222222,1,0
74,"The train set contains 91,188,648 examples ( 1.5B tokens ) .",Dataset and baselines .,Dataset and baselines .,text-classification,2,4,0.2352941176470588,73,0.7849462365591398,5,0.2777777777777778,1,0
75,"The validation has 930,497 examples and the test set 543,424 .",Dataset and baselines .,Dataset and baselines .,text-classification,2,5,0.2941176470588235,74,0.7956989247311828,6,0.3333333333333333,1,0
76,"The vocabulary size is 297,141 and there are 312,116 unique tags .",Dataset and baselines .,Dataset and baselines .,text-classification,2,6,0.3529411764705882,75,0.8064516129032258,7,0.3888888888888889,1,0
77,We will release a script that recreates this dataset so that our numbers could be reproduced .,Dataset and baselines .,Dataset and baselines .,text-classification,2,7,0.4117647058823529,76,0.8172043010752689,8,0.4444444444444444,1,0
78,We report precision at 1 .,Dataset and baselines .,,text-classification,2,8,0.4705882352941176,77,0.8279569892473119,9,0.5,1,0
79,We consider a frequency - based baseline which predicts the most frequent tag .,Dataset and baselines .,We report precision at 1 .,text-classification,2,9,0.5294117647058824,78,0.8387096774193549,10,0.5555555555555556,1,0
80,"We also compare with Tagspace ( Weston et al. , 2014 ) , which is a tag prediction model similar to ours , but based on the Wsabie model of .",Dataset and baselines .,We report precision at 1 .,text-classification,2,10,0.5882352941176471,79,0.8494623655913979,11,0.6111111111111112,1,0
81,"While the Tagspace model is described using convolutions , we consider the linear version , which achieves comparable performance but is much faster .",Dataset and baselines .,We report precision at 1 .,text-classification,2,11,0.6470588235294118,80,0.8602150537634409,12,0.6666666666666666,1,0
82,Results and training time . and 200 .,Dataset and baselines .,,text-classification,2,12,0.7058823529411765,81,0.8709677419354839,13,0.7222222222222222,1,0
83,"Both models achieve a similar performance with a small hidden layer , but adding bigrams gives us a significant boost inaccuracy .",Dataset and baselines .,Results and training time . and 200 .,text-classification,2,13,0.7647058823529411,82,0.8817204301075269,14,0.7777777777777778,1,0
84,"At test time , Tagspace needs to compute the scores for all the classes which makes it relatively slow , while our fast inference gives a significant speed - up when the number of classes is large ( more than 300 K here ) .",Dataset and baselines .,Results and training time . and 200 .,text-classification,2,14,0.8235294117647058,83,0.8924731182795699,15,0.8333333333333334,1,1
85,"Overall , we are more than an order of magnitude faster to obtain model with a better quality .",Dataset and baselines .,Results and training time . and 200 .,text-classification,2,15,0.8823529411764706,84,0.9032258064516128,16,0.8888888888888888,1,1
86,The speedup of the test phase is even more significant ( a 600 speedup ) .,Dataset and baselines .,Results and training time . and 200 .,text-classification,2,16,0.9411764705882352,85,0.913978494623656,17,0.9444444444444444,1,0
87,shows some qualitative examples .,Dataset and baselines .,Results and training time . and 200 .,text-classification,2,17,1.0,86,0.9247311827956988,18,1.0,1,0
88,Discussion and conclusion,,,text-classification,2,0,0.0,87,0.935483870967742,0,0.0,1,0
89,"In this work , we propose a simple baseline method for text classification .",Discussion and conclusion,Discussion and conclusion,text-classification,2,1,0.2,88,0.946236559139785,1,0.2,0,0
90,"Unlike unsupervisedly trained word vectors from word2vec , our word features can be averaged together to form good sentence representations .",Discussion and conclusion,Discussion and conclusion,text-classification,2,2,0.4,89,0.956989247311828,2,0.4,0,0
91,"In several tasks , fastText obtains performance on par with recently proposed methods inspired by deep learning , while being much faster .",Discussion and conclusion,Discussion and conclusion,text-classification,2,3,0.6,90,0.967741935483871,3,0.6,0,0
92,"Although deep neural networks have in theory much higher representational power than shallow models , it is not clear if simple text classification problems such as sentiment analysis are the right ones to evaluate them .",Discussion and conclusion,Discussion and conclusion,text-classification,2,4,0.8,91,0.978494623655914,4,0.8,0,0
93,We will publish our code so that the research community can easily build on top of our work .,Discussion and conclusion,Discussion and conclusion,text-classification,2,5,1.0,92,0.989247311827957,5,1.0,0,0
1,title,,,text-classification,3,0,0.0,0,0.0,0,0.0,1,0
2,On the Role of Text Preprocessing in Neural Network Architectures : An Evaluation Study on Text Categorization and Sentiment Analysis,title,title,text-classification,3,1,0.0,1,0.008130081300813,1,0.0,1,1
3,abstract,,,text-classification,3,0,0.0,2,0.016260162601626,0,0.0,1,0
4,"Text preprocessing is often the first step in the pipeline of a Natural Language Processing ( NLP ) system , with potential impact in its final performance .",abstract,abstract,text-classification,3,1,0.1428571428571428,3,0.024390243902439,1,0.1428571428571428,1,1
5,"Despite its importance , text preprocessing has not received much attention in the deep learning literature .",abstract,abstract,text-classification,3,2,0.2857142857142857,4,0.032520325203252,2,0.2857142857142857,1,0
6,"In this paper we investigate the impact of simple text preprocessing decisions ( particularly tokenizing , lemmatizing , lowercasing and multiword grouping ) on the performance of a standard neural text classifier .",abstract,abstract,text-classification,3,3,0.4285714285714285,5,0.040650406504065,3,0.4285714285714285,1,0
7,We perform an extensive evaluation on standard benchmarks from text categorization and sentiment analysis .,abstract,abstract,text-classification,3,4,0.5714285714285714,6,0.048780487804878,4,0.5714285714285714,1,0
8,"While our experiments show that a simple tokenization of input text is generally adequate , they also highlight significant degrees of variability across preprocessing techniques .",abstract,abstract,text-classification,3,5,0.7142857142857143,7,0.056910569105691,5,0.7142857142857143,1,0
9,"This reveals the importance of paying attention to this usually - overlooked step in the pipeline , particularly when comparing different models .",abstract,abstract,text-classification,3,6,0.8571428571428571,8,0.065040650406504,6,0.8571428571428571,1,0
10,"Finally , our evaluation provides insights into the best preprocessing practices for training word embeddings .",abstract,abstract,text-classification,3,7,1.0,9,0.073170731707317,7,1.0,1,0
11,Introduction,,,text-classification,3,0,0.0,10,0.08130081300813,0,0.0,1,0
12,"Words are often considered as the basic constituents of texts for many languages , including English .",Introduction,Introduction,text-classification,3,1,0.02,11,0.089430894308943,1,0.0476190476190476,1,0
13,The first module in an NLP pipeline is a tokenizer which transforms texts to sequences of words .,Introduction,Introduction,text-classification,3,2,0.04,12,0.0975609756097561,2,0.0952380952380952,1,0
14,"However , in practise , other preprocessing techniques can be ( and are ) further used together with tokenization .",Introduction,Introduction,text-classification,3,3,0.06,13,0.1056910569105691,3,0.1428571428571428,1,0
15,"These include lemmatization , lowercasing and 1 Note that although word - based models are mainstream in NLP in general and text classification in particular , recent work has also considered other linguistic units , such as characters or word senses .",Introduction,Introduction,text-classification,3,4,0.08,14,0.1138211382113821,4,0.1904761904761904,1,0
16,"These techniques require a different kind of preprocessing and , while they have been shown effective in various settings , in this work we only focus on the mainstream word - based models .",Introduction,Introduction,text-classification,3,5,0.1,15,0.1219512195121951,5,0.238095238095238,1,0
17,"multiword grouping , among others .",Introduction,Introduction,text-classification,3,6,0.12,16,0.1300813008130081,6,0.2857142857142857,1,0
18,"Although these preprocessing decisions have been studied in the context of conventional text classification techniques , little attention has been paid to them in the more recent neural - based models .",Introduction,Introduction,text-classification,3,7,0.14,17,0.1382113821138211,7,0.3333333333333333,1,0
19,"The most similar study to ours is , which analyzed different encoding levels for English and Asian languages such as Chinese , Japanese and Korean .",Introduction,Introduction,text-classification,3,8,0.16,18,0.1463414634146341,8,0.3809523809523809,1,0
20,"As opposed to our work , their analysis was focused on UTF - 8 bytes , characters , words , romanized characters and romanized words as encoding levels , rather than the preprocessing techniques analyzed in this paper .",Introduction,Introduction,text-classification,3,9,0.18,19,0.1544715447154471,9,0.4285714285714285,1,0
21,"Additionally , word embeddings have been shown to play an important role in boosting the generalization capabilities of neural systems .",Introduction,Introduction,text-classification,3,10,0.2,20,0.1626016260162601,10,0.4761904761904761,1,0
22,"However , while some studies have focused on intrinsically analyzing the role of lemmatization in their underlying training corpus , the impact on their extrinsic performance when integrated into a neural network architecture has remained understudied .",Introduction,Introduction,text-classification,3,11,0.22,21,0.1707317073170731,11,0.5238095238095238,1,0
23,"In this paper we focus on the role of preprocessing the input text , particularly in how it is split into individual ( meaning - bearing ) tokens and how it affects the performance of standard neural text classification models based on .",Introduction,Introduction,text-classification,3,12,0.24,22,0.1788617886178861,12,0.5714285714285714,1,1
24,"CNNs have proven to be effective in a wide range of NLP applications , in - cluding text classification tasks such as topic categorization and polarity detection , which are the tasks considered in this work .",Introduction,Introduction,text-classification,3,13,0.26,23,0.1869918699186991,13,0.6190476190476191,1,0
25,The goal of our evaluation study is to find answers to the following two questions :,Introduction,Introduction,text-classification,3,14,0.28,24,0.1951219512195122,14,0.6666666666666666,1,0
26,1 .,Introduction,Introduction,text-classification,3,15,0.3,25,0.2032520325203252,15,0.7142857142857143,1,0
27,Are neural network architectures ( in particular CNNs ) affected by seemingly small preprocessing decisions in the input text ?,Introduction,Introduction,text-classification,3,16,0.32,26,0.2113821138211382,16,0.7619047619047619,1,0
28,2 .,Introduction,Introduction,text-classification,3,17,0.34,27,0.2195121951219512,17,0.8095238095238095,1,0
29,Does the preprocessing of the embeddings ' underlying training corpus have an impact on the final performance of a state - of - the - art neural network text classifier ?,Introduction,Introduction,text-classification,3,18,0.36,28,0.2276422764227642,18,0.8571428571428571,1,0
30,"According to our experiments in topic categorization and polarity detection , these decisions are important in certain cases .",Introduction,Introduction,text-classification,3,19,0.38,29,0.2357723577235772,19,0.9047619047619048,1,0
31,"Moreover , we shed some light on the motivations of each preprocessing decision and provide some hints on how to normalize the input corpus to better suit each setting .",Introduction,Introduction,text-classification,3,20,0.4,30,0.2439024390243902,20,0.9523809523809524,1,0
32,The accompanying materials of this submission can be downloaded at the following repository : github.com/pedrada88/preproc-textclassification .,Introduction,Introduction,text-classification,3,21,0.42,31,0.2520325203252032,21,1.0,1,1
33,Text Preprocessing,Introduction,,text-classification,3,22,0.44,32,0.2601626016260163,0,0.0,1,0
34,"Given an input text , words are gathered as input units of classification models through tokenization .",Introduction,Text Preprocessing,text-classification,3,23,0.46,33,0.2682926829268293,1,0.2,1,0
35,We refer to the corpus which is only tokenized as vanilla .,Introduction,Text Preprocessing,text-classification,3,24,0.48,34,0.2764227642276423,2,0.4,1,0
36,"For example , given the sentence "" Apple is asking its manufacturers to move Mac - Book Air production to the United States . "" ( running example ) , the vanilla tokenized text would be as follows ( white spaces delimiting different word units ) :",Introduction,Text Preprocessing,text-classification,3,25,0.5,35,0.2845528455284553,3,0.6,1,0
37,Apple is asking its manufacturers to move MacBook Air production to the United States .,Introduction,Text Preprocessing,text-classification,3,26,0.52,36,0.2926829268292683,4,0.8,1,0
38,"We additionally consider three simple preprocessing techniques to be applied to an input text : lowercasing ( Section 2.1 ) , lemmatizing ( Section 2.2 ) and multiword grouping ( Section 2.3 ) .",Introduction,Text Preprocessing,text-classification,3,27,0.54,37,0.3008130081300813,5,1.0,1,0
39,Lowercasing,Introduction,,text-classification,3,28,0.56,38,0.3089430894308943,0,0.0,1,0
40,This is the simplest preprocessing technique which consists of lowercasing each single token of the input text :,Introduction,Lowercasing,text-classification,3,29,0.58,39,0.3170731707317073,1,0.2,1,0
41,apple is asking its manufacturers to move macbook air production to the united states .,Introduction,Lowercasing,text-classification,3,30,0.6,40,0.3252032520325203,2,0.4,1,0
42,"Due to its simplicity , lowercasing has been a popular practice in modules of deep learning libraries and word embedding packages .",Introduction,Lowercasing,text-classification,3,31,0.62,41,0.3333333333333333,3,0.6,1,0
43,"Despite its desirable property of reducing sparsity and vocabulary size , lowercasing may negatively impact system 's performance by increasing ambiguity .",Introduction,Lowercasing,text-classification,3,32,0.64,42,0.3414634146341463,4,0.8,1,0
44,"For instance , the Apple company in our example and the apple fruit would be considered as identical entities .",Introduction,Lowercasing,text-classification,3,33,0.66,43,0.3495934959349593,5,1.0,1,0
45,Lemmatizing,Introduction,,text-classification,3,34,0.68,44,0.3577235772357723,0,0.0,1,0
46,The process of lemmatizing consists of replacing a given token with its corresponding lemma :,Introduction,Lemmatizing,text-classification,3,35,0.7,45,0.3658536585365853,1,0.1666666666666666,1,0
47,Apple be ask its manufacturer to move Mac - Book Air production to the United States .,Introduction,Lemmatizing,text-classification,3,36,0.72,46,0.3739837398373983,2,0.3333333333333333,1,0
48,Lemmatization has been traditionally a standard preprocessing technique for linear text classification systems .,Introduction,Lemmatizing,text-classification,3,37,0.74,47,0.3821138211382114,3,0.5,1,0
49,"However , it is rarely used as a preprocessing stage in neural - based systems .",Introduction,Lemmatizing,text-classification,3,38,0.76,48,0.3902439024390244,4,0.6666666666666666,1,0
50,"The main idea behind lemmatization is to reduce sparsity , as different inflected forms of the same lemma may occur infrequently ( or not at all ) during training .",Introduction,Lemmatizing,text-classification,3,39,0.78,49,0.3983739837398374,5,0.8333333333333334,1,0
51,"However , this may come at the cost of neglecting important syntactic nuances .",Introduction,Lemmatizing,text-classification,3,40,0.8,50,0.4065040650406504,6,1.0,1,0
52,Multiword grouping,Introduction,,text-classification,3,41,0.82,51,0.4146341463414634,0,0.0,1,0
53,This last preprocessing technique consists of grouping consecutive tokens together into a single token if found in a given inventory :,Introduction,Multiword grouping,text-classification,3,42,0.84,52,0.4227642276422764,1,0.1111111111111111,1,0
54,Apple is asking its manufacturers to move MacBook Air production to the United States .,Introduction,Multiword grouping,text-classification,3,43,0.86,53,0.4308943089430894,2,0.2222222222222222,1,0
55,"The motivation behind this step lies in the idiosyncratic nature of multiword expressions , e.g. United States in the example .",Introduction,Multiword grouping,text-classification,3,44,0.88,54,0.4390243902439024,3,0.3333333333333333,1,0
56,The meaning of these multiword expressions are often hardly traceable from their individual tokens .,Introduction,Multiword grouping,text-classification,3,45,0.9,55,0.4471544715447154,4,0.4444444444444444,1,0
57,"As a result , treating multiwords as single units may lead to better training of a given model .",Introduction,Multiword grouping,text-classification,3,46,0.92,56,0.4552845528455284,5,0.5555555555555556,1,0
58,"Because of this , word embedding toolkits such as Word2vec propose statistical approaches for extracting these multiwords , or directly include multiwords along with single words in their pretrained embedding spaces .",Introduction,Multiword grouping,text-classification,3,47,0.94,57,0.4634146341463415,6,0.6666666666666666,1,0
59,"We considered two tasks for our experiments : topic categorization , i.e. assigning a topic to a given document from a pre-defined set of topics , and polarity detection , i.e. detecting if the sentiment of a given piece of text is positive or negative .",Introduction,Multiword grouping,text-classification,3,48,0.96,58,0.4715447154471545,7,0.7777777777777778,1,0
60,Two different settings were studied : ( 1 ) word embedding 's training corpus and the evaluation dataset were preprocessed in a similar manner ( Section 3.2 ) ; and ( 2 ) the two were preprocessed differently ( Section 3.3 ) .,Introduction,Multiword grouping,text-classification,3,49,0.98,59,0.4796747967479675,8,0.8888888888888888,1,0
61,In what follows we describe the common experimental setting as well as the datasets and preprocessing used for the evaluation .,Introduction,Multiword grouping,text-classification,3,50,1.0,60,0.4878048780487805,9,1.0,1,0
62,Experimental setup,,,text-classification,3,0,0.0,61,0.4959349593495935,0,0.0,1,0
63,We tried with two classification models .,Experimental setup,,text-classification,3,1,0.1428571428571428,62,0.5040650406504065,1,0.02,1,1
64,"The first one is a standard CNN model similar to that of , using ReLU as non-linear activation function .",Experimental setup,We tried with two classification models .,text-classification,3,2,0.2857142857142857,63,0.5121951219512195,2,0.04,1,1
65,"In the second model , we add a recurrent layer ( specifically an LSTM ) before passing the pooled features directly to the fully connected softmax layer .",Experimental setup,We tried with two classification models .,text-classification,3,3,0.4285714285714285,64,0.5203252032520326,3,0.06,1,1
66,The inclusion of this LSTM layer has been shown to be able to effectively replace multiple layers of convolution and be beneficial particularly for large inputs .,Experimental setup,We tried with two classification models .,text-classification,3,4,0.5714285714285714,65,0.5284552845528455,4,0.08,1,0
67,"These models were used for both topic categorization and polarity detection tasks , with slight hyperparameter variations given their different natures ( mainly in their text size ) which were fixed across all datasets .",Experimental setup,We tried with two classification models .,text-classification,3,5,0.7142857142857143,66,0.5365853658536586,5,0.1,1,0
68,The embedding layer was initialized using 300 - dimensional CBOW Word2vec embeddings trained on the 3B - word UMBC WebBase corpus with standard hyperparameters,Experimental setup,We tried with two classification models .,text-classification,3,6,0.8571428571428571,67,0.5447154471544715,6,0.12,1,1
69,4 .,Experimental setup,We tried with two classification models .,text-classification,3,7,1.0,68,0.5528455284552846,7,0.14,1,0
70,Evaluation datasets .,,,text-classification,3,0,0.0,69,0.5609756097560976,8,0.16,1,0
71,"For the topic categorization task we used the BBC news dataset 5 , 20 News , Reuters 6 and The code for this CNN implementation is the same as in , which is available at https://github.com/pilehvar/sensecnn",Evaluation datasets .,Evaluation datasets .,text-classification,3,1,0.0384615384615384,70,0.5691056910569106,9,0.18,1,0
72,4 Context window of 5 words and hierarchical softmax .,Evaluation datasets .,Evaluation datasets .,text-classification,3,2,0.0769230769230769,71,0.5772357723577236,10,0.2,1,0
73,5 http://mlg.ucd.ie/datasets/bbc.html,Evaluation datasets .,Evaluation datasets .,text-classification,3,3,0.1153846153846153,72,0.5853658536585366,11,0.22,1,0
74,"6 Due to the large number of labels in the original Reuters ( i.e. 91 ) and to be consistent with the other datasets , we reduce the dataset to its 8 most frequent labels , a reduction already performed in previous works .",Evaluation datasets .,Evaluation datasets .,text-classification,3,4,0.1538461538461538,73,0.5934959349593496,12,0.24,1,0
75,Preprocessing .,Evaluation datasets .,,text-classification,3,5,0.1923076923076923,74,0.6016260162601627,13,0.26,1,0
76,Four different techniques ( see Section 2 ) were used to preprocess the datasets as well as the corpus which was used to train word embeddings ( i.e. UMBC ) .,Evaluation datasets .,Preprocessing .,text-classification,3,6,0.2307692307692307,75,0.6097560975609756,14,0.28,1,0
77,For tokenization and lemmatization we relied on Stanford CoreNLP .,Evaluation datasets .,Preprocessing .,text-classification,3,7,0.2692307692307692,76,0.6178861788617886,15,0.3,1,0
78,"As for multiwords , we used the phrases from the pre-trained Google News Word2vec vectors , which were obtained using a simple statistical approach .",Evaluation datasets .,Preprocessing .,text-classification,3,8,0.3076923076923077,77,0.6260162601626016,16,0.32,1,0
79,12 shows the accuracy 13 of the classification models using our four preprocessing techniques .,Evaluation datasets .,Preprocessing .,text-classification,3,9,0.3461538461538461,78,0.6341463414634146,17,0.34,1,0
80,We observe a certain variability of results depending on the preprocessing techniques used ( aver -7 ftp://medir.ohsu.edu/pub/ohsumed,Evaluation datasets .,Preprocessing .,text-classification,3,10,0.3846153846153846,79,0.6422764227642277,18,0.36,1,0
81,8 Both PL04 and PL05 were downloaded from http://www.cs.cornell.edu/people/pabo/movie-review-data/,Evaluation datasets .,Preprocessing .,text-classification,3,11,0.4230769230769231,80,0.6504065040650406,19,0.38,1,0
82,9 http://www.rottentomatoes.com,Evaluation datasets .,Preprocessing .,text-classification,3,12,0.4615384615384615,81,0.6585365853658537,20,0.4,1,0
83,"10 We mapped the numerical value of phrases to either negative ( from 0 to 0.4 ) or positive ( from 0.6 to 1 ) , removing the neutral phrases according to the scale ( from 0.4 to 0.6 ) .",Evaluation datasets .,Preprocessing .,text-classification,3,13,0.5,82,0.6666666666666666,21,0.42,1,0
84,"For the datasets with train - test partitions , the sizes of the test sets are the following : 7,532 for 20 News ; 12,733 for Ohsumed ; 25,000 for IMDb ; and 1,000 for RTC .",Evaluation datasets .,Preprocessing .,text-classification,3,14,0.5384615384615384,83,0.6747967479674797,22,0.44,1,0
85,For future work it would be interesting to explore more complex methods to learn embeddings for multiword expressions .,Evaluation datasets .,Preprocessing .,text-classification,3,15,0.5769230769230769,84,0.6829268292682927,23,0.46,1,0
86,Computed by averaging accuracy of two different runs .,Evaluation datasets .,,text-classification,3,16,0.6153846153846154,85,0.6910569105691057,24,0.48,1,0
87,"The statistical significance was calculated according to an unpaired t- test at the 5 % significance level . age variability 14 of 2.4 % for the CNN + LSTM model , including a statistical significance gap in seven of the nine datasets ) , which proves the influence of preprocessing on the final results .",Evaluation datasets .,Computed by averaging accuracy of two different runs .,text-classification,3,17,0.6538461538461539,86,0.6991869918699187,25,0.5,1,0
88,It is perhaps not surprising that the lowest variance of results is seen in the datasets with the larger training data ( i.e. RTC and Stanford ) .,Evaluation datasets .,Computed by averaging accuracy of two different runs .,text-classification,3,18,0.6923076923076923,87,0.7073170731707317,26,0.52,1,0
89,"This suggests that the preprocessing decisions are not so important when the training data is large enough , but they are indeed relevant in benchmarks where the training data is limited .",Evaluation datasets .,Computed by averaging accuracy of two different runs .,text-classification,3,19,0.7307692307692307,88,0.7154471544715447,27,0.54,1,0
90,"As far as the individual preprocessing techniques are concerned , the vanilla setting ( tokenization only ) proves to be consistent across datasets and tasks , as it performs in the same ballpark as the best result in 8 of the 9 datasets for both models ( with no noticeable differences between topic categorization and polarity detection ) .",Evaluation datasets .,Computed by averaging accuracy of two different runs .,text-classification,3,20,0.7692307692307693,89,0.7235772357723578,28,0.56,1,0
91,"The only topic categorization dataset in which tokenization does not seem enough is Ohsumed , which , unlike the more general nature of other categorization datasets ( news ) , belongs to a specialized domain ( medical ) for which fine - grained distinctions are required to classify cardiovascular diseases .",Evaluation datasets .,Computed by averaging accuracy of two different runs .,text-classification,3,21,0.8076923076923077,90,0.7317073170731707,29,0.58,1,0
92,"In particular for this dataset , word embeddings trained on a general - domain corpus like UMBC may not accurately capture the specialized meaning of medical terms and hence , sparsity becomes an issue .",Evaluation datasets .,Computed by averaging accuracy of two different runs .,text-classification,3,22,0.8461538461538461,91,0.7398373983739838,30,0.6,1,0
93,"In fact , lowercasing and lemmatizing , which are mainly aimed at reducing sparsity , outperform the vanilla setting by over six points in the CNN + LSTM setting and clearly outperform the other preprocessing techniques on the single CNN model as well .",Evaluation datasets .,Computed by averaging accuracy of two different runs .,text-classification,3,23,0.8846153846153846,92,0.7479674796747967,31,0.62,1,0
94,Experiment 1 : Preprocessing effect,Evaluation datasets .,,text-classification,3,24,0.9230769230769232,93,0.7560975609756098,32,0.64,1,0
95,"Nevertheless , the use of more complex preprocessing techniques such as lemmatization and multiword grouping does not help in general .",Evaluation datasets .,Experiment 1 : Preprocessing effect,text-classification,3,25,0.9615384615384616,94,0.7642276422764228,33,0.66,1,1
96,"Even though lemmatization has proved useful in conventional linear models as an effective way to deal with sparsity , neural network architectures seem to be more capable of overcoming sparsity thanks to the generalization power of word embeddings .",Evaluation datasets .,Experiment 1 : Preprocessing effect,text-classification,3,26,1.0,95,0.7723577235772358,34,0.68,1,0
97,Experiment 2 : Cross-preprocessing,,,text-classification,3,0,0.0,96,0.7804878048780488,35,0.7,1,0
98,This experiment aims at studying the impact of using different word embeddings ( with differently preprocessed training corpora ) on tokenized datasets ( vanilla setting ) .,Experiment 2 : Cross-preprocessing,Experiment 2 : Cross-preprocessing,text-classification,3,1,0.0666666666666666,97,0.7886178861788617,36,0.72,1,0
99,shows the results for this experiment .,Experiment 2 : Cross-preprocessing,Experiment 2 : Cross-preprocessing,text-classification,3,2,0.1333333333333333,98,0.7967479674796748,37,0.74,1,0
100,"In this experiment we observe a different trend , with multiwordenhanced vectors exhibiting a better performance both on the single CNN model ( best overall performance in seven of the nine datasets ) and on the CNN + LSTM model ( best performance in four datasets and in the same ballpark as the best results in four of the remaining five datasets ) .",Experiment 2 : Cross-preprocessing,Experiment 2 : Cross-preprocessing,text-classification,3,3,0.2,99,0.8048780487804879,38,0.76,1,1
101,In this case the same set of words is learnt but single tokens inside multiword expressions are not trained .,Experiment 2 : Cross-preprocessing,Experiment 2 : Cross-preprocessing,text-classification,3,4,0.2666666666666666,100,0.8130081300813008,39,0.78,1,0
102,"Instead , these single tokens are considered in isolation only , without the added noise when considered inside the multiword expression as well .",Experiment 2 : Cross-preprocessing,Experiment 2 : Cross-preprocessing,text-classification,3,5,0.3333333333333333,101,0.8211382113821138,40,0.8,1,0
103,"For instance , the word Apple has a clearly different meaning in isolation from the one inside :",Experiment 2 : Cross-preprocessing,Experiment 2 : Cross-preprocessing,text-classification,3,6,0.4,102,0.8292682926829268,41,0.82,1,0
104,Cross - preprocessing evaluation : accuracy on the topic categorization and polarity detection tasks using different sets of word embeddings to initialize the embedding layer of the two classifiers .,Experiment 2 : Cross-preprocessing,Experiment 2 : Cross-preprocessing,text-classification,3,7,0.4666666666666667,103,0.8373983739837398,42,0.84,1,0
105,All datasets were preprocessed similarly according to the vanilla setting .,Experiment 2 : Cross-preprocessing,Experiment 2 : Cross-preprocessing,text-classification,3,8,0.5333333333333333,104,0.8455284552845529,43,0.86,1,0
106,indicates results that are statistically significant with respect to the top result .,Experiment 2 : Cross-preprocessing,Experiment 2 : Cross-preprocessing,text-classification,3,9,0.6,105,0.8536585365853658,44,0.88,1,0
107,"the multiword expression Big Apple , hence it can be seen as beneficial not to train the word",Experiment 2 : Cross-preprocessing,Experiment 2 : Cross-preprocessing,text-classification,3,10,0.6666666666666666,106,0.8617886178861789,45,0.9,1,0
108,Apple when part of this multiword expression .,Experiment 2 : Cross-preprocessing,,text-classification,3,11,0.7333333333333333,107,0.8699186991869918,46,0.92,1,0
109,"Interestingly , using multiword - wise embeddings on the vanilla setting leads to consistently better results than using them on the same multiwordgrouped preprocessed dataset in eight of the nine datasets .",Experiment 2 : Cross-preprocessing,Apple when part of this multiword expression .,text-classification,3,12,0.8,108,0.8780487804878049,47,0.94,1,1
110,"This could provide hints on the excellent results provided by pre-trained Word2vec embeddings trained on the Google News corpus , which learns multiwords similarly to our setting .",Experiment 2 : Cross-preprocessing,Apple when part of this multiword expression .,text-classification,3,13,0.8666666666666667,109,0.8861788617886179,48,0.96,1,0
111,"Apart from this somewhat surprising finding , the use of the embeddings trained on a simple tokenized corpus ( i.e. vanilla ) proved again competitive , as different preprocessing techniques such as lowercasing and lemmatizing do not seem to help .",Experiment 2 : Cross-preprocessing,Apple when part of this multiword expression .,text-classification,3,14,0.9333333333333332,110,0.8943089430894309,49,0.98,1,1
112,"In fact , the relatively weaker performance of lemmatization and lowercasing in this crossprocessing experiment is somehow expected as the coverage of word embeddings in vanilla - tokenized datasets is limited , e.g. , many entities which are capitalized in the datasets are not covered in the case of lowercasing , and inflected forms are missing in the case of lemmatizing .",Experiment 2 : Cross-preprocessing,Apple when part of this multiword expression .,text-classification,3,15,1.0,111,0.902439024390244,50,1.0,1,0
113,Conclusions,,,text-classification,3,0,0.0,112,0.9105691056910568,0,0.0,1,0
114,In this paper we analyzed the impact of simple text preprocessing decisions on the performance of a standard word - based neural text classifier .,Conclusions,Conclusions,text-classification,3,1,0.1,113,0.91869918699187,1,0.1,0,0
115,Our evaluations highlight the importance of being careful in the choice of how to preprocess our data and to be consistent when comparing different systems .,Conclusions,Conclusions,text-classification,3,2,0.2,114,0.926829268292683,2,0.2,0,0
116,"In general , a simple tokenization works equally or better than more complex pre-processing techniques such as lemmatization or multiword grouping , except for domain - specific datasets ( such as the medical dataset in our experiments ) in which sole tokenization performs poorly .",Conclusions,Conclusions,text-classification,3,3,0.3,115,0.934959349593496,3,0.3,0,0
117,"Additionally , word embeddings trained on multiword - grouped corpora perform surprisingly well when applied to simple tokenized datasets .",Conclusions,Conclusions,text-classification,3,4,0.4,116,0.943089430894309,4,0.4,0,0
118,"This property has often been overlooked and , to the best of our knowledge , we test the hypothesis for the first time .",Conclusions,Conclusions,text-classification,3,5,0.5,117,0.951219512195122,5,0.5,0,0
119,"In fact , this finding could partially explain the long - lasting success of pre-trained Word2vec embeddings , which specifically learn multiword embeddings as part of their pipeline .",Conclusions,Conclusions,text-classification,3,6,0.6,118,0.959349593495935,6,0.6,0,0
120,"Moreover , our analysis shows that there is a high variance in the results depending on the preprocessing choice ( 2.4 % on average for the best performing model ) , especially when the training data is not large enough to generalize .",Conclusions,Conclusions,text-classification,3,7,0.7,119,0.967479674796748,7,0.7,0,0
121,"Further analysis and experimentation would be required to fully understand the significance of these results ; but , this work can be viewed as a starting point for studying the impact of text preprocessing in deep learning models .",Conclusions,Conclusions,text-classification,3,8,0.8,120,0.975609756097561,8,0.8,0,0
122,We hope that our findings will encourage future researchers to carefully select and report these preprocessing decisions when evaluating or comparing different models .,Conclusions,Conclusions,text-classification,3,9,0.9,121,0.983739837398374,9,0.9,0,0
123,"Finally , as future work , we plan to extend our analysis to other tasks ( e.g. question answering ) , languages ( particularly morphologically rich languages for which these results may vary ) and preprocessing techniques ( e.g. stopword removal or part - of - speech tagging ) .",Conclusions,Conclusions,text-classification,3,10,1.0,122,0.991869918699187,10,1.0,0,0
1,title,,,text-classification,4,0,0.0,0,0.0,0,0.0,1,0
2,Learning Context - Sensitive Convolutional Filters for Text Processing,title,,text-classification,4,1,0.0,1,0.004524886877828,1,0.0,1,1
3,abstract,,,text-classification,4,0,0.0,2,0.0090497737556561,0,0.0,1,0
4,Convolutional neural networks ( CNNs ) have recently emerged as a popular building block for natural language processing ( NLP ) .,abstract,abstract,text-classification,4,1,0.1428571428571428,3,0.0135746606334841,1,0.1428571428571428,1,0
5,"Despite their success , most existing CNN models employed in NLP share the same learned ( and static ) set of filters for all input sentences .",abstract,abstract,text-classification,4,2,0.2857142857142857,4,0.0180995475113122,2,0.2857142857142857,1,0
6,"In this paper , we consider an approach of using a small meta network to learn contextsensitive convolutional filters for text processing .",abstract,abstract,text-classification,4,3,0.4285714285714285,5,0.0226244343891402,3,0.4285714285714285,1,1
7,The role of meta network is to abstract the contextual information of a sentence or document into a set of input -aware filters .,abstract,abstract,text-classification,4,4,0.5714285714285714,6,0.0271493212669683,4,0.5714285714285714,1,0
8,"We further generalize this framework to model sentence pairs , where a bidirectional filter generation mechanism is introduced to encapsulate co-dependent sentence representations .",abstract,abstract,text-classification,4,5,0.7142857142857143,7,0.0316742081447963,5,0.7142857142857143,1,0
9,"In our benchmarks on four different tasks , including ontology classification , sentiment analysis , answer sentence selection , and paraphrase identification , our proposed model , a modified CNN with context - sensitive filters , consistently outperforms the standard CNN and attention - based CNN baselines .",abstract,abstract,text-classification,4,6,0.8571428571428571,8,0.0361990950226244,6,0.8571428571428571,1,0
10,"By visualizing the learned context - sensitive filters , we further validate and rationalize the effectiveness of proposed framework .",abstract,abstract,text-classification,4,7,1.0,9,0.0407239819004524,7,1.0,1,0
11,Introduction,,,text-classification,4,0,0.0,10,0.0452488687782805,0,0.0,1,0
12,"In the last few years , convolutional neural networks ( CNNs ) have demonstrated remarkable progress in various natural language processing applications , including sentence / document classification , text sequence matching , generic text representations , language modeling , machine translation and abstractive sentence summarization .",Introduction,Introduction,text-classification,4,1,0.0434782608695652,11,0.0497737556561085,1,0.0434782608695652,1,0
13,CNNs are typically applied to tasks where feature extrac-tion and a corresponding supervised task are approached jointly .,Introduction,Introduction,text-classification,4,2,0.0869565217391304,12,0.0542986425339366,2,0.0869565217391304,1,0
14,"As an encoder network for text , CNNs typically convolve a set of filters , of window size n , with an inputsentence embedding matrix obtained via word2vec or Glove .",Introduction,Introduction,text-classification,4,3,0.1304347826086956,13,0.0588235294117647,3,0.1304347826086956,1,0
15,"Different filter sizes n maybe used within the same model , exploiting meaningful semantic features from different n-gram fragments .",Introduction,Introduction,text-classification,4,4,0.1739130434782608,14,0.0633484162895927,4,0.1739130434782608,1,0
16,"The learned weights of CNN filters , inmost cases , are assumed to be fixed regardless of the input text .",Introduction,Introduction,text-classification,4,5,0.217391304347826,15,0.0678733031674208,5,0.217391304347826,1,0
17,"As a result , the rich contextual information inherent in natural language sequences may not be fully captured .",Introduction,Introduction,text-classification,4,6,0.2608695652173913,16,0.0723981900452488,6,0.2608695652173913,1,0
18,"As demonstrated in , the context of a word tends to greatly influence its contribution to the final supervised tasks .",Introduction,Introduction,text-classification,4,7,0.3043478260869565,17,0.0769230769230769,7,0.3043478260869565,1,0
19,"This observation is consistent with the following intuition : when reading different types of documents , e.g. , academic papers or newspaper articles , people tend to adopt distinct strategies for better and more effective understanding , leveraging the fact that the same words or phrases may have different meaning or imply different things , depending on context .",Introduction,Introduction,text-classification,4,8,0.3478260869565217,18,0.0814479638009049,8,0.3478260869565217,1,0
20,Several research efforts have sought to incorporate contextual information into CNNs to adaptively extract text representations .,Introduction,Introduction,text-classification,4,9,0.391304347826087,19,0.085972850678733,9,0.391304347826087,1,0
21,"One common strategy is the attention mechanism , which is typically employed on top of a CNN ( or Long Short - Term Memory ( LSTM ) ) layer to guide the extraction of semantic features .",Introduction,Introduction,text-classification,4,10,0.4347826086956521,20,0.090497737556561,10,0.4347826086956521,1,0
22,"For the embedding of a single sentence , proposed a selfattentive model that attends to different parts of a sentence and combines them into multiple vector representations .",Introduction,Introduction,text-classification,4,11,0.4782608695652174,21,0.0950226244343891,11,0.4782608695652174,1,0
23,"However , their model needs considerably more parameters to achieve performance gains over traditional CNNs .",Introduction,Introduction,text-classification,4,12,0.5217391304347826,22,0.0995475113122171,12,0.5217391304347826,1,0
24,"To match sentence pairs , introduced an attentionbased CNN model , which re-weights the convolution inputs or outputs , to extract interdepen - dent sentence representations . ; explore a compare and aggregate framework to directly capture the wordby - word matching between two paired sentences .",Introduction,Introduction,text-classification,4,13,0.5652173913043478,23,0.1040723981900452,13,0.5652173913043478,1,0
25,"However , these approaches suffer from the problem of high matching complexity , since a similarity matrix between pairwise words needs to be computed , and thus it is computationally inefficient or even prohibitive when applied to long sentences .",Introduction,Introduction,text-classification,4,14,0.6086956521739131,24,0.1085972850678733,14,0.6086956521739131,1,0
26,"In this paper , we propose a generic approach to learn context - sensitive convolutional filters for natural language understanding .",Introduction,Introduction,text-classification,4,15,0.6521739130434783,25,0.1131221719457013,15,0.6521739130434783,1,1
27,"In contrast to traditional CNNs , the convolution operation in our framework does not have a fixed set of filters , and thus provides the network with stronger modeling flexibility and capacity .",Introduction,Introduction,text-classification,4,16,0.6956521739130435,26,0.1176470588235294,16,0.6956521739130435,1,1
28,"Specifically , we introduce a meta network to generate a set of contextsensitive filters , conditioned on specific input sentences ; these filters are adaptively applied to either the same ( Section 3.2 ) or different ( Section 3.3 ) text sequences .",Introduction,Introduction,text-classification,4,17,0.7391304347826086,27,0.1221719457013574,17,0.7391304347826086,1,1
29,"In this manner , the learned filters vary from sentence to sentence and allow for more fine - grained feature abstraction .",Introduction,Introduction,text-classification,4,18,0.782608695652174,28,0.1266968325791855,18,0.782608695652174,1,1
30,"Moreover , since the generated filters in our framework can adapt to different conditional information available ( labels or paired sentences ) , they can be naturally generalized to model sentence pairs .",Introduction,Introduction,text-classification,4,19,0.8260869565217391,29,0.1312217194570135,19,0.8260869565217391,1,0
31,"In this regard , we propose a novel bidirectional filter generation mechanism to allow interactions between sentence pairs while constructing context - sensitive representations .",Introduction,Introduction,text-classification,4,20,0.8695652173913043,30,0.1357466063348416,20,0.8695652173913043,1,1
32,"We investigate the effectiveness of our Adaptive Context - sensitive CNN ( ACNN ) framework on several text processing tasks : ontology classification , sentiment analysis , answer sentence selection and paraphrase identification .",Introduction,Introduction,text-classification,4,21,0.9130434782608696,31,0.1402714932126696,21,0.9130434782608696,1,0
33,We show that the proposed methods consistently outperforms the standard CNN and attention - based CNN baselines .,Introduction,Introduction,text-classification,4,22,0.9565217391304348,32,0.1447963800904977,22,0.9565217391304348,1,0
34,"Our work provides anew perspective on how to incorporate contextual information into text representations , which can be combined with more sophisticated structures to achieve even better performance in the future .",Introduction,Introduction,text-classification,4,23,1.0,33,0.1493212669683258,23,1.0,1,0
35,Related Work,,,text-classification,4,0,0.0,34,0.1538461538461538,0,0.0,1,0
36,"Learning deep text representations has attracted much attention recently , since they can potentially benefit a wide range of NLP applications .",Related Work,Related Work,text-classification,4,1,0.0833333333333333,35,0.1583710407239819,1,0.0833333333333333,0,0
37,CNNs have been extensively investigated as the encoder networks of natural language .,Related Work,Related Work,text-classification,4,2,0.1666666666666666,36,0.1628959276018099,2,0.1666666666666666,0,0
38,Our work is inline with previous efforts on improving the adaptivity and flexibility of convolutional neural networks .,Related Work,Related Work,text-classification,4,3,0.25,37,0.167420814479638,3,0.25,0,0
39,proposed to enhance the transformation modeling capacity of CNNs by adaptively learning the filter shapes through backpropagation .,Related Work,Related Work,text-classification,4,4,0.3333333333333333,38,0.171945701357466,4,0.3333333333333333,0,0
40,"De introduced an architecture to generate the future frames conditioned on given image ( s ) , by adapting the CNN filter weights to the motion within previous video frames .",Related Work,Related Work,text-classification,4,5,0.4166666666666667,39,0.1764705882352941,5,0.4166666666666667,0,0
41,"Although CNNs have been widely adopted in a large number of NLP applications , improving the adaptivity of vanilla CNN modules has been considerably less studied .",Related Work,Related Work,text-classification,4,6,0.5,40,0.1809954751131221,6,0.5,0,0
42,"To the best of our knowledge , the work reported in this paper is the first attempt to develop more flexible and adjustable CNN architecture for modeling sentences .",Related Work,Related Work,text-classification,4,7,0.5833333333333334,41,0.1855203619909502,7,0.5833333333333334,0,0
43,"Our use of a meta network to generate parameters for another network is directly inspired by the recent success of hypernetworks for textgeneration tasks , and dynamic parameter - prediction for video - frame generation .",Related Work,Related Work,text-classification,4,8,0.6666666666666666,42,0.1900452488687782,8,0.6666666666666666,0,0
44,"In contrast to these works that focus on generation problems , our model is based on context - sensitive CNN filters and is aimed at abstracting more informative and predictive sentence features .",Related Work,Related Work,text-classification,4,9,0.75,43,0.1945701357466063,9,0.75,0,0
45,"Most similar to our work , designed a meta network to generate compositional functions over tree - structured neural networks for encapsulating sentence features .",Related Work,Related Work,text-classification,4,10,0.8333333333333334,44,0.1990950226244343,10,0.8333333333333334,0,0
46,"However , their model is only suitable for encoding individual sentences , while our framework can be readily generalized to capture the interactions between sentence pairs .",Related Work,Related Work,text-classification,4,11,0.9166666666666666,45,0.2036199095022624,11,0.9166666666666666,0,0
47,"Moreover , our framework is based on CNN models , which is advantageous due to fewer parameters and highly parallelizable computations relative to sequential - based models .",Related Work,Related Work,text-classification,4,12,1.0,46,0.2081447963800905,12,1.0,0,0
48,Model,,,text-classification,4,0,0.0,47,0.2126696832579185,0,0.0,1,0
49,Basic CNN for text representations,Model,Model,text-classification,4,1,0.0112359550561797,48,0.2171945701357466,0,0.0,1,0
50,"The CNN architectures in are typically utilized for extracting sentence representations , by a composition of a convolutional layer and a max - pooling operation overall resulting feature maps .",Model,Model,text-classification,4,2,0.0224719101123595,49,0.2217194570135746,1,0.0526315789473684,1,0
51,"Let the words of a sentence of length T ( padded where necessary ) be x 1 , x 2 , ... , x T .",Model,Model,text-classification,4,3,0.0337078651685393,50,0.2262443438914027,2,0.1052631578947368,1,0
52,The sentence can be represented as a matrix X ?,Model,Model,text-classification,4,4,0.0449438202247191,51,0.2307692307692307,3,0.1578947368421052,1,0
53,"R d T , where each column represents a d-dimensional embedding of the corresponding word .",Model,Model,text-classification,4,5,0.0561797752808988,52,0.2352941176470588,4,0.2105263157894736,1,0
54,"In the convolutional layer , a set of filters with weights W ?",Model,Model,text-classification,4,6,0.0674157303370786,53,0.2398190045248869,5,0.2631578947368421,1,0
55,"R Khd is convolved with every window of h words within the sentence , i.e. , {x 1:h , x 2:h+1 , . . . , x T ?h+1:T } , where K is the number of output feature maps ( and filters ) .",Model,Model,text-classification,4,7,0.0786516853932584,54,0.2443438914027149,6,0.3157894736842105,1,0
56,"In this manner , feature maps p for these h-gram text fragments are generated as :",Model,Model,text-classification,4,8,0.0898876404494382,55,0.248868778280543,7,0.3684210526315789,1,0
57,"where i = 1 , 2 , ... , T ? h + 1 and denotes the convolution operator at the ith shift location .",Model,Model,text-classification,4,9,0.1011235955056179,56,0.253393665158371,8,0.4210526315789473,1,0
58,Parameter b ?,Model,Model,text-classification,4,10,0.1123595505617977,57,0.2579185520361991,9,0.4736842105263157,1,0
59,"R K is the bias term and f ( ) is a non-linear function , implemented as a rectified linear unit ( ReLU ) in our experiments .",Model,Model,text-classification,4,11,0.1235955056179775,58,0.2624434389140271,10,0.5263157894736842,1,0
60,"The output feature maps of the convolutional layer , i.e. , p ? R K(T ?h+1 ) are then passed to the pooling layer , which takes the maximum value in every row of p , forming a K-dimensional vector , z.",Model,Model,text-classification,4,12,0.1348314606741573,59,0.2669683257918552,11,0.5789473684210527,1,0
61,This operation attempts to keep the most salient feature detected by every filter and discard the information from less fundamental text fragments .,Model,Model,text-classification,4,13,0.146067415730337,60,0.2714932126696832,12,0.631578947368421,1,0
62,"Moreover , the max - over - time nature of the pooling operation guarantees that the size of the obtained representation is independent of the sentence length .",Model,Model,text-classification,4,14,0.1573033707865168,61,0.2760180995475113,13,0.6842105263157895,1,0
63,"Note that in basic CNN sentence encoders , filter weights are the same for different inputs , which maybe suboptimal for feature extraction , especially in the case where conditional information is available .",Model,Model,text-classification,4,15,0.1685393258426966,62,0.2805429864253393,14,0.7368421052631579,1,0
64,Learning context - sensitive filters,Model,Model,text-classification,4,16,0.1797752808988764,63,0.2850678733031674,15,0.7894736842105263,1,0
65,"The proposed architecture to learn contextsensitive filters is composed of two principal modules : ( i ) a filter generation module , which produces a set of filters conditioned on the input sentence ; and ( ii ) an adaptive convolution module , which applies the generated filters to an input sentence ( this sentence maybe either the same as or different from the first input , as discussed further in Section 3.3 ) .",Model,Model,text-classification,4,17,0.1910112359550561,64,0.2895927601809955,16,0.8421052631578947,1,0
66,"The two modules are jointly differentiable , and the overall architecture can be trained in an end - to - end manner .",Model,Model,text-classification,4,18,0.2022471910112359,65,0.2941176470588235,17,0.8947368421052632,1,0
67,"Since the generated filters are sample - specific , our ACNN feature extractor for text tends to have stronger predictive power than a basic CNN encoder .",Model,Model,text-classification,4,19,0.2134831460674157,66,0.2986425339366516,18,0.9473684210526316,1,0
68,The general ACNN framework is shown schematically in .,Model,Model,text-classification,4,20,0.2247191011235955,67,0.3031674208144796,19,1.0,1,0
69,Convolution module,Model,,text-classification,4,21,0.2359550561797752,68,0.3076923076923077,0,0.0,1,0
70,Context - aware Filters,Model,Convolution module,text-classification,4,22,0.247191011235955,69,0.3122171945701357,1,0.0,1,0
71,Filter generation module,Model,Convolution module,text-classification,4,23,0.2584269662921348,70,0.3167420814479638,0,0.0,1,0
72,"I 'll go back and try other dishes Filter generation module Instead of utilizing fixed filter weights W for different inputs ( as ( 1 ) ) , our model generates a set of filters conditioned on the input sentence X. Given an input X , the filter - generation module can be implemented , in principle , as any deep ( differentiable ) architecture .",Model,Convolution module,text-classification,4,24,0.2696629213483146,71,0.3212669683257919,1,0.05,1,0
73,"However , in order to handle input sentences of variable length common in natural language , we design a generic filter generation module to produce filters with a predefined size .",Model,Convolution module,text-classification,4,25,0.2808988764044944,72,0.3257918552036199,2,0.1,1,0
74,"First , the input X is encapsulated into a fixedlength vector ( code ) z with the dimension of l , via a basic CNN model , where one convolutional layer is employed along with the pooling operation ( as described in Section 3.1 ) .",Model,Convolution module,text-classification,4,26,0.2921348314606741,73,0.3303167420814479,3,0.15,1,0
75,"On top of this hidden representation z , a deconvolutional layer , which performs transposed operations of convolutions , is further applied to produce a unique set of filters for X ( as illustrated in ) :",Model,Convolution module,text-classification,4,27,0.3033707865168539,74,0.334841628959276,4,0.2,1,0
76,(,Model,Convolution module,text-classification,4,28,0.3146067415730337,75,0.3393665158371041,5,0.25,1,0
77,where ?,Model,Convolution module,text-classification,4,29,0.3258426966292135,76,0.3438914027149321,6,0.3,1,0
78,e and ?,Model,Convolution module,text-classification,4,30,0.3370786516853932,77,0.3484162895927601,7,0.35,1,0
79,"dare the learned parameters in each layer of the filter - generating module , respectively .",Model,Convolution module,text-classification,4,31,0.348314606741573,78,0.3529411764705882,8,0.4,1,0
80,"Specifically , we convolve z with a filter of size ( f s , l , k x , k y ) , where f sis the number of generated filters and the kernel size is ( k x , k y ) .",Model,Convolution module,text-classification,4,32,0.3595505617977528,79,0.3574660633484163,9,0.45,1,0
81,"The output will be a tensor of shape ( f s , k x , k y ) .",Model,Convolution module,text-classification,4,33,0.3707865168539326,80,0.3619909502262443,10,0.5,1,0
82,"Since the dimension of hidden representation z is independent of input - sentence length , this framework guarantees that the generated filters are of the same shape and size for every sentence .",Model,Convolution module,text-classification,4,34,0.3820224719101123,81,0.3665158371040724,11,0.55,1,0
83,"Intuitively , the encoding part of filter generation module abstracts the information from sentence X into z .",Model,Convolution module,text-classification,4,35,0.3932584269662921,82,0.3710407239819004,12,0.6,1,0
84,"Based on this representation , the deconvolutional up - sampling layer determines a set of fixedsize , fine - grained filters f for the specific input .",Model,Convolution module,text-classification,4,36,0.4044943820224719,83,0.3755656108597285,13,0.65,1,0
85,Adaptive convolution module,Model,Convolution module,text-classification,4,37,0.4157303370786517,84,0.3800904977375565,14,0.7,1,0
86,The adaptive convolution module takes as inputs the generated filters f and an input sentence .,Model,Convolution module,text-classification,4,38,0.4269662921348314,85,0.3846153846153846,15,0.75,1,0
87,This sentence and the input to the filter - generation module maybe identical ( as in ) or different ( as in .,Model,Convolution module,text-classification,4,39,0.4382022471910112,86,0.3891402714932127,16,0.8,1,0
88,"With the sample - specific filters , the input sentence is adaptively encoded , again , via a basic CNN architecture as in Section 3.1 , i.e. , one convolutional and one pooling layer .",Model,Convolution module,text-classification,4,40,0.449438202247191,87,0.3936651583710407,17,0.85,1,0
89,"Notably , there are no additional parameters in the adaptive convolution module ( no bias term is employed ) .",Model,Convolution module,text-classification,4,41,0.4606741573033708,88,0.3981900452488687,18,0.9,1,0
90,"Our ACNN framework can be seen as a generalization of the basic CNN , which can be represented as an ACNN by setting the outputs of the filter - generation module to a constant , regardless of the contextual information from input sentence ( s ) .",Model,Convolution module,text-classification,4,42,0.4719101123595505,89,0.4027149321266968,19,0.95,1,0
91,"Because of the learning - to - learn nature of the proposed ACNN framework , it tends to have greater representational power than the basic CNN .",Model,Convolution module,text-classification,4,43,0.4831460674157303,90,0.4072398190045249,20,1.0,1,0
92,Extension to text sequence matching,Model,Convolution module,text-classification,4,44,0.4943820224719101,91,0.4117647058823529,0,0.0,1,0
93,"Considering the ability of our ACNN framework to generate context - sensitive filters , it can be naturally generalized to the task of text sequence matching .",Model,Convolution module,text-classification,4,45,0.5056179775280899,92,0.416289592760181,1,0.025,1,0
94,"In this section , we will describe the proposed Adaptive Question Answering ( AdaQA ) model in the context of answer sentence selection task .",Model,Convolution module,text-classification,4,46,0.5168539325842697,93,0.420814479638009,2,0.05,1,0
95,Note that the corresponding model can be readily adapted to other sentence matching problems as well ( see Section 5.2 ) .,Model,Convolution module,text-classification,4,47,0.5280898876404494,94,0.4253393665158371,3,0.075,1,0
96,"Given a factual question q ( associated with a list of candidate answers {a 1 , a 2 , . . . , am } and their corresponding labels y = {y 1 , y 2 , . . . , y m } ) , the goal of the model is to identify the correct answers from the set of candidates .",Model,Convolution module,text-classification,4,48,0.5393258426966292,95,0.4298642533936652,4,0.1,1,0
97,"For i = 1 , 2 , . . . , m , if a i correctly answers q , then y i = 1 , and otherwise y i =",Model,Convolution module,text-classification,4,49,0.550561797752809,96,0.4343891402714932,5,0.125,1,0
98,"0 . Therefore , the task can be cast as a classification problem where , given an unlabeled question - answer pair ( q i , a i ) , we seek to predict the judgement y i .",Model,Convolution module,text-classification,4,50,0.5617977528089888,97,0.4389140271493212,6,0.15,1,0
99,"Conventionally , a question q and an answer a are independently encoded by two basic CNNs to fixed - length vector representations , denoted h q and ha , respectively .",Model,Convolution module,text-classification,4,51,0.5730337078651685,98,0.4434389140271493,7,0.175,1,0
100,They are then directly employed to predict the judgement y .,Model,Convolution module,text-classification,4,52,0.5842696629213483,99,0.4479638009049774,8,0.2,1,0
101,"This strategy could be suboptimal , since no communication ( information sharing ) occurs between the questionanswer pair until the top prediction layer .",Model,Convolution module,text-classification,4,53,0.5955056179775281,100,0.4524886877828054,9,0.225,1,0
102,"Intuitively , while the model is inferring the representation fora question , if the meaning of the answer is The AdaQA model can be divided into three modules : filter generation , adaptive convolution , and matching modules , as depicted schematically in .",Model,Convolution module,text-classification,4,54,0.6067415730337079,101,0.4570135746606334,10,0.25,1,0
103,"Assume there is a question - answer pair to be matched , represented by word - embedding matrices , i.e. Q ? R Tqd and A ?",Model,Convolution module,text-classification,4,55,0.6179775280898876,102,0.4615384615384615,11,0.275,1,0
104,"R Tad , where dis the embedding dimension and T q and Ta are respective sentence lengths .",Model,Convolution module,text-classification,4,56,0.6292134831460674,103,0.4660633484162896,12,0.3,1,0
105,"First , they are passed to two filter - generation modules , to produce two sets of filters that encapsulate features of the corresponding input sentences .",Model,Convolution module,text-classification,4,57,0.6404494382022472,104,0.4705882352941176,13,0.325,1,0
106,"Similar to the setup in Section 3.2 , we also employ a two - step process to produce the filters .",Model,Convolution module,text-classification,4,58,0.651685393258427,105,0.4751131221719457,14,0.35,1,0
107,"For a question Q , the generating process is :",Model,Convolution module,text-classification,4,59,0.6629213483146067,106,0.4796380090497738,15,0.375,1,0
108,"where CNN and DCNN denote the basic CNN unit and deconvolution layer , respectively , as discussed in Section 2.1 .",Model,Convolution module,text-classification,4,60,0.6741573033707865,107,0.4841628959276018,16,0.4,1,0
109,Parameters ?,Model,Convolution module,text-classification,4,61,0.6853932584269663,108,0.4886877828054298,17,0.425,1,0
110,q e and ?,Model,Convolution module,text-classification,4,62,0.6966292134831461,109,0.4932126696832579,18,0.45,1,0
111,q dare to be learned .,Model,Convolution module,text-classification,4,63,0.7078651685393258,110,0.497737556561086,19,0.475,1,0
112,"The same process can be utilized to produce encodings z a and filters fa for the answer input , A , with parameters ?",Model,Convolution module,text-classification,4,64,0.7191011235955056,111,0.502262443438914,20,0.5,1,0
113,a e and ?,Model,Convolution module,text-classification,4,65,0.7303370786516854,112,0.5067873303167421,21,0.525,1,0
114,"ad , respectively .",Model,Convolution module,text-classification,4,66,0.7415730337078652,113,0.5113122171945701,22,0.55,1,0
115,"The two sets of filter weights are then passed to adaptive convolution modules , along with Q and A , to obtain the extracted question and answer embeddings .",Model,Convolution module,text-classification,4,67,0.7528089887640449,114,0.5158371040723982,23,0.575,1,0
116,"That is , the question embedding is convolved with the filters produced by the answer and vise versa (? q and ?",Model,Convolution module,text-classification,4,68,0.7640449438202247,115,0.5203619909502263,24,0.6,1,0
117,a are the bias terms to be learned ) .,Model,Convolution module,text-classification,4,69,0.7752808988764045,116,0.5248868778280543,25,0.625,1,0
118,The key idea is to abstract informa-tion from the answer ( or question ) that is pertinent to the corresponding question ( or answer ) .,Model,Convolution module,text-classification,4,70,0.7865168539325843,117,0.5294117647058824,26,0.65,1,0
119,"Compared to a Siamese CNN architecture , our model selectively encapsulates the most important features for judgement prediction , removing less vital information .",Model,Convolution module,text-classification,4,71,0.797752808988764,118,0.5339366515837104,27,0.675,1,0
120,We then employ the question and answer representations h q ?,Model,Convolution module,text-classification,4,72,0.8089887640449438,119,0.5384615384615384,28,0.7,1,0
121,"Rn h , ha ?",Model,Convolution module,text-classification,4,73,0.8202247191011236,120,0.5429864253393665,29,0.725,1,0
122,Rn h as inputs to the matching module ( where n h is the dimension of question / answer embeddings ) .,Model,Convolution module,text-classification,4,74,0.8314606741573034,121,0.5475113122171946,30,0.75,1,0
123,"Following , the matching function is defined as :",Model,Convolution module,text-classification,4,75,0.8426966292134831,122,0.5520361990950227,31,0.775,1,0
124,where ?,Model,Convolution module,text-classification,4,76,0.8539325842696629,123,0.5565610859728507,32,0.8,1,0
125,"and denote an element - wise subtraction and element - wise product , respectively .",Model,Convolution module,text-classification,4,77,0.8651685393258427,124,0.5610859728506787,33,0.825,1,0
126,[h a ; h b ] indicates that ha and h bare stacked as column vectors .,Model,Convolution module,text-classification,4,78,0.8764044943820225,125,0.5656108597285068,34,0.85,1,0
127,The resulting matching vector t ?,Model,Convolution module,text-classification,4,79,0.8876404494382022,126,0.5701357466063348,35,0.875,1,0
128,R 4n h is then sent through an MLP layer ( with sigmoid activation function and parameters ?,Model,Convolution module,text-classification,4,80,0.898876404494382,127,0.5746606334841629,36,0.9,1,0
129,"to be learned ) to model the desired conditional distribution p ( y i = 1|h q , ha ) .",Model,Convolution module,text-classification,4,81,0.9101123595505618,128,0.579185520361991,37,0.925,1,0
130,"Notably , we share the weights of filter generating networks for both the question and answer , so that the model adaptivity for answer selection can be improved without an excessive increase in the number of parameters .",Model,Convolution module,text-classification,4,82,0.9213483146067416,129,0.583710407239819,38,0.95,1,0
131,All three modules in AdaQA model are jointly trained end - to - end .,Model,Convolution module,text-classification,4,83,0.9325842696629212,130,0.5882352941176471,39,0.975,1,0
132,"Note that the AdaQA model proposed can be readily adapted to other sentence matching tasks , such as paraphrase identification ( see Section 5.2 ) .",Model,Convolution module,text-classification,4,84,0.9438202247191012,131,0.5927601809954751,40,1.0,1,0
133,Connections to attention mechanism,Model,Convolution module,text-classification,4,85,0.9550561797752808,132,0.5972850678733032,0,0.0,1,0
134,"The adaptive context - sensitive filter generation mechanism proposed here bears close resemblance to attention mechanism ) widely adopted in the NLP community , in the sense that both methods intend to incorporate rich contextual information into text representations .",Model,Convolution module,text-classification,4,86,0.9662921348314608,133,0.6018099547511312,1,0.25,1,0
135,"However , attention is typically operated on top of the hidden units preprocessed by CNN or LSTM layers , and assigns different weights to each unit according to a context vector .",Model,Convolution module,text-classification,4,87,0.9775280898876404,134,0.6063348416289592,2,0.5,1,0
136,"By contrast , in our context - sensitive filter generation mechanism , the contextual information is inherently encoded into the convolutional filters , which directly interact with the input sentence during the convolution encoding operation .",Model,Convolution module,text-classification,4,88,0.9887640449438202,135,0.6108597285067874,3,0.75,1,0
137,"Notably , according to our experiments , the proposed filter generation module can be readily combined with ( standard ) attention mechanisms to further enhance the modeling expressiveness of CNN encoder .",Model,Convolution module,text-classification,4,89,1.0,136,0.6153846153846154,4,1.0,1,0
138,Experimental Setup,,,text-classification,4,0,0.0,137,0.6199095022624435,0,0.0,1,0
139,Datasets,Experimental Setup,,text-classification,4,1,0.0833333333333333,138,0.6244343891402715,1,0.0303030303030303,1,0
140,We investigate the effectiveness of the proposed ACNN framework on both document classification and text sequence matching tasks .,Experimental Setup,Datasets,text-classification,4,2,0.1666666666666666,139,0.6289592760180995,2,0.0606060606060606,1,0
141,"Specifically , we consider two large - scale document classification datasets :",Experimental Setup,Datasets,text-classification,4,3,0.25,140,0.6334841628959276,3,0.0909090909090909,1,0
142,"Yelp Reviews Polarity , and DBPedia ontology datasets .",Experimental Setup,Datasets,text-classification,4,4,0.3333333333333333,141,0.6380090497737556,4,0.1212121212121212,1,0
143,"For Yelp reviews , we seek to predict a binary label ( positive or negative ) regarding one review about a restaurant .",Experimental Setup,Datasets,text-classification,4,5,0.4166666666666667,142,0.6425339366515838,5,0.1515151515151515,1,0
144,"DBpedia is extracted from Wikipedia by crowd - sourcing and is categorized into 14 non-overlapping ontology classes , including Company , Athlete , Natural Place , etc .",Experimental Setup,Datasets,text-classification,4,6,0.5,143,0.6470588235294118,6,0.1818181818181818,1,0
145,"We sample 15 % of the training data as the validation set , to select hyperparameters for our models and perform early stopping .",Experimental Setup,Datasets,text-classification,4,7,0.5833333333333334,144,0.6515837104072398,7,0.2121212121212121,1,0
146,"For sentence matching , we evaluate the AdaQA model on two datasets for open - domain question answering : Wiki QA and SelQA .",Experimental Setup,Datasets,text-classification,4,8,0.6666666666666666,145,0.6561085972850679,8,0.2424242424242424,1,0
147,"Given a question , the task is to rank the corresponding candidate answers , which , in the case of WikiQA , are sentences extracted from the summary section of a related Wikipedia article .",Experimental Setup,Datasets,text-classification,4,9,0.75,146,0.6606334841628959,9,0.2727272727272727,1,0
148,"To facilitate comparison with existing results , we truncate the candidate answers to a maximum length of 40 tokens for all experiments on the WikiQA dataset .",Experimental Setup,Datasets,text-classification,4,10,0.8333333333333334,147,0.665158371040724,10,0.303030303030303,1,0
149,"We also consider the task of paraphrase identification with the Quora Question Pairs dataset , with the same data splits as in .",Experimental Setup,Datasets,text-classification,4,11,0.9166666666666666,148,0.669683257918552,11,0.3333333333333333,1,0
150,A summary of all datasets is presented in .,Experimental Setup,Datasets,text-classification,4,12,1.0,149,0.6742081447963801,12,0.3636363636363636,1,0
151,Training Details,,,text-classification,4,0,0.0,150,0.6787330316742082,13,0.3939393939393939,1,0
152,"For the document classification experiments , we randomly initialize the word embeddings uniformly within [ ? 0.001 , 0.001 ] and update them during training .",Training Details,Training Details,text-classification,4,1,0.0769230769230769,151,0.6832579185520362,14,0.4242424242424242,1,1
153,"For the generated filters , we set the window size as h = 5 , with K = 100 feature maps ( the dimension of z is set as 100 ) .",Training Details,Training Details,text-classification,4,2,0.1538461538461538,152,0.6877828054298643,15,0.4545454545454545,1,1
154,"For direct comparison , we employ the same filter shape / size settings as in our basic CNN implementation .",Training Details,Training Details,text-classification,4,3,0.2307692307692307,153,0.6923076923076923,16,0.4848484848484848,1,0
155,"A one - layer architec - ture is utilized for both the CNN baseline and the ACNN model , since we did not observe significant performance gains with a multilayer architecture .",Training Details,Training Details,text-classification,4,4,0.3076923076923077,154,0.6968325791855203,17,0.5151515151515151,1,1
156,"The minibatch size is set as 128 , and a dropout rate of 0.2 is utilized on the embedding layer .",Training Details,Training Details,text-classification,4,5,0.3846153846153846,155,0.7013574660633484,18,0.5454545454545454,1,1
157,"We observed that a larger dropout rate ( e.g. , 0.5 ) will hurt performance on document classifications and make training significantly slower .",Training Details,Training Details,text-classification,4,6,0.4615384615384615,156,0.7058823529411765,19,0.5757575757575758,1,0
158,"For the sentence matching tasks , we initialized the word embeddings with 50 - dimensional Glove word vectors pretrained from Wikipedia 2014 and Gigaword 5 for all model variants .",Training Details,Training Details,text-classification,4,7,0.5384615384615384,157,0.7104072398190046,20,0.6060606060606061,1,1
159,"As for the filters , we set the window size as h = 5 , with K = 300 feature maps .",Training Details,Training Details,text-classification,4,8,0.6153846153846154,158,0.7149321266968326,21,0.6363636363636364,1,1
160,"As described in Section 3.3 , the vector t , output from the matching module , is fed to the prediction layer , implemented as a one - layer MLP followed by the sigmoid function .",Training Details,Training Details,text-classification,4,9,0.6923076923076923,159,0.7194570135746606,22,0.6666666666666666,1,0
161,"We use Adam to train the models , with a learning rate of 3 10 ?4 .",Training Details,Training Details,text-classification,4,10,0.7692307692307693,160,0.7239819004524887,23,0.696969696969697,1,1
162,"Dropout , with a rate of 0.5 , is employed on the word embedding layer .",Training Details,Training Details,text-classification,4,11,0.8461538461538461,161,0.7285067873303167,24,0.7272727272727273,1,1
163,The hyperparameters are selected by choosing the best model on the validation set .,Training Details,Training Details,text-classification,4,12,0.9230769230769232,162,0.7330316742081447,25,0.7575757575757576,1,0
164,All models are implemented with TensorFlow and are trained using one NVIDIA GeForce GTX TITAN X GPU with 12 GB memory .,Training Details,Training Details,text-classification,4,13,1.0,163,0.7375565610859729,26,0.7878787878787878,1,1
165,Baselines,,,text-classification,4,0,0.0,164,0.7420814479638009,27,0.8181818181818182,1,0
166,"For document classification , we consider several baseline models : ( i ) ngrams , a bag - of - means method based on TFIDF representations built by choosing the 500,000 most frequent n-grams ( up to 5 - grams ) from the training set and use their corresponding counts as features ; ( ii ) small / large word CNN : 6 layer word - based convolutional networks , with 256/1024 features at each layer , denoted as small / large , respectively ; ( iii ) deep CNN : deep convolutional neural networks with 9/17 /29 layers .",Baselines,Baselines,text-classification,4,1,0.5,165,0.746606334841629,28,0.8484848484848485,1,1
167,"To evaluate the effectiveness of proposed AdaQA model , we compare it with several CNN - based sequence matching baselines , including Vanilla CNN , attentive pooling networks , and ABCNN ( where an attention mechanism is employed over the two sentence representations ) .",Baselines,Baselines,text-classification,4,2,1.0,166,0.751131221719457,29,0.8787878787878788,1,0
168,Evaluation Metrics,,,text-classification,4,0,0.0,167,0.755656108597285,30,0.9090909090909092,1,0
169,"For document categorization and paraphrase identification tasks , we em - , are reported by , and are reported by .",Evaluation Metrics,Evaluation Metrics,text-classification,4,1,0.3333333333333333,168,0.7601809954751131,31,0.9393939393939394,1,0
170,ploy the percentage of correct predictions on the test set to evaluate and compare different models .,Evaluation Metrics,Evaluation Metrics,text-classification,4,2,0.6666666666666666,169,0.7647058823529411,32,0.9696969696969696,1,0
171,"For the answer sentence selection task , mean average precision ( MAP ) and mean reciprocal rank ( MRR ) are utilized as the corresponding evaluation metrics .",Evaluation Metrics,Evaluation Metrics,text-classification,4,3,1.0,170,0.7692307692307693,33,1.0,1,0
172,Experimental Results,,,text-classification,4,0,0.0,171,0.7737556561085973,0,0.0,1,0
173,Document Classification,Experimental Results,,text-classification,4,1,0.0204081632653061,172,0.7782805429864253,0,0.0,1,0
174,"To explicitly explore whether our ACNN model can leverage the input-aware filter weights for better sentence representation , we perform a comparison between the basic CNN and ACNN models with only a single filter , which are denoted as S - CNN , S - ACNN , respectively ( this setting may not yield best overall performance , since only a single filter is used , but it allows us to isolate the impact of adaptivity ) .",Experimental Results,Document Classification,text-classification,4,2,0.0408163265306122,173,0.7828054298642534,1,0.0714285714285714,1,0
175,"As illustrated in , S - ACNN significantly outperforms S - CNN on both datasets , demonstrating the advantage of the filtergeneration module in our ACNN framework .",Experimental Results,Document Classification,text-classification,4,3,0.0612244897959183,174,0.7873303167420814,2,0.1428571428571428,1,1
176,"As a result , with only one convolutional filter and thus very limited modeling capacity , our S - ACNN model tends to be much more expressive than the basic CNN model , due to the flexibility of applying different filters to different sentences .",Experimental Results,Document Classification,text-classification,4,4,0.0816326530612244,175,0.7918552036199095,3,0.2142857142857142,1,0
177,We further experiment on both ACNN and CNN models with multiple filters .,Experimental Results,Document Classification,text-classification,4,5,0.1020408163265306,176,0.7963800904977375,4,0.2857142857142857,1,0
178,The corresponding document categorization accuracies are presented in .,Experimental Results,Document Classification,text-classification,4,6,0.1224489795918367,177,0.8009049773755657,5,0.3571428571428571,1,0
179,"Although we only use one convolution layer for our ACNN model , it already outperforms other CNN baseline methods with much deeper architectures .",Experimental Results,Document Classification,text-classification,4,7,0.1428571428571428,178,0.8054298642533937,6,0.4285714285714285,1,1
180,"Moreover , our method exhibits higher accuracy than n-grams , which is a very strong baseline as shown in .",Experimental Results,Document Classification,text-classification,4,8,0.1632653061224489,179,0.8099547511312217,7,0.5,1,0
181,We attribute the superior performance of the ACNN framework to its stronger ( adaptive ) feature - extraction ability .,Experimental Results,Document Classification,text-classification,4,9,0.1836734693877551,180,0.8144796380090498,8,0.5714285714285714,1,0
182,"Moreover , our M - ACNN also achieves slightly better performance than self - attentive sentence embeddings proposed in , which requires significant more parameters than our method .",Experimental Results,Document Classification,text-classification,4,10,0.2040816326530612,181,0.8190045248868778,9,0.6428571428571429,1,1
183,Effect of number of filters,Experimental Results,Document Classification,text-classification,4,11,0.2244897959183673,182,0.8235294117647058,10,0.7142857142857143,1,0
184,"To further demonstrate that the performance gains in document categorization experiments originates from the improved adaptivity of our ACNN framework , we implement the basic CNN model with different numbers of filter sizes , ranging from 1 to 1000 .",Experimental Results,Document Classification,text-classification,4,12,0.2448979591836734,183,0.8280542986425339,11,0.7857142857142857,1,0
185,"As illustrated in ( a ) , when the filter size is larger than 100 , the test accuracy of the standard CNN model does not show any noticeable improvement with more filters .",Experimental Results,Document Classification,text-classification,4,13,0.2653061224489796,184,0.832579185520362,12,0.8571428571428571,1,0
186,"More importantly , even with a filter size of 1000 , the classification accuracy of the CNN is worse than that of the ACNN model with the filter number restricted to 100 .",Experimental Results,Document Classification,text-classification,4,14,0.2857142857142857,185,0.8371040723981901,13,0.9285714285714286,1,0
187,"Given these observations , we believe that the boosted categorization accuracy does come from the improved flexibility and thus better feature extraction of our ACNN framework .",Experimental Results,Document Classification,text-classification,4,15,0.3061224489795918,186,0.8416289592760181,14,1.0,1,0
188,Answer Sentence Selection,Experimental Results,,text-classification,4,16,0.3265306122448979,187,0.8461538461538461,0,0.0,1,0
189,"To elucidate the role of different parts ( modules ) in our AdaQA model , we implement several model variants for comparison : ( i ) a "" vanilla "" CNN model that independently encodes two sentence representations for matching ; ( ii ) a self - adaptive , and marked with are from .",Experimental Results,Answer Sentence Selection,text-classification,4,17,0.3469387755102041,188,0.8506787330316742,1,0.0555555555555555,1,0
190,"ACNN - based model where the question / answer sentence generates adaptive filters only to convolve with the input itself ; ( iii ) a one - way ACNN model where only the answer sentence representation is extracted with adaptive filters , which are generated conditioned on the question ; ( iv ) a two - way AdaQA model as described in Section 2.4 , where both sentences are adaptively encoded , with filters generated conditioned on the other sequence ; ( v ) considering that the proposed filter generation mechanism is complementary to the attention layer typically employed in sequence matching tasks ( see Section 3.4 ) , we experiment with another model variant that combines the proposed context - sensitive filter generation mechanism with the multi-perspective attention layer introduced in .",Experimental Results,Answer Sentence Selection,text-classification,4,18,0.3673469387755102,189,0.8552036199095022,2,0.1111111111111111,1,0
191,"show experimental results of our models on WikiQA and Se lQA datasets , along with other state - of - the - art methods .",Experimental Results,Answer Sentence Selection,text-classification,4,19,0.3877551020408163,190,0.8597285067873304,3,0.1666666666666666,1,0
192,"Note that the self - adaptive ACNN model variant , which generates filters only for the input itself ( without any interactions before the top matching module ) , slightly outperforms the vanilla CNN Siamese model .",Experimental Results,Answer Sentence Selection,text-classification,4,20,0.4081632653061224,191,0.8642533936651584,4,0.2222222222222222,1,0
193,"Combined with the results in document categorization experiments , we believe that our ACNN framework , in its simplest form , can be utilized as a powerful feature extractor for transforming natural language sentences into fixed - length vectors .",Experimental Results,Answer Sentence Selection,text-classification,4,21,0.4285714285714285,192,0.8687782805429864,5,0.2777777777777778,1,0
194,"More importantly , our two - way AdaQA model exhibits superior results compared with the one - way variant as well as other CNN - based baseline models on the WikiQA dataset .",Experimental Results,Answer Sentence Selection,text-classification,4,22,0.4489795918367347,193,0.8733031674208145,6,0.3333333333333333,1,0
195,This observation indicates that the bidirectional filter gener - Model Accuracy Siamese - CNN 0.7960,Experimental Results,Answer Sentence Selection,text-classification,4,23,0.4693877551020408,194,0.8778280542986425,7,0.3888888888888889,1,0
196,Multi-Perspective-CNN 0 . 8138 AdaQA ( two-way ) 0.8516 AdaQA ( two-way ) + att. 0.8794 ation mechanism is strongly associated with the performance gains .,Experimental Results,Answer Sentence Selection,text-classification,4,24,0.4897959183673469,195,0.8823529411764706,8,0.4444444444444444,1,0
197,"While combined with the multi-perspective attention layers , adopted after the ACNN encoding layer , our two - way AdaQA model achieves even better performance .",Experimental Results,Answer Sentence Selection,text-classification,4,25,0.5102040816326531,196,0.8868778280542986,9,0.5,1,0
198,"This suggests that the proposed strategy is complementary , in terms of the incorporation of rich contextual information , to the standard attention mechanism .",Experimental Results,Answer Sentence Selection,text-classification,4,26,0.5306122448979592,197,0.8914027149321267,10,0.5555555555555556,1,0
199,"The same trend is also observed on the SelQA dataset ( as shown in ) , which is a much larger dataset than Wiki QA .",Experimental Results,Answer Sentence Selection,text-classification,4,27,0.5510204081632653,198,0.8959276018099548,11,0.6111111111111112,1,0
200,"Notably , our model yields significantly better results than an attentive pooling network and ABCNN ( attention - based CNN ) baselines .",Experimental Results,Answer Sentence Selection,text-classification,4,28,0.5714285714285714,199,0.9004524886877828,12,0.6666666666666666,1,1
201,"We attribute the improvement to two potential advantages of our AdaQA model : ( i ) for the two previous baseline methods , the interaction between question and answer takes place either before or after convolution .",Experimental Results,Answer Sentence Selection,text-classification,4,29,0.5918367346938775,200,0.9049773755656108,13,0.7222222222222222,1,0
202,"However , in our AdaQA model , the communication between two sentences is inherent in the convolution operation , and thus can provide the abstracted features with more flexibility ; ( ii ) the bidirectional filter generation mechanism in our AdaQA model generates co-dependent representations for the question and candidate answer , which could enable the model to recover from initial local maxima corresponding to incorrect predictions .",Experimental Results,Answer Sentence Selection,text-classification,4,30,0.6122448979591837,201,0.9095022624434388,14,0.7777777777777778,1,0
203,Paragraph Identification,Experimental Results,,text-classification,4,31,0.6326530612244898,202,0.9140271493212668,15,0.8333333333333334,1,0
204,"Considering that the proposed AdaQA model can be readily generalized to other text sequence matching problems , we further evaluate the proposed framework on the paraphrase identification task with the Quora question pairs dataset .",Experimental Results,Paragraph Identification,text-classification,4,32,0.6530612244897959,203,0.918552036199095,16,0.8888888888888888,1,0
205,"To ensure a fair comparison , we employ the same data splits as in .",Experimental Results,Paragraph Identification,text-classification,4,33,0.673469387755102,204,0.9230769230769232,17,0.9444444444444444,1,0
206,"As illustrated in , our twoway AdaQA model again exhibits superior performances compared with basic CNN models ( as reported in ) .",Experimental Results,Paragraph Identification,text-classification,4,34,0.6938775510204082,205,0.9276018099547512,18,1.0,1,0
207,Discussion,Experimental Results,,text-classification,4,35,0.7142857142857143,206,0.9321266968325792,0,0.0,1,0
208,Reasoning ability,Experimental Results,,text-classification,4,36,0.7346938775510204,207,0.9366515837104072,1,0.1666666666666666,1,0
209,"To associate the improved answer sentence selection results with the reasoning capabilities of our AdaQA model , we further categorize the questions in the WikiQA test set into 5 types containing : ' What ' , ' Where ' , ' How ' , ' When ' or ' Who ' .",Experimental Results,Reasoning ability,text-classification,4,37,0.7551020408163265,208,0.9411764705882352,2,0.3333333333333333,1,0
210,We then calculate the MAP scores of the basic CNN and our AdaQA model on different question types .,Experimental Results,Reasoning ability,text-classification,4,38,0.7755102040816326,209,0.9457013574660632,3,0.5,1,0
211,"Similar to the findings in , we observe that the ' How ' question is the hardest to answer , with the lowest MAP scores .",Experimental Results,Reasoning ability,text-classification,4,39,0.7959183673469388,210,0.9502262443438914,4,0.6666666666666666,1,0
212,"However , our AdaQA model improves most over the basic CNN on the ' How ' type question , see ( b ) .",Experimental Results,Reasoning ability,text-classification,4,40,0.8163265306122449,211,0.9547511312217196,5,0.8333333333333334,1,0
213,"Further comparing our results with NASM in , our AdaQA model ( with a MAP score of 0.579 ) outperforms their reported ' How ' question MAP scores ( 0.524 ) by a large margin , indicating that the adaptive convolutional filter - generation mechanism improves the model 's ability to read and reason over natural language sentences .",Experimental Results,Reasoning ability,text-classification,4,41,0.8367346938775511,212,0.9592760180995475,6,1.0,1,0
214,Filter visualization,Experimental Results,,text-classification,4,42,0.8571428571428571,213,0.9638009049773756,0,0.0,1,0
215,"To better understand what information has been encoded into our contextsensitive filters , we visualize one of the filters for sentences within the test set ( on the DBpedia dataset ) with t- SNE .",Experimental Results,Filter visualization,text-classification,4,43,0.8775510204081632,214,0.9683257918552036,1,0.1428571428571428,1,0
216,The corresponding results are shown in ( c ) .,Experimental Results,Filter visualization,text-classification,4,44,0.8979591836734694,215,0.9728506787330315,2,0.2857142857142857,1,0
217,"It can be observed that the filters for documents with the same label ( ontology ) are grouped into clusters , indicating that for different types of document , ACNN has leveraged distinct convolutional filters for better feature extraction .",Experimental Results,Filter visualization,text-classification,4,45,0.9183673469387756,216,0.9773755656108596,3,0.4285714285714285,1,0
218,"We presented a context - sensitive convolutional filter - generation mechanism , introducing a meta network to adaptively produce a set of input -aware filters .",Experimental Results,Filter visualization,text-classification,4,46,0.9387755102040816,217,0.9819004524886876,4,0.5714285714285714,1,0
219,"In this manner , the filter weights vary from sample to sample , providing the CNN encoder network with more modeling flexibility and capacity .",Experimental Results,Filter visualization,text-classification,4,47,0.9591836734693876,218,0.986425339366516,5,0.7142857142857143,1,0
220,"This framework is further generalized to model question - answer sentence pairs , leveraging a twoway feature abstraction process .",Experimental Results,Filter visualization,text-classification,4,48,0.979591836734694,219,0.990950226244344,6,0.8571428571428571,1,0
221,"We evaluate our models on several document - categorization and sentence matching benchmarks , and they consistently outperform the standard CNN and attentionbased CNN baselines , demonstrating the effectiveness of our framework .",Experimental Results,Filter visualization,text-classification,4,49,1.0,220,0.995475113122172,7,1.0,1,0
1,title,,,text-classification,5,0,0.0,0,0.0,0,0.0,1,0
2,Universal Language Model Fine - tuning for Text Classification,title,title,text-classification,5,1,0.0,1,0.0039682539682539,1,0.0,1,1
3,abstract,,,text-classification,5,0,0.0,2,0.0079365079365079,0,0.0,1,0
4,"Inductive transfer learning has greatly impacted computer vision , but existing approaches in NLP still require task - specific modifications and training from scratch .",abstract,abstract,text-classification,5,1,0.2,3,0.0119047619047619,1,0.2,1,0
5,"We propose Universal Language Model Fine - tuning ( ULMFiT ) , an effective transfer learning method that can be applied to any task in NLP , and introduce techniques that are key for fine - tuning a language model .",abstract,abstract,text-classification,5,2,0.4,4,0.0158730158730158,2,0.4,1,0
6,"Our method significantly outperforms the state - of - the - art on six text classification tasks , reducing the error by 18 - 24 % on the majority of datasets .",abstract,abstract,text-classification,5,3,0.6,5,0.0198412698412698,3,0.6,1,0
7,"Furthermore , with only 100 labeled examples , it matches the performance of training from scratch on 100 more data .",abstract,abstract,text-classification,5,4,0.8,6,0.0238095238095238,4,0.8,1,0
8,We opensource our pretrained models and code 1 .,abstract,,text-classification,5,5,1.0,7,0.0277777777777777,5,1.0,1,0
9,Introduction,,,text-classification,5,0,0.0,8,0.0317460317460317,0,0.0,1,0
10,Inductive transfer learning has had a large impact on computer vision ( CV ) .,Introduction,Introduction,text-classification,5,1,0.0384615384615384,9,0.0357142857142857,1,0.0384615384615384,1,0
11,"Applied CV models ( including object detection , classification , and segmentation ) are rarely trained from scratch , but instead are fine - tuned from models that have been pretrained on ImageNet , MS - COCO , and other datasets .",Introduction,Introduction,text-classification,5,2,0.0769230769230769,10,0.0396825396825396,2,0.0769230769230769,1,0
12,"Text classification is a category of Natural Language Processing ( NLP ) tasks with real - world applications such as spam , fraud , and bot detection , emergency response , and commercial document classification , such as for legal discovery .",Introduction,Introduction,text-classification,5,3,0.1153846153846153,11,0.0436507936507936,3,0.1153846153846153,1,0
13,1 http://nlp.fast.ai/ulmfit.,Introduction,Introduction,text-classification,5,4,0.1538461538461538,12,0.0476190476190476,4,0.1538461538461538,1,0
14,Equal contribution .,Introduction,,text-classification,5,5,0.1923076923076923,13,0.0515873015873015,5,0.1923076923076923,1,0
15,"Jeremy focused on the algorithm development and implementation , Sebastian focused on the experiments and writing .",Introduction,Equal contribution .,text-classification,5,6,0.2307692307692307,14,0.0555555555555555,6,0.2307692307692307,1,0
16,"While Deep Learning models have achieved state - of - the - art on many NLP tasks , these models are trained from scratch , requiring large datasets , and days to converge .",Introduction,Equal contribution .,text-classification,5,7,0.2692307692307692,15,0.0595238095238095,7,0.2692307692307692,1,0
17,Research in NLP focused mostly on transductive transfer .,Introduction,,text-classification,5,8,0.3076923076923077,16,0.0634920634920634,8,0.3076923076923077,1,0
18,"For inductive transfer , fine - tuning pretrained word embeddings , a simple transfer technique that only targets a model 's first layer , has had a large impact in practice and is used inmost state - of - the - art models .",Introduction,Research in NLP focused mostly on transductive transfer .,text-classification,5,9,0.3461538461538461,17,0.0674603174603174,9,0.3461538461538461,1,0
19,"Recent approaches that concatenate embeddings derived from other tasks with the input at different layers ) still train the main task model from scratch and treat pretrained embeddings as fixed parameters , limiting their usefulness .",Introduction,Research in NLP focused mostly on transductive transfer .,text-classification,5,10,0.3846153846153846,18,0.0714285714285714,10,0.3846153846153846,1,0
20,"In light of the benefits of pretraining , we should be able to do better than randomly initializing the remaining parameters of our models .",Introduction,Research in NLP focused mostly on transductive transfer .,text-classification,5,11,0.4230769230769231,19,0.0753968253968253,11,0.4230769230769231,1,0
21,"However , inductive transfer via finetuning has been unsuccessful for NLP .",Introduction,Research in NLP focused mostly on transductive transfer .,text-classification,5,12,0.4615384615384615,20,0.0793650793650793,12,0.4615384615384615,1,0
22,"first proposed finetuning a language model ( LM ) but require millions of in - domain documents to achieve good performance , which severely limits its applicability .",Introduction,Research in NLP focused mostly on transductive transfer .,text-classification,5,13,0.5,21,0.0833333333333333,13,0.5,1,0
23,We show that not the idea of LM fine - tuning but our lack of knowledge of how to train them effectively has been hindering wider adoption .,Introduction,Research in NLP focused mostly on transductive transfer .,text-classification,5,14,0.5384615384615384,22,0.0873015873015873,14,0.5384615384615384,1,0
24,LMs overfit to small datasets and suffered catastrophic forgetting when fine - tuned with a classifier .,Introduction,Research in NLP focused mostly on transductive transfer .,text-classification,5,15,0.5769230769230769,23,0.0912698412698412,15,0.5769230769230769,1,0
25,"Compared to CV , NLP models are typically more shallow and thus require different fine - tuning methods .",Introduction,Research in NLP focused mostly on transductive transfer .,text-classification,5,16,0.6153846153846154,24,0.0952380952380952,16,0.6153846153846154,1,0
26,"We propose anew method , Universal Language Model Fine - tuning ( ULMFiT ) that addresses these issues and enables robust inductive transfer learning for any NLP task , akin to fine - tuning Image Net models :",Introduction,Research in NLP focused mostly on transductive transfer .,text-classification,5,17,0.6538461538461539,25,0.0992063492063492,17,0.6538461538461539,1,0
27,The same 3 - layer LSTM architecturewith the same hyperparameters and no additions other than tuned dropout hyperparametersoutperforms highly engineered models and trans - fer learning approaches on six widely studied text classification tasks .,Introduction,Research in NLP focused mostly on transductive transfer .,text-classification,5,18,0.6923076923076923,26,0.1031746031746031,18,0.6923076923076923,1,0
28,"On IMDb , with 100 labeled examples , ULMFiT matches the performance of training from scratch with 10 and - given 50 k unlabeled examples - with 100 more data .",Introduction,Research in NLP focused mostly on transductive transfer .,text-classification,5,19,0.7307692307692307,27,0.1071428571428571,19,0.7307692307692307,1,0
29,Contributions,Introduction,,text-classification,5,20,0.7692307692307693,28,0.1111111111111111,20,0.7692307692307693,1,0
30,Our contributions are the following :,Introduction,Contributions,text-classification,5,21,0.8076923076923077,29,0.115079365079365,21,0.8076923076923077,1,0
31,"1 ) We propose Universal Language Model Fine - tuning ( ULMFiT ) , a method that can be used to achieve CV - like transfer learning for any task for NLP .",Introduction,Contributions,text-classification,5,22,0.8461538461538461,30,0.119047619047619,22,0.8461538461538461,1,1
32,"2 ) We propose discriminative fine - tuning , slanted triangular learning rates , and gradual unfreezing , novel techniques to retain previous knowledge and avoid catastrophic forgetting during fine - tuning .",Introduction,Contributions,text-classification,5,23,0.8846153846153846,31,0.123015873015873,23,0.8846153846153846,1,1
33,"3 ) We significantly outperform the state - of - the - art on six representative text classification datasets , with an error reduction of 18 - 24 % on the majority of datasets .",Introduction,Contributions,text-classification,5,24,0.9230769230769232,32,0.1269841269841269,24,0.9230769230769232,1,0
34,4 ) We show that our method enables extremely sample - efficient transfer learning and perform an extensive ablation analysis .,Introduction,Contributions,text-classification,5,25,0.9615384615384616,33,0.1309523809523809,25,0.9615384615384616,1,0
35,5 ) We make the pretrained models and our code available to enable wider adoption .,Introduction,Contributions,text-classification,5,26,1.0,34,0.1349206349206349,26,1.0,1,0
36,Related work,,,text-classification,5,0,0.0,35,0.1388888888888889,0,0.0,1,0
37,Transfer learning in CV Features in deep neural networks in CV have been observed to transition from general to task - specific from the first to the last layer .,Related work,Related work,text-classification,5,1,0.009090909090909,36,0.1428571428571428,1,0.009090909090909,0,0
38,"For this reason , most work in CV focuses on transferring the first layers of the model .",Related work,Related work,text-classification,5,2,0.0181818181818181,37,0.1468253968253968,2,0.0181818181818181,0,0
39,Sharif achieve state - of - theart results using features of an Image Net model as input to a simple classifier .,Related work,Related work,text-classification,5,3,0.0272727272727272,38,0.1507936507936507,3,0.0272727272727272,0,0
40,"In recent years , this approach has been superseded by fine - tuning either the last or several of the last layers of a pretrained model and leaving the remaining layers frozen .",Related work,Related work,text-classification,5,4,0.0363636363636363,39,0.1547619047619047,4,0.0363636363636363,0,0
41,Hypercolumns,Related work,,text-classification,5,5,0.0454545454545454,40,0.1587301587301587,5,0.0454545454545454,0,0
42,"In NLP , only recently have methods been proposed that go beyond transferring word embeddings .",Related work,Hypercolumns,text-classification,5,6,0.0545454545454545,41,0.1626984126984127,6,0.0545454545454545,0,0
43,The prevailing approach is to pretrain embeddings that capture additional context via other tasks .,Related work,Hypercolumns,text-classification,5,7,0.0636363636363636,42,0.1666666666666666,7,0.0636363636363636,0,0
44,"Embeddings at different levels are then used as features , concatenated either with the word embeddings or with the inputs at intermediate layers .",Related work,Hypercolumns,text-classification,5,8,0.0727272727272727,43,0.1706349206349206,8,0.0727272727272727,0,0
45,"This method is known as hypercolumns in CV 2 and is used by , who use language modeling , paraphrasing , entailment , and Machine Translation ( MT ) respectively for pretraining .",Related work,Hypercolumns,text-classification,5,9,0.0818181818181818,44,0.1746031746031746,9,0.0818181818181818,0,0
46,"require engineered custom architectures , while we show state - of - the - art performance with the same basic architecture across a range of tasks .",Related work,Hypercolumns,text-classification,5,10,0.0909090909090909,45,0.1785714285714285,10,0.0909090909090909,0,0
47,"In CV , hypercolumns have been nearly entirely superseded by end - to - end fine - tuning .",Related work,Hypercolumns,text-classification,5,11,0.1,46,0.1825396825396825,11,0.1,0,0
48,Multi - task learning,Related work,,text-classification,5,12,0.109090909090909,47,0.1865079365079365,12,0.109090909090909,0,0
49,A related direction is multi-task learning ( MTL ) .,Related work,Multi - task learning,text-classification,5,13,0.1181818181818181,48,0.1904761904761904,13,0.1181818181818181,0,0
50,This is the approach taken by and who add a language modeling objective to the model that is trained jointly with the main task model .,Related work,Multi - task learning,text-classification,5,14,0.1272727272727272,49,0.1944444444444444,14,0.1272727272727272,0,0
51,"MTL requires the tasks to be trained from scratch every time , which makes it inefficient and often requires careful weighting of the taskspecific objective functions .",Related work,Multi - task learning,text-classification,5,15,0.1363636363636363,50,0.1984126984126984,15,0.1363636363636363,0,0
52,Fine - tuning,Related work,,text-classification,5,16,0.1454545454545454,51,0.2023809523809523,16,0.1454545454545454,0,0
53,"Fine- tuning has been used successfully to transfer between similar tasks , e.g. in QA , for distantly supervised sentiment analysis , or MT domains but has been shown to fail between unrelated ones .",Related work,Fine - tuning,text-classification,5,17,0.1545454545454545,52,0.2063492063492063,17,0.1545454545454545,0,0
54,"also fine - tune a language model , but overfit with 10 k labeled examples and require millions of in - domain documents for good performance .",Related work,Fine - tuning,text-classification,5,18,0.1636363636363636,53,0.2103174603174603,18,0.1636363636363636,0,0
55,"In contrast , ULMFiT leverages general - domain pretraining and novel finetuning techniques to prevent overfitting even with only 100 labeled examples and achieves state - of the - art results also on small datasets .",Related work,Fine - tuning,text-classification,5,19,0.1727272727272727,54,0.2142857142857142,19,0.1727272727272727,0,0
56,Universal Language Model Fine- tuning,Related work,Fine - tuning,text-classification,5,20,0.1818181818181818,55,0.2182539682539682,20,0.1818181818181818,0,0
57,"We are interested in the most general inductive transfer learning setting for NLP ( Pan and Yang , 2010 ) :",Related work,Fine - tuning,text-classification,5,21,0.1909090909090909,56,0.2222222222222222,21,0.1909090909090909,0,0
58,"Given a static source task T Sand any target task T T with T S = T T , we would like to improve performance on T T .",Related work,Fine - tuning,text-classification,5,22,0.2,57,0.2261904761904762,22,0.2,0,0
59,Language modeling can be seen as the ideal source task and a counterpart of ImageNet for NLP :,Related work,Fine - tuning,text-classification,5,23,0.209090909090909,58,0.2301587301587301,23,0.209090909090909,0,0
60,"It captures many facets of language relevant for downstream tasks , such as long - term dependencies , hierarchical relations , and sentiment .",Related work,Fine - tuning,text-classification,5,24,0.2181818181818181,59,0.2341269841269841,24,0.2181818181818181,0,0
61,"In contrast to tasks like and entailment , it provides data in near- unlimited quantities for most domains and languages .",Related work,Fine - tuning,text-classification,5,25,0.2272727272727272,60,0.238095238095238,25,0.2272727272727272,0,0
62,"Additionally , a pretrained LM can be easily adapted to the idiosyncrasies of a target",Related work,Fine - tuning,text-classification,5,26,0.2363636363636363,61,0.242063492063492,26,0.2363636363636363,0,0
63,The full LM is fine - tuned on target task data using discriminative fine - tuning ( ' Discr ' ) and slanted triangular learning rates ( STLR ) to learn task - specific features .,Related work,Fine - tuning,text-classification,5,27,0.2454545454545454,62,0.246031746031746,27,0.2454545454545454,0,0
64,c),Related work,Fine - tuning,text-classification,5,28,0.2545454545454545,63,0.25,28,0.2545454545454545,0,0
65,"The classifier is fine - tuned on the target task using gradual unfreezing , ' Discr ' , and STLR to preserve low - level representations and adapt high - level ones ( shaded : unfreezing stages ; black : frozen ) .",Related work,Fine - tuning,text-classification,5,29,0.2636363636363636,64,0.2539682539682539,29,0.2636363636363636,0,0
66,"task , which we show significantly improves performance ( see Section 5 ) .",Related work,Fine - tuning,text-classification,5,30,0.2727272727272727,65,0.2579365079365079,30,0.2727272727272727,0,0
67,"Moreover , language modeling already is a key component of existing tasks such as MT and dialogue modeling .",Related work,Fine - tuning,text-classification,5,31,0.2818181818181818,66,0.2619047619047619,31,0.2818181818181818,0,0
68,"Formally , language modeling induces a hypothesis space H that should be useful for many other NLP tasks .",Related work,Fine - tuning,text-classification,5,32,0.2909090909090909,67,0.2658730158730158,32,0.2909090909090909,0,0
69,"We propose Universal Language Model Finetuning ( ULMFiT ) , which pretrains a language model ( LM ) on a large general - domain corpus and fine - tunes it on the target task using novel techniques .",Related work,Fine - tuning,text-classification,5,33,0.3,68,0.2698412698412698,33,0.3,0,0
70,The method is universal in the sense that it meets these practical criteria :,Related work,Fine - tuning,text-classification,5,34,0.3090909090909091,69,0.2738095238095238,34,0.3090909090909091,0,0
71,"1 ) It works across tasks varying in document size , number , and label type ; 2 ) it uses a single architecture and training process ; 3 ) it requires no custom feature engineering or preprocessing ; and 4 ) it does not require additional in - domain documents or labels .",Related work,Fine - tuning,text-classification,5,35,0.3181818181818182,70,0.2777777777777778,35,0.3181818181818182,0,0
72,"In our experiments , we use the state - of - theart language model AWD - LSTM , a regular LSTM ( with no attention , short - cut connections , or other sophisticated additions ) with various tuned dropout hyperparameters .",Related work,Fine - tuning,text-classification,5,36,0.3272727272727272,71,0.2817460317460317,36,0.3272727272727272,0,0
73,"Analogous to CV , we expect that downstream performance can be improved by using higherperformance language models in the future .",Related work,Fine - tuning,text-classification,5,37,0.3363636363636363,72,0.2857142857142857,37,0.3363636363636363,0,0
74,"ULMFiT consists of the following steps , which we show in : a) General - domain LM pretraining ( 3.1 ) ; b ) target task LM fine - tuning ( 3.2 ) ; and c ) target task classifier fine - tuning ( 3.3 ) .",Related work,Fine - tuning,text-classification,5,38,0.3454545454545454,73,0.2896825396825397,38,0.3454545454545454,0,0
75,We discuss these in the following sections .,Related work,,text-classification,5,39,0.3545454545454545,74,0.2936507936507936,39,0.3545454545454545,0,0
76,General - domain LM pretraining,Related work,,text-classification,5,40,0.3636363636363636,75,0.2976190476190476,40,0.3636363636363636,0,0
77,An Image Net - like corpus for language should be large and capture general properties of language .,Related work,General - domain LM pretraining,text-classification,5,41,0.3727272727272727,76,0.3015873015873015,41,0.3727272727272727,0,0
78,"We pretrain the language model on Wikitext - 103 consisting of 28,595 preprocessed Wikipedia articles and 103 million words .",Related work,General - domain LM pretraining,text-classification,5,42,0.3818181818181818,77,0.3055555555555556,42,0.3818181818181818,0,0
79,Pretraining is most beneficial for tasks with small datasets and enables generalization even with 100 labeled examples .,Related work,General - domain LM pretraining,text-classification,5,43,0.3909090909090909,78,0.3095238095238095,43,0.3909090909090909,0,0
80,"We leave the exploration of more diverse pretraining corpora to future work , but expect that they would boost performance .",Related work,General - domain LM pretraining,text-classification,5,44,0.4,79,0.3134920634920635,44,0.4,0,0
81,"While this stage is the most expensive , it only needs to be performed once and improves performance and convergence of downstream models .",Related work,General - domain LM pretraining,text-classification,5,45,0.4090909090909091,80,0.3174603174603174,45,0.4090909090909091,0,0
82,Target task LM fine - tuning,Related work,,text-classification,5,46,0.4181818181818181,81,0.3214285714285714,46,0.4181818181818181,0,0
83,"No matter how diverse the general - domain data used for pretraining is , the data of the target task will likely come from a different distribution .",Related work,Target task LM fine - tuning,text-classification,5,47,0.4272727272727272,82,0.3253968253968254,47,0.4272727272727272,0,0
84,We thus fine - tune the LM on data of the target task .,Related work,Target task LM fine - tuning,text-classification,5,48,0.4363636363636363,83,0.3293650793650793,48,0.4363636363636363,0,0
85,"Given a pretrained general - domain LM , this stage converges faster as it only needs to adapt to the idiosyncrasies of the target data , and it allows us to train a robust LM even for small datasets .",Related work,Target task LM fine - tuning,text-classification,5,49,0.4454545454545454,84,0.3333333333333333,49,0.4454545454545454,0,0
86,"We propose discriminative fine - tuning and slanted triangular learning rates for fine - tuning the LM , which we introduce in the following .",Related work,Target task LM fine - tuning,text-classification,5,50,0.4545454545454545,85,0.3373015873015873,50,0.4545454545454545,0,0
87,Discriminative fine - tuning,Related work,,text-classification,5,51,0.4636363636363636,86,0.3412698412698413,51,0.4636363636363636,0,0
88,"As different layers capture different types of information , they should be fine - tuned to different extents .",Related work,Discriminative fine - tuning,text-classification,5,52,0.4727272727272727,87,0.3452380952380952,52,0.4727272727272727,0,0
89,"To this end , we propose a novel fine - tuning method , discriminative fine - tuning 3 .",Related work,Discriminative fine - tuning,text-classification,5,53,0.4818181818181818,88,0.3492063492063492,53,0.4818181818181818,0,0
90,"Instead of using the same learning rate for all layers of the model , discriminative fine - tuning allows us to tune each layer with different learning rates .",Related work,Discriminative fine - tuning,text-classification,5,54,0.4909090909090909,89,0.3531746031746032,54,0.4909090909090909,0,0
91,"For context , the regular stochastic gradient descent ( SGD ) update of a model 's parameters ?",Related work,Discriminative fine - tuning,text-classification,5,55,0.5,90,0.3571428571428571,55,0.5,0,0
92,at time step t looks like the following :,Related work,Discriminative fine - tuning,text-classification,5,56,0.509090909090909,91,0.3611111111111111,56,0.509090909090909,0,0
93,where ?,Related work,Discriminative fine - tuning,text-classification,5,57,0.5181818181818182,92,0.365079365079365,57,0.5181818181818182,0,0
94,is the learning rate and ? ?,Related work,Discriminative fine - tuning,text-classification,5,58,0.5272727272727272,93,0.369047619047619,58,0.5272727272727272,0,0
95,J ( ? ) is the gradient with regard to the model 's objective function .,Related work,Discriminative fine - tuning,text-classification,5,59,0.5363636363636364,94,0.373015873015873,59,0.5363636363636364,0,0
96,"For discriminative fine - tuning , we split the parameters ? into {?",Related work,Discriminative fine - tuning,text-classification,5,60,0.5454545454545454,95,0.376984126984127,60,0.5454545454545454,0,0
97,"1 , . . . , ?",Related work,Discriminative fine - tuning,text-classification,5,61,0.5545454545454546,96,0.3809523809523809,61,0.5545454545454546,0,0
98,L } where ?,Related work,Discriminative fine - tuning,text-classification,5,62,0.5636363636363636,97,0.3849206349206349,62,0.5636363636363636,0,0
99,l contains the parameters of the model at the l - th layer and L is the number of layers of the model .,Related work,Discriminative fine - tuning,text-classification,5,63,0.5727272727272728,98,0.3888888888888889,63,0.5727272727272728,0,0
100,"Similarly , we obtain {? 1 , . . . , ?",Related work,Discriminative fine - tuning,text-classification,5,64,0.5818181818181818,99,0.3928571428571428,64,0.5818181818181818,0,0
101,L } where ?,Related work,Discriminative fine - tuning,text-classification,5,65,0.5909090909090909,100,0.3968253968253968,65,0.5909090909090909,0,0
102,l is the learning rate of the l - th layer .,Related work,Discriminative fine - tuning,text-classification,5,66,0.6,101,0.4007936507936508,66,0.6,0,0
103,The SGD update with discriminative finetuning is then the following :,Related work,Discriminative fine - tuning,text-classification,5,67,0.6090909090909091,102,0.4047619047619047,67,0.6090909090909091,0,0
104,We empirically found it to work well to first choose the learning rate ?,Related work,Discriminative fine - tuning,text-classification,5,68,0.6181818181818182,103,0.4087301587301587,68,0.6181818181818182,0,0
105,L of the last layer by fine - tuning only the last layer and using ?,Related work,Discriminative fine - tuning,text-classification,5,69,0.6272727272727273,104,0.4126984126984127,69,0.6272727272727273,0,0
106,l?1 = ? l / 2.6 as the learning rate for lower layers .,Related work,Discriminative fine - tuning,text-classification,5,70,0.6363636363636364,105,0.4166666666666667,70,0.6363636363636364,0,0
107,Slanted triangular learning rates,Related work,,text-classification,5,71,0.6454545454545455,106,0.4206349206349206,71,0.6454545454545455,0,0
108,"For adapting its parameters to task - specific features , we would like the model to quickly converge to a suitable region of the parameter space in the beginning of training and then refine its parameters .",Related work,Slanted triangular learning rates,text-classification,5,72,0.6545454545454545,107,0.4246031746031746,72,0.6545454545454545,0,0
109,Using the same learning rate ( LR ) or an annealed learning rate throughout training is not the best way to achieve this behaviour .,Related work,Slanted triangular learning rates,text-classification,5,73,0.6636363636363637,108,0.4285714285714285,73,0.6636363636363637,0,0
110,"Instead , we propose slanted triangular learning rates ( STLR ) , which first linearly increases the learning rate and then linearly decays it according to the following update schedule , which can be seen in :",Related work,Slanted triangular learning rates,text-classification,5,74,0.6727272727272727,109,0.4325396825396825,74,0.6727272727272727,0,0
111,"where T is the number of training iterations 4 , cut f rac is the fraction of iterations we increase 3 An unrelated method of the same name exists for deep Boltzmann machines",Related work,Slanted triangular learning rates,text-classification,5,75,0.6818181818181818,110,0.4365079365079365,75,0.6818181818181818,0,0
112,"In other words , the number of epochs times the number of updates per epoch .",Related work,Slanted triangular learning rates,text-classification,5,76,0.6909090909090909,111,0.4404761904761904,76,0.6909090909090909,0,0
113,"the LR , cut is the iteration when we switch from increasing to decreasing the LR , p is the fraction of the number of iterations we have increased or will decrease the LR respectively , ratio specifies how much smaller the lowest LR is from the maximum LR ? max , and ?",Related work,Slanted triangular learning rates,text-classification,5,77,0.7,112,0.4444444444444444,77,0.7,0,0
114,t is the learning rate at iteration t.,Related work,Slanted triangular learning rates,text-classification,5,78,0.7090909090909091,113,0.4484126984126984,78,0.7090909090909091,0,0
115,"We generally use cut f rac = 0.1 , ratio = 32 and ? max = 0.01 .",Related work,Slanted triangular learning rates,text-classification,5,79,0.7181818181818181,114,0.4523809523809524,79,0.7181818181818181,0,0
116,"STLR modifies triangular learning rates ( Smith , 2017 ) with a short increase and along decay period , which we found key for good performance .",Related work,Slanted triangular learning rates,text-classification,5,80,0.7272727272727273,115,0.4563492063492063,80,0.7272727272727273,0,0
117,"In Section 5 , we compare against aggressive cosine annealing , a similar schedule that has recently been used to achieve state - of - the - art performance in CV .",Related work,Slanted triangular learning rates,text-classification,5,81,0.7363636363636363,116,0.4603174603174603,81,0.7363636363636363,0,0
118,6 : The slanted triangular learning rate schedule used for ULMFiT as a function of the number of training iterations .,Related work,Slanted triangular learning rates,text-classification,5,82,0.7454545454545455,117,0.4642857142857143,82,0.7454545454545455,0,0
119,Target task classifier fine - tuning,Related work,,text-classification,5,83,0.7545454545454545,118,0.4682539682539682,83,0.7545454545454545,0,0
120,"Finally , for fine - tuning the classifier , we augment the pretrained language model with two additional linear blocks .",Related work,Target task classifier fine - tuning,text-classification,5,84,0.7636363636363637,119,0.4722222222222222,84,0.7636363636363637,0,0
121,"Following standard practice for CV classifiers , each block uses batch normalization and dropout , with ReLU activations for the intermediate layer and a softmax activation that outputs a probability distribution over target classes at the last layer .",Related work,Target task classifier fine - tuning,text-classification,5,85,0.7727272727272727,120,0.4761904761904761,85,0.7727272727272727,0,0
122,Note that the parameters in these task - specific classifier layers are the only ones that are learned from scratch .,Related work,Target task classifier fine - tuning,text-classification,5,86,0.7818181818181819,121,0.4801587301587302,86,0.7818181818181819,0,0
123,The first linear layer takes as the input the pooled last hidden layer states .,Related work,Target task classifier fine - tuning,text-classification,5,87,0.7909090909090909,122,0.4841269841269841,87,0.7909090909090909,0,0
124,Concat pooling,Related work,,text-classification,5,88,0.8,123,0.4880952380952381,88,0.8,0,0
125,"The signal in text classification tasks is often contained in a few words , which may occur anywhere in the document .",Related work,Concat pooling,text-classification,5,89,0.8090909090909091,124,0.492063492063492,89,0.8090909090909091,0,0
126,"As input documents can consist of hundreds of words , information may get lost if we only consider the last hidden state of the model .",Related work,Concat pooling,text-classification,5,90,0.8181818181818182,125,0.496031746031746,90,0.8181818181818182,0,0
127,"For this reason , we concatenate the hidden state at the last time step h T of the document with both the max - pooled and the mean - pooled representation of the hidden states over as many time steps as fit in GPU memory H = {h 1 , . . . , h T }:",Related work,Concat pooling,text-classification,5,91,0.8272727272727273,126,0.5,91,0.8272727272727273,0,0
128,where [ ] is concatenation .,Related work,Concat pooling,text-classification,5,92,0.8363636363636363,127,0.503968253968254,92,0.8363636363636363,0,0
129,Fine - tuning the target classifier is the most critical part of the transfer learning method .,Related work,Concat pooling,text-classification,5,93,0.8454545454545455,128,0.5079365079365079,93,0.8454545454545455,0,0
130,"Overly aggressive fine - tuning will cause catastrophic forgetting , eliminating the benefit of the information captured through language modeling ; too cautious fine - tuning will lead to slow convergence ( and resultant overfitting ) .",Related work,Concat pooling,text-classification,5,94,0.8545454545454545,129,0.5119047619047619,94,0.8545454545454545,0,0
131,"Besides discriminative finetuning and triangular learning rates , we propose gradual unfreezing for fine - tuning the classifier .",Related work,Concat pooling,text-classification,5,95,0.8636363636363636,130,0.5158730158730159,95,0.8636363636363636,0,0
132,Gradual unfreezing,Related work,,text-classification,5,96,0.8727272727272727,131,0.5198412698412699,96,0.8727272727272727,0,0
133,"Rather than fine - tuning all layers at once , which risks catastrophic forgetting , we propose to gradually unfreeze the model starting from the last layer as this contains the least general knowledge :",Related work,Gradual unfreezing,text-classification,5,97,0.8818181818181818,132,0.5238095238095238,97,0.8818181818181818,0,0
134,We first unfreeze the last layer and fine - tune all unfrozen layers for one epoch .,Related work,Gradual unfreezing,text-classification,5,98,0.8909090909090909,133,0.5277777777777778,98,0.8909090909090909,0,0
135,"We then unfreeze the next lower frozen layer and repeat , until we finetune all layers until convergence at the last iteration .",Related work,Gradual unfreezing,text-classification,5,99,0.9,134,0.5317460317460317,99,0.9,0,0
136,"This is similar to ' chain - thaw ' , except that we add a layer at a time to the set of ' thawed ' layers , rather than only training a single layer at a time .",Related work,Gradual unfreezing,text-classification,5,100,0.9090909090909092,135,0.5357142857142857,100,0.9090909090909092,0,0
137,"While discriminative fine - tuning , slanted triangular learning rates , and gradual unfreezing all are beneficial on their own , we show in Section 5 that they complement each other and enable our method to perform well across diverse datasets .",Related work,Gradual unfreezing,text-classification,5,101,0.9181818181818182,136,0.5396825396825397,101,0.9181818181818182,0,0
138,BPTT for Text Classification ( BPT3C ),Related work,Gradual unfreezing,text-classification,5,102,0.9272727272727272,137,0.5436507936507936,102,0.9272727272727272,0,0
139,Language models are trained with backpropagation through time ( BPTT ) to enable gradient propagation for large input sequences .,Related work,Gradual unfreezing,text-classification,5,103,0.9363636363636364,138,0.5476190476190477,103,0.9363636363636364,0,0
140,"In order to make fine - tuning a classifier for large documents feasible , we propose BPTT for Text Classification ( BPT3C ) :",Related work,Gradual unfreezing,text-classification,5,104,0.9454545454545454,139,0.5515873015873016,104,0.9454545454545454,0,0
141,We divide the document into fixedlength batches of size b.,Related work,Gradual unfreezing,text-classification,5,105,0.9545454545454546,140,0.5555555555555556,105,0.9545454545454546,0,0
142,"At the beginning of each batch , the model is initialized with the final state of the previous batch ; we keep track of the hidden states for mean and max - pooling ; gradients are back - propagated to the batches whose hidden states contributed to the final prediction .",Related work,Gradual unfreezing,text-classification,5,106,0.9636363636363636,141,0.5595238095238095,106,0.9636363636363636,0,0
143,"In practice , we use variable length backpropagation sequences .",Related work,Gradual unfreezing,text-classification,5,107,0.9727272727272728,142,0.5634920634920635,107,0.9727272727272728,0,0
144,"Bidirectional language model Similar to existing work ( Peters et al. , 2017 , 2018 ) , we are not limited to fine - tuning a unidirectional language model .",Related work,Gradual unfreezing,text-classification,5,108,0.9818181818181818,143,0.5674603174603174,108,0.9818181818181818,0,0
145,"For all our experiments , we pretrain both a forward and a backward LM .",Related work,Gradual unfreezing,text-classification,5,109,0.990909090909091,144,0.5714285714285714,109,0.990909090909091,0,0
146,We fine - tune a classifier for each LM independently using BPT3C and average the classifier predictions .,Related work,Gradual unfreezing,text-classification,5,110,1.0,145,0.5753968253968254,110,1.0,0,0
147,Experiments,,,text-classification,5,0,0.0,146,0.5793650793650794,0,0.0,1,0
148,"While our approach is equally applicable to sequence labeling tasks , we focus on text classification tasks in this work due to their important realworld applications .",Experiments,Experiments,text-classification,5,1,0.0,147,0.5833333333333334,1,0.0,1,0
149,Experimental setup,,,text-classification,5,0,0.0,148,0.5873015873015873,0,0.0,1,0
150,Datasets and tasks,Experimental setup,,text-classification,5,1,0.1111111111111111,149,0.5912698412698413,1,0.1111111111111111,1,0
151,"We evaluate our method on six widely - studied datasets , with varying numbers of documents and varying document length , used by state - of - the - art text classification and transfer learning approaches as instances of three common text classification tasks : sentiment analysis , question classification , and topic classification .",Experimental setup,Datasets and tasks,text-classification,5,2,0.2222222222222222,150,0.5952380952380952,2,0.2222222222222222,1,0
152,We show the statistics for each dataset and task in .,Experimental setup,Datasets and tasks,text-classification,5,3,0.3333333333333333,151,0.5992063492063492,3,0.3333333333333333,1,0
153,TBCNN 4.0 Virtual 5.9 LSTM- CNN 3.9 ULMFiT ( ours ) 4.6 ULMFiT ( ours ) 3.6 : Test error rates ( % ) on text classification datasets used by .,Experimental setup,Datasets and tasks,text-classification,5,4,0.4444444444444444,152,0.6031746031746031,4,0.4444444444444444,1,0
154,Topic classification,Experimental setup,,text-classification,5,5,0.5555555555555556,153,0.6071428571428571,5,0.5555555555555556,1,0
155,"For topic classification , we evaluate on the large - scale AG news and DBpedia ontology datasets created by .",Experimental setup,Topic classification,text-classification,5,6,0.6666666666666666,154,0.6111111111111112,6,0.6666666666666666,1,0
156,Pre-processing,Experimental setup,,text-classification,5,7,0.7777777777777778,155,0.6150793650793651,7,0.7777777777777778,1,0
157,We use the same pre-processing as in earlier work .,Experimental setup,Pre-processing,text-classification,5,8,0.8888888888888888,156,0.6190476190476191,8,0.8888888888888888,1,0
158,"In addition , to allow the language model to capture aspects that might be relevant for classification , we add special tokens for upper-case words , elongation , and repetition .",Experimental setup,Pre-processing,text-classification,5,9,1.0,157,0.623015873015873,9,1.0,1,0
159,Hyperparameters,,,text-classification,5,0,0.0,158,0.626984126984127,0,0.0,1,0
160,We are interested in a model that performs robustly across a diverse set of tasks .,Hyperparameters,Hyperparameters,text-classification,5,1,0.125,159,0.6309523809523809,1,0.0833333333333333,1,0
161,"To this end , if not mentioned otherwise , we use the same set of hyperparameters across tasks , which we tune on the IMDb validation set .",Hyperparameters,Hyperparameters,text-classification,5,2,0.25,160,0.6349206349206349,2,0.1666666666666666,1,0
162,"We use the AWD - LSTM language model with an embedding size of 400 , 3 layers , 1150 hidden activations per layer , and a BPTT batch size of 70 .",Hyperparameters,Hyperparameters,text-classification,5,3,0.375,161,0.6388888888888888,3,0.25,1,1
163,"We apply dropout of 0.4 to layers , 0.3 to RNN layers , 0.4 to input embedding layers , 0.05 to embedding layers , and weight dropout of 0.5 to the RNN hidden - to - hidden matrix .",Hyperparameters,Hyperparameters,text-classification,5,4,0.5,162,0.6428571428571429,4,0.3333333333333333,1,1
164,The classifier has a hidden layer of size 50 .,Hyperparameters,Hyperparameters,text-classification,5,5,0.625,163,0.6468253968253969,5,0.4166666666666667,1,1
165,"We use Adam with ? 1 = 0.7 instead of the default ? 1 = 0.9 and ? 2 = 0.99 , similar to .",Hyperparameters,Hyperparameters,text-classification,5,6,0.75,164,0.6507936507936508,6,0.5,1,1
166,"We use a batch size of 64 , abase learning rate of 0.004 and 0.01 for finetuning the LM and the classifier respectively , and tune the number of epochs on the validation set of each task 7 .",Hyperparameters,Hyperparameters,text-classification,5,7,0.875,165,0.6547619047619048,7,0.5833333333333334,1,1
167,We otherwise use the same practices used in .,Hyperparameters,,text-classification,5,8,1.0,166,0.6587301587301587,8,0.6666666666666666,1,0
168,Baselines and comparison models,,,text-classification,5,0,0.0,167,0.6626984126984127,9,0.75,1,0
169,"For each task , we compare against the current state - of - theart .",Baselines and comparison models,Baselines and comparison models,text-classification,5,1,0.3333333333333333,168,0.6666666666666666,10,0.8333333333333334,1,0
170,"For the IMDb and TREC - 6 datasets , we compare against CoVe , a stateof - the - art transfer learning method for NLP .",Baselines and comparison models,Baselines and comparison models,text-classification,5,2,0.6666666666666666,169,0.6706349206349206,11,0.9166666666666666,1,0
171,"For the AG , Yelp , and DBpedia datasets , we compare against the state - of - the - art text categorization method by .",Baselines and comparison models,Baselines and comparison models,text-classification,5,3,1.0,170,0.6746031746031746,12,1.0,1,0
172,Results,,,text-classification,5,0,0.0,171,0.6785714285714286,0,0.0,1,0
173,"For consistency , we report all results as error rates ( lower is better ) .",Results,Results,text-classification,5,1,0.0454545454545454,172,0.6825396825396826,1,0.0666666666666666,1,0
174,We show the test error rates on the IMDb and TREC - 6 datasets used by in .,Results,Results,text-classification,5,2,0.0909090909090909,173,0.6865079365079365,2,0.1333333333333333,1,0
175,"Our method outperforms both CoVe , a state - of - the - art transfer learning method based on hypercolumns , as well as the state - of - the - art on both datasets .",Results,Results,text-classification,5,3,0.1363636363636363,174,0.6904761904761905,3,0.2,1,1
176,"On IMDb , we reduce the error dramatically by 43.9 % and 22 % with regard to CoVe and the state - of - the - art respectively .",Results,Results,text-classification,5,4,0.1818181818181818,175,0.6944444444444444,4,0.2666666666666666,1,1
177,"This is promising as the existing stateof - the - art requires complex architectures , multiple forms of attention and sophisticated embedding schemes , while our method employs a regular LSTM with dropout .",Results,Results,text-classification,5,5,0.2272727272727272,176,0.6984126984126984,5,0.3333333333333333,1,0
178,"We note that the language model fine - tuning approach of only achieves an error of 7.64 vs. 4.6 for our method on IMDb , demonstrating the benefit of transferring knowledge from a large Image Net - like corpus using our fine - tuning techniques .",Results,Results,text-classification,5,6,0.2727272727272727,177,0.7023809523809523,6,0.4,1,0
179,IMDb in particular is reflective of realworld datasets :,Results,Results,text-classification,5,7,0.3181818181818182,178,0.7063492063492064,7,0.4666666666666667,1,0
180,"It s documents are generally a few paragraphs long - similar to emails ( e.g for legal discovery ) and online comments ( e.g for community management ) ; and sentiment analysis is similar to many commercial applications , e.g. product response tracking and support email routing .",Results,Results,text-classification,5,8,0.3636363636363636,179,0.7103174603174603,8,0.5333333333333333,1,0
181,"On TREC - 6 , our improvement - similar as the improvements of state - of - the - art approaches - is not statistically significant , due to the small size of the 500 - examples test set .",Results,Results,text-classification,5,9,0.4090909090909091,180,0.7142857142857143,9,0.6,1,1
182,"Nevertheless , the competitive performance on TREC - 6 demonstrates that our model performs well across different dataset sizes and can deal with examples that range from single sentences - in the case of TREC - 6to several paragraphs for IMDb .",Results,Results,text-classification,5,10,0.4545454545454545,181,0.7182539682539683,10,0.6666666666666666,1,0
183,"Note that despite pretraining on more than two orders of magnitude less data than the 7 million sentence pairs used by , we consistently outperform their approach on both datasets .",Results,Results,text-classification,5,11,0.5,182,0.7222222222222222,11,0.7333333333333333,1,0
184,"We show the test error rates on the larger AG , DBpedia , Yelp - bi , and Yelp - full datasets in .",Results,Results,text-classification,5,12,0.5454545454545454,183,0.7261904761904762,12,0.8,1,0
185,Our method again outperforms the state - of the - art significantly .,Results,Results,text-classification,5,13,0.5909090909090909,184,0.7301587301587301,13,0.8666666666666667,1,0
186,"On AG , we observe a similarly dramatic error reduction by 23.7 % compared to the state - of - the - art .",Results,Results,text-classification,5,14,0.6363636363636364,185,0.7341269841269841,14,0.9333333333333332,1,1
187,"On DBpedia , Yelp - bi , and Yelp - full , we reduce the error by 4.8 % , 18.2 % , 2.0 % respectively .",Results,Results,text-classification,5,15,0.6818181818181818,186,0.7380952380952381,15,1.0,1,1
188,Analysis,Results,,text-classification,5,16,0.7272727272727273,187,0.7420634920634921,0,0.0,1,0
189,"In order to assess the impact of each contribution , we perform a series of analyses and ablations .",Results,Analysis,text-classification,5,17,0.7727272727272727,188,0.746031746031746,1,0.1666666666666666,1,0
190,"We run experiments on three corpora , IMDb , TREC - 6 , and AG that are representative of different tasks , genres , and sizes .",Results,Analysis,text-classification,5,18,0.8181818181818182,189,0.75,2,0.3333333333333333,1,0
191,"For all experiments , we split off 10 % of the training set and report error rates on this validation set with unidirectional LMs .",Results,Analysis,text-classification,5,19,0.8636363636363636,190,0.753968253968254,3,0.5,1,0
192,We fine - tune the classifier for 50 epochs and train all methods but ULMFiT with early stopping .,Results,Analysis,text-classification,5,20,0.9090909090909092,191,0.7579365079365079,4,0.6666666666666666,1,0
193,Low - shot learning,Results,,text-classification,5,21,0.9545454545454546,192,0.7619047619047619,5,0.8333333333333334,1,0
194,One of the main benefits of transfer learning is being able to train a model for,Results,Low - shot learning,text-classification,5,22,1.0,193,0.7658730158730159,6,1.0,1,0
195,Pretraining,,,text-classification,5,0,0.0,194,0.7698412698412699,0,0.0,1,0
196,IMDb TREC - 6 AG Without pretraining 5.63 10.67 5.52 With pretraining 5.00 5.69 5.38 : Validation error rates for ULMFiT with and without pretraining .,Pretraining,Pretraining,text-classification,5,1,0.1111111111111111,195,0.7738095238095238,1,0.1111111111111111,1,0
197,a task with a small number of labels .,Pretraining,Pretraining,text-classification,5,2,0.2222222222222222,196,0.7777777777777778,2,0.2222222222222222,1,0
198,We evaluate ULMFiT on different numbers of labeled examples in two settings : only labeled examples are used for LM fine - tuning ( 'supervised ' ) ; and all task data is available and can be used to fine - tune the LM ( ' semi-supervised ' ) .,Pretraining,Pretraining,text-classification,5,3,0.3333333333333333,197,0.7817460317460317,3,0.3333333333333333,1,0
199,We compare ULM - FiT to training from scratch - which is necessary for hypercolumn - based approaches .,Pretraining,Pretraining,text-classification,5,4,0.4444444444444444,198,0.7857142857142857,4,0.4444444444444444,1,0
200,"We split off balanced fractions of the training data , keep the validation set fixed , and use the same hyperparameters as before .",Pretraining,Pretraining,text-classification,5,5,0.5555555555555556,199,0.7896825396825397,5,0.5555555555555556,1,0
201,We show the results in .,Pretraining,,text-classification,5,6,0.6666666666666666,200,0.7936507936507936,6,0.6666666666666666,1,0
202,"On IMDb and AG , supervised ULMFiT with only 100 labeled examples matches the performance of training from scratch with 10 and 20 more data respectively , clearly demonstrating the benefit of general - domain LM pretraining .",Pretraining,We show the results in .,text-classification,5,7,0.7777777777777778,201,0.7976190476190477,7,0.7777777777777778,1,1
203,"If we allow ULMFiT to also utilize unlabeled examples ( 50 k for IMDb , 100 k for AG ) , at 100 labeled examples , we match the performance of training from scratch with 50 and 100 more data on AG and IMDb respectively .",Pretraining,We show the results in .,text-classification,5,8,0.8888888888888888,202,0.8015873015873016,8,0.8888888888888888,1,0
204,"On TREC - 6 , ULMFiT significantly improves upon training from scratch ; as examples are shorter and fewer , supervised and semi-supervised ULMFiT achieve similar results .",Pretraining,We show the results in .,text-classification,5,9,1.0,203,0.8055555555555556,9,1.0,1,1
205,Impact of pretraining,,,text-classification,5,0,0.0,204,0.8095238095238095,0,0.0,1,0
206,We compare using no pretraining with pretraining on WikiText - 103 in .,Impact of pretraining,Impact of pretraining,text-classification,5,1,0.0238095238095238,205,0.8134920634920635,1,0.0294117647058823,1,0
207,"Pretraining is most useful for small and medium - sized datasets , which are most common in commercial applications .",Impact of pretraining,Impact of pretraining,text-classification,5,2,0.0476190476190476,206,0.8174603174603174,2,0.0588235294117647,1,1
208,"However , even for large datasets , pretraining improves performance .",Impact of pretraining,Impact of pretraining,text-classification,5,3,0.0714285714285714,207,0.8214285714285714,3,0.088235294117647,1,0
209,Impact of LM quality,Impact of pretraining,,text-classification,5,4,0.0952380952380952,208,0.8253968253968254,4,0.1176470588235294,1,0
210,"In order to gauge the importance of choosing an appropriate LM , we compare a vanilla LM with the same hyperparameters without any dropout 8 with the AWD - LSTM LM with tuned dropout parameters in .",Impact of pretraining,Impact of LM quality,text-classification,5,5,0.119047619047619,209,0.8293650793650794,5,0.1470588235294117,1,0
211,"Using our fine - tuning techniques , even a regular LM reaches surprisingly good performance on the larger datasets .",Impact of pretraining,Impact of LM quality,text-classification,5,6,0.1428571428571428,210,0.8333333333333334,6,0.1764705882352941,1,1
212,"On the smaller TREC - 6 , a vanilla LM without dropout runs the risk of overfitting , which decreases performance .",Impact of pretraining,Impact of LM quality,text-classification,5,7,0.1666666666666666,211,0.8373015873015873,7,0.2058823529411764,1,1
213,Impact of LM fine - tuning,Impact of pretraining,,text-classification,5,8,0.1904761904761904,212,0.8412698412698413,8,0.2352941176470588,1,0
214,"We compare no finetuning against fine - tuning the full model ( ' Full ' ) , the most commonly used fine - tuning method , with and without discriminative fine - tuning ( ' Discr ' ) and slanted triangular learning rates ( ' Stlr ' ) in .",Impact of pretraining,Impact of LM fine - tuning,text-classification,5,9,0.2142857142857142,213,0.8452380952380952,9,0.2647058823529412,1,0
215,Fine - tuning the LM is most beneficial for larger datasets .,Impact of pretraining,Impact of LM fine - tuning,text-classification,5,10,0.238095238095238,214,0.8492063492063492,10,0.2941176470588235,1,1
216,"' Discr ' and ' Stlr ' improve performance across all three datasets and are necessary on the smaller TREC - 6 , where regular fine - tuning is not beneficial .",Impact of pretraining,Impact of LM fine - tuning,text-classification,5,11,0.2619047619047619,215,0.8531746031746031,11,0.3235294117647059,1,0
217,Impact of classifier fine - tuning,Impact of pretraining,,text-classification,5,12,0.2857142857142857,216,0.8571428571428571,12,0.3529411764705882,1,0
218,"We compare training from scratch , fine - tuning the full model ( ' Full ' ) , only fine - tuning the last layer ( ' Last ' ) , ' Chain - thaw ' , and gradual unfreezing ( ' Freez ' ) .",Impact of pretraining,Impact of classifier fine - tuning,text-classification,5,13,0.3095238095238095,217,0.8611111111111112,13,0.3823529411764705,1,0
219,We furthermore assess the importance of discriminative fine - tuning ( ' Discr ' ) and slanted triangular learning rates ( ' Stlr ' ) .,Impact of pretraining,Impact of classifier fine - tuning,text-classification,5,14,0.3333333333333333,218,0.8650793650793651,14,0.4117647058823529,1,0
220,"We compare the latter to an alternative , aggressive cosine annealing schedule ( ' Cos ' ) .",Impact of pretraining,Impact of classifier fine - tuning,text-classification,5,15,0.3571428571428571,219,0.8690476190476191,15,0.4411764705882353,1,0
221,"We use a learning rate ? L = 0.01 for ' Discr ' , learning rates",Impact of pretraining,Impact of classifier fine - tuning,text-classification,5,16,0.3809523809523809,220,0.873015873015873,16,0.4705882352941176,1,0
222,8,Impact of pretraining,Impact of classifier fine - tuning,text-classification,5,17,0.4047619047619047,221,0.876984126984127,17,0.5,1,0
223,"To avoid overfitting , we only train the vanilla LM classifier for 5 epochs and keep dropout of 0.4 in the classifier . of 0.001 and 0.0001 for the last and all other layers respectively for ' Chain - thaw ' as in , and a learning rate of 0.001 otherwise .",Impact of pretraining,Impact of classifier fine - tuning,text-classification,5,18,0.4285714285714285,222,0.8809523809523809,18,0.5294117647058824,1,0
224,We show the results in .,Impact of pretraining,,text-classification,5,19,0.4523809523809524,223,0.8849206349206349,19,0.5588235294117647,1,0
225,"Fine - tuning the classifier significantly improves over training from scratch , particularly on the small TREC - 6 . ' Last ' , the standard fine - tuning method in CV , severely underfits and is never able to lower the training error to 0 . ' Chainthaw ' achieves competitive performance on the smaller datasets , but is outperformed significantly on the large AG .",Impact of pretraining,We show the results in .,text-classification,5,20,0.4761904761904761,224,0.8888888888888888,20,0.5882352941176471,1,1
226,' Freez ' provides similar performance as ' Full ' .,Impact of pretraining,We show the results in .,text-classification,5,21,0.5,225,0.8928571428571429,21,0.6176470588235294,1,0
227,"' Discr ' consistently boosts the performance of ' Full ' and ' Freez ' , except for the large AG .",Impact of pretraining,We show the results in .,text-classification,5,22,0.5238095238095238,226,0.8968253968253969,22,0.6470588235294118,1,0
228,"Cosine annealing is competitive with slanted triangular learning rates on large data , but under-performs on smaller datasets .",Impact of pretraining,We show the results in .,text-classification,5,23,0.5476190476190477,227,0.9007936507936508,23,0.6764705882352942,1,0
229,"Finally , full ULMFiT classifier fine - tuning ( bottom row ) achieves the best performance on IMDB and TREC - 6 and competitive performance on AG .",Impact of pretraining,We show the results in .,text-classification,5,24,0.5714285714285714,228,0.9047619047619048,24,0.7058823529411765,1,0
230,"Importantly , ULMFiT is the only method that shows excellent performance across the board - and is therefore the only universal method .",Impact of pretraining,We show the results in .,text-classification,5,25,0.5952380952380952,229,0.9087301587301588,25,0.7352941176470589,1,0
231,Classifier fine - tuning behavior,Impact of pretraining,,text-classification,5,26,0.6190476190476191,230,0.9126984126984128,26,0.7647058823529411,1,0
232,"While our results demonstrate that how we fine - tune the classifier makes a significant difference , fine - tuning for inductive transfer is currently under-explored in NLP as it mostly has been thought to be unhelpful .",Impact of pretraining,Classifier fine - tuning behavior,text-classification,5,27,0.6428571428571429,231,0.9166666666666666,27,0.7941176470588235,1,0
233,"To better understand the fine - tuning behavior of our model , we compare the validation error of the classifier fine - tuned with ULMFiT and ' Full ' during training in .",Impact of pretraining,Classifier fine - tuning behavior,text-classification,5,28,0.6666666666666666,232,0.9206349206349206,28,0.8235294117647058,1,0
234,"On all datasets , fine - tuning the full model leads to the lowest error comparatively early in training , e.g. already after the first epoch on IMDb .",Impact of pretraining,Classifier fine - tuning behavior,text-classification,5,29,0.6904761904761905,233,0.9246031746031746,29,0.8529411764705882,1,0
235,The error then increases as the model starts to overfit and knowledge captured through pretraining is lost .,Impact of pretraining,Classifier fine - tuning behavior,text-classification,5,30,0.7142857142857143,234,0.9285714285714286,30,0.8823529411764706,1,0
236,"In contrast , ULMFiT is more stable and suffers from no such catastrophic forgetting ; performance remains similar or improves until late epochs , which shows the positive effect of the learning rate schedule .",Impact of pretraining,Classifier fine - tuning behavior,text-classification,5,31,0.7380952380952381,235,0.9325396825396826,31,0.9117647058823528,1,0
237,Impact of bidirectionality,Impact of pretraining,,text-classification,5,32,0.7619047619047619,236,0.9365079365079364,32,0.9411764705882352,1,0
238,"At the cost of training a second model , ensembling the predictions of a forward and backwards LM - classifier brings a performance boost of around 0.5 - 0.7 .",Impact of pretraining,Impact of bidirectionality,text-classification,5,33,0.7857142857142857,237,0.9404761904761904,33,0.9705882352941176,1,1
239,On IMD b we lower the test error from 5.30 of a single model to 4.58 for the bidirectional model .,Impact of pretraining,Impact of bidirectionality,text-classification,5,34,0.8095238095238095,238,0.9444444444444444,34,1.0,1,1
240,Discussion and future directions,Impact of pretraining,,text-classification,5,35,0.8333333333333334,239,0.9484126984126984,0,0.0,1,0
241,"While we have shown that ULMFiT can achieve state - of - the - art performance on widely used text classification tasks , we believe that language model fine - tuning will be particularly useful in the following settings compared to existing transfer learning approaches : a) NLP for non-English languages , where training data for supervised pretraining tasks is scarce ; b ) new NLP tasks where no state - of - the - art architecture exists ; and c) tasks with limited amounts of labeled data ( and some amounts of unlabeled data ) .",Impact of pretraining,Discussion and future directions,text-classification,5,36,0.8571428571428571,240,0.9523809523809524,1,0.1428571428571428,1,0
242,"Given that transfer learning and particularly fine - tuning for NLP is under - explored , many future directions are possible .",Impact of pretraining,Discussion and future directions,text-classification,5,37,0.8809523809523809,241,0.9563492063492064,2,0.2857142857142857,1,0
243,"One possible direction is to improve language model pretraining and fine - tuning and make them more scalable : for Image Net , predicting far fewer classes only incurs a small performance drop , while recent work shows that an alignment between source and target task label sets is important ) - focusing on predicting a subset of words such as the most frequent ones might retain most of the performance while speeding up training .",Impact of pretraining,Discussion and future directions,text-classification,5,38,0.9047619047619048,242,0.9603174603174603,3,0.4285714285714285,1,0
244,"Language modeling can also be augmented with additional tasks in a multi-task learning fashion or enriched with additional supervision , e.g. syntax - sensitive dependencies to create a model that is more general or better suited for certain downstream tasks , ideally in a weakly - supervised manner to retain its universal properties .",Impact of pretraining,Discussion and future directions,text-classification,5,39,0.9285714285714286,243,0.9642857142857144,4,0.5714285714285714,1,0
245,Another direction is to apply the method to novel tasks and models .,Impact of pretraining,Discussion and future directions,text-classification,5,40,0.9523809523809524,244,0.9682539682539684,5,0.7142857142857143,1,0
246,"While an extension to sequence labeling is straightforward , other tasks with more complex interactions such as entailment or question answering may require novel ways to pretrain and fine - tune .",Impact of pretraining,Discussion and future directions,text-classification,5,41,0.9761904761904762,245,0.9722222222222222,6,0.8571428571428571,1,0
247,"Finally , while we have provided a series of analyses and ablations , more studies are required to better understand what knowledge a pretrained language model captures , how this changes during fine - tuning , and what information different tasks require .",Impact of pretraining,Discussion and future directions,text-classification,5,42,1.0,246,0.9761904761904762,7,1.0,1,0
248,Conclusion,,,text-classification,5,0,0.0,247,0.98015873015873,0,0.0,1,0
249,"We have proposed ULMFiT , an effective and extremely sample - efficient transfer learning method that can be applied to any NLP task .",Conclusion,Conclusion,text-classification,5,1,0.25,248,0.984126984126984,1,0.25,0,0
250,We have also proposed several novel fine - tuning techniques that in conjunction prevent catastrophic forgetting and enable robust learning across a diverse range of tasks .,Conclusion,Conclusion,text-classification,5,2,0.5,249,0.988095238095238,2,0.5,0,0
251,Our method significantly outperformed existing transfer learning techniques and the stateof - the - art on six representative text classification tasks .,Conclusion,Conclusion,text-classification,5,3,0.75,250,0.992063492063492,3,0.75,0,0
252,We hope that our results will catalyze new developments in transfer learning for NLP .,Conclusion,Conclusion,text-classification,5,4,1.0,251,0.996031746031746,4,1.0,0,0
1,title,,,text-classification,6,0,0.0,0,0.0,0,0.0,1,0
2,Universal Sentence Encoder,title,,text-classification,6,1,0.0,1,0.0067567567567567,1,0.0,1,1
3,abstract,,,text-classification,6,0,0.0,2,0.0135135135135135,0,0.0,1,0
4,We present models for encoding sentences into embedding vectors that specifically target transfer learning to other NLP tasks .,abstract,abstract,text-classification,6,1,0.1111111111111111,3,0.0202702702702702,1,0.1111111111111111,1,1
5,The models are efficient and result in accurate performance on diverse transfer tasks .,abstract,abstract,text-classification,6,2,0.2222222222222222,4,0.027027027027027,2,0.2222222222222222,1,0
6,Two variants of the encoding models allow for trade - offs between accuracy and compute resources .,abstract,abstract,text-classification,6,3,0.3333333333333333,5,0.0337837837837837,3,0.3333333333333333,1,0
7,"For both variants , we investigate and report the relationship between model complexity , resource consumption , the availability of transfer task training data , and task performance .",abstract,abstract,text-classification,6,4,0.4444444444444444,6,0.0405405405405405,4,0.4444444444444444,1,0
8,Comparisons are made with baselines that use word level transfer learning via pretrained word embeddings as well as baselines do not use any transfer learning .,abstract,abstract,text-classification,6,5,0.5555555555555556,7,0.0472972972972973,5,0.5555555555555556,1,0
9,We find that transfer learning using sentence embeddings tends to outperform word level transfer .,abstract,abstract,text-classification,6,6,0.6666666666666666,8,0.054054054054054,6,0.6666666666666666,1,1
10,"With transfer learning via sentence embeddings , we observe surprisingly good performance with minimal amounts of supervised training data fora transfer task .",abstract,abstract,text-classification,6,7,0.7777777777777778,9,0.0608108108108108,7,0.7777777777777778,1,1
11,We obtain encouraging results on Word Embedding Association Tests ( WEAT ) targeted at detecting model bias .,abstract,abstract,text-classification,6,8,0.8888888888888888,10,0.0675675675675675,8,0.8888888888888888,1,0
12,Our pre-trained sentence encoding models are made freely available for download and on TF Hub .,abstract,abstract,text-classification,6,9,1.0,11,0.0743243243243243,9,1.0,1,0
13,Introduction,,,text-classification,6,0,0.0,12,0.081081081081081,0,0.0,1,0
14,Limited amounts of training data are available for many NLP tasks .,Introduction,Introduction,text-classification,6,1,0.0666666666666666,13,0.0878378378378378,1,0.0666666666666666,1,0
15,This presents a challenge for data hungry deep learning methods .,Introduction,Introduction,text-classification,6,2,0.1333333333333333,14,0.0945945945945946,2,0.1333333333333333,1,0
16,"Given the high cost of annotating supervised training data , very large training sets are usually not available for most research or industry NLP tasks .",Introduction,Introduction,text-classification,6,3,0.2,15,0.1013513513513513,3,0.2,1,0
17,Many models address the problem by implicitly performing limited transfer learning through the use of pre-trained word embeddings such as those produced by word2vec or Glo Ve .,Introduction,Introduction,text-classification,6,4,0.2666666666666666,16,0.1081081081081081,4,0.2666666666666666,1,0
18,"However , recent work has demonstrated strong transfer task performance using pre-trained sentence level embeddings .",Introduction,Introduction,text-classification,6,5,0.3333333333333333,17,0.1148648648648648,5,0.3333333333333333,1,0
19,"In this paper , we present two models for producing sentence embeddings that demonstrate good transfer to a number of other of other NLP tasks .",Introduction,Introduction,text-classification,6,6,0.4,18,0.1216216216216216,6,0.4,1,0
20,We include experiments with varying amounts of transfer task training data to illustrate the relationship between transfer task performance and training set size .,Introduction,Introduction,text-classification,6,7,0.4666666666666667,19,0.1283783783783783,7,0.4666666666666667,1,0
21,We find that our sentence embeddings can be used to obtain surprisingly good task performance with remarkably little task specific training data .,Introduction,Introduction,text-classification,6,8,0.5333333333333333,20,0.1351351351351351,8,0.5333333333333333,1,0
22,The sentence encoding models are made publicly available on TF Hub .,Introduction,Introduction,text-classification,6,9,0.6,21,0.1418918918918918,9,0.6,1,0
23,Engineering characteristics of models used for transfer learning are an important consideration .,Introduction,Introduction,text-classification,6,10,0.6666666666666666,22,0.1486486486486486,10,0.6666666666666666,1,0
24,We discuss modeling trade - offs regarding memory requirements as well as compute time on CPU and GPU .,Introduction,Introduction,text-classification,6,11,0.7333333333333333,23,0.1554054054054054,11,0.7333333333333333,1,0
25,Resource consumption comparisons are made for sentences of varying lengths .,Introduction,Introduction,text-classification,6,12,0.8,24,0.1621621621621621,12,0.8,1,0
26,"import tensorflow_hub as hub embed = hub.Module ( "" https://tfhub.dev/google/ "" "" universal- sentence - encoder / 1 "" ) embedding = embed ( [",Introduction,Introduction,text-classification,6,13,0.8666666666666667,25,0.1689189189189189,13,0.8666666666666667,1,0
27,""" The quick brown fox jumps over the lazy dog . "" ] )",Introduction,Introduction,text-classification,6,14,0.9333333333333332,26,0.1756756756756756,14,0.9333333333333332,1,0
28,Listing 1 : Python example code for using the universal sentence encoder .,Introduction,Introduction,text-classification,6,15,1.0,27,0.1824324324324324,15,1.0,1,0
29,Model Toolkit,,,text-classification,6,0,0.0,28,0.1891891891891892,0,0.0,1,0
30,We make available two new models for encoding sentences into embedding vectors .,Model Toolkit,Model Toolkit,text-classification,6,1,0.032258064516129,29,0.1959459459459459,1,0.125,1,0
31,"One makes use of the transformer architecture , while the other is formulated as a deep averaging network ( DAN ) .",Model Toolkit,Model Toolkit,text-classification,6,2,0.064516129032258,30,0.2027027027027027,2,0.25,1,0
32,Both models are implemented in TensorFlow and are available to download from TF Hub : 1 https://tfhub.dev/google/universal-sentence-encoder/1,Model Toolkit,Model Toolkit,text-classification,6,3,0.0967741935483871,31,0.2094594594594594,3,0.375,1,1
33,The models take as input English strings and produce as output a fixed dimensional embedding representation of the string .,Model Toolkit,Model Toolkit,text-classification,6,4,0.1290322580645161,32,0.2162162162162162,4,0.5,1,0
34,Listing 1 provides a minimal code snippet to convert a sentence into a tensor containing its sentence embedding .,Model Toolkit,Model Toolkit,text-classification,6,5,0.1612903225806451,33,0.2229729729729729,5,0.625,1,0
35,The embedding tensor can be used directly or incorporated into larger model graphs for specific tasks .,Model Toolkit,Model Toolkit,text-classification,6,6,0.1935483870967742,34,0.2297297297297297,6,0.75,1,0
36,"As illustrated in , the sentence embeddings can be trivially used to compute sentence level semantic similarity scores that achieve excellent performance on the semantic textual similarity ( STS ) Benchmark .",Model Toolkit,Model Toolkit,text-classification,6,7,0.2258064516129032,35,0.2364864864864864,7,0.875,1,0
37,"When included within larger models , the sentence encoding models can be fine tuned for specific tasks using gradient based updates .",Model Toolkit,Model Toolkit,text-classification,6,8,0.2580645161290322,36,0.2432432432432432,8,1.0,1,0
38,Encoders,Model Toolkit,,text-classification,6,9,0.2903225806451613,37,0.25,0,0.0,1,0
39,We introduce the model architecture for our two encoding models in this section .,Model Toolkit,Encoders,text-classification,6,10,0.3225806451612903,38,0.2567567567567567,1,0.25,1,0
40,Our two encoders have different design goals .,Model Toolkit,Encoders,text-classification,6,11,0.3548387096774194,39,0.2635135135135135,2,0.5,1,0
41,One based on the transformer architecture targets high accuracy at the cost of greater model complexity and resource consumption .,Model Toolkit,Encoders,text-classification,6,12,0.3870967741935484,40,0.2702702702702703,3,0.75,1,0
42,The other targets efficient inference with slightly reduced accuracy .,Model Toolkit,Encoders,text-classification,6,13,0.4193548387096774,41,0.277027027027027,4,1.0,1,0
43,Transformer,Model Toolkit,,text-classification,6,14,0.4516129032258064,42,0.2837837837837837,0,0.0,1,0
44,The transformer based sentence encoding model constructs sentence embeddings using the encoding sub - graph of the transformer architecture .,Model Toolkit,Transformer,text-classification,6,15,0.4838709677419355,43,0.2905405405405405,1,0.0588235294117647,1,1
45,This sub - graph uses attention to compute context aware representations of words in a sentence that take into account both the ordering and identity of all the other words .,Model Toolkit,Transformer,text-classification,6,16,0.5161290322580645,44,0.2972972972972973,2,0.1176470588235294,1,1
46,The context aware word representations are converted to a fixed length sentence encoding vector by computing the element - wise sum of the representations at each word position .,Model Toolkit,Transformer,text-classification,6,17,0.5483870967741935,45,0.304054054054054,3,0.1764705882352941,1,1
47,The encoder takes as input a lowercased PTB tokenized string and outputs a 512 dimensional vector as the sentence embedding .,Model Toolkit,Transformer,text-classification,6,18,0.5806451612903226,46,0.3108108108108108,4,0.2352941176470588,1,0
48,The encoding model is designed to be as general purpose as possible .,Model Toolkit,Transformer,text-classification,6,19,0.6129032258064516,47,0.3175675675675675,5,0.2941176470588235,1,1
49,This is accomplished by using multi-task learning whereby a single encoding model is used to feed multiple downstream tasks .,Model Toolkit,Transformer,text-classification,6,20,0.6451612903225806,48,0.3243243243243243,6,0.3529411764705882,1,1
50,The supported tasks include : a Skip - Thought like task for the unsupervised learning from arbitrary running text ; a conversational input - response task for the inclusion of parsed conversational data ; and classification tasks for training on supervised data .,Model Toolkit,Transformer,text-classification,6,21,0.6774193548387096,49,0.3310810810810811,7,0.4117647058823529,1,0
51,The Skip - Thought task replaces the LSTM used in the original formulation with a model based on the Transformer architecture .,Model Toolkit,Transformer,text-classification,6,22,0.7096774193548387,50,0.3378378378378378,8,0.4705882352941176,1,0
52,"As will be shown in the experimental results below , the transformer based encoder achieves the best overall transfer task performance .",Model Toolkit,Transformer,text-classification,6,23,0.7419354838709677,51,0.3445945945945945,9,0.5294117647058824,1,0
53,"However , this comes at the cost of compute time and memory usage scaling dramatically with sentence length .",Model Toolkit,Transformer,text-classification,6,24,0.7741935483870968,52,0.3513513513513513,10,0.5882352941176471,1,0
54,Deep Averaging Network ( DAN ),Model Toolkit,Transformer,text-classification,6,25,0.8064516129032258,53,0.3581081081081081,11,0.6470588235294118,1,0
55,The second encoding model makes use of a deep averaging network ( DAN ) whereby input embeddings for words and bi-grams are first averaged together and then passed through a feedforward deep neural network ( DNN ) to produce sentence embeddings .,Model Toolkit,Transformer,text-classification,6,26,0.8387096774193549,54,0.3648648648648648,12,0.7058823529411765,1,1
56,"Similar to the Transformer encoder , the DAN encoder takes as input a lowercased PTB tokenized string and outputs a 512 dimensional sentence embedding .",Model Toolkit,Transformer,text-classification,6,27,0.8709677419354839,55,0.3716216216216216,13,0.7647058823529411,1,1
57,The DAN encoder is trained similarly to the Transformer based encoder .,Model Toolkit,Transformer,text-classification,6,28,0.9032258064516128,56,0.3783783783783784,14,0.8235294117647058,1,0
58,We make use of mul-titask learning whereby a single DAN encoder is used to supply sentence embeddings for multiple downstream tasks .,Model Toolkit,Transformer,text-classification,6,29,0.935483870967742,57,0.3851351351351351,15,0.8823529411764706,1,1
59,The primary advantage of the DAN encoder is that compute time is linear in the length of the input sequence .,Model Toolkit,Transformer,text-classification,6,30,0.967741935483871,58,0.3918918918918919,16,0.9411764705882352,1,1
60,"Similar to , our results demonstrate that DANs achieve strong baseline performance on text classification tasks .",Model Toolkit,Transformer,text-classification,6,31,1.0,59,0.3986486486486486,17,1.0,1,0
61,Encoder Training Data,,,text-classification,6,0,0.0,60,0.4054054054054054,0,0.0,1,0
62,Unsupervised training data for the sentence encoding models are drawn from a variety of web sources .,Encoder Training Data,Encoder Training Data,text-classification,6,1,0.05,61,0.4121621621621621,1,0.25,1,0
63,"The sources are Wikipedia , web news , web question - answer pages and discussion forums .",Encoder Training Data,Encoder Training Data,text-classification,6,2,0.1,62,0.4189189189189189,2,0.5,1,0
64,We augment unsupervised learning with training on supervised data from the Stanford Natural Language Inference ( SNLI ) corpus .,Encoder Training Data,Encoder Training Data,text-classification,6,3,0.15,63,0.4256756756756756,3,0.75,1,0
65,"Similar to the findings of , we observe that training to SNLI improves transfer performance .",Encoder Training Data,Encoder Training Data,text-classification,6,4,0.2,64,0.4324324324324324,4,1.0,1,0
66,Transfer Tasks,Encoder Training Data,,text-classification,6,5,0.25,65,0.4391891891891892,0,0.0,1,0
67,This section presents an overview of the data used for the transfer learning experiments and the Word Embedding Association Test ( WEAT ) data used to characterize model bias .,Encoder Training Data,Transfer Tasks,text-classification,6,6,0.3,66,0.4459459459459459,1,0.0666666666666666,1,0
68,"summarizes the number of samples provided by the test portion of each evaluation set and , when available , the size of the dev and training data .",Encoder Training Data,Transfer Tasks,text-classification,6,7,0.35,67,0.4527027027027027,2,0.1333333333333333,1,0
69,MR : Movie review snippet sentiment on a five star scale .,Encoder Training Data,Transfer Tasks,text-classification,6,8,0.4,68,0.4594594594594595,3,0.2,1,1
70,CR : Sentiment of sentences mined from customer reviews .,Encoder Training Data,Transfer Tasks,text-classification,6,9,0.45,69,0.4662162162162162,4,0.2666666666666666,1,1
71,SUBJ : Subjectivity of sentences from movie reviews and plot summaries .,Encoder Training Data,Transfer Tasks,text-classification,6,10,0.5,70,0.4729729729729729,5,0.3333333333333333,1,1
72,MPQA : Phrase level opinion polarity from news data .,Encoder Training Data,Transfer Tasks,text-classification,6,11,0.55,71,0.4797297297297297,6,0.4,1,1
73,TREC : Fine grained question classification sourced from TREC .,Encoder Training Data,Transfer Tasks,text-classification,6,12,0.6,72,0.4864864864864865,7,0.4666666666666667,1,1
74,SST : Binary phrase level sentiment classification .,Encoder Training Data,Transfer Tasks,text-classification,6,13,0.65,73,0.4932432432432432,8,0.5333333333333333,1,1
75,STS Benchmark : Semantic textual similarity ( STS ) between sentence pairs scored by Pearson correlation with human judgments .,Encoder Training Data,Transfer Tasks,text-classification,6,14,0.7,74,0.5,9,0.6,1,1
76,WEAT : Word pairs from the psychology literature on implicit association tests ( IAT ) that are used to characterize model bias .,Encoder Training Data,Transfer Tasks,text-classification,6,15,0.75,75,0.5067567567567568,10,0.6666666666666666,1,1
77,"For sentence classification transfer tasks , the output of the transformer and DAN sentence encoders are provided to a task specific DNN .",Encoder Training Data,Transfer Tasks,text-classification,6,16,0.8,76,0.5135135135135135,11,0.7333333333333333,1,0
78,"For the pairwise semantic similarity task , we directly assess the similarity of the sentence embeddings produced by our two encoders .",Encoder Training Data,Transfer Tasks,text-classification,6,17,0.85,77,0.5202702702702703,12,0.8,1,0
79,"As shown Eq. 1 , we first compute the cosine similarity of the two sentence embeddings and then use arccos to convert the cosine similarity into an angular distance .",Encoder Training Data,Transfer Tasks,text-classification,6,18,0.9,78,0.527027027027027,13,0.8666666666666667,1,0
80,"5 sim ( u , v ) = 1 ? arccos u v | | u || | | v|| /?",Encoder Training Data,Transfer Tasks,text-classification,6,19,0.95,79,0.5337837837837838,14,0.9333333333333332,1,0
81,( 1 ),Encoder Training Data,Transfer Tasks,text-classification,6,20,1.0,80,0.5405405405405406,15,1.0,1,0
82,Baselines,,,text-classification,6,0,0.0,81,0.5472972972972973,0,0.0,1,0
83,"For each transfer task , we include baselines that only make use of word level transfer and baselines that make use of no transfer learning at all .",Baselines,Baselines,text-classification,6,1,0.2,82,0.5540540540540541,1,0.2,1,0
84,"For word level transfer , we use word embeddings from a word2 vec skip - gram model trained on a corpus of news data .",Baselines,Baselines,text-classification,6,2,0.4,83,0.5608108108108109,2,0.4,1,0
85,The pretrained word embeddings are included as input to two model types : a convolutional neural network models ( CNN ) ; a DAN .,Baselines,Baselines,text-classification,6,3,0.6,84,0.5675675675675675,3,0.6,1,0
86,The baselines that use pretrained word embeddings allow us to contrast word versus sentence level transfer .,Baselines,Baselines,text-classification,6,4,0.8,85,0.5743243243243243,4,0.8,1,0
87,Additional baseline CNN and DAN models are trained without using any pretrained word or sentence embeddings .,Baselines,Baselines,text-classification,6,5,1.0,86,0.581081081081081,5,1.0,1,1
88,Combined Transfer Models,,,text-classification,6,0,0.0,87,0.5878378378378378,0,0.0,1,0
89,We explore combining the sentence and word level transfer models by concatenating their representations prior to feeding the combined representation :,Combined Transfer Models,Combined Transfer Models,text-classification,6,1,0.0909090909090909,88,0.5945945945945946,1,0.0909090909090909,1,0
90,Model performance on transfer tasks .,Combined Transfer Models,Combined Transfer Models,text-classification,6,2,0.1818181818181818,89,0.6013513513513513,2,0.1818181818181818,1,0
91,USE,Combined Transfer Models,,text-classification,6,3,0.2727272727272727,90,0.6081081081081081,3,0.2727272727272727,1,0
92,T is the universal sentence encoder ( USE ) using Transformer .,Combined Transfer Models,USE,text-classification,6,4,0.3636363636363636,91,0.6148648648648649,4,0.3636363636363636,1,0
93,USE,Combined Transfer Models,,text-classification,6,5,0.4545454545454545,92,0.6216216216216216,5,0.4545454545454545,1,0
94,Dis the universal encoder DAN model .,Combined Transfer Models,USE,text-classification,6,6,0.5454545454545454,93,0.6283783783783784,6,0.5454545454545454,1,0
95,"Models tagged with w2 v w.e. make use of pre-training word2vec skip - gram embeddings for the transfer task model , while models tagged with lrn w.e. use randomly initialized word embeddings that are learned only on the transfer task data .",Combined Transfer Models,USE,text-classification,6,7,0.6363636363636364,94,0.6351351351351351,7,0.6363636363636364,1,0
96,Accuracy is reported for all evaluations except STS Bench where we report the Pearson correlation of the similarity scores with human judgments .,Combined Transfer Models,USE,text-classification,6,8,0.7272727272727273,95,0.6418918918918919,8,0.7272727272727273,1,0
97,Pairwise similarity scores are computed directly using the sentence embeddings from the universal sentence encoder as in Eq. ( 1 ) .,Combined Transfer Models,USE,text-classification,6,9,0.8181818181818182,96,0.6486486486486487,9,0.8181818181818182,1,0
98,to the transfer task classification layers .,Combined Transfer Models,USE,text-classification,6,10,0.9090909090909092,97,0.6554054054054054,10,0.9090909090909092,1,0
99,"For completeness , we also explore concatenating the representations from sentence level transfer models with the baseline models that do not make use of word level transfer learning .",Combined Transfer Models,USE,text-classification,6,11,1.0,98,0.6621621621621622,11,1.0,1,0
100,Experiments,,,text-classification,6,0,0.0,99,0.668918918918919,0,0.0,1,0
101,Transfer task model hyperparamaters are tuned using a combination of Vizier and light manual tuning .,Experiments,Experiments,text-classification,6,1,0.1111111111111111,100,0.6756756756756757,1,0.1111111111111111,1,0
102,"When available , model hyperparameters are tuned using task dev sets .",Experiments,Experiments,text-classification,6,2,0.2222222222222222,101,0.6824324324324325,2,0.2222222222222222,1,0
103,"Otherwise , hyperparameters are tuned by crossvalidation on the task training data when available or the evaluation test data when neither training nor dev data are provided .",Experiments,Experiments,text-classification,6,3,0.3333333333333333,102,0.6891891891891891,3,0.3333333333333333,1,0
104,Training repeats ten times for each transfer task model with different randomly initialized weights and we report evaluation results by averaging across runs .,Experiments,Experiments,text-classification,6,4,0.4444444444444444,103,0.6959459459459459,4,0.4444444444444444,1,0
105,Transfer learning is critically important when training data fora target task is limited .,Experiments,Experiments,text-classification,6,5,0.5555555555555556,104,0.7027027027027027,5,0.5555555555555556,1,0
106,We explore the impact on task performance of varying the amount of training data available for the task both with and without the use of transfer learning .,Experiments,Experiments,text-classification,6,6,0.6666666666666666,105,0.7094594594594594,6,0.6666666666666666,1,0
107,"Contrasting the transformer and DAN based encoders , we demonstrate trade - offs in model complexity and the amount of data required to reach a desired level of accuracy on a task .",Experiments,Experiments,text-classification,6,7,0.7777777777777778,106,0.7162162162162162,7,0.7777777777777778,1,0
108,"To assess bias in our encoding models , we evaluate the strength of various associations learned by our model on WEAT word lists .",Experiments,Experiments,text-classification,6,8,0.8888888888888888,107,0.722972972972973,8,0.8888888888888888,1,0
109,We compare our result to those of who discovered that word embeddings could be used to reproduce human performance on implicit association tasks for both benign and potentially undesirable associations .,Experiments,Experiments,text-classification,6,9,1.0,108,0.7297297297297297,9,1.0,1,0
110,Results,,,text-classification,6,0,0.0,109,0.7364864864864865,0,0.0,1,0
111,Transfer task performance is summarized in Table 2 . ,Results,Results,text-classification,6,1,0.032258064516129,110,0.7432432432432432,1,0.0833333333333333,1,0
112,We observe that transfer learning from the transformer based sentence encoder usually performs as good or better than transfer learning from the DAN encoder .,Results,Results,text-classification,6,2,0.064516129032258,111,0.75,2,0.1666666666666666,1,1
113,"Hoewver , transfer learning using the simpler and fast DAN encoder can for some tasks perform as well or better than the more sophisticated transformer encoder .",Results,Results,text-classification,6,3,0.0967741935483871,112,0.7567567567567568,3,0.25,1,0
114,Models that make use of sentence level transfer learning tend to perform better than models that only use word level transfer .,Results,Results,text-classification,6,4,0.1290322580645161,113,0.7635135135135135,4,0.3333333333333333,1,1
115,The best performance on most tasks is obtained by models that make use of both sentence and word level transfer .,Results,Results,text-classification,6,5,0.1612903225806451,114,0.7702702702702703,5,0.4166666666666667,1,0
116,illustrates transfer task performance for varying amounts of training data .,Results,Results,text-classification,6,6,0.1935483870967742,115,0.777027027027027,6,0.5,1,0
117,"We observe that , for smaller quantities of data , sentence level transfer learning can achieve surprisingly good task performance .",Results,Results,text-classification,6,7,0.2258064516129032,116,0.7837837837837838,7,0.5833333333333334,1,1
118,"As the training set size increases , models that do not make use of transfer learning approach the performance of the other models .",Results,Results,text-classification,6,8,0.2580645161290322,117,0.7905405405405406,8,0.6666666666666666,1,1
119,contrasts 's findings on bias within GloVe embeddings with the DAN variant of the universal encoder .,Results,Results,text-classification,6,9,0.2903225806451613,118,0.7972972972972973,9,0.75,1,0
120,"Similar to GloVe , our model reproduces human associations between flowers vs. insects and pleasantness vs. unpleasantness .",Results,Results,text-classification,6,10,0.3225806451612903,119,0.8040540540540541,10,0.8333333333333334,1,0
121,"However , our model demonstrates weaker associations than GloVe for probes targeted at revealing at ageism , racism and sexism .",Results,Results,text-classification,6,11,0.3548387096774194,120,0.8108108108108109,11,0.9166666666666666,1,0
122,The differences in word association patterns can be attributed to differences in the training data composition and the mixture of tasks used to train the sentence embeddings .,Results,Results,text-classification,6,12,0.3870967741935484,121,0.8175675675675675,12,1.0,1,0
123,Discussion,Results,,text-classification,6,13,0.4193548387096774,122,0.8243243243243243,0,0.0,1,0
124,Transfer learning leads to performance improvements on many tasks .,Results,Discussion,text-classification,6,14,0.4516129032258064,123,0.831081081081081,1,0.2,1,0
125,Using transfer learning is more critical when less training data is available .,Results,Discussion,text-classification,6,15,0.4838709677419355,124,0.8378378378378378,2,0.4,1,0
126,"When task performance is close , the correct modeling choice should take into account engineering trade - offs regarding the memory and compute 6 Researchers and developers are strongly encouraged to independently verify whether biases in their overall model or model components impacts their use case .",Results,Discussion,text-classification,6,16,0.5161290322580645,125,0.8445945945945946,3,0.6,1,0
127,For resources on ML fairness visit https://developers.google.com/machinelearning/fairness-overview/.,Results,Discussion,text-classification,6,17,0.5483870967741935,126,0.8513513513513513,4,0.8,1,0
128,resource requirements introduced by the different models that could be used .,Results,Discussion,text-classification,6,18,0.5806451612903226,127,0.8581081081081081,5,1.0,1,0
129,Resource Usage,Results,,text-classification,6,19,0.6129032258064516,128,0.8648648648648649,0,0.0,1,0
130,This section describes memory and compute resource usage for the transformer and DAN sentence encoding models for different sentence lengths .,Results,Resource Usage,text-classification,6,20,0.6451612903225806,129,0.8716216216216216,1,0.0833333333333333,1,0
131,Figure 2 plots model resource usage against sentence length .,Results,Resource Usage,text-classification,6,21,0.6774193548387096,130,0.8783783783783784,2,0.1666666666666666,1,0
132,Compute Usage,Results,,text-classification,6,22,0.7096774193548387,131,0.8851351351351351,3,0.25,1,0
133,"The transformer model time complexity is O ( n 2 ) in sentence length , while the DAN model is O ( n ) .",Results,Compute Usage,text-classification,6,23,0.7419354838709677,132,0.8918918918918919,4,0.3333333333333333,1,0
134,"As seen in ( a - b ) , for short sentences , the transformer encoding model is only moderately slower than the much simpler DAN model .",Results,Compute Usage,text-classification,6,24,0.7741935483870968,133,0.8986486486486487,5,0.4166666666666667,1,0
135,"However , compute time for transformer increases noticeably as sentence length increases .",Results,Compute Usage,text-classification,6,25,0.8064516129032258,134,0.9054054054054054,6,0.5,1,0
136,"In contrast , the compute time for the DAN model stays nearly constant as sentence length is increased .",Results,Compute Usage,text-classification,6,26,0.8387096774193549,135,0.9121621621621622,7,0.5833333333333334,1,0
137,"Since the DAN model is remarkably computational efficient , using GPUs over CPUs will often have a much larger practical impact for the transformer based encoder .",Results,Compute Usage,text-classification,6,27,0.8709677419354839,136,0.918918918918919,8,0.6666666666666666,1,0
138,Memory Usage,Results,,text-classification,6,28,0.9032258064516128,137,0.9256756756756755,9,0.75,1,0
139,"The transformer model space complexity also scales quadratically , O ( n 2 ) , in sentence length , while the DAN model space complexity is constant in the length of the sentence . Similar to compute usage , memory usage for the transformer model increases quickly with sentence length , while the memory usage for the DAN model remains constant .",Results,Memory Usage,text-classification,6,29,0.935483870967742,138,0.9324324324324323,10,0.8333333333333334,1,0
140,"We note that , for the DAN model , memory usage is dominated by the parameters used to store the model unigram and bigram embeddings .",Results,Memory Usage,text-classification,6,30,0.967741935483871,139,0.9391891891891893,11,0.9166666666666666,1,0
141,"Since the transformer model only needs to store unigram embeddings , for short sequences it requires nearly half as much memory as the DAN model .",Results,Memory Usage,text-classification,6,31,1.0,140,0.945945945945946,12,1.0,1,0
142,Conclusion,,,text-classification,6,0,0.0,141,0.9527027027027029,0,0.0,1,0
143,Both the transformer and DAN based universal encoding models provide sentence level embeddings that demonstrate strong transfer performance on a number of NLP tasks .,Conclusion,Conclusion,text-classification,6,1,0.1666666666666666,142,0.9594594594594594,1,0.1666666666666666,0,0
144,The sentence level embeddings surpass the performance of transfer learning using word level embeddings alone .,Conclusion,Conclusion,text-classification,6,2,0.3333333333333333,143,0.9662162162162162,2,0.3333333333333333,0,0
145,Models that make use of sentence and word level transfer achieve the best overall performance .,Conclusion,Conclusion,text-classification,6,3,0.5,144,0.972972972972973,3,0.5,0,0
146,We observe that transfer learning is most helpful when limited training data is available for the transfer task .,Conclusion,Conclusion,text-classification,6,4,0.6666666666666666,145,0.9797297297297296,4,0.6666666666666666,0,0
147,The encoding models make different trade - offs regarding accuracy and model complexity that should be considered when choosing the best model fora particular application .,Conclusion,Conclusion,text-classification,6,5,0.8333333333333334,146,0.9864864864864864,5,0.8333333333333334,0,0
148,The pre-trained encoding models will be made publicly available for research and use in applications that can benefit from a better understanding of natural language .,Conclusion,Conclusion,text-classification,6,6,1.0,147,0.9932432432432432,6,1.0,0,0
1,title,,,text-classification,7,0,0.0,0,0.0,0,0.0,1,0
2,Investigating Capsule Networks with Dynamic Routing for Text Classification,title,,text-classification,7,1,0.0,1,0.0041152263374485,1,0.0,1,1
3,abstract,,,text-classification,7,0,0.0,2,0.0082304526748971,0,0.0,1,0
4,"In this study , we explore capsule networks with dynamic routing for text classification .",abstract,abstract,text-classification,7,1,0.1428571428571428,3,0.0123456790123456,1,0.1428571428571428,1,0
5,"We propose three strategies to stabilize the dynamic routing process to alleviate the disturbance of some noise capsules which may contain "" background "" information or have not been successfully trained .",abstract,abstract,text-classification,7,2,0.2857142857142857,4,0.0164609053497942,2,0.2857142857142857,1,0
6,A series of experiments are conducted with capsule networks on six text classification benchmarks .,abstract,abstract,text-classification,7,3,0.4285714285714285,5,0.0205761316872428,3,0.4285714285714285,1,0
7,"Capsule networks achieve competitive results over the compared baseline methods on 4 out of 6 datasets , which shows the effectiveness of capsule networks for text classification .",abstract,abstract,text-classification,7,4,0.5714285714285714,6,0.0246913580246913,4,0.5714285714285714,1,0
8,We additionally show that capsule networks exhibit significant improvement when transfer single - label to multi-label text classification over the competitors .,abstract,abstract,text-classification,7,5,0.7142857142857143,7,0.0288065843621399,5,0.7142857142857143,1,0
9,"To the best of our knowledge , this is the first work that capsule networks have been empirically investigated for text modeling 1 . * Corresponding author ( min.yang@siat.ac.cn )",abstract,abstract,text-classification,7,6,0.8571428571428571,8,0.0329218106995884,6,0.8571428571428571,1,0
10,1 Codes are publicly available at : https://github.com/andyweizhao/capsule_text_classification .,abstract,abstract,text-classification,7,7,1.0,9,0.037037037037037,7,1.0,1,1
11,Introduction,,,text-classification,7,0,0.0,10,0.0411522633744856,0,0.0,1,0
12,Modeling articles or sentences computationally is a fundamental topic in natural language processing .,Introduction,Introduction,text-classification,7,1,0.037037037037037,11,0.0452674897119341,1,0.037037037037037,1,1
13,"It could be as simple as a keyword / phrase matching problem , but it could also be a nontrivial problem if compositions , hierarchies , and structures of texts are considered .",Introduction,Introduction,text-classification,7,2,0.074074074074074,12,0.0493827160493827,2,0.074074074074074,1,0
14,"For example , a news article which mentions a single phrase "" US election "" maybe categorized into the political news with high probability .",Introduction,Introduction,text-classification,7,3,0.1111111111111111,13,0.0534979423868312,3,0.1111111111111111,1,0
15,"But it could be very difficult fora computer to predict which presidential candidate is favored by its author , or whether the author 's view in the article is more liberal or more conservative .",Introduction,Introduction,text-classification,7,4,0.1481481481481481,14,0.0576131687242798,4,0.1481481481481481,1,0
16,"Earlier efforts in modeling texts have achieved limited success on text categorization using a simple bag - of - words classifier , implying understanding the meaning of the individual word or n-gram is a necessary step towards more sophisticated models .",Introduction,Introduction,text-classification,7,5,0.1851851851851851,15,0.0617283950617283,5,0.1851851851851851,1,1
17,"It is therefore not a surprise that distributed representations of words , a.k.a. word embeddings , have received great attention from NLP community addressing the question "" what "" to be modeled at the basic level .",Introduction,Introduction,text-classification,7,6,0.2222222222222222,16,0.0658436213991769,6,0.2222222222222222,1,0
18,"In order to model higher level concepts and facts in texts , an NLP researcher has to think cautiously the so - called "" what "" question : what is actually modeled beyond word meanings .",Introduction,Introduction,text-classification,7,7,0.2592592592592592,17,0.0699588477366255,7,0.2592592592592592,1,0
19,"A common approach to the question is to treat the texts as sequences and focus on their spatial patterns , whose representatives include convolutional neural networks ( CNNs ) and long shortterm memory networks ( LSTMs ) .",Introduction,Introduction,text-classification,7,8,0.2962962962962963,18,0.074074074074074,8,0.2962962962962963,1,0
20,"Another common approach is to completely ignore the order of words but focus on their compositions as a collection , whose representatives include probabilistic topic modeling and Earth Mover 's Distance based modeling .",Introduction,Introduction,text-classification,7,9,0.3333333333333333,19,0.0781893004115226,9,0.3333333333333333,1,0
21,"Those two approaches , albeit quite different from the computational perspective , actually follow a common measure to be diagnosed regarding their answers to the "" what "" question .",Introduction,Introduction,text-classification,7,10,0.3703703703703703,20,0.0823045267489712,10,0.3703703703703703,1,0
22,"In neural network approaches , spatial patterns aggregated at lower levels contribute to representing higher level concepts .",Introduction,Introduction,text-classification,7,11,0.4074074074074074,21,0.0864197530864197,11,0.4074074074074074,1,0
23,"Here , they form a recursive process to articulate what to be modeled .",Introduction,Introduction,text-classification,7,12,0.4444444444444444,22,0.0905349794238683,12,0.4444444444444444,1,0
24,"For example , CNN builds convolutional feature detectors to extract local patterns from a window of vector sequences and uses max - pooling to select the most prominent ones .",Introduction,Introduction,text-classification,7,13,0.4814814814814814,23,0.0946502057613168,13,0.4814814814814814,1,0
25,It then hierarchically builds such pattern extraction pipelines at multiple levels .,Introduction,Introduction,text-classification,7,14,0.5185185185185185,24,0.0987654320987654,14,0.5185185185185185,1,0
26,"Being a spatially sensitive model , CNN pays a price for the inefficiency of replicating feature detectors on a grid .",Introduction,Introduction,text-classification,7,15,0.5555555555555556,25,0.102880658436214,15,0.5555555555555556,1,0
27,"As argued in , one has to choose between replicating detectors whose size grows exponentially with the number of dimensions , or increasing the volume of the labeled training set in a similar exponential way .",Introduction,Introduction,text-classification,7,16,0.5925925925925926,26,0.1069958847736625,16,0.5925925925925926,1,0
28,"On the other hand , methods that are spatially insensitive are perfectly efficient at the inference time regardless of any order of words or local patterns .",Introduction,Introduction,text-classification,7,17,0.6296296296296297,27,0.1111111111111111,17,0.6296296296296297,1,0
29,"However , they are unavoidably more restricted to encode rich structures presented in a sequence .",Introduction,Introduction,text-classification,7,18,0.6666666666666666,28,0.1152263374485596,18,0.6666666666666666,1,0
30,Improving the efficiency to encode spatial patterns while keeping the flexibility of their representation capability is thus a central issue .,Introduction,Introduction,text-classification,7,19,0.7037037037037037,29,0.1193415637860082,19,0.7037037037037037,1,0
31,A recent method called capsule network introduced by possesses this attractive potential to address the aforementioned issue .,Introduction,Introduction,text-classification,7,20,0.7407407407407407,30,0.1234567901234567,20,0.7407407407407407,1,0
32,They introduce an iterative routing process to decide the credit attribution between nodes from lower and higher layers .,Introduction,Introduction,text-classification,7,21,0.7777777777777778,31,0.1275720164609053,21,0.7777777777777778,1,0
33,A metaphor ( also as an argument ) they made is that human visual system intelligently assigns parts to wholes at the inference time without hard - coding patterns to be perspective relevant .,Introduction,Introduction,text-classification,7,22,0.8148148148148148,32,0.1316872427983539,22,0.8148148148148148,1,0
34,"As an outcome , their model could encode the intrinsic spatial relationship between apart and a whole constituting viewpoint invariant knowledge that automatically generalizes to novel viewpoints .",Introduction,Introduction,text-classification,7,23,0.8518518518518519,33,0.1358024691358024,23,0.8518518518518519,1,0
35,"In our work , we follow a similar spirit to use this technique in modeling texts .",Introduction,Introduction,text-classification,7,24,0.8888888888888888,34,0.139917695473251,24,0.8888888888888888,1,0
36,"Three strategies are proposed to stabilize the dynamic routing process to alleviate the disturbance of some noise capsules which may contain "" background "" information such as stop words and the words that are unrelated to specific categories .",Introduction,Introduction,text-classification,7,25,0.925925925925926,35,0.1440329218106996,25,0.925925925925926,1,0
37,We conduct a series of experiments with capsule networks on top of the pre-trained word vectors for six text classification benchmarks .,Introduction,Introduction,text-classification,7,26,0.9629629629629628,36,0.1481481481481481,26,0.9629629629629628,1,0
38,"More importantly , we show that capsule networks achieves significant improvement when transferring singlelabel to multi-label text classifications over strong baseline methods .",Introduction,Introduction,text-classification,7,27,1.0,37,0.1522633744855967,27,1.0,1,0
39,Our Model,,,text-classification,7,0,0.0,38,0.1563786008230452,0,0.0,1,0
40,"Our capsule network , depicted in , is a variant of the capsule networks proposed in .",Our Model,Our Model,text-classification,7,1,0.0106382978723404,39,0.1604938271604938,1,0.0555555555555555,1,1
41,"It consists of four layers : ngram convolutional layer , primary capsule layer , convolutional capsule layer , and fully connected capsule layer .",Our Model,Our Model,text-classification,7,2,0.0212765957446808,40,0.1646090534979424,2,0.1111111111111111,1,1
42,"In addition , we explore two capsule frameworks to integrate these four components in different ways .",Our Model,Our Model,text-classification,7,3,0.0319148936170212,41,0.1687242798353909,3,0.1666666666666666,1,1
43,"In the rest of this section , we elaborate the key components in detail .",Our Model,Our Model,text-classification,7,4,0.0425531914893617,42,0.1728395061728395,4,0.2222222222222222,1,0
44,N - gram Convolutional Layer,Our Model,Our Model,text-classification,7,5,0.0531914893617021,43,0.176954732510288,5,0.2777777777777778,1,0
45,This layer is a standard convolutional layer which extracts n-gram features at different positions of a sentence through various convolutional filters .,Our Model,Our Model,text-classification,7,6,0.0638297872340425,44,0.1810699588477366,6,0.3333333333333333,1,1
46,Suppose x ?,Our Model,Our Model,text-classification,7,7,0.0744680851063829,45,0.1851851851851851,7,0.3888888888888889,1,0
47,R LV denotes the input sentence representation where L is the length of the sentence and V is the embedding size of words .,Our Model,Our Model,text-classification,7,8,0.0851063829787234,46,0.1893004115226337,8,0.4444444444444444,1,0
48,Let xi ?,Our Model,Our Model,text-classification,7,9,0.0957446808510638,47,0.1934156378600823,9,0.5,1,0
49,RV be the V - dimensional word vector corresponding to the i - th word in the sentence .,Our Model,Our Model,text-classification,7,10,0.1063829787234042,48,0.1975308641975308,10,0.5555555555555556,1,0
50,Let W a ? R K 1,Our Model,Our Model,text-classification,7,11,0.1170212765957446,49,0.2016460905349794,11,0.6111111111111112,1,0
51,"V be the filter for the convolution operation , where K 1 is the N - gram size while sliding over a sentence for the purpose of detecting features at different positions .",Our Model,Our Model,text-classification,7,12,0.1276595744680851,50,0.205761316872428,12,0.6666666666666666,1,0
52,A filter W a convolves with the word - window x i:i+K 1 ? 1 at each possible position ( with stride of 1 ) to produce a column feature map ma ?,Our Model,Our Model,text-classification,7,13,0.1382978723404255,51,0.2098765432098765,13,0.7222222222222222,1,0
53,"R L?K 1 + 1 , each element ma i ?",Our Model,Our Model,text-classification,7,14,0.1489361702127659,52,0.2139917695473251,14,0.7777777777777778,1,0
54,R of the feature map is produced by,Our Model,Our Model,text-classification,7,15,0.1595744680851064,53,0.2181069958847736,15,0.8333333333333334,1,0
55,"where is element - wise multiplication , b 0 is a bias term , and f is a nonlinear activate function ( i.e. , ReLU ) .",Our Model,Our Model,text-classification,7,16,0.1702127659574468,54,0.2222222222222222,16,0.8888888888888888,1,0
56,We have described the process by which one feature is extracted from one filter .,Our Model,Our Model,text-classification,7,17,0.1808510638297872,55,0.2263374485596707,17,0.9444444444444444,1,0
57,"Hence , fora = 1 , . . . , B , totally B filters with the same N - gram size , one can generate B feature maps which can be rearranged as",Our Model,Our Model,text-classification,7,18,0.1914893617021276,56,0.2304526748971193,18,1.0,1,0
58,Primary Capsule Layer,Our Model,,text-classification,7,19,0.202127659574468,57,0.2345679012345679,0,0.0,1,0
59,This is the first capsule layer in which the capsules replace the scalar - output feature detectors of CNNs with vector- output capsules to preserve the instantiated parameters such as the local order of words and semantic representations of words .,Our Model,Primary Capsule Layer,text-classification,7,20,0.2127659574468085,58,0.2386831275720164,1,0.1428571428571428,1,1
60,Suppose pi ?,Our Model,Primary Capsule Layer,text-classification,7,21,0.2234042553191489,59,0.242798353909465,2,0.2857142857142857,1,0
61,"Rd denotes the instantiated parameters of a capsule , where dis the dimension of the capsule .",Our Model,Primary Capsule Layer,text-classification,7,22,0.2340425531914893,60,0.2469135802469135,3,0.4285714285714285,1,0
62,Let W b ? R,Our Model,Primary Capsule Layer,text-classification,7,23,0.2446808510638297,61,0.2510288065843621,4,0.5714285714285714,1,0
63,Bd be the filter shared in different sliding windows .,Our Model,Primary Capsule Layer,text-classification,7,24,0.2553191489361702,62,0.2551440329218107,5,0.7142857142857143,1,0
64,"For each matrix multiplication , we have a window sliding over each Ngram vector denoted as M i ?",Our Model,Primary Capsule Layer,text-classification,7,25,0.2659574468085106,63,0.2592592592592592,6,0.8571428571428571,1,0
65,"R B , then the corresponding N - gram phrases in the form of capsule are produced with",Our Model,Primary Capsule Layer,text-classification,7,26,0.2765957446808511,64,0.2633744855967078,7,1.0,1,0
66,ConvCaps Capsule,Our Model,,text-classification,7,27,0.2872340425531915,65,0.2674897119341564,0,0.0,1,0
67,"Probability column - list of capsules p ? R ( L?K 1 + 1 ) d , each capsule pi ?",Our Model,ConvCaps Capsule,text-classification,7,28,0.2978723404255319,66,0.2716049382716049,1,0.0588235294117647,1,0
68,Rd in the column - list is computed as,Our Model,ConvCaps Capsule,text-classification,7,29,0.3085106382978723,67,0.2757201646090535,2,0.1176470588235294,1,0
69,"where g is nonlinear squash function through the entire vector , b 1 is the capsule bias term .",Our Model,ConvCaps Capsule,text-classification,7,30,0.3191489361702128,68,0.279835390946502,3,0.1764705882352941,1,0
70,"For all C filters , the generated capsule feature maps can be rearranged as",Our Model,ConvCaps Capsule,text-classification,7,31,0.3297872340425531,69,0.2839506172839506,4,0.2352941176470588,1,0
71,where totally ( L ? K 1 + 1 ) C d-dimensional vectors are collected as capsules in P .,Our Model,ConvCaps Capsule,text-classification,7,32,0.3404255319148936,70,0.2880658436213992,5,0.2941176470588235,1,0
72,Child - Parent Relationships,Our Model,,text-classification,7,33,0.351063829787234,71,0.2921810699588477,6,0.3529411764705882,1,0
73,"As argued in , capsule network tries to address the representational limitation and exponential inefficiencies of convolutions with transformation matrices .",Our Model,Child - Parent Relationships,text-classification,7,34,0.3617021276595745,72,0.2962962962962963,7,0.4117647058823529,1,0
74,It allows the networks to automatically learn child - parent ( or partwhole ) relationships .,Our Model,Child - Parent Relationships,text-classification,7,35,0.3723404255319149,73,0.3004115226337449,8,0.4705882352941176,1,0
75,"In text classification tasks , different sentences with the same category are supposed to have the similar topic but with different viewpoints .",Our Model,Child - Parent Relationships,text-classification,7,36,0.3829787234042553,74,0.3045267489711934,9,0.5294117647058824,1,0
76,"In this paper , we explore two different types of transformation matrices to generate prediction vector ( vote ) j|i ?",Our Model,Child - Parent Relationships,text-classification,7,37,0.3936170212765957,75,0.3086419753086419,10,0.5882352941176471,1,0
77,Rd from it s child capsule i to the parent capsule j.,Our Model,Child - Parent Relationships,text-classification,7,38,0.4042553191489361,76,0.3127572016460905,11,0.6470588235294118,1,0
78,The first one shares weights W t 1 ?,Our Model,Child - Parent Relationships,text-classification,7,39,0.4148936170212766,77,0.3168724279835391,12,0.7058823529411765,1,0
79,"RN dd across child capsules in the layer below , where N is the number of parent capsules in the layer above .",Our Model,Child - Parent Relationships,text-classification,7,40,0.425531914893617,78,0.3209876543209876,13,0.7647058823529411,1,0
80,"Formally , each corresponding vote can be computed by :",Our Model,Child - Parent Relationships,text-classification,7,41,0.4361702127659574,79,0.3251028806584362,14,0.8235294117647058,1,0
81,where u i is a child - capsule in the layer below and b j|i is the capsule bias term .,Our Model,Child - Parent Relationships,text-classification,7,42,0.4468085106382978,80,0.3292181069958848,15,0.8823529411764706,1,0
82,"In the second design , we replace the shared weight matrix W t 1 j with non-shared weight matrix W t 2 i , j , where the weight matrices W t 2 ?",Our Model,Child - Parent Relationships,text-classification,7,43,0.4574468085106383,81,0.3333333333333333,16,0.9411764705882352,1,0
83,R HN dd and H is the number of child capsules in the layer below .,Our Model,Child - Parent Relationships,text-classification,7,44,0.4680851063829787,82,0.3374485596707819,17,1.0,1,0
84,Dynamic Routing,Our Model,,text-classification,7,45,0.4787234042553192,83,0.3415637860082304,0,0.0,1,0
85,The basic idea of dynamic routing is to construct a non-linear map in an iterative manner ensuring that the output of each capsule gets sent to an appropriate parent in the subsequent layer :,Our Model,Dynamic Routing,text-classification,7,46,0.4893617021276595,84,0.345679012345679,1,0.1111111111111111,1,1
86,"For each potential parent , the capsule network can increase or decrease the connection strength by dynamic routing , which is more effective than the primitive routing strategies such as max - pooling in CNN that essentially detects whether a feature is present in any position of the text , but loses spatial information about the feature .",Our Model,Dynamic Routing,text-classification,7,47,0.5,85,0.3497942386831276,2,0.2222222222222222,1,0
87,We explore three strategies to boost the accuracy of routing process by alleviating the disturbance of some noisy capsules :,Our Model,Dynamic Routing,text-classification,7,48,0.5106382978723404,86,0.3539094650205761,3,0.3333333333333333,1,0
88,Orphan Category,Our Model,,text-classification,7,49,0.5212765957446809,87,0.3580246913580246,4,0.4444444444444444,1,0
89,"Inspired by , an additional "" orphan "" category is added to the network , which can capture the "" background "" information of the text such as stop words and the words that are unrelated to specific categories , helping the capsule network model the child - parent relationship more efficiently .",Our Model,Orphan Category,text-classification,7,50,0.5319148936170213,88,0.3621399176954732,5,0.5555555555555556,1,0
90,"Adding "" orphan "" category in the text is more effective than in image since there is no single consistent "" background "" object in images , while the stop words are consistent in texts such as predicate "" s "" , "" am "" and pronouns "" his "" , "" she "" .",Our Model,Orphan Category,text-classification,7,51,0.5425531914893617,89,0.3662551440329218,6,0.6666666666666666,1,0
91,Leaky - Softmax,Our Model,,text-classification,7,52,0.5531914893617021,90,0.3703703703703703,7,0.7777777777777778,1,0
92,We explore Leaky - Softmax in the place of standard softmax while updating connection strength between the children capsules and their parents .,Our Model,Leaky - Softmax,text-classification,7,53,0.5638297872340425,91,0.3744855967078189,8,0.8888888888888888,1,0
93,"Despite the orphan category in the last capsule layer , we also need a light - weight method between two consecutive layers to route the noise child capsules to extra dimension without any additional parameters and computation consuming .",Our Model,Leaky - Softmax,text-classification,7,54,0.574468085106383,92,0.3786008230452675,9,1.0,1,0
94,Coefficients Amendment,Our Model,,text-classification,7,55,0.5851063829787234,93,0.382716049382716,0,0.0,1,0
95,We also attempt to use the probability of existence of child capsules in the layer below to iteratively amend the connection strength as Eq.6 .,Our Model,Coefficients Amendment,text-classification,7,56,0.5957446808510638,94,0.3868312757201646,1,0.0833333333333333,1,0
96,"Algorithm 1 : Dynamic Routing Algorithm 1 procedure ROUTING ( j|i , j|i , r , l ) 2 Initialize the logits of coupling coefficients b j|i = 0 3 for r iterations do 4 for all capsule i in layer land capsule j in layer l + 1 :",Our Model,Coefficients Amendment,text-classification,7,57,0.6063829787234043,95,0.3909465020576131,2,0.1666666666666666,1,0
97,for all capsule i in layer land capsule j in,Our Model,Coefficients Amendment,text-classification,7,58,0.6170212765957447,96,0.3950617283950617,3,0.25,1,0
98,"Given each prediction vector j|i and its probability of existence j|i , where j|i = i , each iterative coupling coefficient of connection strength c j|i is updated by",Our Model,Coefficients Amendment,text-classification,7,59,0.6276595744680851,97,0.3991769547325103,4,0.3333333333333333,1,0
99,where b j|i is the logits of coupling coefficients .,Our Model,Coefficients Amendment,text-classification,7,60,0.6382978723404256,98,0.4032921810699588,5,0.4166666666666667,1,0
100,Each parent capsule v j in the layer above is a weighted sum overall prediction vectors j|i :,Our Model,Coefficients Amendment,text-classification,7,61,0.648936170212766,99,0.4074074074074074,6,0.5,1,0
101,"where a j is the probabilities of parent capsules , g is nonlinear squash function through the entire vector .",Our Model,Coefficients Amendment,text-classification,7,62,0.6595744680851063,100,0.411522633744856,7,0.5833333333333334,1,0
102,"Once all of the parent capsules are produced , each coupling coefficient b j|i is updated by :",Our Model,Coefficients Amendment,text-classification,7,63,0.6702127659574468,101,0.4156378600823045,8,0.6666666666666666,1,0
103,"For simplicity of notation , the parent capsules and their probabilities in the layer above are denoted as v , a = Routing ( )",Our Model,Coefficients Amendment,text-classification,7,64,0.6808510638297872,102,0.419753086419753,9,0.75,1,0
104,"where denotes all of the child capsules in the layer below , v denotes all of the parent - capsules and their probabilities a.",Our Model,Coefficients Amendment,text-classification,7,65,0.6914893617021277,103,0.4238683127572016,10,0.8333333333333334,1,0
105,Our dynamic routing algorithm is summarized in Algorithm,Our Model,Coefficients Amendment,text-classification,7,66,0.7021276595744681,104,0.4279835390946502,11,0.9166666666666666,1,0
106,1 .,Our Model,Coefficients Amendment,text-classification,7,67,0.7127659574468085,105,0.4320987654320987,12,1.0,1,0
107,Convolutional Capsule Layer,Our Model,,text-classification,7,68,0.723404255319149,106,0.4362139917695473,0,0.0,1,0
108,"In this layer , each capsule is connected only to a local region K 2 C spatially in the layer below .",Our Model,Convolutional Capsule Layer,text-classification,7,69,0.7340425531914894,107,0.4403292181069959,1,0.1111111111111111,1,1
109,Those capsules in the region multiply transformation matrices to learn child - parent relationships followed by routing by agreement to produce parent capsules in the layer above .,Our Model,Convolutional Capsule Layer,text-classification,7,70,0.7446808510638298,108,0.4444444444444444,2,0.2222222222222222,1,1
110,Suppose W c 1 ? R Ddd and W c 2 ?,Our Model,Convolutional Capsule Layer,text-classification,7,71,0.7553191489361702,109,0.448559670781893,3,0.3333333333333333,1,0
111,R K,Our Model,,text-classification,7,72,0.7659574468085106,110,0.4526748971193415,4,0.4444444444444444,1,0
112,"2 CDdd denote shared and non-shared weights , respectively , where K 2 C is the number of child capsules in a local region in the layer below , Dis the number of parent capsules which the child capsules are sent to .",Our Model,R K,text-classification,7,73,0.776595744680851,111,0.4567901234567901,5,0.5555555555555556,1,0
113,"When the transformation matrices are shared across the child capsules , each potential parent - capsule j|i is produced b?",Our Model,R K,text-classification,7,74,0.7872340425531915,112,0.4609053497942387,6,0.6666666666666666,1,0
114,"where b j|i is the capsule bias term , u i is a child capsule in a local region K 2 C and W c 1 j is the j th matrix in tensor W c 1 .",Our Model,R K,text-classification,7,75,0.7978723404255319,113,0.4650205761316872,7,0.7777777777777778,1,0
115,"Then , we use routingby - agreement to produce parent capsules feature maps totally ( L?K 1 ? K 2 + 2 ) D d-dimensional capsules in this layer .",Our Model,R K,text-classification,7,76,0.8085106382978723,114,0.4691358024691358,8,0.8888888888888888,1,0
116,"When using the non-shared weights across the child capsules , we replace the transformation matrix W c 1 j in Eq. ( 10 ) with W c 2 j .",Our Model,R K,text-classification,7,77,0.8191489361702128,115,0.4732510288065844,9,1.0,1,0
117,Fully Connected Capsule Layer,Our Model,,text-classification,7,78,0.8297872340425532,116,0.4773662551440329,0,0.0,1,0
118,The capsules in the layer below are flattened into a list of capsules and fed into fully connected capsule layer in which capsules are multiplied by transformation matrix W d 1 ?,Our Model,Fully Connected Capsule Layer,text-classification,7,79,0.8404255319148937,117,0.4814814814814814,1,0.1666666666666666,1,1
119,R Edd or W d 2 ?,Our Model,Fully Connected Capsule Layer,text-classification,7,80,0.851063829787234,118,0.48559670781893,2,0.3333333333333333,1,0
120,R HEdd followed by routing - by - agreement to produce final capsule v j ?,Our Model,Fully Connected Capsule Layer,text-classification,7,81,0.8617021276595744,119,0.4897119341563786,3,0.5,1,0
121,Rd and its probability a j ?,Our Model,Fully Connected Capsule Layer,text-classification,7,82,0.8723404255319149,120,0.4938271604938271,4,0.6666666666666666,1,0
122,R for each category .,Our Model,Fully Connected Capsule Layer,text-classification,7,83,0.8829787234042553,121,0.4979423868312757,5,0.8333333333333334,1,0
123,"Here , H is the number of child capsules in the layer below , E is the number of categories plus an extra orphan category .",Our Model,Fully Connected Capsule Layer,text-classification,7,84,0.8936170212765957,122,0.5020576131687243,6,1.0,1,0
124,The Architectures of Capsule Network,Our Model,,text-classification,7,85,0.9042553191489362,123,0.5061728395061729,0,0.0,1,0
125,We explore two capsule architectures ( denoted as Capsule - A and Capsule - B ) to integrate these four,Our Model,The Architectures of Capsule Network,text-classification,7,86,0.9148936170212766,124,0.5102880658436214,1,0.1111111111111111,1,1
126,"Capsule - B Capsule - A starts with an embedding layer which transforms each word in the corpus to a 300 - dimensional ( V = 300 ) word vector , followed by a 3 - gram ( K 1 = 3 ) convolutional layer with 32 filters ( B = 32 ) and astride of 1 with ReLU non-linearity .",Our Model,The Architectures of Capsule Network,text-classification,7,87,0.925531914893617,125,0.51440329218107,2,0.2222222222222222,1,0
127,"All the other layers are capsule layers starting with a B d primary capsule layer with 32 filters ( C = 32 ) , followed by a 3 C d d ( K 2 = 3 ) convolutional capsule layer with 16 filters ( D = 16 ) and a fully connected capsule layer in sequence .",Our Model,The Architectures of Capsule Network,text-classification,7,88,0.9361702127659576,126,0.5185185185185185,3,0.3333333333333333,1,0
128,Each capsule has 16 - dimensional ( d = 16 ) instantiated parameters and their length ( norm ) can describe the probability of the existence of capsules .,Our Model,The Architectures of Capsule Network,text-classification,7,89,0.946808510638298,127,0.522633744855967,4,0.4444444444444444,1,0
129,"The capsule layers are connected by the transformation matrices , and each connection is also multiplied by a routing coefficient that is dynamically computed by routing by agreement mechanism .",Our Model,The Architectures of Capsule Network,text-classification,7,90,0.9574468085106383,128,0.5267489711934157,5,0.5555555555555556,1,0
130,"The basic structure of Capsule - B is similar to Capsule - A except that we adopt three parallel networks with filter windows ( N ) of 3 , 4 , 5 in the N - gram convolutional layer ( see ) .",Our Model,The Architectures of Capsule Network,text-classification,7,91,0.9680851063829788,129,0.5308641975308642,6,0.6666666666666666,1,0
131,The final output of the fully connected capsule layer is fed into the average pooling to produce the final results .,Our Model,The Architectures of Capsule Network,text-classification,7,92,0.9787234042553192,130,0.5349794238683128,7,0.7777777777777778,1,0
132,"In this way , Capsule - B can learn more meaningful and comprehensive text representation .",Our Model,The Architectures of Capsule Network,text-classification,7,93,0.9893617021276596,131,0.5390946502057613,8,0.8888888888888888,1,0
133,3 Experimental Setup,Our Model,The Architectures of Capsule Network,text-classification,7,94,1.0,132,0.5432098765432098,9,1.0,1,0
134,Experimental Datasets,,,text-classification,7,0,0.0,133,0.5473251028806584,0,0.0,1,0
135,"In order to evaluate the effectiveness of our model , we conduct a series of experiments on six bench - marks including : movie reviews ( MR ) , Stanford Sentiment Treebankan extension of MR ( SST - 2 ) , Subjectivity dataset ( Subj ) , TREC question dataset ( TREC ) , customer review ( CR ) , and AG 's news corpus .",Experimental Datasets,Experimental Datasets,text-classification,7,1,0.3333333333333333,134,0.551440329218107,1,0.3333333333333333,1,0
136,"These benchmarks cover several text classification tasks such as sentiment classification , question categorization , news categorization .",Experimental Datasets,Experimental Datasets,text-classification,7,2,0.6666666666666666,135,0.5555555555555556,2,0.6666666666666666,1,0
137,The detailed statistics are presented in,Experimental Datasets,Experimental Datasets,text-classification,7,3,1.0,136,0.5596707818930041,3,1.0,1,0
138,Implementation Details,,,text-classification,7,0,0.0,137,0.5637860082304527,0,0.0,1,0
139,"In the experiments , we use 300 - dimensional word2vec vectors to initialize embedding vectors .",Implementation Details,Implementation Details,text-classification,7,1,0.25,138,0.5679012345679012,1,0.25,1,1
140,We conduct mini-batch with size 50 for AG 's news and size 25 for other datasets .,Implementation Details,Implementation Details,text-classification,7,2,0.5,139,0.5720164609053497,2,0.5,1,1
141,We use Adam optimization algorithm with 1e - 3 learning rate to train the model .,Implementation Details,Implementation Details,text-classification,7,3,0.75,140,0.5761316872427984,3,0.75,1,1
142,We use 3 iteration of routing for all datasets since it optimizes the loss faster and converges to a lower loss at the end .,Implementation Details,Implementation Details,text-classification,7,4,1.0,141,0.5802469135802469,4,1.0,1,1
143,Baseline methods,,,text-classification,7,0,0.0,142,0.5843621399176955,0,0.0,1,0
144,"In the experiments , we evaluate and compare our model with several strong baseline methods including : LSTM / Bi - LSTM , tree - structured LSTM ( Tree - LSTM ) , LSTM regularized by linguistic knowledge ( LR - LSTM ) , CNNrand / CNN - static / CNN - non-static ( Kim , 2014 ) , very deep convolutional network ( VD - CNN ) , and character - level convolutional network ( CL - CNN ) .",Baseline methods,Baseline methods,text-classification,7,1,0.0,143,0.588477366255144,1,0.0,1,1
145,Experimental Results,,,text-classification,7,0,0.0,144,0.5925925925925926,0,0.0,1,0
146,Quantitative Evaluation,,,text-classification,7,0,0.0,145,0.5967078189300411,0,0.0,1,0
147,"In our experiments , the evaluation metric is classification accuracy .",Quantitative Evaluation,Quantitative Evaluation,text-classification,7,1,0.25,146,0.6008230452674898,1,0.25,1,0
148,We summarize the experimental results in .,Quantitative Evaluation,Quantitative Evaluation,text-classification,7,2,0.5,147,0.6049382716049383,2,0.5,1,0
149,"From the results , we observe that the capsule networks achieve best results on 4 out of 6 benchmarks , which verifies the effectiveness of the capsule networks .",Quantitative Evaluation,Quantitative Evaluation,text-classification,7,3,0.75,148,0.6090534979423868,3,0.75,1,1
150,"In particular , our model substantially and consistently outperforms",Quantitative Evaluation,Quantitative Evaluation,text-classification,7,4,1.0,149,0.6131687242798354,4,1.0,1,0
151,Ablation Study,,,text-classification,7,0,0.0,150,0.6172839506172839,0,0.0,1,0
152,"To analyze the effect of varying different components of our capsule architecture for text classification , we also report the ablation test of the capsule - B model in terms of using different setups of the capsule network .",Ablation Study,Ablation Study,text-classification,7,1,0.0181818181818181,151,0.6213991769547325,1,0.0357142857142857,1,0
153,The experimental results are summarized in .,Ablation Study,Ablation Study,text-classification,7,2,0.0363636363636363,152,0.6255144032921811,2,0.0714285714285714,1,0
154,"Generally , all three proposed dynamic routing strategies contribute to the effectiveness of Capsule - B by alleviating the disturbance of some noise capsules which may contain "" background "" information such as stop words and the words that are unrelated to specific categories .",Ablation Study,Ablation Study,text-classification,7,3,0.0545454545454545,153,0.6296296296296297,3,0.1071428571428571,1,1
155,More comprehensive comparison results are demonstrated in . 4 in Supplementary Material .,Ablation Study,Ablation Study,text-classification,7,4,0.0727272727272727,154,0.6337448559670782,4,0.1428571428571428,1,0
156,Single - Label to Multi - Label Text Classification,Ablation Study,,text-classification,7,5,0.0909090909090909,155,0.6378600823045267,5,0.1785714285714285,1,0
157,Capsule network demonstrates promising performance in single - label text classification which as - signs a label from a predefined set to a text ( see ) .,Ablation Study,Single - Label to Multi - Label Text Classification,text-classification,7,6,0.109090909090909,156,0.6419753086419753,6,0.2142857142857142,1,0
158,"Multi-label text classification is , however , a more challenging practical problem .",Ablation Study,Single - Label to Multi - Label Text Classification,text-classification,7,7,0.1272727272727272,157,0.6460905349794238,7,0.25,1,0
159,"From singlelabel to multi-label ( with n category labels ) text classification , the label space is expanded from n to 2 n , thus more training is required to cover the whole label space .",Ablation Study,Single - Label to Multi - Label Text Classification,text-classification,7,8,0.1454545454545454,158,0.6502057613168725,8,0.2857142857142857,1,0
160,"For single - label texts , it is practically easy to collect and annotate the samples .",Ablation Study,Single - Label to Multi - Label Text Classification,text-classification,7,9,0.1636363636363636,159,0.654320987654321,9,0.3214285714285714,1,0
161,"However , the burden of collection and annotation fora large scale multi-label text dataset is generally extremely high .",Ablation Study,Single - Label to Multi - Label Text Classification,text-classification,7,10,0.1818181818181818,160,0.6584362139917695,10,0.3571428571428571,1,0
162,"How deep neural networks ( e.g. , CNN and LSTM ) best cope with multi-label text classification still remains a problem since obtaining large scale of multi-label dataset is a timeconsuming and expensive process .",Ablation Study,Single - Label to Multi - Label Text Classification,text-classification,7,11,0.2,161,0.6625514403292181,11,0.3928571428571428,1,0
163,"In this section , we investigate the capability of capsule network on multi-label text classification by using only the single - label samples as training data .",Ablation Study,Single - Label to Multi - Label Text Classification,text-classification,7,12,0.2181818181818181,162,0.6666666666666666,12,0.4285714285714285,1,0
164,"With feature property as part of the information extracted by capsules , we may generalize the model better to multi-label text classification without an over extensive amount of labeled data .",Ablation Study,Single - Label to Multi - Label Text Classification,text-classification,7,13,0.2363636363636363,163,0.6707818930041153,13,0.4642857142857143,1,0
165,The evaluation is carried on the Reuters - 21578 dataset .,Ablation Study,Single - Label to Multi - Label Text Classification,text-classification,7,14,0.2545454545454545,164,0.6748971193415638,14,0.5,1,0
166,"This dataset consists of 10,788 documents from the Reuters financial newswire service , where each document contains either multiple labels or a single label .",Ablation Study,Single - Label to Multi - Label Text Classification,text-classification,7,15,0.2727272727272727,165,0.6790123456790124,15,0.5357142857142857,1,0
167,We reprocess the corpus to evaluate the capability of capsule networks of transferring from single - label to multi-label text classification .,Ablation Study,Single - Label to Multi - Label Text Classification,text-classification,7,16,0.2909090909090909,166,0.6831275720164609,16,0.5714285714285714,1,0
168,"For dev and training , we only use the single - label documents in the Reuters dev and training sets .",Ablation Study,Single - Label to Multi - Label Text Classification,text-classification,7,17,0.3090909090909091,167,0.6872427983539094,17,0.6071428571428571,1,0
169,"For testing , Reuters - Multi - label only uses the multi-label documents in testing dataset , while Reuters - Full includes all documents in test set .",Ablation Study,Single - Label to Multi - Label Text Classification,text-classification,7,18,0.3272727272727272,168,0.691358024691358,18,0.6428571428571429,1,0
170,The characteristics of these two datasets are described in .,Ablation Study,Single - Label to Multi - Label Text Classification,text-classification,7,19,0.3454545454545454,169,0.6954732510288066,19,0.6785714285714286,1,0
171,"Following ( Sorower , 2010 ) , we adopt Micro Averaged Precision ( Precision ) , Micro Averaged Recall ( Recall ) and Micro Averaged F1 scores ( F1 ) as the evaluation metrics for multi-label text classification .",Ablation Study,Single - Label to Multi - Label Text Classification,text-classification,7,20,0.3636363636363636,170,0.6995884773662552,20,0.7142857142857143,1,0
172,"Any of these scores are firstly computed on individual class labels and then averaged overall classes , called label - based measures .",Ablation Study,Single - Label to Multi - Label Text Classification,text-classification,7,21,0.3818181818181818,171,0.7037037037037037,21,0.75,1,0
173,"In addition , we also measure the Exact Match Ratio ( ER ) which considers partially correct prediction as incorrect and only counts fully correct samples .",Ablation Study,Single - Label to Multi - Label Text Classification,text-classification,7,22,0.4,172,0.7078189300411523,22,0.7857142857142857,1,0
174,The experimental results are summarized in .,Ablation Study,Single - Label to Multi - Label Text Classification,text-classification,7,23,0.4181818181818181,173,0.7119341563786008,23,0.8214285714285714,1,0
175,"From the results , we can observe that the capsule networks have substantial and significant improvement in terms of all four evaluation metrics over the strong baseline methods on the test sets in both Reuters - Multi-label and Reuters - Full datasets .",Ablation Study,Single - Label to Multi - Label Text Classification,text-classification,7,24,0.4363636363636363,174,0.7160493827160493,24,0.8571428571428571,1,0
176,"In particular , larger improvement is achieved on Reuters - Multi - label dataset which only contains the multi-label documents in the test set .",Ablation Study,Single - Label to Multi - Label Text Classification,text-classification,7,25,0.4545454545454545,175,0.720164609053498,25,0.8928571428571429,1,0
177,This is within our expectation since the capsule network is capable of preserving the instantiated parameters of the categories trained by singlelabel documents .,Ablation Study,Single - Label to Multi - Label Text Classification,text-classification,7,26,0.4727272727272727,176,0.7242798353909465,26,0.9285714285714286,1,0
178,The capsule network has much stronger transferring capability than the conventional deep neural networks .,Ablation Study,Single - Label to Multi - Label Text Classification,text-classification,7,27,0.4909090909090909,177,0.7283950617283951,27,0.9642857142857144,1,0
179,"In addition , the good results on Reuters - Full also indicate that the capsule network has robust superiority over competitors on single - label documents .",Ablation Study,Single - Label to Multi - Label Text Classification,text-classification,7,28,0.509090909090909,178,0.7325102880658436,28,1.0,1,0
180,Connection Strength Visualization,Ablation Study,,text-classification,7,29,0.5272727272727272,179,0.7366255144032922,0,0.0,1,0
181,"To visualize the connection strength between capsule layers clearly , we remove the convolutional capsule layer and make the primary capsule layer followed by the fully connected capsule layer directly , where the primary capsules denote N-gram phrases in the form of capsules .",Ablation Study,Connection Strength Visualization,text-classification,7,30,0.5454545454545454,180,0.7407407407407407,1,0.0833333333333333,1,0
182,"The connection strength shows the importance of each primary capsule for text categories , acting like a parallel attention mechanism .",Ablation Study,Connection Strength Visualization,text-classification,7,31,0.5636363636363636,181,0.7448559670781894,2,0.1666666666666666,1,0
183,This should allow the capsule networks to recognize multiple categories in the text even though the model is trained on singlelabel documents .,Ablation Study,Connection Strength Visualization,text-classification,7,32,0.5818181818181818,182,0.7489711934156379,3,0.25,1,0
184,"Due to space reasons , we choose a multilabel document from Reuters - Multi - label test set whose category labels ( i.e. , Interest Rates and Money / Foreign Exchange ) are correctly predicted ( fully correct ) by our model with high confidence ( p > 0.8 ) to report in .",Ablation Study,Connection Strength Visualization,text-classification,7,33,0.6,183,0.7530864197530864,4,0.3333333333333333,1,0
185,"The categoryspecific phrases such as "" interest rates "" and "" foreign exchange "" are highlighted with red color .",Ablation Study,Connection Strength Visualization,text-classification,7,34,0.6181818181818182,184,0.757201646090535,5,0.4166666666666667,1,0
186,We use the tag cloud to visualize the 3 - gram phrases for Interest Rates and Money / Foreign Exchange categories .,Ablation Study,Connection Strength Visualization,text-classification,7,35,0.6363636363636364,185,0.7613168724279835,6,0.5,1,0
187,"The stronger the connection strength , the bigger the font size .",Ablation Study,Connection Strength Visualization,text-classification,7,36,0.6545454545454545,186,0.7654320987654321,7,0.5833333333333334,1,0
188,"From the results , we observe that capsule networks can correctly recognize and cluster the important phrases with respect to the text categories .",Ablation Study,Connection Strength Visualization,text-classification,7,37,0.6727272727272727,187,0.7695473251028807,8,0.6666666666666666,1,0
189,"The histograms are used to show the intensity of connection strengths between primary capsules and the fully connected capsules , as shown in To experimentally verify the convergence of the routing algorithm , we also plot learning curve to show the training loss overtime with different iterations of routing .",Ablation Study,Connection Strength Visualization,text-classification,7,38,0.6909090909090909,188,0.7736625514403292,9,0.75,1,0
190,"From , we observe that the Capsule - B with 3 or 5 iterations of routing optimizes the loss faster and converges to a lower loss at the end than the capsule network with 1 iteration .",Ablation Study,Connection Strength Visualization,text-classification,7,39,0.7090909090909091,189,0.7777777777777778,10,0.8333333333333334,1,0
191,U.K .,Ablation Study,,text-classification,7,40,0.7272727272727273,190,0.7818930041152263,11,0.9166666666666666,1,0
192,MONEY RATES FIRM ON LAWSON STERLING TARGETS,Ablation Study,U.K .,text-classification,7,41,0.7454545454545455,191,0.7860082304526749,12,1.0,1,0
193,Interest Rates,Ablation Study,,text-classification,7,42,0.7636363636363637,192,0.7901234567901234,0,0.0,1,0
194,Money / Foreign Exchange Interest rates on the London money market were slightly firmer on news U.K .,Ablation Study,Interest Rates,text-classification,7,43,0.7818181818181819,193,0.7942386831275721,1,0.0909090909090909,1,0
195,"Chancellor of the Exchequer Nigel Lawson had stated target rates for sterling against the dollar and mark , dealers said .",Ablation Study,Interest Rates,text-classification,7,44,0.8,194,0.7983539094650206,2,0.1818181818181818,1,0
196,"They said this had come as a surprise and expected the targets , 2.90 marks and 1.60 dlrs , to be promptly tested in the foreign exchange markets .",Ablation Study,Interest Rates,text-classification,7,45,0.8181818181818182,195,0.8024691358024691,3,0.2727272727272727,1,0
197,Sterling opened 0.3 points lower in trade weighted terms at 71.3 .,Ablation Study,Interest Rates,text-classification,7,46,0.8363636363636363,196,0.8065843621399177,4,0.3636363636363636,1,0
198,Dealers noted the chancellor said he would achieve his goals on sterling by a combination of intervention in currency markets and interest rates .,Ablation Study,Interest Rates,text-classification,7,47,0.8545454545454545,197,0.8106995884773662,5,0.4545454545454545,1,0
199,Operators feel the foreign exchanges are likely to test sterling on the downside and that this seems to make a fall in U.K .,Ablation Study,Interest Rates,text-classification,7,48,0.8727272727272727,198,0.8148148148148148,6,0.5454545454545454,1,0
200,"Base lending rates even less likely in the near term , dealers said .",Ablation Study,Interest Rates,text-classification,7,49,0.8909090909090909,199,0.8189300411522634,7,0.6363636363636364,1,0
201,"The feeling remains in the market , however , that fundamental factors have not really changed and that arise in U.K .",Ablation Study,Interest Rates,text-classification,7,50,0.9090909090909092,200,0.823045267489712,8,0.7272727272727273,1,0
202,Interest rates is not very likely .,Ablation Study,Interest Rates,text-classification,7,51,0.9272727272727272,201,0.8271604938271605,9,0.8181818181818182,1,0
203,"The market is expected to continue at around these levels , reflecting the current 10 pct base rate level , for sometime .",Ablation Study,Interest Rates,text-classification,7,52,0.9454545454545454,202,0.831275720164609,10,0.9090909090909092,1,0
204,The key three months interbank rate was 1 / 16 point firmer at 10 9 - 7 /8 pct .,Ablation Study,Interest Rates,text-classification,7,53,0.9636363636363636,203,0.8353909465020576,11,1.0,1,0
205,Orphan,Ablation Study,,text-classification,7,54,0.9818181818181818,204,0.8395061728395061,0,0.0,1,0
206,Mergers / Acquisitions Money / Foreign Exchange Trade Interest Rates,Ablation Study,Orphan,text-classification,7,55,1.0,205,0.8436213991769548,1,0.0,1,0
207,Related Work,,,text-classification,7,0,0.0,206,0.8477366255144033,0,0.0,1,0
208,"Early methods for text classification adopted the typical features such as bag - of - words , n-grams , and their TF - IDF features as input of machine learning algorithms such as support vector machine ( SVM ) , naive Bayes ( NB ) for classification .",Related Work,Related Work,text-classification,7,1,0.05,207,0.8518518518518519,1,0.05,0,0
209,"However , these models usually heavily relied on laborious feature engineering or massive extra linguistic resources .",Related Work,Related Work,text-classification,7,2,0.1,208,0.8559670781893004,2,0.1,0,0
210,Recent advances in deep neural networks and representation learning have substantially improved the performance of text classification tasks .,Related Work,Related Work,text-classification,7,3,0.15,209,0.8600823045267489,3,0.15,0,0
211,"The dominant approaches are recurrent neural net -works , in particular LSTMs and CNNs. reported on a series of experiments with CNNs trained on top of pre-trained word vectors for sentence - level classification tasks .",Related Work,Related Work,text-classification,7,4,0.2,210,0.8641975308641975,4,0.2,0,0
212,The CNN models improved upon the state of the art on 4 out of 7 tasks .,Related Work,Related Work,text-classification,7,5,0.25,211,0.8683127572016461,5,0.25,0,0
213,offered an empirical exploration on the use of character - level convolutional networks ( Convnets ) for text classification and the experiments showed that Convnets outperformed the traditional models .,Related Work,Related Work,text-classification,7,6,0.3,212,0.8724279835390947,6,0.3,0,0
214,"proposed a simple and efficient text classification method fastText , which could be trained on a billion words within ten minutes .",Related Work,Related Work,text-classification,7,7,0.35,213,0.8765432098765432,7,0.35,0,0
215,proposed a very deep convolutional networks ( with 29 convolutional layers ) for text classification .,Related Work,Related Work,text-classification,7,8,0.4,214,0.8806584362139918,8,0.4,0,0
216,generalized the LSTM to the tree - structured network topologies ( Tree - LSTM ) that achieved best results on two text classification tasks .,Related Work,Related Work,text-classification,7,9,0.45,215,0.8847736625514403,9,0.45,0,0
217,"Recently , a novel type of neural network is proposed using the concept of capsules to improve the representational limitations of firstly introduced the concept of "" capsules "" to address the representational limitations of CNNs and RNNs .",Related Work,Related Work,text-classification,7,10,0.5,216,0.8888888888888888,10,0.5,0,0
218,Capsules with transformation matrices allowed networks to automatically learn part - whole relationships .,Related Work,Related Work,text-classification,7,11,0.55,217,0.8930041152263375,11,0.55,0,0
219,"Consequently , proposed capsule networks that replaced the scalar - output feature detectors of CNNs with vector - output capsules and max - pooling with routing - by - agreement .",Related Work,Related Work,text-classification,7,12,0.6,218,0.897119341563786,12,0.6,0,0
220,The capsule network has shown its potential by achieving a state - of - the - art result on MNIST data .,Related Work,Related Work,text-classification,7,13,0.65,219,0.9012345679012346,13,0.65,0,0
221,"Unlike max - pooling in CNN , however , Capsule network do not throwaway information about the precise position of the entity within the region .",Related Work,Related Work,text-classification,7,14,0.7,220,0.9053497942386832,14,0.7,0,0
222,"For lowlevel capsules , location information is placecoded by which capsule is active .",Related Work,Related Work,text-classification,7,15,0.75,221,0.9094650205761317,15,0.75,0,0
223,further tested out the application of capsule networks on CIFAR data with higher dimensionality .,Related Work,Related Work,text-classification,7,16,0.8,222,0.9135802469135802,16,0.8,0,0
224,"proposed anew iterative routing procedure between capsule layers based on the EM algorithm , which achieves significantly better accuracy on the small NORB data set .",Related Work,Related Work,text-classification,7,17,0.85,223,0.9176954732510288,17,0.85,0,0
225,generalized existing routing methods within the framework of weighted kernel density estimation .,Related Work,Related Work,text-classification,7,18,0.9,224,0.9218106995884774,18,0.9,0,0
226,"To date , no work investigates the performance of capsule networks in NLP tasks .",Related Work,Related Work,text-classification,7,19,0.95,225,0.925925925925926,19,0.95,0,0
227,This study herein takes the lead in this topic .,Related Work,Related Work,text-classification,7,20,1.0,226,0.9300411522633744,20,1.0,0,0
228,Conclusion,,,text-classification,7,0,0.0,227,0.934156378600823,0,0.0,1,0
229,"In this paper , we investigated capsule networks with dynamic routing for text classification .",Conclusion,Conclusion,text-classification,7,1,0.0666666666666666,228,0.9382716049382716,1,0.25,0,0
230,Three strategies were proposed to boost the performance of the dynamic routing process to alleviate the disturbance of noisy capsules .,Conclusion,Conclusion,text-classification,7,2,0.1333333333333333,229,0.9423868312757202,2,0.5,0,0
231,Extensive experiments on six text classification benchmarks show the effectiveness of capsule networks in text classification .,Conclusion,Conclusion,text-classification,7,3,0.2,230,0.9465020576131687,3,0.75,0,0
232,"More importantly , capsule networks also show significant improvement when transferring single - label to multi-label text classifications over strong baseline methods .",Conclusion,Conclusion,text-classification,7,4,0.2666666666666666,231,0.9506172839506172,4,1.0,0,0
233,Supplementary Material,Conclusion,,text-classification,7,5,0.3333333333333333,232,0.9547325102880658,0,0.0,0,0
234,"To better demonstrate the orphan and other categories with top unigrams , we remove the convolutional capsule layer and make the primary capsule layer followed by the fully connected capsule layer directly , similar to the settings in section 5.1 .",Conclusion,Supplementary Material,text-classification,7,6,0.4,233,0.9588477366255144,1,0.1111111111111111,0,0
235,"Here , the primary capsules denote uni-grams in the form of capsules .",Conclusion,Supplementary Material,text-classification,7,7,0.4666666666666667,234,0.9629629629629628,2,0.2222222222222222,0,0
236,"We picked top - 20 uni-gram ( words ) from four categories ( i.e. , Orphan category , Trade category , Money Exchange category and Interest Rates category ) sorted by their connection strengths .",Conclusion,Supplementary Material,text-classification,7,8,0.5333333333333333,235,0.9670781893004116,3,0.3333333333333333,0,0
237,"Money / Foreign Exchange Following is the text of a statement by the Group of Seven - the U.S. , Japan , West Germany , France , Britain , Italy and Canada - issued after a Washington meeting yesterday .",Conclusion,Supplementary Material,text-classification,7,9,0.6,236,0.97119341563786,4,0.4444444444444444,0,0
238,1 . The finance ministers and central bank governors of seven major industrial countries met today .,Conclusion,Supplementary Material,text-classification,7,10,0.6666666666666666,237,0.9753086419753086,5,0.5555555555555556,0,0
239,They continued the process of multilateral surveillance of their economies pursuant to the arrangements for strengthened economic policy coordination agreed at the 1986 Tokyo summit of their heads of state or government .,Conclusion,Supplementary Material,text-classification,7,11,0.7333333333333333,238,0.9794238683127572,6,0.6666666666666666,0,0
240,"2 . The ministers and governors reaffirmed the commitment to the cooperative approach agreed at the recent Paris meeting , and noted the progress achieved in implementing the undertakings embodied in the Louvre Agreement .",Conclusion,Supplementary Material,text-classification,7,12,0.8,239,0.9835390946502056,7,0.7777777777777778,0,0
241,"In this connection they welcomed the proposals just announced by the governing Liberal Democratic Party in Japan for extraordinary and urgent measures to stimulate Japan 's economy through early implementation of a large supplementary budget exceeding those of previous years , as well as unprecedented front - end loading of public works expenditures .",Conclusion,Supplementary Material,text-classification,7,13,0.8666666666666667,240,0.9876543209876544,8,0.8888888888888888,0,0
242,They concluded that present and prospective progress in implementing the policy undertakings at the Louvre and in this statement provided a basis for continuing close cooperation to foster the stability of exchange rates .,Conclusion,Supplementary Material,text-classification,7,14,0.9333333333333332,241,0.9917695473251028,9,1.0,0,0
243,Index,Conclusion,,text-classification,7,15,1.0,242,0.9958847736625516,0,0.0,0,0
1,title,,,text-classification,8,0,0.0,0,0.0,0,0.0,1,0
2,Baseline Needs More Love : On Simple Word - Embedding - Based Models and Associated Pooling Mechanisms,title,title,text-classification,8,1,0.0,1,0.0037174721189591,1,0.0,1,1
3,abstract,,,text-classification,8,0,0.0,2,0.0074349442379182,0,0.0,1,0
4,"Many deep learning architectures have been proposed to model the compositionality in text sequences , requiring a substantial number of parameters and expensive computations .",abstract,abstract,text-classification,8,1,0.1428571428571428,3,0.0111524163568773,1,0.1428571428571428,1,1
5,"However , there has not been a rigorous evaluation regarding the added value of sophisticated compositional functions .",abstract,abstract,text-classification,8,2,0.2857142857142857,4,0.0148698884758364,2,0.2857142857142857,1,0
6,"In this paper , we conduct a point - by - point comparative study between Simple Word - Embeddingbased Models ( SWEMs ) , consisting of parameter - free pooling operations , relative to word - embedding - based RNN / CNN models .",abstract,abstract,text-classification,8,3,0.4285714285714285,5,0.0185873605947955,3,0.4285714285714285,1,0
7,"Surprisingly , SWEMs exhibit comparable or even superior performance in the majority of cases considered .",abstract,abstract,text-classification,8,4,0.5714285714285714,6,0.0223048327137546,4,0.5714285714285714,1,0
8,"Based upon this understanding , we propose two additional pooling strategies over learned word embeddings : ( i ) a max - pooling operation for improved interpretability ; and ( ii ) a hierarchical pooling operation , which preserves spatial ( n - gram ) information within text sequences .",abstract,abstract,text-classification,8,5,0.7142857142857143,7,0.0260223048327137,5,0.7142857142857143,1,0
9,"We present experiments on 17 datasets encompassing three tasks : ( i ) ( long ) document classification ; ( ii ) text sequence matching ; and ( iii ) short text tasks , including classification and tagging .",abstract,abstract,text-classification,8,6,0.8571428571428571,8,0.0297397769516728,6,0.8571428571428571,1,0
10,The source code and datasets can be obtained from https://github.com/dinghanshen/SWEM .,abstract,abstract,text-classification,8,7,1.0,9,0.0334572490706319,7,1.0,1,1
11,Introduction,,,text-classification,8,0,0.0,10,0.037174721189591,0,0.0,1,0
12,"Word embeddings , learned from massive unstructured text data , are widely - adopted building blocks for Natural Language Processing ( NLP ) .",Introduction,Introduction,text-classification,8,1,0.0454545454545454,11,0.0408921933085501,1,0.0454545454545454,1,0
13,"By representing each word as a fixed - length vector , these embeddings can group semantically similar words , while implicitly encoding rich linguis - tic regularities and patterns .",Introduction,Introduction,text-classification,8,2,0.0909090909090909,12,0.0446096654275092,2,0.0909090909090909,1,0
14,"Leveraging the word - embedding construct , many deep architectures have been proposed to model the compositionality in variable - length text sequences .",Introduction,Introduction,text-classification,8,3,0.1363636363636363,13,0.0483271375464684,3,0.1363636363636363,1,1
15,"These methods range from simple operations like addition , to more sophisticated compositional functions such as Recurrent Neural Networks ( RNNs ) , Convolutional Neural Networks ( CNNs ) and Recursive Neural Networks .",Introduction,Introduction,text-classification,8,4,0.1818181818181818,14,0.0520446096654275,4,0.1818181818181818,1,0
16,"Models with more expressive compositional functions , e.g. , RNNs or CNNs , have demonstrated impressive results ; however , they are typically computationally expensive , due to the need to estimate hundreds of thousands , if not millions , of parameters .",Introduction,Introduction,text-classification,8,5,0.2272727272727272,15,0.0557620817843866,5,0.2272727272727272,1,0
17,"In contrast , models with simple compositional functions often compute a sentence or document embedding by simply adding , or averaging , over the word embedding of each sequence element obtained via , e.g. , word2vec , or Glo Ve .",Introduction,Introduction,text-classification,8,6,0.2727272727272727,16,0.0594795539033457,6,0.2727272727272727,1,0
18,"Generally , such a Simple Word - Embedding - based Model ( SWEM ) does not explicitly account for spatial , word - order information within a text sequence .",Introduction,Introduction,text-classification,8,7,0.3181818181818182,17,0.0631970260223048,7,0.3181818181818182,1,0
19,"However , they possess the desirable property of having significantly fewer parameters , enjoying much faster training , relative to RNN - or CNN - based models .",Introduction,Introduction,text-classification,8,8,0.3636363636363636,18,0.0669144981412639,8,0.3636363636363636,1,0
20,"Hence , there is a computation - vs. - expressiveness tradeoff regarding how to model the compositionality of a text sequence .",Introduction,Introduction,text-classification,8,9,0.4090909090909091,19,0.070631970260223,9,0.4090909090909091,1,0
21,"In this paper , we conduct an extensive experimental investigation to understand when , and why , simple pooling strategies , operated over word embeddings alone , already carry sufficient information for natural language understanding .",Introduction,Introduction,text-classification,8,10,0.4545454545454545,20,0.0743494423791821,10,0.4545454545454545,1,1
22,"To ac- count for the distinct nature of various NLP tasks that may require different semantic features , we compare SWEM - based models with existing recurrent and convolutional networks in a pointby - point manner .",Introduction,Introduction,text-classification,8,11,0.5,21,0.0780669144981412,11,0.5,1,1
23,"Specifically , we consider 17 datasets , including three distinct NLP tasks : document classification ( Yahoo news , Yelp reviews , etc. ) , natural language sequence matching ( SNLI , WikiQA , etc. ) and ( short ) sentence classification / tagging ( Stanford sentiment treebank , .",Introduction,Introduction,text-classification,8,12,0.5454545454545454,22,0.0817843866171003,12,0.5454545454545454,1,0
24,"Surprisingly , SWEMs exhibit comparable or even superior performance in the majority of cases considered .",Introduction,Introduction,text-classification,8,13,0.5909090909090909,23,0.0855018587360594,13,0.5909090909090909,1,0
25,"In order to validate our experimental findings , we conduct additional investigations to understand to what extent the word - order information is utilized / required to make predictions on different tasks .",Introduction,Introduction,text-classification,8,14,0.6363636363636364,24,0.0892193308550185,14,0.6363636363636364,1,0
26,"We observe that in text representation tasks , many words ( e.g. , stop words , or words that are not related to sentiment or topic ) do not meaningfully contribute to the final predictions ( e.g. , sentiment label ) .",Introduction,Introduction,text-classification,8,15,0.6818181818181818,25,0.0929368029739776,15,0.6818181818181818,1,0
27,"Based upon this understanding , we propose to leverage a max - pooling operation directly over the word embedding matrix of a given sequence , to select its most salient features .",Introduction,Introduction,text-classification,8,16,0.7272727272727273,26,0.0966542750929368,16,0.7272727272727273,1,0
28,"This strategy is demonstrated to extract complementary features relative to the standard averaging operation , while resulting in a more interpretable model .",Introduction,Introduction,text-classification,8,17,0.7727272727272727,27,0.1003717472118959,17,0.7727272727272727,1,0
29,"Inspired by a case study on sentiment analysis tasks , we further propose a hierarchical pooling strategy to abstract and preserve the spatial information in the final representations .",Introduction,Introduction,text-classification,8,18,0.8181818181818182,28,0.104089219330855,18,0.8181818181818182,1,0
30,"This strategy is demonstrated to exhibit comparable empirical results to LSTM and CNN on tasks that are sensitive to word - order features , while maintaining the favorable properties of not having compositional parameters , thus fast training .",Introduction,Introduction,text-classification,8,19,0.8636363636363636,29,0.1078066914498141,19,0.8636363636363636,1,0
31,"Our work presents a simple yet strong baseline for text representation learning that is widely ignored in benchmarks , and highlights the general computation - vs. - expressiveness tradeoff associated with appropriately selecting compositional functions for distinct NLP problems .",Introduction,Introduction,text-classification,8,20,0.9090909090909092,30,0.1115241635687732,20,0.9090909090909092,1,0
32,"Furthermore , we quantitatively show that the word - embeddingbased text classification tasks can have the similar level of difficulty regardless of the employed models , using the subspace training to constrain the trainable parameters .",Introduction,Introduction,text-classification,8,21,0.9545454545454546,31,0.1152416356877323,21,0.9545454545454546,1,0
33,"Thus , according to Occam 's razor , simple models are preferred .",Introduction,Introduction,text-classification,8,22,1.0,32,0.1189591078066914,22,1.0,1,0
34,Related Work,,,text-classification,8,0,0.0,33,0.1226765799256505,0,0.0,1,0
35,"A fundamental goal in NLP is to develop expressive , yet computationally efficient compositional functions that can capture the linguistic structure of natural language sequences .",Related Work,Related Work,text-classification,8,1,0.0769230769230769,34,0.1263940520446096,1,0.0769230769230769,0,0
36,"Recently , several studies have suggested that on certain NLP applications , much simpler word - embedding - based architectures exhibit comparable or even superior performance , compared with more - sophisticated models using recurrence or convolutions .",Related Work,Related Work,text-classification,8,2,0.1538461538461538,35,0.1301115241635687,2,0.1538461538461538,0,0
37,"Although complex compositional functions are avoided in these models , additional modules , such as attention layers , are employed on top of the word embedding layer .",Related Work,Related Work,text-classification,8,3,0.2307692307692307,36,0.1338289962825278,3,0.2307692307692307,0,0
38,"As a result , the specific role that the word embedding plays in these models is not emphasized ( or explicit ) , which distracts from understanding how important the word embeddings alone are to the observed superior performance .",Related Work,Related Work,text-classification,8,4,0.3076923076923077,37,0.137546468401487,4,0.3076923076923077,0,0
39,"Moreover , several recent studies have shown empirically that the advantages of distinct compositional functions are highly dependent on the specific task .",Related Work,Related Work,text-classification,8,5,0.3846153846153846,38,0.1412639405204461,5,0.3846153846153846,0,0
40,"Therefore , it is of interest to study the practical value of the additional expressiveness , on a wide variety of NLP problems .",Related Work,Related Work,text-classification,8,6,0.4615384615384615,39,0.1449814126394052,6,0.4615384615384615,0,0
41,"SWEMs bear close resemblance to Deep Averaging Network ( DAN ) or fast - Text , where they show that average pooling achieves promising results on certain NLP tasks .",Related Work,Related Work,text-classification,8,7,0.5384615384615384,40,0.1486988847583643,7,0.5384615384615384,0,0
42,"However , there exist several key differences that make our work unique .",Related Work,Related Work,text-classification,8,8,0.6153846153846154,41,0.1524163568773234,8,0.6153846153846154,0,0
43,"First , we explore a series of pooling operations , rather than only average - pooling .",Related Work,Related Work,text-classification,8,9,0.6923076923076923,42,0.1561338289962825,9,0.6923076923076923,0,0
44,"Specifically , a hierarchical pooling operation is introduced to incorporate spatial information , which demonstrates superior results on sentiment analysis , relative to average pooling .",Related Work,Related Work,text-classification,8,10,0.7692307692307693,43,0.1598513011152416,10,0.7692307692307693,0,0
45,"Second , our work not only explores when simple pooling operations are enough , but also investigates the underlying reasons , i.e. , what semantic features are required for distinct NLP problems .",Related Work,Related Work,text-classification,8,11,0.8461538461538461,44,0.1635687732342007,11,0.8461538461538461,0,0
46,"Third , DAN and fast Text only focused on one or two problems at a time , thus a comprehensive study regarding the effectiveness of various compositional functions on distinct NLP tasks , e.g. , categorizing short sentence / long documents , matching natural language sentences , has heretofore been absent .",Related Work,Related Work,text-classification,8,12,0.9230769230769232,45,0.1672862453531598,12,0.9230769230769232,0,0
47,"In response , our work seeks to perform a comprehensive comparison with respect to simple - vs. - complex compositional func- tions , across a wide range of NLP problems , and reveals some general rules for rationally selecting models to tackle different tasks .",Related Work,Related Work,text-classification,8,13,1.0,46,0.1710037174721189,13,1.0,0,0
48,Models & training,,,text-classification,8,0,0.0,47,0.174721189591078,0,0.0,1,0
49,"Consider a text sequence represented as X ( either a sentence or a document ) , composed of a sequence of words : {w 1 , w 2 , .... , w L } , where L is the number of tokens , i.e. , the sentence / document length .",Models & training,Models & training,text-classification,8,1,0.0333333333333333,48,0.1784386617100371,1,0.2,1,0
50,"Let {v 1 , v 2 , .... , v L } denote the respective word embeddings for each token , where v l ?",Models & training,Models & training,text-classification,8,2,0.0666666666666666,49,0.1821561338289963,2,0.4,1,0
51,R K .,Models & training,,text-classification,8,3,0.1,50,0.1858736059479553,3,0.6,1,0
52,"The compositional function , X ? z , aims to combine word embeddings into a fixed - length sentence / document representation z .",Models & training,R K .,text-classification,8,4,0.1333333333333333,51,0.1895910780669145,4,0.8,1,0
53,"These representations are then used to make predictions about sequence X. Below , we describe different types of functions considered in this work .",Models & training,R K .,text-classification,8,5,0.1666666666666666,52,0.1933085501858736,5,1.0,1,0
54,Recurrent Sequence Encoder,Models & training,R K .,text-classification,8,6,0.2,53,0.1970260223048327,0,0.0,1,0
55,"A widely adopted compositional function is defined in a recurrent manner : the model successively takes word vector v tat position t , along with the hidden unit h t?1 from the last position t ?",Models & training,R K .,text-classification,8,7,0.2333333333333333,54,0.2007434944237918,1,0.2,1,0
56,"1 , to update the current hidden unit via",Models & training,R K .,text-classification,8,8,0.2666666666666666,55,0.2044609665427509,2,0.4,1,0
57,"To address the issue of learning long - term dependencies , f ( ) is often defined as Long Short - Term Memory ( LSTM ) , which employs gates to control the flow of information abstracted from a sequence .",Models & training,R K .,text-classification,8,9,0.3,56,0.20817843866171,3,0.6,1,0
58,We omit the details of the LSTM and refer the interested readers to the work by for further explanation .,Models & training,R K .,text-classification,8,10,0.3333333333333333,57,0.2118959107806691,4,0.8,1,0
59,"Intuitively , the LSTM encodes a text sequence considering its word - order information , but yields additional compositional parameters that must be learned .",Models & training,R K .,text-classification,8,11,0.3666666666666666,58,0.2156133828996282,5,1.0,1,0
60,Convolutional Sequence Encoder,Models & training,R K .,text-classification,8,12,0.4,59,0.2193308550185873,0,0.0,1,0
61,The Convolutional Neural Network ( CNN ) architecture is another strategy extensively employed as the compositional function to encode text sequences .,Models & training,R K .,text-classification,8,13,0.4333333333333333,60,0.2230483271375464,1,0.0555555555555555,1,0
62,"The convolution operation considers windows of n consecutive words within the sequence , where a set of filters ( to be learned ) are applied to these word windows to generate corresponding feature maps .",Models & training,R K .,text-classification,8,14,0.4666666666666667,61,0.2267657992565055,2,0.1111111111111111,1,0
63,"Subsequently , an aggregation operation ( such as max - pooling ) is used on top of the feature maps to abstract the most salient semantic features , resulting in the final representation .",Models & training,R K .,text-classification,8,15,0.5,62,0.2304832713754646,3,0.1666666666666666,1,0
64,"For most experiments , we consider a single - layer CNN text model .",Models & training,R K .,text-classification,8,16,0.5333333333333333,63,0.2342007434944238,4,0.2222222222222222,1,0
65,"However , Deep CNN text models have also been developed , and are considered in a few of our experiments .",Models & training,R K .,text-classification,8,17,0.5666666666666667,64,0.2379182156133829,5,0.2777777777777778,1,0
66,Simple Word - Embedding Model,Models & training,R K .,text-classification,8,18,0.6,65,0.241635687732342,6,0.3333333333333333,1,0
67,( SWEM ),Models & training,R K .,text-classification,8,19,0.6333333333333333,66,0.2453531598513011,7,0.3888888888888889,1,0
68,"To investigate the raw modeling capacity of word embeddings , we consider a class of models with no additional compositional parameters to encode natural language sequences , termed SWEMs .",Models & training,R K .,text-classification,8,20,0.6666666666666666,67,0.2490706319702602,8,0.4444444444444444,1,0
69,"Among them , the simplest strategy is to compute the element - wise average over word vectors fora given sequence :",Models & training,R K .,text-classification,8,21,0.7,68,0.2527881040892193,9,0.5,1,0
70,"The model in can be seen as an average pooling operation , which takes the mean over each of the K dimensions for all word embeddings , resulting in a representation z with the same dimension as the embedding itself , termed here SWEM - aver .",Models & training,R K .,text-classification,8,22,0.7333333333333333,69,0.2565055762081784,10,0.5555555555555556,1,0
71,"Intuitively , z takes the information of every sequence element into account via the addition operation .",Models & training,R K .,text-classification,8,23,0.7666666666666667,70,0.2602230483271375,11,0.6111111111111112,1,0
72,Max Pooling,Models & training,,text-classification,8,24,0.8,71,0.2639405204460966,12,0.6666666666666666,1,0
73,"Motivated by the observation that , in general , only a small number of key words contribute to final predictions , we propose another SWEM variant , that extracts the most salient features from every word - embedding dimension , by taking the maximum value along each dimension of the word vectors .",Models & training,Max Pooling,text-classification,8,25,0.8333333333333334,72,0.2676579925650557,13,0.7222222222222222,1,0
74,This strategy is similar to the max - over - time pooling operation in convolutional neural networks :,Models & training,Max Pooling,text-classification,8,26,0.8666666666666667,73,0.2713754646840148,14,0.7777777777777778,1,0
75,We denote this model variant as SWEM - max .,Models & training,Max Pooling,text-classification,8,27,0.9,74,0.275092936802974,15,0.8333333333333334,1,0
76,"Here the j - th component of z is the maximum element in the set {v 1 j , . . . , v Lj } , where v 1j is , for example , the j - th component of v 1 .",Models & training,Max Pooling,text-classification,8,28,0.9333333333333332,75,0.2788104089219331,16,0.8888888888888888,1,0
77,"With this pooling operation , those words that are unimportant or unrelated to the corresponding tasks will be ignored in the encoding process ( as the components of the embedding vectors will have small amplitude ) , unlike SWEM - aver where every word contributes equally to the representation .",Models & training,Max Pooling,text-classification,8,29,0.9666666666666668,76,0.2825278810408922,17,0.9444444444444444,1,0
78,"Considering that SWEM - aver and SWEM - max are complementary , in the sense of accounting for different types of information from text sequences ,",Models & training,Max Pooling,text-classification,8,30,1.0,77,0.2862453531598513,18,1.0,1,0
79,Model,,,text-classification,8,0,0.0,78,0.2899628252788104,0,0.0,1,0
80,Parameter s Speed CNN 541K 171s LSTM 1.8M 598s SWEM 61K 63s,Model,Model,text-classification,8,1,0.0294117647058823,79,0.2936802973977695,1,0.1428571428571428,1,0
81,"Interestingly , for the sentiment analysis tasks , both CNN and LSTM compositional functions perform better than SWEM , suggesting that wordorder information maybe required for analyzing sentiment orientations .",Model,Model,text-classification,8,2,0.0588235294117647,80,0.2973977695167286,2,0.2857142857142857,1,1
82,"This finding is consistent with , where they hypothesize that the positional information of a word in text sequences maybe beneficial to predict sentiment .",Model,Model,text-classification,8,3,0.088235294117647,81,0.3011152416356877,3,0.4285714285714285,1,0
83,"This is intuitively reasonable since , for instance , the phrase "" not really good "" and "" really not good "" convey different levels of negative sentiment , while being different only by their word orderings .",Model,Model,text-classification,8,4,0.1176470588235294,82,0.3048327137546468,4,0.5714285714285714,1,0
84,"Contrary to SWEM , CNN and LSTM models can both capture this type of information via convolutional filters or recurrent transition functions .",Model,Model,text-classification,8,5,0.1470588235294117,83,0.3085501858736059,5,0.7142857142857143,1,0
85,"However , as suggested above , such word - order patterns maybe much less useful for predicting the topic of a document .",Model,Model,text-classification,8,6,0.1764705882352941,84,0.312267657992565,6,0.8571428571428571,1,0
86,"This maybe attributed to the fact that word embeddings alone already provide sufficient topic information of a document , at least when the text sequences considered are relatively long .",Model,Model,text-classification,8,7,0.2058823529411764,85,0.3159851301115242,7,1.0,1,0
87,Parameters,Model,,text-classification,8,8,0.2352941176470588,86,0.3197026022304832,0,0.0,1,0
88,Complexity Sequential,Model,,text-classification,8,9,0.2647058823529412,87,0.3234200743494423,1,0.0384615384615384,1,0
89,"Ops we also propose a third SWEM variant , where the two abstracted features are concatenated together to form the sentence embeddings , denoted here as SWEM - concat .",Model,Complexity Sequential,text-classification,8,10,0.2941176470588235,88,0.3271375464684015,2,0.0769230769230769,1,0
90,"For all SWEM variants , there are no additional compositional parameters to be learned .",Model,Complexity Sequential,text-classification,8,11,0.3235294117647059,89,0.3308550185873606,3,0.1153846153846153,1,0
91,"As a result , the models only exploit intrinsic word embedding information for predictions .",Model,Complexity Sequential,text-classification,8,12,0.3529411764705882,90,0.3345724907063197,4,0.1538461538461538,1,0
92,"Hierarchical Pooling Both SWEM - aver and SWEM - max do not take word - order or spatial information into consideration , which could be useful for certain NLP applications .",Model,Complexity Sequential,text-classification,8,13,0.3823529411764705,91,0.3382899628252788,5,0.1923076923076923,1,0
93,"So motivated , we further propose a hierarchical pooling layer .",Model,Complexity Sequential,text-classification,8,14,0.4117647058823529,92,0.3420074349442379,6,0.2307692307692307,1,0
94,"Let v i:i+n?1 refer to the local window consisting of n consecutive words words ,",Model,Complexity Sequential,text-classification,8,15,0.4411764705882353,93,0.345724907063197,7,0.2692307692307692,1,0
95,"First , an average - pooling is performed on each local window , v i:i+n?1 .",Model,Complexity Sequential,text-classification,8,16,0.4705882352941176,94,0.3494423791821561,8,0.3076923076923077,1,0
96,The extracted features from all windows are further down - sampled with a global max - pooling operation on top of the representations for every window .,Model,Complexity Sequential,text-classification,8,17,0.5,95,0.3531598513011152,9,0.3461538461538461,1,0
97,We call this approach SWEM - hier due to its layered pooling .,Model,Complexity Sequential,text-classification,8,18,0.5294117647058824,96,0.3568773234200743,10,0.3846153846153846,1,0
98,"This strategy preserves the local spatial information of a text sequence in the sense that it keeps track of how the sentence / document is constructed from individual word windows , i.e. , n-grams .",Model,Complexity Sequential,text-classification,8,19,0.5588235294117647,97,0.3605947955390334,11,0.4230769230769231,1,0
99,This formulation is related to bag - of - n- grams method .,Model,Complexity Sequential,text-classification,8,20,0.5882352941176471,98,0.3643122676579926,12,0.4615384615384615,1,0
100,"However , SWEM - hier learns fixed - length representations for the n-grams that appear in the corpus , rather than just capturing their occurrences via count features , which may potentially advantageous for prediction purposes .",Model,Complexity Sequential,text-classification,8,21,0.6176470588235294,99,0.3680297397769517,13,0.5,1,0
101,Parameters & Computation,Model,,text-classification,8,22,0.6470588235294118,100,0.3717472118959107,14,0.5384615384615384,1,0
102,Comparison,Model,,text-classification,8,23,0.6764705882352942,101,0.3754646840148699,15,0.5769230769230769,1,0
103,"We compare CNN , LSTM and SWEM wrt their parameters and computational speed .",Model,Comparison,text-classification,8,24,0.7058823529411765,102,0.379182156133829,16,0.6153846153846154,1,0
104,"K denotes the dimension of word embeddings , as above .",Model,Comparison,text-classification,8,25,0.7352941176470589,103,0.3828996282527881,17,0.6538461538461539,1,0
105,"For the CNN , we use n to denote the filter width ( assumed constant for all filters , for simplicity of analysis , but in practice variable n is commonly used ) .",Model,Comparison,text-classification,8,26,0.7647058823529411,104,0.3866171003717472,18,0.6923076923076923,1,0
106,We defined as the dimension of the final sequence representation .,Model,Comparison,text-classification,8,27,0.7941176470588235,105,0.3903345724907063,19,0.7307692307692307,1,0
107,"Specifically , d represents the dimension of hidden units or the number of filters in LSTM or CNN , respectively .",Model,Comparison,text-classification,8,28,0.8235294117647058,106,0.3940520446096654,20,0.7692307692307693,1,0
108,We first examine the number of compositional parameters for each model .,Model,Comparison,text-classification,8,29,0.8529411764705882,107,0.3977695167286245,21,0.8076923076923077,1,0
109,"As shown in , both the CNN and LSTM have a large number of parameters , to model the semantic compositionality of text sequences , whereas SWEM has no such parameters .",Model,Comparison,text-classification,8,30,0.8823529411764706,108,0.4014869888475836,22,0.8461538461538461,1,0
110,"Similar to , we then consider the computational complexity and the minimum number of sequential operations required for each model .",Model,Comparison,text-classification,8,31,0.9117647058823528,109,0.4052044609665427,23,0.8846153846153846,1,0
111,SWEM tends to be more efficient than CNN and LSTM in terms of computation complexity .,Model,Comparison,text-classification,8,32,0.9411764705882352,110,0.4089219330855018,24,0.9230769230769232,1,0
112,"For example , considering the case where K = d , SWEM is faster than CNN or LSTM by a factor of nd or d , respectively .",Model,Comparison,text-classification,8,33,0.9705882352941176,111,0.4126394052044609,25,0.9615384615384616,1,0
113,"Further , the computations in SWEM are highly parallelizable , unlike LSTM that requires O ( L ) sequential steps .",Model,Comparison,text-classification,8,34,1.0,112,0.4163568773234201,26,1.0,1,0
114,Experiments,,,text-classification,8,0,0.0,113,0.4200743494423792,0,0.0,1,0
115,"We evaluate different compositional functions on a wide variety of supervised tasks , including document categorization , text sequence matching ( given a sentence pair , X 1 , X 2 , predict their relationship , y) as well as ( short ) sentence classification .",Experiments,Experiments,text-classification,8,1,0.0625,114,0.4237918215613382,1,0.0625,1,0
116,"We experiment on 17 datasets concerning natural language understanding , with corresponding data statistics summarized in the Supplementary Material .",Experiments,Experiments,text-classification,8,2,0.125,115,0.4275092936802974,2,0.125,1,0
117,We use Glo Ve word embeddings with K = 300 as initialization for all our models .,Experiments,Experiments,text-classification,8,3,0.1875,116,0.4312267657992565,3,0.1875,1,1
118,"Out - Of - Vocabulary ( OOV ) words are initialized from a uniform distribution with range [ ? 0.01 , 0.01 ] .",Experiments,Experiments,text-classification,8,4,0.25,117,0.4349442379182156,4,0.25,1,1
119,"The Glo Ve embeddings are employed in two ways to learn refined word embeddings : ( i ) directly updating each word embedding during training ; and ( ii ) training a 300 dimensional Multilayer Perceptron ( MLP ) layer with ReLU activation , with Glo Ve embeddings as input to the MLP and with output defining the refined word embeddings .",Experiments,Experiments,text-classification,8,5,0.3125,118,0.4386617100371747,5,0.3125,1,1
120,The latter approach corresponds to learning an MLP model that adapts GloVe embeddings to the dataset and task of interest .,Experiments,Experiments,text-classification,8,6,0.375,119,0.4423791821561338,6,0.375,1,0
121,The advantages of these two methods differ from dataset to dataset .,Experiments,Experiments,text-classification,8,7,0.4375,120,0.4460966542750929,7,0.4375,1,0
122,We choose the better strategy based on their corresponding performances on the validation set .,Experiments,Experiments,text-classification,8,8,0.5,121,0.449814126394052,8,0.5,1,0
123,"The final classifier is implemented as an MLP layer with dimension selected from the set [ 100 , 300 , 500 , 1000 ] , followed by a sigmoid or softmax function , depending on the specific task .",Experiments,Experiments,text-classification,8,9,0.5625,122,0.4535315985130111,9,0.5625,1,0
124,"Adam ) is used to optimize all models , with learning rate selected from .",Experiments,Experiments,text-classification,8,10,0.625,123,0.4572490706319702,10,0.625,1,1
125,"Surprisingly , on topic prediction tasks , our SWEM model exhibits stronger performances , relative to both LSTM and CNN compositional architectures , this by leveraging both the average and max - pooling features from word embeddings .",Experiments,Experiments,text-classification,8,11,0.6875,124,0.4609665427509293,11,0.6875,1,1
126,"Specifically , our SWEM - concat model even outperforms a 29 - layer deep CNN model , when predicting topics .",Experiments,Experiments,text-classification,8,12,0.75,125,0.4646840148698885,12,0.75,1,0
127,"On the ontology classification problem ( DBpedia dataset ) , we observe the same trend , that SWEM exhibits comparable or even superior results , relative to CNN or LSTM models .",Experiments,Experiments,text-classification,8,13,0.8125,126,0.4684014869888476,13,0.8125,1,1
128,"Since there are no compositional parameters in SWEM , our models have an order of magnitude fewer parameters ( excluding embeddings ) than LSTM or CNN , and are considerably more computationally efficient .",Experiments,Experiments,text-classification,8,14,0.875,127,0.4721189591078067,14,0.875,1,0
129,"As illustrated in Table 4 , SWEM - concat achieves better results on Yahoo !",Experiments,Experiments,text-classification,8,15,0.9375,128,0.4758364312267658,15,0.9375,1,0
130,"Answer than CNN / LSTM , with only 61 K parameters ( one - tenth the number of LSTM parameters , or one - third the number of CNN parameters ) , while taking a fraction of the training time relative to the CNN or LSTM .",Experiments,Experiments,text-classification,8,16,1.0,129,0.4795539033457249,16,1.0,1,0
131,Interpreting model predictions,,,text-classification,8,0,0.0,130,0.483271375464684,0,0.0,1,0
132,"Although the proposed SWEM - max variant generally performs a slightly worse than SWEM - aver , it extracts complementary features from SWEMaver , and hence inmost cases SWEM - concat exhibits the best performance among all SWEM variants .",Interpreting model predictions,Interpreting model predictions,text-classification,8,1,0.0138888888888888,131,0.4869888475836431,1,0.0588235294117647,1,0
133,"More importantly , we found that the word embeddings learned from SWEM - max tend to be sparse .",Interpreting model predictions,Interpreting model predictions,text-classification,8,2,0.0277777777777777,132,0.4907063197026022,2,0.1176470588235294,1,0
134,We trained our SWEM - max model on the Yahoo datasets ( randomly initialized ) .,Interpreting model predictions,Interpreting model predictions,text-classification,8,3,0.0416666666666666,133,0.4944237918215613,3,0.1764705882352941,1,0
135,"With the learned embeddings , we plot the values for each of the word embedding dimensions , for the entire vocabulary .",Interpreting model predictions,Interpreting model predictions,text-classification,8,4,0.0555555555555555,134,0.4981412639405204,4,0.2352941176470588,1,0
136,"As shown in , most of the values are highly concentrated around zero , indicating that the word embeddings learned are very sparse .",Interpreting model predictions,Interpreting model predictions,text-classification,8,5,0.0694444444444444,135,0.5018587360594795,5,0.2941176470588235,1,0
137,"On the contrary , the Glo Ve word embeddings , for the same vocabulary , are considerably denser than the embeddings learned from SWEM - max .",Interpreting model predictions,Interpreting model predictions,text-classification,8,6,0.0833333333333333,136,0.5055762081784386,6,0.3529411764705882,1,0
138,"This suggests that the model may only depend on a few key words , among the entire vocabulary , for predictions ( since most words do not contribute to the max - pooling operation in SWEM - max ) .",Interpreting model predictions,Interpreting model predictions,text-classification,8,7,0.0972222222222222,137,0.5092936802973977,7,0.4117647058823529,1,0
139,"Through the embedding , the model learns the important words fora given task ( those words with non -zero embedding components ) .",Interpreting model predictions,Interpreting model predictions,text-classification,8,8,0.1111111111111111,138,0.5130111524163569,8,0.4705882352941176,1,0
140,"In this regard , the nature of max - pooling pro - cess gives rise to a more interpretable model .",Interpreting model predictions,Interpreting model predictions,text-classification,8,9,0.125,139,0.516728624535316,9,0.5294117647058824,1,0
141,"For a document , only the word with largest value in each embedding dimension is employed for the final representation .",Interpreting model predictions,Interpreting model predictions,text-classification,8,10,0.1388888888888889,140,0.5204460966542751,10,0.5882352941176471,1,0
142,"Thus , we suspect that semantically similar words may have large values in some shared dimensions .",Interpreting model predictions,Interpreting model predictions,text-classification,8,11,0.1527777777777778,141,0.5241635687732342,11,0.6470588235294118,1,0
143,"So motivated , after training the SWEM - max model on the Yahoo dataset , we selected five words with the largest values , among the entire vocabulary , for each word embedding dimension ( these words are selected preferentially in the corresponding dimension , by the max operation ) .",Interpreting model predictions,Interpreting model predictions,text-classification,8,12,0.1666666666666666,142,0.5278810408921933,12,0.7058823529411765,1,0
144,"As shown in , the words chosen wrt each embedding dimension are indeed highly relevant and correspond to a common topic ( the topics are inferred from words ) .",Interpreting model predictions,Interpreting model predictions,text-classification,8,13,0.1805555555555555,143,0.5315985130111525,13,0.7647058823529411,1,0
145,"For example , the words in the first column of are all political terms , which could be assigned to the Politics & Government topic .",Interpreting model predictions,Interpreting model predictions,text-classification,8,14,0.1944444444444444,144,0.5353159851301115,14,0.8235294117647058,1,0
146,Note that our model can even learn locally interpretable structure that is not explicitly indicated by the label information .,Interpreting model predictions,Interpreting model predictions,text-classification,8,15,0.2083333333333333,145,0.5390334572490706,15,0.8823529411764706,1,0
147,"For instance , all words in the fifth column are Chemistry - related .",Interpreting model predictions,Interpreting model predictions,text-classification,8,16,0.2222222222222222,146,0.5427509293680297,16,0.9411764705882352,1,0
148,"However , we do not have a chemistry label in the dataset , and regardless they should belong to the Science topic .",Interpreting model predictions,Interpreting model predictions,text-classification,8,17,0.2361111111111111,147,0.5464684014869888,17,1.0,1,0
149,Text Sequence Matching,Interpreting model predictions,Interpreting model predictions,text-classification,8,18,0.25,148,0.550185873605948,0,0.0,1,0
150,"To gain a deeper understanding regarding the modeling capacity of word embeddings , we further investigate the problem of sentence matching , including natural language inference , answer sentence selection and paraphrase identification .",Interpreting model predictions,Interpreting model predictions,text-classification,8,19,0.2638888888888889,149,0.5539033457249071,1,0.0476190476190476,1,0
151,The corresponding performance metrics are shown in .,Interpreting model predictions,,text-classification,8,20,0.2777777777777778,150,0.5576208178438662,2,0.0952380952380952,1,0
152,"Surprisingly , on most of the datasets considered ( except WikiQA ) , SWEM demonstrates the best results compared with those with CNN or the LSTM encoder .",Interpreting model predictions,The corresponding performance metrics are shown in .,text-classification,8,21,0.2916666666666667,151,0.5613382899628253,3,0.1428571428571428,1,1
153,"Notably , on SNLI dataset , we observe that SWEM - max performs the best among all SWEM variants , consistent with the findings in Nie and Bansal ( 2017 ) ; , that max - pooling over BiLSTM hidden units outperforms average pooling operation on SNLI dataset .",Interpreting model predictions,The corresponding performance metrics are shown in .,text-classification,8,22,0.3055555555555556,152,0.5650557620817844,4,0.1904761904761904,1,1
154,"As a result , with only 120K parameters , our SWEM - max achieves a test accuracy of 83.8 % , which is very competitive among state - of the - art sentence encoding - based models ( in terms of both performance and number of parameters )",Interpreting model predictions,The corresponding performance metrics are shown in .,text-classification,8,23,0.3194444444444444,153,0.5687732342007435,5,0.238095238095238,1,0
155,1 .,Interpreting model predictions,The corresponding performance metrics are shown in .,text-classification,8,24,0.3333333333333333,154,0.5724907063197026,6,0.2857142857142857,1,0
156,"The strong results of the SWEM approach on these tasks may stem from the fact that when matching natural language sentences , it is sufficient inmost cases to simply model the word - level alignments between two sequences .",Interpreting model predictions,The corresponding performance metrics are shown in .,text-classification,8,25,0.3472222222222222,155,0.5762081784386617,7,0.3333333333333333,1,0
157,"From this perspective , word - order information becomes much less useful for predicting relationship between sentences .",Interpreting model predictions,The corresponding performance metrics are shown in .,text-classification,8,26,0.3611111111111111,156,0.5799256505576208,8,0.3809523809523809,1,0
158,"Moreover , considering the simpler model architecture of SWEM , they could be much easier to be optimized than LSTM or CNN - based models , and thus give rise to better empirical results .",Interpreting model predictions,The corresponding performance metrics are shown in .,text-classification,8,27,0.375,157,0.5836431226765799,9,0.4285714285714285,1,0
159,Importance of word - order information,Interpreting model predictions,,text-classification,8,28,0.3888888888888889,158,0.587360594795539,10,0.4761904761904761,1,0
160,"One possible disadvantage of SWEM is that it ignores the word - order information within a text sequence , which could be potentially captured by CNN - or LSTM - based models .",Interpreting model predictions,Importance of word - order information,text-classification,8,29,0.4027777777777778,159,0.5910780669144982,11,0.5238095238095238,1,0
161,"However , we empirically found that except for sentiment analysis , SWEM exhibits similar or even superior performance as the CNN or LSTM on a variety of tasks .",Interpreting model predictions,Importance of word - order information,text-classification,8,30,0.4166666666666667,160,0.5947955390334573,12,0.5714285714285714,1,0
162,"In this regard , one natural question would be : how important are word - order features for these tasks ?",Interpreting model predictions,Importance of word - order information,text-classification,8,31,0.4305555555555556,161,0.5985130111524164,13,0.6190476190476191,1,0
163,"To this end , we randomly shuffle the words for every sentence in the training set , while keeping the original word order for samples in the test set .",Interpreting model predictions,Importance of word - order information,text-classification,8,32,0.4444444444444444,162,0.6022304832713755,14,0.6666666666666666,1,0
164,The motivation here is to remove the word - order features from the training set and examine how sensitive the performance on different tasks are to word - order information .,Interpreting model predictions,Importance of word - order information,text-classification,8,33,0.4583333333333333,163,0.6059479553903345,15,0.7142857142857143,1,0
165,We use LSTM as the model for this purpose since it can captures wordorder information from the original training set .,Interpreting model predictions,Importance of word - order information,text-classification,8,34,0.4722222222222222,164,0.6096654275092936,16,0.7619047619047619,1,0
166,The results on three distinct tasks are shown in .,Interpreting model predictions,Importance of word - order information,text-classification,8,35,0.4861111111111111,165,0.6133828996282528,17,0.8095238095238095,1,0
167,"Somewhat surprisingly , for Yahoo and SNLI datasets , the LSTM model trained on shuffled training set shows comparable accuracies to those trained on the original dataset , indicating that word - order information does not contribute significantly on these two problems , i.e. , topic categorization and textual entailment .",Interpreting model predictions,Importance of word - order information,text-classification,8,36,0.5,166,0.6171003717472119,18,0.8571428571428571,1,0
168,"However , on the Yelp polarity dataset , the results drop noticeably , further suggesting that word - order does matter for sentiment analysis ( as indicated above from a different perspective ) .",Interpreting model predictions,Importance of word - order information,text-classification,8,37,0.5138888888888888,167,0.620817843866171,19,0.9047619047619048,1,0
169,"Notably , the performance of LSTM on the Yelp dataset with a shuffled training set is very close to our results with SWEM , indicating that the main difference between LSTM and SWEM maybe due to the ability of the former to capture word - order features .",Interpreting model predictions,Importance of word - order information,text-classification,8,38,0.5277777777777778,168,0.6245353159851301,20,0.9523809523809524,1,0
170,Both observations are in consistent with our experimental results in the previous section .,Interpreting model predictions,Importance of word - order information,text-classification,8,39,0.5416666666666666,169,0.6282527881040892,21,1.0,1,0
171,Case Study,Interpreting model predictions,,text-classification,8,40,0.5555555555555556,170,0.6319702602230484,0,0.0,1,0
172,"To understand what type of sentences are sensitive to word - order information , we further show those samples that are wrongly predicted because of the shuffling of training data in .",Interpreting model predictions,Case Study,text-classification,8,41,0.5694444444444444,171,0.6356877323420075,1,0.0526315789473684,1,0
173,"Taking the first sentence as an example , several words in the review are generally positive , i.e. friendly , nice , okay , great and likes .",Interpreting model predictions,Case Study,text-classification,8,42,0.5833333333333334,172,0.6394052044609665,2,0.1052631578947368,1,0
174,"However , the most vital features for predicting the sentiment of this sentence could be the phrase / sentence ' is just okay ' , ' not great ' or ' makes me wonder why everyone likes ' , which can not be captured without considering word - order features .",Interpreting model predictions,Case Study,text-classification,8,43,0.5972222222222222,173,0.6431226765799256,3,0.1578947368421052,1,0
175,It is worth noting the hints for predictions in this case are actually ngram phrases from the input document .,Interpreting model predictions,Case Study,text-classification,8,44,0.6111111111111112,174,0.6468401486988847,4,0.2105263157894736,1,0
176,SWEM - hier for sentiment analysis,Interpreting model predictions,,text-classification,8,45,0.625,175,0.6505576208178439,5,0.2631578947368421,1,0
177,"As demonstrated in Section 4.2.1 , word - order information plays a vital role for sentiment analysis tasks .",Interpreting model predictions,SWEM - hier for sentiment analysis,text-classification,8,46,0.6388888888888888,176,0.654275092936803,6,0.3157894736842105,1,1
178,"However , according to the case study above , the most important features for sentiment prediction maybe some key n-gram phrase / words from Negative :",Interpreting model predictions,SWEM - hier for sentiment analysis,text-classification,8,47,0.6527777777777778,177,0.6579925650557621,7,0.3684210526315789,1,1
179,Friendly staff and nice selection of vegetarian options .,Interpreting model predictions,,text-classification,8,48,0.6666666666666666,178,0.6617100371747212,8,0.4210526315789473,1,0
180,"Food is just okay , not great .",Interpreting model predictions,,text-classification,8,49,0.6805555555555556,179,0.6654275092936803,9,0.4736842105263157,1,0
181,Makes me wonder why everyone likes food fight so much .,Interpreting model predictions,"Food is just okay , not great .",text-classification,8,50,0.6944444444444444,180,0.6691449814126395,10,0.5263157894736842,1,0
182,Positive :,Interpreting model predictions,"Food is just okay , not great .",text-classification,8,51,0.7083333333333334,181,0.6728624535315985,11,0.5789473684210527,1,0
183,"The store is small , but it carries specialties that are difficult to find in Pittsburgh .",Interpreting model predictions,"Food is just okay , not great .",text-classification,8,52,0.7222222222222222,182,0.6765799256505576,12,0.631578947368421,1,0
184,I was particularly excited to find middle eastern chili sauce and chocolate covered turkish delights .,Interpreting model predictions,"Food is just okay , not great .",text-classification,8,53,0.7361111111111112,183,0.6802973977695167,13,0.6842105263157895,1,0
185,the input document .,Interpreting model predictions,"Food is just okay , not great .",text-classification,8,54,0.75,184,0.6840148698884758,14,0.7368421052631579,1,0
186,"We hypothesize that incorporating information about the local word - order , i.e. , n-gram features , is likely to largely mitigate the limitations of the above three SWEM variants .",Interpreting model predictions,"Food is just okay , not great .",text-classification,8,55,0.7638888888888888,185,0.6877323420074349,15,0.7894736842105263,1,0
187,"Inspired by this observation , we propose using another simple pooling operation termed as hierarchical ( SWEM - hier ) , as detailed in Section 3.3 .",Interpreting model predictions,"Food is just okay , not great .",text-classification,8,56,0.7777777777777778,186,0.6914498141263941,16,0.8421052631578947,1,0
188,We evaluate this method on the two documentlevel sentiment analysis tasks and the results are shown in the last row of .,Interpreting model predictions,"Food is just okay , not great .",text-classification,8,57,0.7916666666666666,187,0.6951672862453532,17,0.8947368421052632,1,0
189,"SWEM - hier greatly outperforms the other three SWEM variants , and the corresponding accuracies are comparable to the results of CNN or LSTM ) .",Interpreting model predictions,"Food is just okay , not great .",text-classification,8,58,0.8055555555555556,188,0.6988847583643123,18,0.9473684210526316,1,1
190,"This indicates that the proposed hierarchical pooling operation manages to abstract spatial ( word - order ) information from the input sequence , which is beneficial for performance in sentiment analysis tasks .",Interpreting model predictions,"Food is just okay , not great .",text-classification,8,59,0.8194444444444444,189,0.7026022304832714,19,1.0,1,0
191,Short Sentence Processing,Interpreting model predictions,"Food is just okay , not great .",text-classification,8,60,0.8333333333333334,190,0.7063197026022305,0,0.0,1,0
192,We now consider sentence - classification tasks ( with approximately 20 words on average ) .,Interpreting model predictions,"Food is just okay , not great .",text-classification,8,61,0.8472222222222222,191,0.7100371747211895,1,0.0909090909090909,1,0
193,"We experiment on three sentiment classification datasets , i.e. , MR , SST - 1 , SST - 2 , as well as subjectivity classification ( Subj ) and question classification ( TREC ) .",Interpreting model predictions,"Food is just okay , not great .",text-classification,8,62,0.8611111111111112,192,0.7137546468401487,2,0.1818181818181818,1,0
194,The corresponding results are shown in .,Interpreting model predictions,,text-classification,8,63,0.875,193,0.7174721189591078,3,0.2727272727272727,1,0
195,"Compared with CNN / LSTM compositional functions , SWEM yields inferior accuracies on sentiment analysis datasets , consistent with our observation in the case of document categorization .",Interpreting model predictions,The corresponding results are shown in .,text-classification,8,64,0.8888888888888888,194,0.7211895910780669,4,0.3636363636363636,1,1
196,"However , SWEM exhibits comparable performance on the other two tasks , again with much less parameters and faster training .",Interpreting model predictions,The corresponding results are shown in .,text-classification,8,65,0.9027777777777778,195,0.724907063197026,5,0.4545454545454545,1,1
197,"Further , we investigate two sequence tagging tasks : the standard CoNLL2000 chunking and CoNLL2003 NER datasets .",Interpreting model predictions,The corresponding results are shown in .,text-classification,8,66,0.9166666666666666,196,0.7286245353159851,6,0.5454545454545454,1,0
198,"Results are shown in the Supplementary Material , where LSTM and CNN again perform better than SWEMs .",Interpreting model predictions,The corresponding results are shown in .,text-classification,8,67,0.9305555555555556,197,0.7323420074349443,7,0.6363636363636364,1,0
199,"Generally , SWEM is less effective at extracting representations from short sentences than from long documents .",Interpreting model predictions,The corresponding results are shown in .,text-classification,8,68,0.9444444444444444,198,0.7360594795539034,8,0.7272727272727273,1,0
200,"This maybe due to the fact that fora shorter text sequence , word - order features tend to be more important since the semantic information provided byword embeddings alone is relatively limited .",Interpreting model predictions,The corresponding results are shown in .,text-classification,8,69,0.9583333333333334,199,0.7397769516728625,9,0.8181818181818182,1,0
201,"Moreover , we note that the results on these relatively small datasets are highly sensitive to model regularization techniques due to the overfitting issues .",Interpreting model predictions,The corresponding results are shown in .,text-classification,8,70,0.9722222222222222,200,0.7434944237918215,10,0.9090909090909092,1,0
202,"In this regard , one interesting future direction maybe to develop specific regularization strategies for the SWEM framework , and thus make them work better on small sentence classification datasets .",Interpreting model predictions,The corresponding results are shown in .,text-classification,8,71,0.9861111111111112,201,0.7472118959107806,11,1.0,1,0
203,Discussion,Interpreting model predictions,,text-classification,8,72,1.0,202,0.7509293680297398,0,0.0,1,0
204,Comparison via subspace training,,,text-classification,8,0,0.0,203,0.7546468401486989,0,0.0,1,0
205,We use subspace training to measure the model complexity in text classification problems .,Comparison via subspace training,Comparison via subspace training,text-classification,8,1,0.037037037037037,204,0.758364312267658,1,0.0625,1,0
206,"It constrains the optimization of the trainable parameters in a subspace of low dimension d , the intrinsic dimension dint defines the minimum d that yield a good solution .",Comparison via subspace training,Comparison via subspace training,text-classification,8,2,0.074074074074074,205,0.7620817843866171,2,0.125,1,0
207,"Two models are studied : the SWEM - max variant , and the CNN model including a convolutional layer followed by a FC layer .",Comparison via subspace training,Comparison via subspace training,text-classification,8,3,0.1111111111111111,206,0.7657992565055762,3,0.1875,1,0
208,We consider two settings :,Comparison via subspace training,Comparison via subspace training,text-classification,8,4,0.1481481481481481,207,0.7695167286245354,4,0.25,1,0
209,"( 1 ) The word embeddings are randomly intialized , and optimized jointly with the model parameters .",Comparison via subspace training,Comparison via subspace training,text-classification,8,5,0.1851851851851851,208,0.7732342007434945,5,0.3125,1,0
210,We show the performance of direct and subspace training on AG News dataset in ( a ) ( b ) .,Comparison via subspace training,Comparison via subspace training,text-classification,8,6,0.2222222222222222,209,0.7769516728624535,6,0.375,1,0
211,The two models trained via direct method share almost identical perfomrnace on training and testing .,Comparison via subspace training,Comparison via subspace training,text-classification,8,7,0.2592592592592592,210,0.7806691449814126,7,0.4375,1,0
212,"The subspace training yields similar accuracy with direct training for very small d , even when model parameters are not trained at all ( d = 0 ) .",Comparison via subspace training,Comparison via subspace training,text-classification,8,8,0.2962962962962963,211,0.7843866171003717,8,0.5,1,0
213,"This is because the word embeddings have the full degrees of freedom to adjust to achieve good solutions , regardless of the employed models .",Comparison via subspace training,Comparison via subspace training,text-classification,8,9,0.3333333333333333,212,0.7881040892193308,9,0.5625,1,0
214,SWEM seems to have an easier loss landspace than CNN for word embeddings to find the best solutions .,Comparison via subspace training,Comparison via subspace training,text-classification,8,10,0.3703703703703703,213,0.79182156133829,10,0.625,1,0
215,"According to Occam 's razor , simple models are preferred , if all else are the same .",Comparison via subspace training,Comparison via subspace training,text-classification,8,11,0.4074074074074074,214,0.7955390334572491,11,0.6875,1,0
216,"( 2 ) The pre-trained GloVe are frozen for the word embeddings , and only the model parameters are optimized .",Comparison via subspace training,Comparison via subspace training,text-classification,8,12,0.4444444444444444,215,0.7992565055762082,12,0.75,1,0
217,"The results on testing datasets of AG News and Yelp P. are shown in ( c ) ( d ) , respectively .",Comparison via subspace training,Comparison via subspace training,text-classification,8,13,0.4814814814814814,216,0.8029739776951673,13,0.8125,1,0
218,"SWEM shows significantly higher accuracy than CNN fora large range of low subspace dimension , indicating that SWEM is more parameter - efficient to get a decent solution .",Comparison via subspace training,Comparison via subspace training,text-classification,8,14,0.5185185185185185,217,0.8066914498141264,14,0.875,1,0
219,"In ( c ) , if we set the performance threshold Model MR SST - 1 SST - 2 Subj TREC RAE 77.7 43.2 82.4 --MV-RNN 79.0 44.4 82.9 --LSTM - 46.4 84.9 --RNN 77.2 --93.7 90.2 Constituency Tree-LSTM - 51.0 88.0 -- Dynamic CNN - 48.5 86.8 - 93.0 CNN 81 as 80 % testing accuracy , SWEM exhibits a lower dint than CNN on AG News dataset .",Comparison via subspace training,Comparison via subspace training,text-classification,8,15,0.5555555555555556,218,0.8104089219330854,15,0.9375,1,0
220,"However , in , CNN can leverage more trainable parameters to achieve higher accuracy when dis large .",Comparison via subspace training,Comparison via subspace training,text-classification,8,16,0.5925925925925926,219,0.8141263940520446,16,1.0,1,0
221,Linear classifiers,Comparison via subspace training,,text-classification,8,17,0.6296296296296297,220,0.8178438661710037,0,0.0,1,0
222,"To further investigate the quality of representations learned from SWEMs , we employ a linear classifier on top of the representations for prediction , instead of a non-linear MLP layer as in the previous section .",Comparison via subspace training,Linear classifiers,text-classification,8,18,0.6666666666666666,221,0.8215613382899628,1,0.25,1,0
223,It turned out that utilizing a linear classifier only leads to a very small performance drop for both Yahoo !,Comparison via subspace training,Linear classifiers,text-classification,8,19,0.7037037037037037,222,0.8252788104089219,2,0.5,1,0
224,Ans. ( from 73.53 % to 73.18 % ) and Yelp P. datasets ( from 93.76 % to 93.66 % ) .,Comparison via subspace training,Linear classifiers,text-classification,8,20,0.7407407407407407,223,0.828996282527881,3,0.75,1,0
225,This observation highlights that SWEMs are able to extract robust and informative sentence representations despite their simplicity .,Comparison via subspace training,Linear classifiers,text-classification,8,21,0.7777777777777778,224,0.8327137546468402,4,1.0,1,0
226,Extension to other languages,Comparison via subspace training,,text-classification,8,22,0.8148148148148148,225,0.8364312267657993,0,0.0,1,0
227,"We have also tried our SWEM - concat and SWE Mhier models on Sogou news corpus ( with the same experimental setup as ) , which is a Chinese dataset represented by Pinyin ( a phonetic romanization of Chinese ) .",Comparison via subspace training,Extension to other languages,text-classification,8,23,0.8518518518518519,226,0.8401486988847584,1,0.2,1,0
228,"SWEMconcat yields an accuracy of 91.3 % , while SWEM - hier ( with a local window size of 5 ) obtains an accuracy of 96.2 % on the test set .",Comparison via subspace training,Extension to other languages,text-classification,8,24,0.8888888888888888,227,0.8438661710037175,2,0.4,1,0
229,"Notably , the performance of SWEM - hier is comparable to the best accuracies of CNN ( 95.6 % ) and LSTM ( 95.2 % ) , as reported in .",Comparison via subspace training,Extension to other languages,text-classification,8,25,0.925925925925926,228,0.8475836431226765,3,0.6,1,0
230,"This indicates that hierarchical pooling is more suitable than average / max pooling for Chinese text classification , by taking spatial information into account .",Comparison via subspace training,Extension to other languages,text-classification,8,26,0.9629629629629628,229,0.8513011152416357,4,0.8,1,0
231,It also implies that Chinese is more sensitive to local word - order features than English .,Comparison via subspace training,Extension to other languages,text-classification,8,27,1.0,230,0.8550185873605948,5,1.0,1,0
232,Conclusions,,,text-classification,8,0,0.0,231,0.8587360594795539,0,0.0,1,0
233,"We have performed a comparative study between SWEM ( with parameter - free pooling operations ) and CNN or LSTM - based models , to represent text sequences on 17 NLP datasets .",Conclusions,Conclusions,text-classification,8,1,0.027027027027027,232,0.862453531598513,1,0.0357142857142857,0,0
234,"We further validated our experimental findings through additional exploration , and revealed some general rules for rationally selecting compositional functions for distinct problems .",Conclusions,Conclusions,text-classification,8,2,0.054054054054054,233,0.8661710037174721,2,0.0714285714285714,0,0
235,Our findings regarding when ( and why ) simple pooling operations are enough for text sequence representations are summarized as follows :,Conclusions,Conclusions,text-classification,8,3,0.081081081081081,234,0.8698884758364313,3,0.1071428571428571,0,0
236,"Simple pooling operations are surprisingly effective at representing longer documents ( with hundreds of words ) , while recurrent / convolutional compositional functions are most effective when constructing representations for short sentences .",Conclusions,Conclusions,text-classification,8,4,0.1081081081081081,235,0.8736059479553904,4,0.1428571428571428,0,0
237,Sentiment analysis tasks are more sensitive to word - order features than topic categorization tasks .,Conclusions,Conclusions,text-classification,8,5,0.1351351351351351,236,0.8773234200743495,5,0.1785714285714285,0,0
238,"However , a simple hierarchical pooling layer proposed here achieves comparable results to LSTM / CNN on sentiment analysis tasks .",Conclusions,Conclusions,text-classification,8,6,0.1621621621621621,237,0.8810408921933085,6,0.2142857142857142,0,0
239,"To match natural language sentences , e.g. , textual entailment , answer sentence selection , etc. , simple pooling operations already exhibit similar or even superior results , compared to CNN and LSTM .",Conclusions,Conclusions,text-classification,8,7,0.1891891891891892,238,0.8847583643122676,7,0.25,0,0
240,"We consider a wide range of text - representationbased tasks in this paper , including document categorization , text sequence matching and ( short ) sentence classification .",Conclusions,Conclusions,text-classification,8,8,0.2162162162162162,239,0.8884758364312267,8,0.2857142857142857,0,0
241,"For document classification tasks , we use the same data splits in ( downloaded from https://goo.gl/QaRpr7 ) ; for short sentence classification , we employ the same training / testing data and preprocessing procedure with .",Conclusions,Conclusions,text-classification,8,9,0.2432432432432432,240,0.8921933085501859,9,0.3214285714285714,0,0
242,"The statistics and corresponding types of these datasets are summarized in Datasets #w #c Train Types SWEM - CRF indicates that CRF is directly operated on top of the word embedding layer and make predictions for each word ( there is no contextual / word - order information before CRF layer , compared to CNN - CRF or BI - LSTM - CRF ) .",Conclusions,Conclusions,text-classification,8,10,0.2702702702702703,241,0.895910780669145,10,0.3571428571428571,0,0
243,"As shown above , CNN - CRF and BI - LSTM - CRF consistently outperform SWEM - CRF on both sequence tagging tasks , although the training takes around 4 to 5 times longer ( for BI - LSTM - CRF ) than SWEM - CRF .",Conclusions,Conclusions,text-classification,8,11,0.2972972972972973,242,0.8996282527881041,11,0.3928571428571428,0,0
244,"This suggests that for chunking and NER , compositional functions such as LSTM or CNN are very necessary , because of the sequential ( order-sensitive ) nature of sequence tagging tasks .",Conclusions,Conclusions,text-classification,8,12,0.3243243243243243,243,0.9033457249070632,12,0.4285714285714285,0,0
245,What are the key words used for predictions ?,Conclusions,Conclusions,text-classification,8,13,0.3513513513513513,244,0.9070631970260224,13,0.4642857142857143,0,0
246,"Given the sparsity of word embeddings , one natural question would be : What are those key words that are leveraged by the model to make predictions ?",Conclusions,Conclusions,text-classification,8,14,0.3783783783783784,245,0.9107806691449816,14,0.5,0,0
247,"To this end , after training SWEM - max on Yahoo !",Conclusions,Conclusions,text-classification,8,15,0.4054054054054054,246,0.9144981412639404,15,0.5357142857142857,0,0
248,"Answer dataset , we selected the top - 10 words ( with the maximum values in that dimension ) for every word embedding dimension .",Conclusions,Conclusions,text-classification,8,16,0.4324324324324324,247,0.9182156133828996,16,0.5714285714285714,0,0
249,The results are visualized in .,Conclusions,,text-classification,8,17,0.4594594594594595,248,0.9219330855018588,17,0.6071428571428571,0,0
250,"These words are indeed very predictive since they are likely to occur in documents with a specific topic , as discussed above .",Conclusions,The results are visualized in .,text-classification,8,18,0.4864864864864865,249,0.9256505576208178,18,0.6428571428571429,0,0
251,"Another interesting observation is that the frequencies of these words are actually quite low in the training set ( e.g. colston : 320 , repubs : 255 win32 : 276 ) , considering the large size of the training set ( 1,400 K ) .",Conclusions,The results are visualized in .,text-classification,8,19,0.5135135135135135,250,0.929368029739777,19,0.6785714285714286,0,0
252,"This suggests that the model is utilizing those relatively rare , yet representative words of each topic for the final predictions .",Conclusions,The results are visualized in .,text-classification,8,20,0.5405405405405406,251,0.933085501858736,20,0.7142857142857143,0,0
253,information of a text sequence is the word embedding .,Conclusions,The results are visualized in .,text-classification,8,21,0.5675675675675675,252,0.9368029739776952,21,0.75,0,0
254,"Thus , it is of interest to see how many word embedding dimensions are needed fora SWEM architecture to perform well .",Conclusions,The results are visualized in .,text-classification,8,22,0.5945945945945946,253,0.9405204460966544,22,0.7857142857142857,0,0
255,"To this end , we vary the dimension from 3 to 1000 and train a SWEMconcat model on the Yahoo dataset .",Conclusions,The results are visualized in .,text-classification,8,23,0.6216216216216216,254,0.9442379182156134,23,0.8214285714285714,0,0
256,"For fair comparison , the word embeddings are randomly initialized in this experiment , since there are no pretrained word vectors , such as GloVe , for some dimensions we consider .",Conclusions,The results are visualized in .,text-classification,8,24,0.6486486486486487,255,0.9479553903345724,24,0.8571428571428571,0,0
257,"As shown in , the model exhibits higher accuracy with larger word embedding dimensions .",Conclusions,The results are visualized in .,text-classification,8,25,0.6756756756756757,256,0.9516728624535316,25,0.8928571428571429,0,0
258,"This is not surprising since with more embedding dimensions , more semantic features could be potentially encapsulated .",Conclusions,The results are visualized in .,text-classification,8,26,0.7027027027027027,257,0.9553903345724908,26,0.9285714285714286,0,0
259,"However , we also observe that even with only 10 dimensions , SWEM demonstrates comparable results relative to the case with 1000 dimensions , suggesting that word embeddings are very efficient at abstracting semantic information into fixed - length vectors .",Conclusions,The results are visualized in .,text-classification,8,27,0.7297297297297297,258,0.9591078066914498,27,0.9642857142857144,0,0
260,"This property indicates that we may further reduce the number of model parameters with lowerdimensional word embeddings , while still achieving competitive results .",Conclusions,The results are visualized in .,text-classification,8,28,0.7567567567567568,259,0.9628252788104088,28,1.0,0,0
261,Sensitivity of compositional functions to sample size,Conclusions,,text-classification,8,29,0.7837837837837838,260,0.966542750929368,0,0.0,0,0
262,"To explore the robustness of different compositional functions , we consider another application scenario , where we only have a limited number of training data , e.g. , when labeled data are expensive to obtain .",Conclusions,Sensitivity of compositional functions to sample size,text-classification,8,30,0.8108108108108109,261,0.9702602230483272,1,0.125,0,0
263,"To investigate this , we re-run the experiments on Yahoo and SNLI datasets , while employing increasing proportions of the original training set .",Conclusions,Sensitivity of compositional functions to sample size,text-classification,8,31,0.8378378378378378,262,0.9739776951672864,2,0.25,0,0
264,"Specifically , we use 0.1 % , 0.2 % , 0.6 % , 1.0 % , 10 % , 100 % for comparison ; the corresponding results are shown in .",Conclusions,Sensitivity of compositional functions to sample size,text-classification,8,32,0.8648648648648649,263,0.9776951672862454,3,0.375,0,0
265,"Surprisingly , SWEM consistently outperforms CNN and LSTM models by a large margin , on a wide range of training data proportions .",Conclusions,Sensitivity of compositional functions to sample size,text-classification,8,33,0.8918918918918919,264,0.9814126394052044,4,0.5,0,0
266,"For instance , with 0.1 % of the training samples from Yahoo dataset ( around 1.4 K labeled data ) , SWEM achieves an accuracy of 56. 10 % , which is much better than that of models with CNN ( 25.32 % ) or LSTM ( 42.37 % ) .",Conclusions,Sensitivity of compositional functions to sample size,text-classification,8,34,0.918918918918919,265,0.9851301115241636,5,0.625,0,0
267,"On the SNLI dataset , we also noticed the same trend that the SWEM architecture result in much better accuracies , with a fraction of training data .",Conclusions,Sensitivity of compositional functions to sample size,text-classification,8,35,0.945945945945946,266,0.9888475836431226,6,0.75,0,0
268,"This observation indicates that overfitting issues in CNN or LSTMbased models on text data mainly stems from overcomplicated compositional functions , rather than the word embedding layer .",Conclusions,Sensitivity of compositional functions to sample size,text-classification,8,36,0.972972972972973,267,0.9925650557620818,7,0.875,0,0
269,"More importantly , SWEM tends to be afar more robust model when only limited data are available for training .",Conclusions,Sensitivity of compositional functions to sample size,text-classification,8,37,1.0,268,0.9962825278810408,8,1.0,0,0
1,title,,,text-classification,9,0,0.0,0,0.0,0,0.0,1,0
2,Translations as Additional Contexts for Sentence Classification,title,,text-classification,9,1,0.0,1,0.0039682539682539,1,0.0,1,1
3,abstract,,,text-classification,9,0,0.0,2,0.0079365079365079,0,0.0,1,0
4,"In sentence classification tasks , additional contexts , such as the neighboring sentences , may improve the accuracy of the classifier .",abstract,abstract,text-classification,9,1,0.1428571428571428,3,0.0119047619047619,1,0.1428571428571428,1,0
5,"However , such contexts are domain - dependent and thus can not be used for another classification task with an inappropriate domain .",abstract,abstract,text-classification,9,2,0.2857142857142857,4,0.0158730158730158,2,0.2857142857142857,1,0
6,"In contrast , we propose the use of translated sentences as domain - free context that is always available regardless of the domain .",abstract,abstract,text-classification,9,3,0.4285714285714285,5,0.0198412698412698,3,0.4285714285714285,1,0
7,"We find that naive feature expansion of translations gains only marginal improvements and may decrease the performance of the classifier , due to possible inaccurate translations thus producing noisy sentence vectors .",abstract,abstract,text-classification,9,4,0.5714285714285714,6,0.0238095238095238,4,0.5714285714285714,1,0
8,"To this end , we present multiple context fixing attachment ( MCFA ) , a series of modules attached to multiple sentence vectors to fix the noise in the vectors using the other sentence vectors as context .",abstract,abstract,text-classification,9,5,0.7142857142857143,7,0.0277777777777777,5,0.7142857142857143,1,0
9,"We show that our method performs competitively compared to previous models , achieving best classification performance on multiple data sets .",abstract,abstract,text-classification,9,6,0.8571428571428571,8,0.0317460317460317,6,0.8571428571428571,1,0
10,We are the first to use translations as domainfree contexts for sentence classification .,abstract,abstract,text-classification,9,7,1.0,9,0.0357142857142857,7,1.0,1,0
11,Introduction,,,text-classification,9,0,0.0,10,0.0396825396825396,0,0.0,1,0
12,"One of the primary tasks in natural language processing ( NLP ) is sentence classification , where given a sentence ( e.g. a sentence of a review ) as input , we are tasked to classify it into one of multiple classes ( e.g. into positive or negative ) .",Introduction,Introduction,text-classification,9,1,0.0135135135135135,11,0.0436507936507936,1,0.024390243902439,1,1
13,"This task is important as it is widely used in almost all subareas of NLP such as sentiment classification for sentiment analysis and question type classification for question answering , to name a few .",Introduction,Introduction,text-classification,9,2,0.027027027027027,12,0.0476190476190476,2,0.048780487804878,1,0
14,"While past methods require feature engineering , recent methods enjoy neural - based methods to automatically encode the sentences into low - dimensional dense vectors .",Introduction,Introduction,text-classification,9,3,0.0405405405405405,13,0.0515873015873015,3,0.073170731707317,1,0
15,"Despite the success of these methods , the major challenge in this task is that extracting features from a single sentence limits the performance .",Introduction,Introduction,text-classification,9,4,0.054054054054054,14,0.0555555555555555,4,0.0975609756097561,1,0
16,"To overcome this limitation , recent works attempted to augment different kinds of features to the sentence , such as the neighboring sentences and the topics of the sentences .",Introduction,Introduction,text-classification,9,5,0.0675675675675675,15,0.0595238095238095,5,0.1219512195121951,1,0
17,"However , these methods used domain - dependent contexts that are only effective when the domain of the task is appropriate .",Introduction,Introduction,text-classification,9,6,0.081081081081081,16,0.0634920634920634,6,0.1463414634146341,1,0
18,"For one thing , neighboring sentences may not be available in some tasks such as question type classification .",Introduction,Introduction,text-classification,9,7,0.0945945945945946,17,0.0674603174603174,7,0.1707317073170731,1,0
19,"Moreover , topics inferred using topic models may produce less useful topics when the data set is domain - specific such as movie review sentiment classification .",Introduction,Introduction,text-classification,9,8,0.1081081081081081,18,0.0714285714285714,8,0.1951219512195122,1,0
20,"In this paper , we propose the usage of translations as compelling and effective domain - free contexts , or contexts that are always available no matter what the task domain is .",Introduction,Introduction,text-classification,9,9,0.1216216216216216,19,0.0753968253968253,9,0.2195121951219512,1,0
21,We observe two opportunities when using translations .,Introduction,Introduction,text-classification,9,10,0.1351351351351351,20,0.0793650793650793,10,0.2439024390243902,1,0
22,"First , each language has its own linguistic and cultural characteristics that may contain different signals to effectively classify a specific class .",Introduction,Introduction,text-classification,9,11,0.1486486486486486,21,0.0833333333333333,11,0.2682926829268293,1,0
23,contrasts the sentence vectors of the original English sentences and their Arabictranslated sentences in the question type classification task .,Introduction,Introduction,text-classification,9,12,0.1621621621621621,22,0.0873015873015873,12,0.2926829268292683,1,0
24,A yellow circle signifies a clear separation of a class .,Introduction,Introduction,text-classification,9,13,0.1756756756756756,23,0.0912698412698412,13,0.3170731707317073,1,0
25,"For example , the green class , or the numeric question type , is circled in the Arabic space as it is clearly separated from other classes , while such separation can not be observed in English .",Introduction,Introduction,text-classification,9,14,0.1891891891891892,24,0.0952380952380952,14,0.3414634146341463,1,0
26,"Meanwhile , location type questions ( in orange ) are better classified in English .",Introduction,Introduction,text-classification,9,15,0.2027027027027027,25,0.0992063492063492,15,0.3658536585365853,1,0
27,"Second , the original sentences may include languagespecific ambiguity , which maybe resolved when presented with its translations .",Introduction,Introduction,text-classification,9,16,0.2162162162162162,26,0.1031746031746031,16,0.3902439024390244,1,0
28,"Consider the example English sentence "" The movie is terribly amazing "" for the sentiment classification task .",Introduction,Introduction,text-classification,9,17,0.2297297297297297,27,0.1071428571428571,17,0.4146341463414634,1,0
29,"In this case , terribly can be used in both positive and negative sense , thus introduces ambiguity in the sentence .",Introduction,Introduction,text-classification,9,18,0.2432432432432432,28,0.1111111111111111,18,0.4390243902439024,1,0
30,"When translated to Korean , it becomes "" ? ? ?? ?? ? ? ? ?? ? ?? ? ? ? ? ? ? ?? ? ?? ?? ? "" which means "" The movie is greatly magnificent "" , removing the ambiguity .",Introduction,Introduction,text-classification,9,19,0.2567567567567567,29,0.115079365079365,19,0.4634146341463415,1,0
31,The above two observations hold only when translations are supported for ( nearly ) arbitrary language pairs with sufficiently high quality .,Introduction,Introduction,text-classification,9,20,0.2702702702702703,30,0.119047619047619,20,0.4878048780487805,1,0
32,"Thankfully , translation services ( e.g. Google Translate )",Introduction,Introduction,text-classification,9,21,0.2837837837837837,31,0.123015873015873,21,0.5121951219512195,1,0
33,"Moreover , recent research on neural machine translation ( NMT ) improved the efficiency and even enabled zero - shot translation of models for languages with no parallel data .",Introduction,Introduction,text-classification,9,22,0.2972972972972973,32,0.1269841269841269,22,0.5365853658536586,1,0
34,"This provides an opportunity to leverage on as many languages as possible to any domain , providing a much wider context compared to the limited contexts provided by past studies .",Introduction,Introduction,text-classification,9,23,0.3108108108108108,33,0.1309523809523809,23,0.5609756097560976,1,0
35,"However , despite the maturity of translation , naively concatenating their vectors to the original sentence vector may introduce more noise than signals .",Introduction,Introduction,text-classification,9,24,0.3243243243243243,34,0.1349206349206349,24,0.5853658536585366,1,0
36,The unaltered translation space on the left of shows an example where translation noises make the two classes indistinguishable .,Introduction,Introduction,text-classification,9,25,0.3378378378378378,35,0.1388888888888889,25,0.6097560975609756,1,0
37,"In this paper , we propose a method to mitigate the possible problems when using translated sentences as context based on the following observations .",Introduction,Introduction,text-classification,9,26,0.3513513513513513,36,0.1428571428571428,26,0.6341463414634146,1,0
38,Suppose there are two translated sentences a and b with slight errors .,Introduction,Introduction,text-classification,9,27,0.3648648648648648,37,0.1468253968253968,27,0.6585365853658537,1,0
39,"We posit that a can be used to fix b when a is used as a context of b , and vice versa",Introduction,Introduction,text-classification,9,28,0.3783783783783784,38,0.1507936507936507,28,0.6829268292682927,1,0
40,1 .,Introduction,Introduction,text-classification,9,29,0.3918918918918919,39,0.1547619047619047,29,0.7073170731707317,1,0
41,"Revisiting the example above , to fix the vector of the English sentence "" The movie is terribly amazing "" , we use the Korean translation to move the vector towards the location where the vector "" The movie is greatly magnificent "" is .",Introduction,Introduction,text-classification,9,30,0.4054054054054054,40,0.1587301587301587,30,0.7317073170731707,1,0
42,"Based on these observations , we present a neural attentionbased multiple context fixing attachment ( MCFA ) .",Introduction,Introduction,text-classification,9,31,0.4189189189189189,41,0.1626984126984127,31,0.7560975609756098,1,1
43,"MCFA is a series of modules that uses all the sentence vectors ( e.g. Arabic , English , Korean , etc. ) as context to fix a sentence vector ( e.g. Korean ) .",Introduction,Introduction,text-classification,9,32,0.4324324324324324,42,0.1666666666666666,32,0.7804878048780488,1,1
44,"Fixing the vectors is done by selectively moving the vectors to a location in the same vector space that better separates the class , as shown in .",Introduction,Introduction,text-classification,9,33,0.4459459459459459,43,0.1706349206349206,33,0.8048780487804879,1,1
45,Noises from translation may cause adverse effects to the vector itself ( e.g. when a noisy vector is directly used for the task ) and relatively to other vectors ( e.g. when a noisy vector is used to fix another noisy vector ) .,Introduction,Introduction,text-classification,9,34,0.4594594594594595,44,0.1746031746031746,34,0.8292682926829268,1,0
46,MCFA computes two sentence usability metrics to control the noise when fixing vectors : ( a ) self usability ? i ( a ) weighs the confidence of using sentence a in solving the task .,Introduction,Introduction,text-classification,9,35,0.4729729729729729,45,0.1785714285714285,35,0.8536585365853658,1,1
47,"( b ) relative usability ? r ( a , b ) weighs the confidence of using sentence a in fixing sentence b.",Introduction,Introduction,text-classification,9,36,0.4864864864864865,46,0.1825396825396825,36,0.8780487804878049,1,1
48,Listed below are the three main strengths of the MCFA attachment .,Introduction,Introduction,text-classification,9,37,0.5,47,0.1865079365079365,37,0.902439024390244,1,0
49,"( 1 ) MCFA is attached after encoding the sentence , which makes it widely adaptable to other models .",Introduction,Introduction,text-classification,9,38,0.5135135135135135,48,0.1904761904761904,38,0.926829268292683,1,0
50,( 2 ) MCFA is extensible and improves the accuracy as the number of translated sentences increases .,Introduction,Introduction,text-classification,9,39,0.527027027027027,49,0.1944444444444444,39,0.951219512195122,1,0
51,"( 3 ) MCFA moves the vectors inside the same space , thus preserves the meaning of vector dimensions .",Introduction,Introduction,text-classification,9,40,0.5405405405405406,50,0.1984126984126984,40,0.975609756097561,1,0
52,"Results show that a convolutional neural network ( CNN ) attached with MCFA significantly improves the classification performance of CNN , achieving state of the 1 Hereon , we mean to "" fix "" as to "" correct , repair , or alter . "" art performance over multiple data sets .",Introduction,Introduction,text-classification,9,41,0.5540540540540541,51,0.2023809523809523,41,1.0,1,0
53,Preliminaries,Introduction,,text-classification,9,42,0.5675675675675675,52,0.2063492063492063,0,0.0,1,0
54,Problem : Translated Sentences as Context,Introduction,,text-classification,9,43,0.581081081081081,53,0.2103174603174603,1,0.03125,1,0
55,"In this paper , the ultimate task that we solve is the sentence classification task where given a sentence and a list of classes , one is task to classify which class ( e.g. positive or negative sentiment ) among the list of classes does the sentence belong .",Introduction,Problem : Translated Sentences as Context,text-classification,9,44,0.5945945945945946,54,0.2142857142857142,2,0.0625,1,0
56,"However , the main challenge that we tackle is the task on how to utilize translated sentences as additional context in order to improve the performance of the classifier .",Introduction,Problem : Translated Sentences as Context,text-classification,9,45,0.6081081081081081,55,0.2182539682539682,3,0.09375,1,0
57,"Specifically , the problem states : given the original sentence s , the goal is to use t 1 , t 2 , ... , tn , or sentences in other languages which are translated from s , as additional context .",Introduction,Problem : Translated Sentences as Context,text-classification,9,46,0.6216216216216216,56,0.2222222222222222,4,0.125,1,0
58,Base Model : Convolutional Neural Network .,Introduction,,text-classification,9,47,0.6351351351351351,57,0.2261904761904762,5,0.15625,1,0
59,The base model used is the convolutional neural network ( CNN ) for sentences .,Introduction,Base Model : Convolutional Neural Network .,text-classification,9,48,0.6486486486486487,58,0.2301587301587301,6,0.1875,1,0
60,It is a simple variation of the original CNN for texts to be used on sentences .,Introduction,Base Model : Convolutional Neural Network .,text-classification,9,49,0.6621621621621622,59,0.2341269841269841,7,0.21875,1,0
61,Let xi ?,Introduction,Base Model : Convolutional Neural Network .,text-classification,9,50,0.6756756756756757,60,0.238095238095238,8,0.25,1,0
62,Rd be the d-dimensional word vector of the i - th word in a sentence of length n .,Introduction,Base Model : Convolutional Neural Network .,text-classification,9,51,0.6891891891891891,61,0.242063492063492,9,0.28125,1,0
63,A convolution operation involves applying a filter matrix W ?,Introduction,Base Model : Convolutional Neural Network .,text-classification,9,52,0.7027027027027027,62,0.246031746031746,10,0.3125,1,0
64,R hd to a window of h words and producing anew feature vector c i using the equation,Introduction,Base Model : Convolutional Neural Network .,text-classification,9,53,0.7162162162162162,63,0.25,11,0.34375,1,0
65,bias vector and f ( . ) is a non-linear function .,Introduction,Base Model : Convolutional Neural Network .,text-classification,9,54,0.7297297297297297,64,0.2539682539682539,12,0.375,1,0
66,"By doing this on all possible windows of words we produce a feature map c = [ c 1 , c 2 , ... ] .",Introduction,Base Model : Convolutional Neural Network .,text-classification,9,55,0.7432432432432432,65,0.2579365079365079,13,0.40625,1,0
67,We then apply a max - over - time pooling operation over the feature map and take the maximum value as the feature vector of the filter .,Introduction,Base Model : Convolutional Neural Network .,text-classification,9,56,0.7567567567567568,66,0.2619047619047619,14,0.4375,1,0
68,We do this on all feature vectors and concatenate all the feature vectors to obtain the final feature vector v.,Introduction,Base Model : Convolutional Neural Network .,text-classification,9,57,0.7702702702702703,67,0.2658730158730158,15,0.46875,1,0
69,We can then use this vector as input features to train a classifier such as logistic regression .,Introduction,Base Model : Convolutional Neural Network .,text-classification,9,58,0.7837837837837838,68,0.2698412698412698,16,0.5,1,0
70,"We use CNN to create sentence vectors for all sentences s , t 1 , t 2 , ... , tn .",Introduction,Base Model : Convolutional Neural Network .,text-classification,9,59,0.7972972972972973,69,0.2738095238095238,17,0.53125,1,0
71,"From hereon , we refer to these vectors as v s , v t1 , v t2 , ... , v tn , respectively .",Introduction,Base Model : Convolutional Neural Network .,text-classification,9,60,0.8108108108108109,70,0.2777777777777778,18,0.5625,1,0
72,We refer to them collectively as V .,Introduction,Base Model : Convolutional Neural Network .,text-classification,9,61,0.8243243243243243,71,0.2817460317460317,19,0.59375,1,0
73,Baseline 1 : Naive Concatenation .,Introduction,Base Model : Convolutional Neural Network .,text-classification,9,62,0.8378378378378378,72,0.2857142857142857,20,0.625,1,0
74,A simple method in order to use the translated sentences as additional context is to naively concatenate their vectors with the vector of the original sentence .,Introduction,Base Model : Convolutional Neural Network .,text-classification,9,63,0.8513513513513513,73,0.2896825396825397,21,0.65625,1,0
75,"That is , we create a wide vectorv = [ v s ; v t1 ; ... ; v tn ] , and use this as the input feature vector of the sentence to the classifier .",Introduction,Base Model : Convolutional Neural Network .,text-classification,9,64,0.8648648648648649,74,0.2936507936507936,22,0.6875,1,0
76,This method works fine if the translated sentences are translated properly .,Introduction,Base Model : Convolutional Neural Network .,text-classification,9,65,0.8783783783783784,75,0.2976190476190476,23,0.71875,1,0
77,"However , sentences translated using machine translation models usually contain incorrect translation .",Introduction,Base Model : Convolutional Neural Network .,text-classification,9,66,0.8918918918918919,76,0.3015873015873015,24,0.75,1,0
78,"In effect , this method will have adverse effects on the overall performance of the classifier .",Introduction,Base Model : Convolutional Neural Network .,text-classification,9,67,0.9054054054054054,77,0.3055555555555556,25,0.78125,1,0
79,This will especially be very evident if the number of additional sentences increases .,Introduction,Base Model : Convolutional Neural Network .,text-classification,9,68,0.918918918918919,78,0.3095238095238095,26,0.8125,1,0
80,Baseline 2 : L2 Regularization .,Introduction,Base Model : Convolutional Neural Network .,text-classification,9,69,0.9324324324324323,79,0.3134920634920635,27,0.84375,1,0
81,"In order to alleviate the problems above , we can use L2 regularization to automatically select useful features by weakening the appropriate weights .",Introduction,Base Model : Convolutional Neural Network .,text-classification,9,70,0.945945945945946,80,0.3174603174603174,28,0.875,1,0
82,The main problem of this method occurs when almost all of the weights coming from the vectors of the translated sentence are weakened .,Introduction,Base Model : Convolutional Neural Network .,text-classification,9,71,0.9594594594594594,81,0.3214285714285714,29,0.90625,1,0
83,This leads to making the additional context vectors useless and to having a similar performance when there are no additional context .,Introduction,Base Model : Convolutional Neural Network .,text-classification,9,72,0.972972972972973,82,0.3253968253968254,30,0.9375,1,0
84,"Ultimately , this method does not make use of the full potential of the additional context .",Introduction,Base Model : Convolutional Neural Network .,text-classification,9,73,0.9864864864864864,83,0.3293650793650793,31,0.96875,1,0
85,usability usability ( a ) Self and relative usability modules,Introduction,Base Model : Convolutional Neural Network .,text-classification,9,74,1.0,84,0.3333333333333333,32,1.0,1,0
86,Model,,,text-classification,9,0,0.0,85,0.3373015873015873,0,0.0,1,0
87,"To solve the problems of the baselines discussed above , we introduce an attention - based neural multiple context fixing attachment ( MCFA ) 2 , a series of modules attached to the sentence vectors V .",Model,Model,text-classification,9,1,0.0192307692307692,86,0.3412698412698413,1,0.2,1,0
88,"MCFA attachment is used to fix the sentence vectors , by slightly modifying the per-dimension values of the vector , before concatenating them into the final feature vector .",Model,Model,text-classification,9,2,0.0384615384615384,87,0.3452380952380952,2,0.4,1,0
89,"The sentence vectors are altered using other sentence vectors as context ( e.g. v t 1 is altered using v s , v t2 , ... , v tn ) .",Model,Model,text-classification,9,3,0.0576923076923076,88,0.3492063492063492,3,0.6,1,0
90,This results to moving the vectors in the same vector space .,Model,Model,text-classification,9,4,0.0769230769230769,89,0.3531746031746032,4,0.8,1,0
91,The full architecture is shown in .,Model,Model,text-classification,9,5,0.0961538461538461,90,0.3571428571428571,5,1.0,1,0
92,Self Usability Module,Model,,text-classification,9,6,0.1153846153846153,91,0.3611111111111111,0,0.0,1,0
93,"To fix a source sentence vector 3 , we use the other sentence vectors as guide to know which dimensions to fix and to what extent do we need to fix them .",Model,Self Usability Module,text-classification,9,7,0.1346153846153846,92,0.365079365079365,1,0.0357142857142857,1,0
94,"However , other vectors might also contain errors which may reflect to the fixing of the source sentence vector .",Model,Self Usability Module,text-classification,9,8,0.1538461538461538,93,0.369047619047619,2,0.0714285714285714,1,0
95,"In order to cope with this , we introduce self usability modules .",Model,Self Usability Module,text-classification,9,9,0.173076923076923,94,0.373015873015873,3,0.1071428571428571,1,0
96,"A self usability module contains the self usability of the vector ? i ( a ) , which measures how confident sentence a is for the task at hand .",Model,Self Usability Module,text-classification,9,10,0.1923076923076923,95,0.376984126984127,4,0.1428571428571428,1,0
97,"For example , an ambiguous sentence ( e.g. "" The movie is terribly amazing "" ) may receive a low self usability , while a clear and definite sentence ( e.g. "" The movie is very good "" ) may receive a high self usability .",Model,Self Usability Module,text-classification,9,11,0.2115384615384615,96,0.3809523809523809,5,0.1785714285714285,1,0
98,"Mathematically , we calculate the self usability of the vector vi of sentence i , denoted as ? i ( v i ) , using the equation",Model,Self Usability Module,text-classification,9,12,0.2307692307692307,97,0.3849206349206349,6,0.2142857142857142,1,0
99,is a matrix to be learned .,Model,Self Usability Module,text-classification,9,13,0.25,98,0.3888888888888889,7,0.25,1,0
100,The produced value is a single real number from 0 to 1 .,Model,Self Usability Module,text-classification,9,14,0.2692307692307692,99,0.3928571428571428,8,0.2857142857142857,1,0
101,We pre-calculate the self usability of all sentence vectors vi ?,Model,Self Usability Module,text-classification,9,15,0.2884615384615384,100,0.3968253968253968,9,0.3214285714285714,1,0
102,V .,Model,,text-classification,9,16,0.3076923076923077,101,0.4007936507936508,10,0.3571428571428571,1,0
103,"These are used in the next module , the relative usability module .",Model,V .,text-classification,9,17,0.3269230769230769,102,0.4047619047619047,11,0.3928571428571428,1,0
104,Relative Usability,Model,,text-classification,9,18,0.3461538461538461,103,0.4087301587301587,12,0.4285714285714285,1,0
105,Module,Model,,text-classification,9,19,0.3653846153846153,104,0.4126984126984127,13,0.4642857142857143,1,0
106,"Relative usability ? r ( a , b ) measures how useful a can be when fixing b , relative to other sentences .",Model,Module,text-classification,9,20,0.3846153846153846,105,0.4166666666666667,14,0.5,1,0
107,"There are two main differences between ? i ( a ) and ? r ( a , b ) .",Model,Module,text-classification,9,21,0.4038461538461538,106,0.4206349206349206,15,0.5357142857142857,1,0
108,"First , ? i ( a ) is calculated before a knows about b while ? r ( a , b ) is calculated when a knows about b.",Model,Module,text-classification,9,22,0.4230769230769231,107,0.4246031746031746,16,0.5714285714285714,1,0
109,"Second , ? r ( a , b ) can below even though ? i ( a ) is not .",Model,Module,text-classification,9,23,0.4423076923076923,108,0.4285714285714285,17,0.6071428571428571,1,0
110,This means that a is notable to help in fixing the wrong information in b .,Model,Module,text-classification,9,24,0.4615384615384615,109,0.4325396825396825,18,0.6428571428571429,1,0
111,"Here , we extend the additive attention module and use it as a method to calculate the relative usability of two sentences of different languages .",Model,Module,text-classification,9,25,0.4807692307692308,110,0.4365079365079365,19,0.6785714285714286,1,0
112,"To better visualize the original attention mechanism , we present the equations below .",Model,Module,text-classification,9,26,0.5,111,0.4404761904761904,20,0.7142857142857143,1,0
113,One major challenge in using the attention mechanism in our problem is that the sentence vectors do not belong to the same vector space .,Model,Module,text-classification,9,27,0.5192307692307693,112,0.4444444444444444,21,0.75,1,0
114,"Moreover , one characteristic of our problem is that the sentence vectors can be both a source and a context vector ( e.g. v scan be both sand ti in Equation 1 ) .",Model,Module,text-classification,9,28,0.5384615384615384,113,0.4484126984126984,22,0.7857142857142857,1,0
115,"Because of these , we can not directly use the additive attention module .",Model,Module,text-classification,9,29,0.5576923076923077,114,0.4523809523809524,23,0.8214285714285714,1,0
116,We extend the module such that ( 1 ) each sentence vector v k has its own projection matrix X k ?,Model,Module,text-classification,9,30,0.5769230769230769,115,0.4563492063492063,24,0.8571428571428571,1,0
117,"R dd , and ( 2 ) each projection matrix X k can be used as projection matrix of both the source ( e.g. when sentence k is the current source ) and the context vectors .",Model,Module,text-classification,9,31,0.5961538461538461,116,0.4603174603174603,25,0.8928571428571429,1,0
118,"Finally , we incorporate the self usability function ?",Model,Module,text-classification,9,32,0.6153846153846154,117,0.4642857142857143,26,0.9285714285714286,1,0
119,i ( v k ) to reflect the self usability of a sentence .,Model,Module,text-classification,9,33,0.6346153846153846,118,0.4682539682539682,27,0.9642857142857144,1,0
120,"Ultimately , the relative usability denoted as ? r ( v i , v j ) is calculated using the equations below , where is the multiplication of a vector and a scalar through broadcasting .",Model,Module,text-classification,9,34,0.6538461538461539,119,0.4722222222222222,28,1.0,1,0
121,Vector Fixing Module,Model,,text-classification,9,35,0.6730769230769231,120,0.4761904761904761,0,0.0,1,0
122,The vector fixing module applies the attention weights to the sentence vectors and creates an integrated context vector .,Model,Vector Fixing Module,text-classification,9,36,0.6923076923076923,121,0.4801587301587302,1,0.0588235294117647,1,0
123,We then use this vector alongside with the source sentence vector to create a weighted gate vector .,Model,Vector Fixing Module,text-classification,9,37,0.7115384615384616,122,0.4841269841269841,2,0.1176470588235294,1,0
124,The weighted gate vector is used to determine to what extent should a dimension of the source sentence vector be altered .,Model,Vector Fixing Module,text-classification,9,38,0.7307692307692307,123,0.4880952380952381,3,0.1764705882352941,1,0
125,The common way to apply the attention weights to the context vectors and create an integrated context vector c i is to directly do weighted sum of all the context vectors .,Model,Vector Fixing Module,text-classification,9,39,0.75,124,0.492063492063492,4,0.2352941176470588,1,0
126,"However , this is not possible because the context vectors are not on the same space .",Model,Vector Fixing Module,text-classification,9,40,0.7692307692307693,125,0.496031746031746,5,0.2941176470588235,1,0
127,"Thus , we use a projection matrix U k ?",Model,Vector Fixing Module,text-classification,9,41,0.7884615384615384,126,0.5,6,0.3529411764705882,1,0
128,R dd to linearly project the sentence vector v k to transform the sentence vectors into a common vector space .,Model,Vector Fixing Module,text-classification,9,42,0.8076923076923077,127,0.503968253968254,7,0.4117647058823529,1,0
129,The integrated context vector c i is then calculated as,Model,Vector Fixing Module,text-classification,9,43,0.8269230769230769,128,0.5079365079365079,8,0.4705882352941176,1,0
130,"Finally , we construct a weighted gate vector wk and use it to fix the source sentence vectors using the equations below , where V k ?",Model,Vector Fixing Module,text-classification,9,44,0.8461538461538461,129,0.5119047619047619,9,0.5294117647058824,1,0
131,R 2dd is a trainable parameter and ?,Model,Vector Fixing Module,text-classification,9,45,0.8653846153846154,130,0.5158730158730159,10,0.5882352941176471,1,0
132,is the element - wise multiplication procedure .,Model,Vector Fixing Module,text-classification,9,46,0.8846153846153846,131,0.5198412698412699,11,0.6470588235294118,1,0
133,The weighted gate vector is a vector of real numbers between 0 and 1 to modify the intensity of per-dimension values of the sentence vector .,Model,Vector Fixing Module,text-classification,9,47,0.903846153846154,132,0.5238095238095238,12,0.7058823529411765,1,0
134,This causes the vector to move in the same vector space towards the correct direction .,Model,Vector Fixing Module,text-classification,9,48,0.9230769230769232,133,0.5277777777777778,13,0.7647058823529411,1,0
135,"An alternative approach to do vector correction is using a residual - style correction , where instead of multiplying agate vector , a residual vector is added to the original vector .",Model,Vector Fixing Module,text-classification,9,49,0.9423076923076924,134,0.5317460317460317,14,0.8235294117647058,1,0
136,"However , this approach makes the correction not interpretable ; it is hard to explain what does adding a value to a specific dimension mean .",Model,Vector Fixing Module,text-classification,9,50,0.9615384615384616,135,0.5357142857142857,15,0.8823529411764706,1,0
137,One major advantage of MCFA is that the corrections in the vectors are interpretable ; the weights in the gate vector correspond to the importance of the per-dimension features of the vector .,Model,Vector Fixing Module,text-classification,9,51,0.9807692307692308,136,0.5396825396825397,16,0.9411764705882352,1,0
138,"The altered vector ? v s , ... , v tn are then concatenated and fed directly as an input vector to the logistic regression classifier for training .",Model,Vector Fixing Module,text-classification,9,52,1.0,137,0.5436507936507936,17,1.0,1,0
139,Experiments,,,text-classification,9,0,0.0,138,0.5476190476190477,0,0.0,1,0
140,Experimental Setting,,,text-classification,9,0,0.0,139,0.5515873015873016,0,0.0,1,0
141,We test our model on four different data sets as listed below and summarized in .,Experimental Setting,Experimental Setting,text-classification,9,1,0.0384615384615384,140,0.5555555555555556,1,0.0384615384615384,1,0
142,( a ) MR 4 : Movie reviews data where the task is to classify whether the review sentence has positive or negative sentiment .,Experimental Setting,Experimental Setting,text-classification,9,2,0.0769230769230769,141,0.5595238095238095,2,0.0769230769230769,1,0
143,( b ) SUBJ : Subjectivity data where the task is to classify whether the sentence is subjective or objective .,Experimental Setting,Experimental Setting,text-classification,9,3,0.1153846153846153,142,0.5634920634920635,3,0.1153846153846153,1,0
144,( c ) CR 5 : Customer reviews where,Experimental Setting,Experimental Setting,text-classification,9,4,0.1538461538461538,143,0.5674603174603174,4,0.1538461538461538,1,0
145,The task is to classify whether the review sentence is positive or negative .,Experimental Setting,Experimental Setting,text-classification,9,5,0.1923076923076923,144,0.5714285714285714,5,0.1923076923076923,1,0
146,( d ) TREC 6 : TREC question data set the task is to classify the type of question .,Experimental Setting,Experimental Setting,text-classification,9,6,0.2307692307692307,145,0.5753968253968254,6,0.2307692307692307,1,0
147,All our data sets are in English .,Experimental Setting,Experimental Setting,text-classification,9,7,0.2692307692307692,146,0.5793650793650794,7,0.2692307692307692,1,0
148,"For the additional contexts , we use ten other languages , selected based on their diversity and their performance on prior experiments : Arabic , Finnish , French , Italian , Korean , Mongolian , Norwegian , Polish , Russian , and Ukranian .",Experimental Setting,Experimental Setting,text-classification,9,8,0.3076923076923077,147,0.5833333333333334,8,0.3076923076923077,1,0
149,We translate the data sets using Google Translate .,Experimental Setting,Experimental Setting,text-classification,9,9,0.3461538461538461,148,0.5873015873015873,9,0.3461538461538461,1,0
150,Tokenization is done using the polyglot library 7 .,Experimental Setting,Experimental Setting,text-classification,9,10,0.3846153846153846,149,0.5912698412698413,10,0.3846153846153846,1,0
151,We experiment on using only one additional context ( N = 1 ) and using all ten languages at once ( N = 10 ) .,Experimental Setting,Experimental Setting,text-classification,9,11,0.4230769230769231,150,0.5952380952380952,11,0.4230769230769231,1,0
152,"For N = 1 , we only show the accuracy of the best classifier for conciseness .",Experimental Setting,Experimental Setting,text-classification,9,12,0.4615384615384615,151,0.5992063492063492,12,0.4615384615384615,1,0
153,"For our CNN , we use rectified linear units and three filters with different window sizes h = 3 , 4 , 5 with 100 feature maps each , following .",Experimental Setting,Experimental Setting,text-classification,9,13,0.5,152,0.6031746031746031,13,0.5,1,1
154,"For the final sentence vector , we concatenate the feature maps to get a 300 - dimension vector .",Experimental Setting,Experimental Setting,text-classification,9,14,0.5384615384615384,153,0.6071428571428571,14,0.5384615384615384,1,1
155,We use dropout on all nonlinear connections with a dropout rate of 0.5 .,Experimental Setting,Experimental Setting,text-classification,9,15,0.5769230769230769,154,0.6111111111111112,15,0.5769230769230769,1,1
156,"We also use an l 2 constraint of 3 , following for accurate comparisons .",Experimental Setting,Experimental Setting,text-classification,9,16,0.6153846153846154,155,0.6150793650793651,16,0.6153846153846154,1,0
157,We use FastText pre-trained vectors 8 for all our data sets and their corresponding additional context .,Experimental Setting,Experimental Setting,text-classification,9,17,0.6538461538461539,156,0.6190476190476191,17,0.6538461538461539,1,0
158,"During training , we use mini-batch size of 50 .",Experimental Setting,Experimental Setting,text-classification,9,18,0.6923076923076923,157,0.623015873015873,18,0.6923076923076923,1,1
159,Training is done via stochastic gradient descent over shuffled mini-batches with the Adadelta update rule .,Experimental Setting,Experimental Setting,text-classification,9,19,0.7307692307692307,158,0.626984126984127,19,0.7307692307692307,1,1
160,We perform early stopping using a random 10 % of the training set as the development set .,Experimental Setting,Experimental Setting,text-classification,9,20,0.7692307692307693,159,0.6309523809523809,20,0.7692307692307693,1,1
161,"We present several competing models , listed below to compare the performance of our model .",Experimental Setting,Experimental Setting,text-classification,9,21,0.8076923076923077,160,0.6349206349206349,21,0.8076923076923077,1,0
162,uses topics as additional contexts and changes the CNN architecture .,Experimental Setting,Experimental Setting,text-classification,9,22,0.8461538461538461,161,0.6388888888888888,22,0.8461538461538461,1,0
163,TopCNN uses two types of topics : word- specific topic and sentence - specific topic ; and ( D ) CNN+ B1 and CNN +,Experimental Setting,Experimental Setting,text-classification,9,23,0.8846153846153846,162,0.6428571428571429,23,0.8846153846153846,1,0
164,B2 are the two baselines presented in this paper .,Experimental Setting,Experimental Setting,text-classification,9,24,0.9230769230769232,163,0.6468253968253969,24,0.9230769230769232,1,0
165,We do not show results from RNN models because they were shown to be less effective in sentence classification in our prior experiments .,Experimental Setting,Experimental Setting,text-classification,9,25,0.9615384615384616,164,0.6507936507936508,25,0.9615384615384616,1,0
166,"For models with additional context , we further use an ensemble classification model using a commonly used method by averaging the class probability scores generated by the multiple variants ( in our model 's case , N = 1 and N = 10 models ) , following .",Experimental Setting,Experimental Setting,text-classification,9,26,1.0,165,0.6547619047619048,26,1.0,1,0
167,Results and Discussion,,,text-classification,9,0,0.0,166,0.6587301587301587,0,0.0,1,0
168,We report the classification accuracy of the competing models in .,Results and Discussion,Results and Discussion,text-classification,9,1,0.0416666666666666,167,0.6626984126984127,1,0.0416666666666666,1,0
169,We show that CNN + MCFA achieves state of the art performance on three of the four data sets and performs competitively on one data set .,Results and Discussion,Results and Discussion,text-classification,9,2,0.0833333333333333,168,0.6666666666666666,2,0.0833333333333333,1,1
170,"When N = 1 , MCFA increases the performance of a normal CNN from 85.0 to 87.6 , beating the current state of the art on the CR data set .",Results and Discussion,Results and Discussion,text-classification,9,3,0.125,169,0.6706349206349206,3,0.125,1,1
171,"When N = 10 , MCFA additionally beats the state of the art on the TREC data set .",Results and Discussion,Results and Discussion,text-classification,9,4,0.1666666666666666,170,0.6746031746031746,4,0.1666666666666666,1,1
172,"Finally , our ensemble classifier additionally outperforms all competing models on the MR data set .",Results and Discussion,Results and Discussion,text-classification,9,5,0.2083333333333333,171,0.6785714285714286,5,0.2083333333333333,1,1
173,"We emphasize that we only use the basic CNN as our sentence encoder for our experiments , yet still achieve state of the art performance :",Results and Discussion,Results and Discussion,text-classification,9,6,0.25,172,0.6825396825396826,6,0.25,1,0
174,Classification accuracies of competing models .,Results and Discussion,Results and Discussion,text-classification,9,7,0.2916666666666667,173,0.6865079365079365,7,0.2916666666666667,1,0
175,"C refers to the additional context , N refers to the number of translations .",Results and Discussion,Results and Discussion,text-classification,9,8,0.3333333333333333,174,0.6904761904761905,8,0.3333333333333333,1,0
176,"In TopCNN , word refers to using word - specific topic while sentence refers to using sentence - specific topic .",Results and Discussion,Results and Discussion,text-classification,9,9,0.375,175,0.6944444444444444,9,0.375,1,0
177,Accuracies colored red are accuracies that perform worse than CNN .,Results and Discussion,Results and Discussion,text-classification,9,10,0.4166666666666667,176,0.6984126984126984,10,0.4166666666666667,1,0
178,Previous state of the art results and the results of our best model are bold - faced .,Results and Discussion,Results and Discussion,text-classification,9,11,0.4583333333333333,177,0.7023809523809523,11,0.4583333333333333,1,0
179,The winning result is underlined .,Results and Discussion,Results and Discussion,text-classification,9,12,0.5,178,0.7063492063492064,12,0.5,1,0
180,"The number inside the parenthesis indicates the increase from the base model , CNN . on most data sets .",Results and Discussion,Results and Discussion,text-classification,9,13,0.5416666666666666,179,0.7103174603174603,13,0.5416666666666666,1,0
181,"Hence , MCFA is successful in effectively using translations as additional context to improve the performance of the classifier .",Results and Discussion,Results and Discussion,text-classification,9,14,0.5833333333333334,180,0.7142857142857143,14,0.5833333333333334,1,0
182,"We compare our model ( CNN + MCFA ) and the baselines discussed above ( CNN + B1 , CNN + B2 ) .",Results and Discussion,Results and Discussion,text-classification,9,15,0.625,181,0.7182539682539683,15,0.625,1,0
183,"On all settings , our model outperforms the baselines .",Results and Discussion,Results and Discussion,text-classification,9,16,0.6666666666666666,182,0.7222222222222222,16,0.6666666666666666,1,0
184,"When N = 10 , the performance of our model increases over the performance when N = 1 , however the performance of CNN + B1 decreases when compared to the performance when N = 1 .",Results and Discussion,Results and Discussion,text-classification,9,17,0.7083333333333334,183,0.7261904761904762,17,0.7083333333333334,1,0
185,We also show the accuracies of the worst classifiers when N = 1 in .,Results and Discussion,Results and Discussion,text-classification,9,18,0.75,184,0.7301587301587301,18,0.75,1,0
186,"On all data sets except SUBJ , the accuracy of CNN + B1 decreases from the base CNN accuracy , while the accuracy of our model always improves from the base CNN accuracy .",Results and Discussion,Results and Discussion,text-classification,9,19,0.7916666666666666,185,0.7341269841269841,19,0.7916666666666666,1,0
187,"This is resolved by CNN + B2 by applying L2 regularization , however the increase in performance is marginal .",Results and Discussion,Results and Discussion,text-classification,9,20,0.8333333333333334,186,0.7380952380952381,20,0.8333333333333334,1,0
188,"We also compare two different kinds of additional context : topics ( TopCNN ) and translations ( CNN + B1 , CNN + B2 , CNN + MCFA ) .",Results and Discussion,Results and Discussion,text-classification,9,21,0.875,187,0.7420634920634921,21,0.875,1,0
189,"Overall , we conclude that translations are better additional contexts than topics .",Results and Discussion,Results and Discussion,text-classification,9,22,0.9166666666666666,188,0.746031746031746,22,0.9166666666666666,1,0
190,"When using a single context ( i.e. TopCNN word , TopCNN sent , and our models when N = 1 ) , translations always outperform topics even when using the baseline methods .",Results and Discussion,Results and Discussion,text-classification,9,23,0.9583333333333334,189,0.75,23,0.9583333333333334,1,0
191,"Using topics as additional context also decreases the performance of the CNN classifier on most data sets , giving an adverse effect to the CNN classifier .",Results and Discussion,Results and Discussion,text-classification,9,24,1.0,190,0.753968253968254,24,1.0,1,0
192,Model Interpretation,,,text-classification,9,0,0.0,191,0.7579365079365079,0,0.0,1,0
193,We first provide examples shown in on how the self usability module determines the score of sentences .,Model Interpretation,Model Interpretation,text-classification,9,1,0.024390243902439,192,0.7619047619047619,1,0.024390243902439,1,0
194,"In the first example , it is hard to classify whether the translated sentence is positive or negative , thus it is given a low self usability score .",Model Interpretation,Model Interpretation,text-classification,9,2,0.048780487804878,193,0.7658730158730159,2,0.048780487804878,1,0
195,"In the second example , although the sentence contains mistranslations , these are minimal and may actually help the classifier by telling it that thirst for violence is not a attention ( negative sentence ) the mothman prophecies , which is mostly a bore , seems to exist only for its climactic setpiece . negative phrase .",Model Interpretation,Model Interpretation,text-classification,9,3,0.073170731707317,194,0.7698412698412699,3,0.073170731707317,1,0
196,"Thus , it is given a high self usability score .",Model Interpretation,Model Interpretation,text-classification,9,4,0.0975609756097561,195,0.7738095238095238,4,0.0975609756097561,1,0
197,shows two data instance examples where we show the attention weights given to the other contexts when fixing a Korean sentence .,Model Interpretation,Model Interpretation,text-classification,9,5,0.1219512195121951,196,0.7777777777777778,5,0.1219512195121951,1,0
198,"The larger the attention weight is , the more the context is used to fix the Korean sentence .",Model Interpretation,Model Interpretation,text-classification,9,6,0.1463414634146341,197,0.7817460317460317,6,0.1463414634146341,1,0
199,In the Original sentence : skip this turd and pick your nose instead because you 're sure to get more out of the latter experience .,Model Interpretation,Model Interpretation,text-classification,9,7,0.1707317073170731,198,0.7857142857142857,7,0.1707317073170731,1,0
200,Korean translation :,Model Interpretation,Model Interpretation,text-classification,9,8,0.1951219512195122,199,0.7896825396825397,8,0.1951219512195122,1,0
201,Human re-translation :,Model Interpretation,Model Interpretation,text-classification,9,9,0.2195121951219512,200,0.7936507936507936,9,0.2195121951219512,1,0
202,"In order to get more from the latter experience , you need to skip this puddle and choose your nose .",Model Interpretation,Model Interpretation,text-classification,9,10,0.2439024390243902,201,0.7976190476190477,10,0.2439024390243902,1,0
203,Self,Model Interpretation,,text-classification,9,11,0.2682926829268293,202,0.8015873015873016,11,0.2682926829268293,1,0
204,Usability : 0.3958 ( a ) Low self usability example Original sentence : michael moore 's latest documentary about america 's thirst for violence is his best film yet . . .,Model Interpretation,Self,text-classification,9,12,0.2926829268292683,203,0.8055555555555556,12,0.2926829268292683,1,0
205,Korean translation :,Model Interpretation,Self,text-classification,9,13,0.3170731707317073,204,0.8095238095238095,13,0.3170731707317073,1,0
206,Human re-translation :,Model Interpretation,Self,text-classification,9,14,0.3414634146341463,205,0.8134920634920635,14,0.3414634146341463,1,0
207,"Michael Moore 's latest American documentary "" Violent Scene "" is his best film yet . . .",Model Interpretation,Self,text-classification,9,15,0.3658536585365853,206,0.8174603174603174,15,0.3658536585365853,1,0
208,Self,Model Interpretation,,text-classification,9,16,0.3902439024390244,207,0.8214285714285714,16,0.3902439024390244,1,0
209,Usability : 1.0000 ( b ) High self usability example you know that ten bucks you 'd spend on a ticket ?,Model Interpretation,Self,text-classification,9,17,0.4146341463414634,208,0.8253968253968254,17,0.4146341463414634,1,0
210,just send it to cranky .,Model Interpretation,Self,text-classification,9,18,0.4390243902439024,209,0.8293650793650794,18,0.4390243902439024,1,0
211,we do n't get paid enough to sit through crap like this .,Model Interpretation,Self,text-classification,9,19,0.4634146341463415,210,0.8333333333333334,19,0.4634146341463415,1,0
212,NN ( altered ),Model Interpretation,Self,text-classification,9,20,0.4878048780487805,211,0.8373015873015873,20,0.4878048780487805,1,0
213,"after scenes of nonsense , you 'll be wistful for the testosteronecharged wizardry of jerry bruckheimer productions , especially because half past dead is like the rock on walmart budget . :",Model Interpretation,Self,text-classification,9,21,0.5121951219512195,212,0.8412698412698413,21,0.5121951219512195,1,0
214,"Two example sentences , from English ( first ) and Korean ( second ) vector spaces , and their nearest neighbors ( NN ) on both the unaltered and altered vector spaces .",Model Interpretation,Self,text-classification,9,22,0.5365853658536586,213,0.8452380952380952,22,0.5365853658536586,1,0
215,We only show the original English sentences for the Korean example for conciseness .,Model Interpretation,Self,text-classification,9,23,0.5609756097560976,214,0.8492063492063492,23,0.5609756097560976,1,0
216,"first example , the Korean sentence contains translation errors ; especially , the words bore and climactic setpiece were not translated and were only spelled using the Korean alphabet .",Model Interpretation,Self,text-classification,9,24,0.5853658536585366,215,0.8531746031746031,24,0.5853658536585366,1,0
217,"In this example , the English attention weight is larger than the Korean attention weight .",Model Interpretation,Self,text-classification,9,25,0.6097560975609756,216,0.8571428571428571,25,0.6097560975609756,1,0
218,"In the second example , the Korean sentence correctly translates all parts of the English sentence , except for the phrase as it does in trouble .",Model Interpretation,Self,text-classification,9,26,0.6341463414634146,217,0.8611111111111112,26,0.6341463414634146,1,0
219,"However , this phrase is not necessary to classify the sentence correctly , and may induce possible vagueness because of the word trouble .",Model Interpretation,Self,text-classification,9,27,0.6585365853658537,218,0.8650793650793651,27,0.6585365853658537,1,0
220,"Thus , the Korean attention weight is larger .",Model Interpretation,Self,text-classification,9,28,0.6829268292682927,219,0.8690476190476191,28,0.6829268292682927,1,0
221,shows the PCA visualization of the unaltered and the altered vectors of four different languages .,Model Interpretation,Self,text-classification,9,29,0.7073170731707317,220,0.873015873015873,29,0.7073170731707317,1,0
222,"In the first example , the unaltered sentence vectors are mostly in the middle of the vector space , making it hard to draw a boundary between the two examples .",Model Interpretation,Self,text-classification,9,30,0.7317073170731707,221,0.876984126984127,30,0.7317073170731707,1,0
223,"After the fixing , the boundary is much clearer .",Model Interpretation,Self,text-classification,9,31,0.7560975609756098,222,0.8809523809523809,31,0.7560975609756098,1,0
224,We also show the English sentence vectors in the second example .,Model Interpretation,Self,text-classification,9,32,0.7804878048780488,223,0.8849206349206349,32,0.7804878048780488,1,0
225,"Even without fixing the unaltered English sentence vectors , it is easy to distinguish both classes .",Model Interpretation,Self,text-classification,9,33,0.8048780487804879,224,0.8888888888888888,33,0.8048780487804879,1,0
226,"After the fix , the sentence vectors in the middle of the space are moved , making the distinction more obvious and clearer .",Model Interpretation,Self,text-classification,9,34,0.8292682926829268,225,0.8928571428571429,34,0.8292682926829268,1,0
227,We also provide quantitative evidence by showing that the Mahalanobis distance between the two classes in the altered vectors are significantly farther than that of the unaltered vectors .,Model Interpretation,Self,text-classification,9,35,0.8536585365853658,226,0.8968253968253969,35,0.8536585365853658,1,0
228,We also show two examples sentences from English and Korean vector spaces and their corresponding nearest neighbors on both the unaltered and altered vector spaces in Table 5 .,Model Interpretation,Self,text-classification,9,36,0.8780487804878049,227,0.9007936507936508,36,0.8780487804878049,1,0
229,"In the first example , the unaltered vector focuses on the meaning of "" wasted yours "" in the sentence , which puts it near sentences regarding wasted time or money .",Model Interpretation,Self,text-classification,9,37,0.902439024390244,228,0.9047619047619048,37,0.902439024390244,1,0
230,"After fixing , the sentence vector focuses its meaning on the slow yet worth - the - wait pace of the movie , thus moving it closer to the correct vectors .",Model Interpretation,Self,text-classification,9,38,0.926829268292683,229,0.9087301587301588,38,0.926829268292683,1,0
231,"In the second example , all three sentences have highly descriptive tones , however , the nearest neighbor on the altered space is hyperbolically negative , comparing the movie to a description unrelated to the movie itself .",Model Interpretation,Self,text-classification,9,39,0.951219512195122,230,0.9126984126984128,39,0.951219512195122,1,0
232,NN ( Unaltered ),Model Interpretation,Self,text-classification,9,40,0.975609756097561,231,0.9166666666666666,40,0.975609756097561,1,0
233,"in the new release of cinema paradiso , the tale has turned from sweet to bittersweet , and when the tears come during that final , beautiful scene , they finally feel absolutely earned .",Model Interpretation,Self,text-classification,9,41,1.0,232,0.9206349206349206,41,1.0,1,0
234,Related Work,,,text-classification,9,0,0.0,233,0.9246031746031746,0,0.0,1,0
235,One way to improve the performance of a sentence classifier is to introduce new context .,Related Work,Related Work,text-classification,9,1,0.0769230769230769,234,0.9285714285714286,1,0.0769230769230769,0,0
236,"Common and obvious kinds of context are the neighboring sentences of the sentence , and the document where the sentence belongs .",Related Work,Related Work,text-classification,9,2,0.1538461538461538,235,0.9325396825396826,2,0.1538461538461538,0,0
237,Topics of the words in the sentence induced by a topic model were also used as contexts .,Related Work,Related Work,text-classification,9,3,0.2307692307692307,236,0.9365079365079364,3,0.2307692307692307,0,0
238,"In this paper , we introduce yet another type of additional context , sentence translations , which to the best of our knowledge have not been used previously .",Related Work,Related Work,text-classification,9,4,0.3076923076923077,237,0.9404761904761904,4,0.3076923076923077,0,0
239,Sentence encoders trained from neural machine translation ( NMT ) systems were also used for transfer learning .,Related Work,Related Work,text-classification,9,5,0.3846153846153846,238,0.9444444444444444,5,0.3846153846153846,0,0
240,demonstrated that altered - length sentence vectors from NMT encoders outperform sentence vectors from monolingual encoders on semantic similarity tasks .,Related Work,Related Work,text-classification,9,6,0.4615384615384615,239,0.9484126984126984,6,0.4615384615384615,0,0
241,Recent work used representation of each word in the sentence to create a sentence representation suitable for multiple NLP tasks .,Related Work,Related Work,text-classification,9,7,0.5384615384615384,240,0.9523809523809524,7,0.5384615384615384,0,0
242,"Our work shares the commonality of using NMT for another task , but instead of using NMT to encode our sentences , we use it to translate the sentences into new contexts .",Related Work,Related Work,text-classification,9,8,0.6153846153846154,241,0.9563492063492064,8,0.6153846153846154,0,0
243,Increasing the number of data instances of the training set has also been explored to improve the performance of a classifier .,Related Work,Related Work,text-classification,9,9,0.6923076923076923,242,0.9603174603174603,9,0.6923076923076923,0,0
244,"Recent methods include the usage of thesaurus , paraphrases , among others .",Related Work,Related Work,text-classification,9,10,0.7692307692307693,243,0.9642857142857144,10,0.7692307692307693,0,0
245,These simple variation techniques are preferred because they are found to be very effective despite their simplicity .,Related Work,Related Work,text-classification,9,11,0.8461538461538461,244,0.9682539682539684,11,0.8461538461538461,0,0
246,"Our work similarly augments training data , not by adding data instances ( vertical augmentation ) , but rather by adding more context ( horizontal augmentation ) .",Related Work,Related Work,text-classification,9,12,0.9230769230769232,245,0.9722222222222222,12,0.9230769230769232,0,0
247,"Though the paraphrase of p can be alternatively used as an augmented context , this could not leverage the added semantics coming from another language , as discussed in Section 1 .",Related Work,Related Work,text-classification,9,13,1.0,246,0.9761904761904762,13,1.0,0,0
248,Conclusion,,,text-classification,9,0,0.0,247,0.98015873015873,0,0.0,1,0
249,This paper investigates the use of translations as better additional contexts for sentence classification .,Conclusion,Conclusion,text-classification,9,1,0.25,248,0.984126984126984,1,0.25,0,0
250,"To answer the problem on mistranslations , we propose multiple context fixing attachment ( MCFA ) to fix the context vectors using other context vectors .",Conclusion,Conclusion,text-classification,9,2,0.5,249,0.988095238095238,2,0.5,0,0
251,We show that our method improves the classification performance and achieves state - of - the - art perfor - mance on multiple data sets .,Conclusion,Conclusion,text-classification,9,3,0.75,250,0.992063492063492,3,0.75,0,0
252,"In our future work , we plan to use and extend our model to other complex NLP tasks .",Conclusion,Conclusion,text-classification,9,4,1.0,251,0.996031746031746,4,1.0,0,0
